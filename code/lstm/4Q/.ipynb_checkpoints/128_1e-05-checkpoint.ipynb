{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"4Q/128_units/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 500\n",
    "learning_rate = 1e-5\n",
    "batch_size = 5\n",
    "early_stop_iters = 15\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 128 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag4',\n",
    "                                       'inflation.lag5',\n",
    "                                       'inflation.lag6',\n",
    "                                       'inflation.lag7']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag4',\n",
    "                                   'unemp.lag5',\n",
    "                                   'unemp.lag6',\n",
    "                                   'unemp.lag7']])\n",
    "train_4lag_oil = np.array(train[['oil.lag4',\n",
    "                                 'oil.lag5',\n",
    "                                 'oil.lag6',\n",
    "                                 'oil.lag7']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag4',\n",
    "                                     'inflation.lag5',\n",
    "                                     'inflation.lag6',\n",
    "                                     'inflation.lag7']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag4',\n",
    "                                 'unemp.lag5',\n",
    "                                 'unemp.lag6',\n",
    "                                 'unemp.lag7']])\n",
    "test_4lag_oil = np.array(test[['oil.lag4',\n",
    "                               'oil.lag5',\n",
    "                               'oil.lag6',\n",
    "                               'oil.lag7']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 128 \n",
      "Learning rate = 1e-05 \n",
      "Epochs = 500 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 15 \n",
      "Learning rate = 1e-05\n",
      "Fold: 1  Epoch: 1  Training loss = 3.3421  Validation loss = 3.5022  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 3.3394  Validation loss = 3.4978  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 3.3353  Validation loss = 3.4913  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 3.3322  Validation loss = 3.4864  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 3.3286  Validation loss = 3.4805  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 3.3253  Validation loss = 3.4751  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 3.3217  Validation loss = 3.4693  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 3.3183  Validation loss = 3.4640  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 3.3143  Validation loss = 3.4576  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 3.3106  Validation loss = 3.4517  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 3.3069  Validation loss = 3.4458  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 3.3035  Validation loss = 3.4403  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 3.3003  Validation loss = 3.4353  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 3.2969  Validation loss = 3.4295  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 3.2937  Validation loss = 3.4243  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 3.2897  Validation loss = 3.4181  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 3.2871  Validation loss = 3.4137  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 3.2840  Validation loss = 3.4086  \n",
      "\n",
      "Fold: 1  Epoch: 19  Training loss = 3.2815  Validation loss = 3.4043  \n",
      "\n",
      "Fold: 1  Epoch: 20  Training loss = 3.2789  Validation loss = 3.3998  \n",
      "\n",
      "Fold: 1  Epoch: 21  Training loss = 3.2764  Validation loss = 3.3958  \n",
      "\n",
      "Fold: 1  Epoch: 22  Training loss = 3.2723  Validation loss = 3.3889  \n",
      "\n",
      "Fold: 1  Epoch: 23  Training loss = 3.2698  Validation loss = 3.3849  \n",
      "\n",
      "Fold: 1  Epoch: 24  Training loss = 3.2660  Validation loss = 3.3786  \n",
      "\n",
      "Fold: 1  Epoch: 25  Training loss = 3.2625  Validation loss = 3.3728  \n",
      "\n",
      "Fold: 1  Epoch: 26  Training loss = 3.2592  Validation loss = 3.3675  \n",
      "\n",
      "Fold: 1  Epoch: 27  Training loss = 3.2568  Validation loss = 3.3634  \n",
      "\n",
      "Fold: 1  Epoch: 28  Training loss = 3.2540  Validation loss = 3.3587  \n",
      "\n",
      "Fold: 1  Epoch: 29  Training loss = 3.2517  Validation loss = 3.3548  \n",
      "\n",
      "Fold: 1  Epoch: 30  Training loss = 3.2478  Validation loss = 3.3483  \n",
      "\n",
      "Fold: 1  Epoch: 31  Training loss = 3.2454  Validation loss = 3.3442  \n",
      "\n",
      "Fold: 1  Epoch: 32  Training loss = 3.2423  Validation loss = 3.3392  \n",
      "\n",
      "Fold: 1  Epoch: 33  Training loss = 3.2393  Validation loss = 3.3340  \n",
      "\n",
      "Fold: 1  Epoch: 34  Training loss = 3.2367  Validation loss = 3.3295  \n",
      "\n",
      "Fold: 1  Epoch: 35  Training loss = 3.2330  Validation loss = 3.3235  \n",
      "\n",
      "Fold: 1  Epoch: 36  Training loss = 3.2298  Validation loss = 3.3183  \n",
      "\n",
      "Fold: 1  Epoch: 37  Training loss = 3.2268  Validation loss = 3.3131  \n",
      "\n",
      "Fold: 1  Epoch: 38  Training loss = 3.2251  Validation loss = 3.3100  \n",
      "\n",
      "Fold: 1  Epoch: 39  Training loss = 3.2222  Validation loss = 3.3049  \n",
      "\n",
      "Fold: 1  Epoch: 40  Training loss = 3.2197  Validation loss = 3.3005  \n",
      "\n",
      "Fold: 1  Epoch: 41  Training loss = 3.2174  Validation loss = 3.2964  \n",
      "\n",
      "Fold: 1  Epoch: 42  Training loss = 3.2150  Validation loss = 3.2922  \n",
      "\n",
      "Fold: 1  Epoch: 43  Training loss = 3.2113  Validation loss = 3.2863  \n",
      "\n",
      "Fold: 1  Epoch: 44  Training loss = 3.2083  Validation loss = 3.2812  \n",
      "\n",
      "Fold: 1  Epoch: 45  Training loss = 3.2043  Validation loss = 3.2745  \n",
      "\n",
      "Fold: 1  Epoch: 46  Training loss = 3.2018  Validation loss = 3.2701  \n",
      "\n",
      "Fold: 1  Epoch: 47  Training loss = 3.1992  Validation loss = 3.2657  \n",
      "\n",
      "Fold: 1  Epoch: 48  Training loss = 3.1964  Validation loss = 3.2607  \n",
      "\n",
      "Fold: 1  Epoch: 49  Training loss = 3.1926  Validation loss = 3.2542  \n",
      "\n",
      "Fold: 1  Epoch: 50  Training loss = 3.1899  Validation loss = 3.2497  \n",
      "\n",
      "Fold: 1  Epoch: 51  Training loss = 3.1869  Validation loss = 3.2442  \n",
      "\n",
      "Fold: 1  Epoch: 52  Training loss = 3.1844  Validation loss = 3.2398  \n",
      "\n",
      "Fold: 1  Epoch: 53  Training loss = 3.1811  Validation loss = 3.2339  \n",
      "\n",
      "Fold: 1  Epoch: 54  Training loss = 3.1789  Validation loss = 3.2299  \n",
      "\n",
      "Fold: 1  Epoch: 55  Training loss = 3.1765  Validation loss = 3.2257  \n",
      "\n",
      "Fold: 1  Epoch: 56  Training loss = 3.1745  Validation loss = 3.2218  \n",
      "\n",
      "Fold: 1  Epoch: 57  Training loss = 3.1723  Validation loss = 3.2180  \n",
      "\n",
      "Fold: 1  Epoch: 58  Training loss = 3.1689  Validation loss = 3.2122  \n",
      "\n",
      "Fold: 1  Epoch: 59  Training loss = 3.1659  Validation loss = 3.2068  \n",
      "\n",
      "Fold: 1  Epoch: 60  Training loss = 3.1639  Validation loss = 3.2031  \n",
      "\n",
      "Fold: 1  Epoch: 61  Training loss = 3.1607  Validation loss = 3.1977  \n",
      "\n",
      "Fold: 1  Epoch: 62  Training loss = 3.1579  Validation loss = 3.1928  \n",
      "\n",
      "Fold: 1  Epoch: 63  Training loss = 3.1555  Validation loss = 3.1883  \n",
      "\n",
      "Fold: 1  Epoch: 64  Training loss = 3.1537  Validation loss = 3.1850  \n",
      "\n",
      "Fold: 1  Epoch: 65  Training loss = 3.1511  Validation loss = 3.1803  \n",
      "\n",
      "Fold: 1  Epoch: 66  Training loss = 3.1467  Validation loss = 3.1730  \n",
      "\n",
      "Fold: 1  Epoch: 67  Training loss = 3.1430  Validation loss = 3.1666  \n",
      "\n",
      "Fold: 1  Epoch: 68  Training loss = 3.1395  Validation loss = 3.1605  \n",
      "\n",
      "Fold: 1  Epoch: 69  Training loss = 3.1370  Validation loss = 3.1560  \n",
      "\n",
      "Fold: 1  Epoch: 70  Training loss = 3.1344  Validation loss = 3.1514  \n",
      "\n",
      "Fold: 1  Epoch: 71  Training loss = 3.1315  Validation loss = 3.1463  \n",
      "\n",
      "Fold: 1  Epoch: 72  Training loss = 3.1291  Validation loss = 3.1420  \n",
      "\n",
      "Fold: 1  Epoch: 73  Training loss = 3.1272  Validation loss = 3.1385  \n",
      "\n",
      "Fold: 1  Epoch: 74  Training loss = 3.1248  Validation loss = 3.1342  \n",
      "\n",
      "Fold: 1  Epoch: 75  Training loss = 3.1219  Validation loss = 3.1289  \n",
      "\n",
      "Fold: 1  Epoch: 76  Training loss = 3.1195  Validation loss = 3.1246  \n",
      "\n",
      "Fold: 1  Epoch: 77  Training loss = 3.1167  Validation loss = 3.1195  \n",
      "\n",
      "Fold: 1  Epoch: 78  Training loss = 3.1146  Validation loss = 3.1158  \n",
      "\n",
      "Fold: 1  Epoch: 79  Training loss = 3.1119  Validation loss = 3.1109  \n",
      "\n",
      "Fold: 1  Epoch: 80  Training loss = 3.1098  Validation loss = 3.1070  \n",
      "\n",
      "Fold: 1  Epoch: 81  Training loss = 3.1069  Validation loss = 3.1019  \n",
      "\n",
      "Fold: 1  Epoch: 82  Training loss = 3.1049  Validation loss = 3.0983  \n",
      "\n",
      "Fold: 1  Epoch: 83  Training loss = 3.1027  Validation loss = 3.0944  \n",
      "\n",
      "Fold: 1  Epoch: 84  Training loss = 3.0999  Validation loss = 3.0892  \n",
      "\n",
      "Fold: 1  Epoch: 85  Training loss = 3.0975  Validation loss = 3.0849  \n",
      "\n",
      "Fold: 1  Epoch: 86  Training loss = 3.0958  Validation loss = 3.0817  \n",
      "\n",
      "Fold: 1  Epoch: 87  Training loss = 3.0934  Validation loss = 3.0772  \n",
      "\n",
      "Fold: 1  Epoch: 88  Training loss = 3.0913  Validation loss = 3.0733  \n",
      "\n",
      "Fold: 1  Epoch: 89  Training loss = 3.0896  Validation loss = 3.0700  \n",
      "\n",
      "Fold: 1  Epoch: 90  Training loss = 3.0871  Validation loss = 3.0653  \n",
      "\n",
      "Fold: 1  Epoch: 91  Training loss = 3.0851  Validation loss = 3.0616  \n",
      "\n",
      "Fold: 1  Epoch: 92  Training loss = 3.0830  Validation loss = 3.0575  \n",
      "\n",
      "Fold: 1  Epoch: 93  Training loss = 3.0807  Validation loss = 3.0532  \n",
      "\n",
      "Fold: 1  Epoch: 94  Training loss = 3.0784  Validation loss = 3.0491  \n",
      "\n",
      "Fold: 1  Epoch: 95  Training loss = 3.0760  Validation loss = 3.0448  \n",
      "\n",
      "Fold: 1  Epoch: 96  Training loss = 3.0743  Validation loss = 3.0416  \n",
      "\n",
      "Fold: 1  Epoch: 97  Training loss = 3.0711  Validation loss = 3.0356  \n",
      "\n",
      "Fold: 1  Epoch: 98  Training loss = 3.0683  Validation loss = 3.0304  \n",
      "\n",
      "Fold: 1  Epoch: 99  Training loss = 3.0670  Validation loss = 3.0279  \n",
      "\n",
      "Fold: 1  Epoch: 100  Training loss = 3.0642  Validation loss = 3.0226  \n",
      "\n",
      "Fold: 1  Epoch: 101  Training loss = 3.0624  Validation loss = 3.0192  \n",
      "\n",
      "Fold: 1  Epoch: 102  Training loss = 3.0601  Validation loss = 3.0150  \n",
      "\n",
      "Fold: 1  Epoch: 103  Training loss = 3.0583  Validation loss = 3.0117  \n",
      "\n",
      "Fold: 1  Epoch: 104  Training loss = 3.0566  Validation loss = 3.0084  \n",
      "\n",
      "Fold: 1  Epoch: 105  Training loss = 3.0543  Validation loss = 3.0039  \n",
      "\n",
      "Fold: 1  Epoch: 106  Training loss = 3.0516  Validation loss = 2.9990  \n",
      "\n",
      "Fold: 1  Epoch: 107  Training loss = 3.0495  Validation loss = 2.9950  \n",
      "\n",
      "Fold: 1  Epoch: 108  Training loss = 3.0482  Validation loss = 2.9925  \n",
      "\n",
      "Fold: 1  Epoch: 109  Training loss = 3.0464  Validation loss = 2.9890  \n",
      "\n",
      "Fold: 1  Epoch: 110  Training loss = 3.0438  Validation loss = 2.9842  \n",
      "\n",
      "Fold: 1  Epoch: 111  Training loss = 3.0419  Validation loss = 2.9804  \n",
      "\n",
      "Fold: 1  Epoch: 112  Training loss = 3.0400  Validation loss = 2.9767  \n",
      "\n",
      "Fold: 1  Epoch: 113  Training loss = 3.0373  Validation loss = 2.9716  \n",
      "\n",
      "Fold: 1  Epoch: 114  Training loss = 3.0354  Validation loss = 2.9677  \n",
      "\n",
      "Fold: 1  Epoch: 115  Training loss = 3.0331  Validation loss = 2.9633  \n",
      "\n",
      "Fold: 1  Epoch: 116  Training loss = 3.0309  Validation loss = 2.9589  \n",
      "\n",
      "Fold: 1  Epoch: 117  Training loss = 3.0283  Validation loss = 2.9542  \n",
      "\n",
      "Fold: 1  Epoch: 118  Training loss = 3.0254  Validation loss = 2.9489  \n",
      "\n",
      "Fold: 1  Epoch: 119  Training loss = 3.0230  Validation loss = 2.9443  \n",
      "\n",
      "Fold: 1  Epoch: 120  Training loss = 3.0205  Validation loss = 2.9394  \n",
      "\n",
      "Fold: 1  Epoch: 121  Training loss = 3.0188  Validation loss = 2.9360  \n",
      "\n",
      "Fold: 1  Epoch: 122  Training loss = 3.0163  Validation loss = 2.9311  \n",
      "\n",
      "Fold: 1  Epoch: 123  Training loss = 3.0140  Validation loss = 2.9266  \n",
      "\n",
      "Fold: 1  Epoch: 124  Training loss = 3.0122  Validation loss = 2.9229  \n",
      "\n",
      "Fold: 1  Epoch: 125  Training loss = 3.0099  Validation loss = 2.9187  \n",
      "\n",
      "Fold: 1  Epoch: 126  Training loss = 3.0082  Validation loss = 2.9153  \n",
      "\n",
      "Fold: 1  Epoch: 127  Training loss = 3.0070  Validation loss = 2.9129  \n",
      "\n",
      "Fold: 1  Epoch: 128  Training loss = 3.0056  Validation loss = 2.9100  \n",
      "\n",
      "Fold: 1  Epoch: 129  Training loss = 3.0041  Validation loss = 2.9071  \n",
      "\n",
      "Fold: 1  Epoch: 130  Training loss = 3.0011  Validation loss = 2.9014  \n",
      "\n",
      "Fold: 1  Epoch: 131  Training loss = 2.9996  Validation loss = 2.8983  \n",
      "\n",
      "Fold: 1  Epoch: 132  Training loss = 2.9967  Validation loss = 2.8928  \n",
      "\n",
      "Fold: 1  Epoch: 133  Training loss = 2.9957  Validation loss = 2.8907  \n",
      "\n",
      "Fold: 1  Epoch: 134  Training loss = 2.9929  Validation loss = 2.8853  \n",
      "\n",
      "Fold: 1  Epoch: 135  Training loss = 2.9920  Validation loss = 2.8836  \n",
      "\n",
      "Fold: 1  Epoch: 136  Training loss = 2.9900  Validation loss = 2.8796  \n",
      "\n",
      "Fold: 1  Epoch: 137  Training loss = 2.9870  Validation loss = 2.8738  \n",
      "\n",
      "Fold: 1  Epoch: 138  Training loss = 2.9855  Validation loss = 2.8706  \n",
      "\n",
      "Fold: 1  Epoch: 139  Training loss = 2.9821  Validation loss = 2.8639  \n",
      "\n",
      "Fold: 1  Epoch: 140  Training loss = 2.9809  Validation loss = 2.8615  \n",
      "\n",
      "Fold: 1  Epoch: 141  Training loss = 2.9798  Validation loss = 2.8590  \n",
      "\n",
      "Fold: 1  Epoch: 142  Training loss = 2.9780  Validation loss = 2.8554  \n",
      "\n",
      "Fold: 1  Epoch: 143  Training loss = 2.9753  Validation loss = 2.8503  \n",
      "\n",
      "Fold: 1  Epoch: 144  Training loss = 2.9734  Validation loss = 2.8464  \n",
      "\n",
      "Fold: 1  Epoch: 145  Training loss = 2.9720  Validation loss = 2.8436  \n",
      "\n",
      "Fold: 1  Epoch: 146  Training loss = 2.9700  Validation loss = 2.8396  \n",
      "\n",
      "Fold: 1  Epoch: 147  Training loss = 2.9679  Validation loss = 2.8353  \n",
      "\n",
      "Fold: 1  Epoch: 148  Training loss = 2.9665  Validation loss = 2.8324  \n",
      "\n",
      "Fold: 1  Epoch: 149  Training loss = 2.9650  Validation loss = 2.8293  \n",
      "\n",
      "Fold: 1  Epoch: 150  Training loss = 2.9629  Validation loss = 2.8250  \n",
      "\n",
      "Fold: 1  Epoch: 151  Training loss = 2.9616  Validation loss = 2.8223  \n",
      "\n",
      "Fold: 1  Epoch: 152  Training loss = 2.9599  Validation loss = 2.8188  \n",
      "\n",
      "Fold: 1  Epoch: 153  Training loss = 2.9588  Validation loss = 2.8163  \n",
      "\n",
      "Fold: 1  Epoch: 154  Training loss = 2.9574  Validation loss = 2.8135  \n",
      "\n",
      "Fold: 1  Epoch: 155  Training loss = 2.9556  Validation loss = 2.8097  \n",
      "\n",
      "Fold: 1  Epoch: 156  Training loss = 2.9535  Validation loss = 2.8056  \n",
      "\n",
      "Fold: 1  Epoch: 157  Training loss = 2.9521  Validation loss = 2.8028  \n",
      "\n",
      "Fold: 1  Epoch: 158  Training loss = 2.9506  Validation loss = 2.7995  \n",
      "\n",
      "Fold: 1  Epoch: 159  Training loss = 2.9482  Validation loss = 2.7945  \n",
      "\n",
      "Fold: 1  Epoch: 160  Training loss = 2.9467  Validation loss = 2.7915  \n",
      "\n",
      "Fold: 1  Epoch: 161  Training loss = 2.9456  Validation loss = 2.7889  \n",
      "\n",
      "Fold: 1  Epoch: 162  Training loss = 2.9444  Validation loss = 2.7864  \n",
      "\n",
      "Fold: 1  Epoch: 163  Training loss = 2.9430  Validation loss = 2.7835  \n",
      "\n",
      "Fold: 1  Epoch: 164  Training loss = 2.9408  Validation loss = 2.7789  \n",
      "\n",
      "Fold: 1  Epoch: 165  Training loss = 2.9393  Validation loss = 2.7759  \n",
      "\n",
      "Fold: 1  Epoch: 166  Training loss = 2.9379  Validation loss = 2.7727  \n",
      "\n",
      "Fold: 1  Epoch: 167  Training loss = 2.9358  Validation loss = 2.7683  \n",
      "\n",
      "Fold: 1  Epoch: 168  Training loss = 2.9338  Validation loss = 2.7643  \n",
      "\n",
      "Fold: 1  Epoch: 169  Training loss = 2.9322  Validation loss = 2.7608  \n",
      "\n",
      "Fold: 1  Epoch: 170  Training loss = 2.9307  Validation loss = 2.7576  \n",
      "\n",
      "Fold: 1  Epoch: 171  Training loss = 2.9285  Validation loss = 2.7528  \n",
      "\n",
      "Fold: 1  Epoch: 172  Training loss = 2.9274  Validation loss = 2.7503  \n",
      "\n",
      "Fold: 1  Epoch: 173  Training loss = 2.9267  Validation loss = 2.7486  \n",
      "\n",
      "Fold: 1  Epoch: 174  Training loss = 2.9254  Validation loss = 2.7458  \n",
      "\n",
      "Fold: 1  Epoch: 175  Training loss = 2.9244  Validation loss = 2.7437  \n",
      "\n",
      "Fold: 1  Epoch: 176  Training loss = 2.9227  Validation loss = 2.7402  \n",
      "\n",
      "Fold: 1  Epoch: 177  Training loss = 2.9217  Validation loss = 2.7377  \n",
      "\n",
      "Fold: 1  Epoch: 178  Training loss = 2.9198  Validation loss = 2.7338  \n",
      "\n",
      "Fold: 1  Epoch: 179  Training loss = 2.9176  Validation loss = 2.7291  \n",
      "\n",
      "Fold: 1  Epoch: 180  Training loss = 2.9158  Validation loss = 2.7250  \n",
      "\n",
      "Fold: 1  Epoch: 181  Training loss = 2.9144  Validation loss = 2.7217  \n",
      "\n",
      "Fold: 1  Epoch: 182  Training loss = 2.9131  Validation loss = 2.7190  \n",
      "\n",
      "Fold: 1  Epoch: 183  Training loss = 2.9113  Validation loss = 2.7150  \n",
      "\n",
      "Fold: 1  Epoch: 184  Training loss = 2.9090  Validation loss = 2.7101  \n",
      "\n",
      "Fold: 1  Epoch: 185  Training loss = 2.9070  Validation loss = 2.7059  \n",
      "\n",
      "Fold: 1  Epoch: 186  Training loss = 2.9054  Validation loss = 2.7022  \n",
      "\n",
      "Fold: 1  Epoch: 187  Training loss = 2.9041  Validation loss = 2.6993  \n",
      "\n",
      "Fold: 1  Epoch: 188  Training loss = 2.9026  Validation loss = 2.6959  \n",
      "\n",
      "Fold: 1  Epoch: 189  Training loss = 2.9011  Validation loss = 2.6926  \n",
      "\n",
      "Fold: 1  Epoch: 190  Training loss = 2.9007  Validation loss = 2.6916  \n",
      "\n",
      "Fold: 1  Epoch: 191  Training loss = 2.8996  Validation loss = 2.6893  \n",
      "\n",
      "Fold: 1  Epoch: 192  Training loss = 2.8971  Validation loss = 2.6836  \n",
      "\n",
      "Fold: 1  Epoch: 193  Training loss = 2.8953  Validation loss = 2.6796  \n",
      "\n",
      "Fold: 1  Epoch: 194  Training loss = 2.8932  Validation loss = 2.6748  \n",
      "\n",
      "Fold: 1  Epoch: 195  Training loss = 2.8919  Validation loss = 2.6720  \n",
      "\n",
      "Fold: 1  Epoch: 196  Training loss = 2.8902  Validation loss = 2.6682  \n",
      "\n",
      "Fold: 1  Epoch: 197  Training loss = 2.8889  Validation loss = 2.6655  \n",
      "\n",
      "Fold: 1  Epoch: 198  Training loss = 2.8879  Validation loss = 2.6631  \n",
      "\n",
      "Fold: 1  Epoch: 199  Training loss = 2.8864  Validation loss = 2.6598  \n",
      "\n",
      "Fold: 1  Epoch: 200  Training loss = 2.8851  Validation loss = 2.6569  \n",
      "\n",
      "Fold: 1  Epoch: 201  Training loss = 2.8843  Validation loss = 2.6549  \n",
      "\n",
      "Fold: 1  Epoch: 202  Training loss = 2.8835  Validation loss = 2.6530  \n",
      "\n",
      "Fold: 1  Epoch: 203  Training loss = 2.8824  Validation loss = 2.6504  \n",
      "\n",
      "Fold: 1  Epoch: 204  Training loss = 2.8808  Validation loss = 2.6468  \n",
      "\n",
      "Fold: 1  Epoch: 205  Training loss = 2.8796  Validation loss = 2.6442  \n",
      "\n",
      "Fold: 1  Epoch: 206  Training loss = 2.8782  Validation loss = 2.6411  \n",
      "\n",
      "Fold: 1  Epoch: 207  Training loss = 2.8768  Validation loss = 2.6379  \n",
      "\n",
      "Fold: 1  Epoch: 208  Training loss = 2.8750  Validation loss = 2.6341  \n",
      "\n",
      "Fold: 1  Epoch: 209  Training loss = 2.8731  Validation loss = 2.6299  \n",
      "\n",
      "Fold: 1  Epoch: 210  Training loss = 2.8707  Validation loss = 2.6244  \n",
      "\n",
      "Fold: 1  Epoch: 211  Training loss = 2.8692  Validation loss = 2.6207  \n",
      "\n",
      "Fold: 1  Epoch: 212  Training loss = 2.8678  Validation loss = 2.6173  \n",
      "\n",
      "Fold: 1  Epoch: 213  Training loss = 2.8662  Validation loss = 2.6137  \n",
      "\n",
      "Fold: 1  Epoch: 214  Training loss = 2.8652  Validation loss = 2.6117  \n",
      "\n",
      "Fold: 1  Epoch: 215  Training loss = 2.8645  Validation loss = 2.6099  \n",
      "\n",
      "Fold: 1  Epoch: 216  Training loss = 2.8630  Validation loss = 2.6065  \n",
      "\n",
      "Fold: 1  Epoch: 217  Training loss = 2.8620  Validation loss = 2.6042  \n",
      "\n",
      "Fold: 1  Epoch: 218  Training loss = 2.8601  Validation loss = 2.5999  \n",
      "\n",
      "Fold: 1  Epoch: 219  Training loss = 2.8589  Validation loss = 2.5972  \n",
      "\n",
      "Fold: 1  Epoch: 220  Training loss = 2.8572  Validation loss = 2.5931  \n",
      "\n",
      "Fold: 1  Epoch: 221  Training loss = 2.8559  Validation loss = 2.5900  \n",
      "\n",
      "Fold: 1  Epoch: 222  Training loss = 2.8543  Validation loss = 2.5864  \n",
      "\n",
      "Fold: 1  Epoch: 223  Training loss = 2.8527  Validation loss = 2.5826  \n",
      "\n",
      "Fold: 1  Epoch: 224  Training loss = 2.8513  Validation loss = 2.5793  \n",
      "\n",
      "Fold: 1  Epoch: 225  Training loss = 2.8503  Validation loss = 2.5771  \n",
      "\n",
      "Fold: 1  Epoch: 226  Training loss = 2.8485  Validation loss = 2.5727  \n",
      "\n",
      "Fold: 1  Epoch: 227  Training loss = 2.8473  Validation loss = 2.5698  \n",
      "\n",
      "Fold: 1  Epoch: 228  Training loss = 2.8462  Validation loss = 2.5671  \n",
      "\n",
      "Fold: 1  Epoch: 229  Training loss = 2.8452  Validation loss = 2.5649  \n",
      "\n",
      "Fold: 1  Epoch: 230  Training loss = 2.8446  Validation loss = 2.5633  \n",
      "\n",
      "Fold: 1  Epoch: 231  Training loss = 2.8435  Validation loss = 2.5604  \n",
      "\n",
      "Fold: 1  Epoch: 232  Training loss = 2.8426  Validation loss = 2.5584  \n",
      "\n",
      "Fold: 1  Epoch: 233  Training loss = 2.8407  Validation loss = 2.5538  \n",
      "\n",
      "Fold: 1  Epoch: 234  Training loss = 2.8393  Validation loss = 2.5504  \n",
      "\n",
      "Fold: 1  Epoch: 235  Training loss = 2.8385  Validation loss = 2.5485  \n",
      "\n",
      "Fold: 1  Epoch: 236  Training loss = 2.8373  Validation loss = 2.5458  \n",
      "\n",
      "Fold: 1  Epoch: 237  Training loss = 2.8363  Validation loss = 2.5434  \n",
      "\n",
      "Fold: 1  Epoch: 238  Training loss = 2.8356  Validation loss = 2.5417  \n",
      "\n",
      "Fold: 1  Epoch: 239  Training loss = 2.8345  Validation loss = 2.5387  \n",
      "\n",
      "Fold: 1  Epoch: 240  Training loss = 2.8331  Validation loss = 2.5356  \n",
      "\n",
      "Fold: 1  Epoch: 241  Training loss = 2.8322  Validation loss = 2.5331  \n",
      "\n",
      "Fold: 1  Epoch: 242  Training loss = 2.8313  Validation loss = 2.5308  \n",
      "\n",
      "Fold: 1  Epoch: 243  Training loss = 2.8306  Validation loss = 2.5290  \n",
      "\n",
      "Fold: 1  Epoch: 244  Training loss = 2.8295  Validation loss = 2.5263  \n",
      "\n",
      "Fold: 1  Epoch: 245  Training loss = 2.8287  Validation loss = 2.5243  \n",
      "\n",
      "Fold: 1  Epoch: 246  Training loss = 2.8277  Validation loss = 2.5218  \n",
      "\n",
      "Fold: 1  Epoch: 247  Training loss = 2.8266  Validation loss = 2.5190  \n",
      "\n",
      "Fold: 1  Epoch: 248  Training loss = 2.8248  Validation loss = 2.5149  \n",
      "\n",
      "Fold: 1  Epoch: 249  Training loss = 2.8240  Validation loss = 2.5129  \n",
      "\n",
      "Fold: 1  Epoch: 250  Training loss = 2.8224  Validation loss = 2.5086  \n",
      "\n",
      "Fold: 1  Epoch: 251  Training loss = 2.8202  Validation loss = 2.5033  \n",
      "\n",
      "Fold: 1  Epoch: 252  Training loss = 2.8192  Validation loss = 2.5009  \n",
      "\n",
      "Fold: 1  Epoch: 253  Training loss = 2.8180  Validation loss = 2.4978  \n",
      "\n",
      "Fold: 1  Epoch: 254  Training loss = 2.8172  Validation loss = 2.4958  \n",
      "\n",
      "Fold: 1  Epoch: 255  Training loss = 2.8166  Validation loss = 2.4941  \n",
      "\n",
      "Fold: 1  Epoch: 256  Training loss = 2.8152  Validation loss = 2.4904  \n",
      "\n",
      "Fold: 1  Epoch: 257  Training loss = 2.8136  Validation loss = 2.4867  \n",
      "\n",
      "Fold: 1  Epoch: 258  Training loss = 2.8123  Validation loss = 2.4836  \n",
      "\n",
      "Fold: 1  Epoch: 259  Training loss = 2.8116  Validation loss = 2.4819  \n",
      "\n",
      "Fold: 1  Epoch: 260  Training loss = 2.8105  Validation loss = 2.4792  \n",
      "\n",
      "Fold: 1  Epoch: 261  Training loss = 2.8090  Validation loss = 2.4755  \n",
      "\n",
      "Fold: 1  Epoch: 262  Training loss = 2.8075  Validation loss = 2.4717  \n",
      "\n",
      "Fold: 1  Epoch: 263  Training loss = 2.8066  Validation loss = 2.4697  \n",
      "\n",
      "Fold: 1  Epoch: 264  Training loss = 2.8054  Validation loss = 2.4668  \n",
      "\n",
      "Fold: 1  Epoch: 265  Training loss = 2.8047  Validation loss = 2.4650  \n",
      "\n",
      "Fold: 1  Epoch: 266  Training loss = 2.8037  Validation loss = 2.4624  \n",
      "\n",
      "Fold: 1  Epoch: 267  Training loss = 2.8019  Validation loss = 2.4580  \n",
      "\n",
      "Fold: 1  Epoch: 268  Training loss = 2.8009  Validation loss = 2.4556  \n",
      "\n",
      "Fold: 1  Epoch: 269  Training loss = 2.7999  Validation loss = 2.4531  \n",
      "\n",
      "Fold: 1  Epoch: 270  Training loss = 2.7991  Validation loss = 2.4510  \n",
      "\n",
      "Fold: 1  Epoch: 271  Training loss = 2.7979  Validation loss = 2.4482  \n",
      "\n",
      "Fold: 1  Epoch: 272  Training loss = 2.7969  Validation loss = 2.4457  \n",
      "\n",
      "Fold: 1  Epoch: 273  Training loss = 2.7960  Validation loss = 2.4435  \n",
      "\n",
      "Fold: 1  Epoch: 274  Training loss = 2.7948  Validation loss = 2.4403  \n",
      "\n",
      "Fold: 1  Epoch: 275  Training loss = 2.7935  Validation loss = 2.4370  \n",
      "\n",
      "Fold: 1  Epoch: 276  Training loss = 2.7928  Validation loss = 2.4352  \n",
      "\n",
      "Fold: 1  Epoch: 277  Training loss = 2.7917  Validation loss = 2.4321  \n",
      "\n",
      "Fold: 1  Epoch: 278  Training loss = 2.7912  Validation loss = 2.4309  \n",
      "\n",
      "Fold: 1  Epoch: 279  Training loss = 2.7902  Validation loss = 2.4281  \n",
      "\n",
      "Fold: 1  Epoch: 280  Training loss = 2.7892  Validation loss = 2.4254  \n",
      "\n",
      "Fold: 1  Epoch: 281  Training loss = 2.7879  Validation loss = 2.4222  \n",
      "\n",
      "Fold: 1  Epoch: 282  Training loss = 2.7867  Validation loss = 2.4189  \n",
      "\n",
      "Fold: 1  Epoch: 283  Training loss = 2.7854  Validation loss = 2.4156  \n",
      "\n",
      "Fold: 1  Epoch: 284  Training loss = 2.7845  Validation loss = 2.4133  \n",
      "\n",
      "Fold: 1  Epoch: 285  Training loss = 2.7833  Validation loss = 2.4103  \n",
      "\n",
      "Fold: 1  Epoch: 286  Training loss = 2.7818  Validation loss = 2.4065  \n",
      "\n",
      "Fold: 1  Epoch: 287  Training loss = 2.7812  Validation loss = 2.4051  \n",
      "\n",
      "Fold: 1  Epoch: 288  Training loss = 2.7804  Validation loss = 2.4028  \n",
      "\n",
      "Fold: 1  Epoch: 289  Training loss = 2.7800  Validation loss = 2.4015  \n",
      "\n",
      "Fold: 1  Epoch: 290  Training loss = 2.7788  Validation loss = 2.3984  \n",
      "\n",
      "Fold: 1  Epoch: 291  Training loss = 2.7775  Validation loss = 2.3951  \n",
      "\n",
      "Fold: 1  Epoch: 292  Training loss = 2.7772  Validation loss = 2.3946  \n",
      "\n",
      "Fold: 1  Epoch: 293  Training loss = 2.7764  Validation loss = 2.3923  \n",
      "\n",
      "Fold: 1  Epoch: 294  Training loss = 2.7758  Validation loss = 2.3905  \n",
      "\n",
      "Fold: 1  Epoch: 295  Training loss = 2.7749  Validation loss = 2.3883  \n",
      "\n",
      "Fold: 1  Epoch: 296  Training loss = 2.7738  Validation loss = 2.3853  \n",
      "\n",
      "Fold: 1  Epoch: 297  Training loss = 2.7730  Validation loss = 2.3831  \n",
      "\n",
      "Fold: 1  Epoch: 298  Training loss = 2.7721  Validation loss = 2.3809  \n",
      "\n",
      "Fold: 1  Epoch: 299  Training loss = 2.7713  Validation loss = 2.3788  \n",
      "\n",
      "Fold: 1  Epoch: 300  Training loss = 2.7704  Validation loss = 2.3763  \n",
      "\n",
      "Fold: 1  Epoch: 301  Training loss = 2.7693  Validation loss = 2.3731  \n",
      "\n",
      "Fold: 1  Epoch: 302  Training loss = 2.7681  Validation loss = 2.3701  \n",
      "\n",
      "Fold: 1  Epoch: 303  Training loss = 2.7672  Validation loss = 2.3677  \n",
      "\n",
      "Fold: 1  Epoch: 304  Training loss = 2.7659  Validation loss = 2.3643  \n",
      "\n",
      "Fold: 1  Epoch: 305  Training loss = 2.7654  Validation loss = 2.3629  \n",
      "\n",
      "Fold: 1  Epoch: 306  Training loss = 2.7649  Validation loss = 2.3615  \n",
      "\n",
      "Fold: 1  Epoch: 307  Training loss = 2.7640  Validation loss = 2.3589  \n",
      "\n",
      "Fold: 1  Epoch: 308  Training loss = 2.7628  Validation loss = 2.3558  \n",
      "\n",
      "Fold: 1  Epoch: 309  Training loss = 2.7621  Validation loss = 2.3539  \n",
      "\n",
      "Fold: 1  Epoch: 310  Training loss = 2.7612  Validation loss = 2.3515  \n",
      "\n",
      "Fold: 1  Epoch: 311  Training loss = 2.7609  Validation loss = 2.3506  \n",
      "\n",
      "Fold: 1  Epoch: 312  Training loss = 2.7599  Validation loss = 2.3480  \n",
      "\n",
      "Fold: 1  Epoch: 313  Training loss = 2.7595  Validation loss = 2.3469  \n",
      "\n",
      "Fold: 1  Epoch: 314  Training loss = 2.7584  Validation loss = 2.3438  \n",
      "\n",
      "Fold: 1  Epoch: 315  Training loss = 2.7569  Validation loss = 2.3396  \n",
      "\n",
      "Fold: 1  Epoch: 316  Training loss = 2.7560  Validation loss = 2.3370  \n",
      "\n",
      "Fold: 1  Epoch: 317  Training loss = 2.7547  Validation loss = 2.3335  \n",
      "\n",
      "Fold: 1  Epoch: 318  Training loss = 2.7543  Validation loss = 2.3322  \n",
      "\n",
      "Fold: 1  Epoch: 319  Training loss = 2.7536  Validation loss = 2.3302  \n",
      "\n",
      "Fold: 1  Epoch: 320  Training loss = 2.7529  Validation loss = 2.3283  \n",
      "\n",
      "Fold: 1  Epoch: 321  Training loss = 2.7524  Validation loss = 2.3267  \n",
      "\n",
      "Fold: 1  Epoch: 322  Training loss = 2.7520  Validation loss = 2.3258  \n",
      "\n",
      "Fold: 1  Epoch: 323  Training loss = 2.7513  Validation loss = 2.3237  \n",
      "\n",
      "Fold: 1  Epoch: 324  Training loss = 2.7506  Validation loss = 2.3217  \n",
      "\n",
      "Fold: 1  Epoch: 325  Training loss = 2.7501  Validation loss = 2.3205  \n",
      "\n",
      "Fold: 1  Epoch: 326  Training loss = 2.7499  Validation loss = 2.3198  \n",
      "\n",
      "Fold: 1  Epoch: 327  Training loss = 2.7496  Validation loss = 2.3190  \n",
      "\n",
      "Fold: 1  Epoch: 328  Training loss = 2.7488  Validation loss = 2.3166  \n",
      "\n",
      "Fold: 1  Epoch: 329  Training loss = 2.7476  Validation loss = 2.3132  \n",
      "\n",
      "Fold: 1  Epoch: 330  Training loss = 2.7467  Validation loss = 2.3104  \n",
      "\n",
      "Fold: 1  Epoch: 331  Training loss = 2.7460  Validation loss = 2.3084  \n",
      "\n",
      "Fold: 1  Epoch: 332  Training loss = 2.7455  Validation loss = 2.3069  \n",
      "\n",
      "Fold: 1  Epoch: 333  Training loss = 2.7449  Validation loss = 2.3051  \n",
      "\n",
      "Fold: 1  Epoch: 334  Training loss = 2.7445  Validation loss = 2.3039  \n",
      "\n",
      "Fold: 1  Epoch: 335  Training loss = 2.7434  Validation loss = 2.3007  \n",
      "\n",
      "Fold: 1  Epoch: 336  Training loss = 2.7429  Validation loss = 2.2993  \n",
      "\n",
      "Fold: 1  Epoch: 337  Training loss = 2.7422  Validation loss = 2.2971  \n",
      "\n",
      "Fold: 1  Epoch: 338  Training loss = 2.7410  Validation loss = 2.2939  \n",
      "\n",
      "Fold: 1  Epoch: 339  Training loss = 2.7398  Validation loss = 2.2904  \n",
      "\n",
      "Fold: 1  Epoch: 340  Training loss = 2.7392  Validation loss = 2.2887  \n",
      "\n",
      "Fold: 1  Epoch: 341  Training loss = 2.7383  Validation loss = 2.2857  \n",
      "\n",
      "Fold: 1  Epoch: 342  Training loss = 2.7377  Validation loss = 2.2842  \n",
      "\n",
      "Fold: 1  Epoch: 343  Training loss = 2.7371  Validation loss = 2.2821  \n",
      "\n",
      "Fold: 1  Epoch: 344  Training loss = 2.7366  Validation loss = 2.2808  \n",
      "\n",
      "Fold: 1  Epoch: 345  Training loss = 2.7357  Validation loss = 2.2781  \n",
      "\n",
      "Fold: 1  Epoch: 346  Training loss = 2.7350  Validation loss = 2.2761  \n",
      "\n",
      "Fold: 1  Epoch: 347  Training loss = 2.7333  Validation loss = 2.2712  \n",
      "\n",
      "Fold: 1  Epoch: 348  Training loss = 2.7326  Validation loss = 2.2691  \n",
      "\n",
      "Fold: 1  Epoch: 349  Training loss = 2.7318  Validation loss = 2.2666  \n",
      "\n",
      "Fold: 1  Epoch: 350  Training loss = 2.7310  Validation loss = 2.2642  \n",
      "\n",
      "Fold: 1  Epoch: 351  Training loss = 2.7303  Validation loss = 2.2624  \n",
      "\n",
      "Fold: 1  Epoch: 352  Training loss = 2.7297  Validation loss = 2.2605  \n",
      "\n",
      "Fold: 1  Epoch: 353  Training loss = 2.7292  Validation loss = 2.2590  \n",
      "\n",
      "Fold: 1  Epoch: 354  Training loss = 2.7289  Validation loss = 2.2580  \n",
      "\n",
      "Fold: 1  Epoch: 355  Training loss = 2.7283  Validation loss = 2.2563  \n",
      "\n",
      "Fold: 1  Epoch: 356  Training loss = 2.7278  Validation loss = 2.2545  \n",
      "\n",
      "Fold: 1  Epoch: 357  Training loss = 2.7275  Validation loss = 2.2537  \n",
      "\n",
      "Fold: 1  Epoch: 358  Training loss = 2.7271  Validation loss = 2.2524  \n",
      "\n",
      "Fold: 1  Epoch: 359  Training loss = 2.7265  Validation loss = 2.2510  \n",
      "\n",
      "Fold: 1  Epoch: 360  Training loss = 2.7260  Validation loss = 2.2497  \n",
      "\n",
      "Fold: 1  Epoch: 361  Training loss = 2.7254  Validation loss = 2.2478  \n",
      "\n",
      "Fold: 1  Epoch: 362  Training loss = 2.7244  Validation loss = 2.2448  \n",
      "\n",
      "Fold: 1  Epoch: 363  Training loss = 2.7237  Validation loss = 2.2427  \n",
      "\n",
      "Fold: 1  Epoch: 364  Training loss = 2.7228  Validation loss = 2.2401  \n",
      "\n",
      "Fold: 1  Epoch: 365  Training loss = 2.7222  Validation loss = 2.2383  \n",
      "\n",
      "Fold: 1  Epoch: 366  Training loss = 2.7216  Validation loss = 2.2362  \n",
      "\n",
      "Fold: 1  Epoch: 367  Training loss = 2.7211  Validation loss = 2.2346  \n",
      "\n",
      "Fold: 1  Epoch: 368  Training loss = 2.7201  Validation loss = 2.2315  \n",
      "\n",
      "Fold: 1  Epoch: 369  Training loss = 2.7193  Validation loss = 2.2293  \n",
      "\n",
      "Fold: 1  Epoch: 370  Training loss = 2.7186  Validation loss = 2.2271  \n",
      "\n",
      "Fold: 1  Epoch: 371  Training loss = 2.7183  Validation loss = 2.2261  \n",
      "\n",
      "Fold: 1  Epoch: 372  Training loss = 2.7180  Validation loss = 2.2253  \n",
      "\n",
      "Fold: 1  Epoch: 373  Training loss = 2.7173  Validation loss = 2.2229  \n",
      "\n",
      "Fold: 1  Epoch: 374  Training loss = 2.7159  Validation loss = 2.2188  \n",
      "\n",
      "Fold: 1  Epoch: 375  Training loss = 2.7145  Validation loss = 2.2146  \n",
      "\n",
      "Fold: 1  Epoch: 376  Training loss = 2.7136  Validation loss = 2.2116  \n",
      "\n",
      "Fold: 1  Epoch: 377  Training loss = 2.7128  Validation loss = 2.2094  \n",
      "\n",
      "Fold: 1  Epoch: 378  Training loss = 2.7122  Validation loss = 2.2073  \n",
      "\n",
      "Fold: 1  Epoch: 379  Training loss = 2.7115  Validation loss = 2.2053  \n",
      "\n",
      "Fold: 1  Epoch: 380  Training loss = 2.7114  Validation loss = 2.2050  \n",
      "\n",
      "Fold: 1  Epoch: 381  Training loss = 2.7110  Validation loss = 2.2038  \n",
      "\n",
      "Fold: 1  Epoch: 382  Training loss = 2.7109  Validation loss = 2.2035  \n",
      "\n",
      "Fold: 1  Epoch: 383  Training loss = 2.7105  Validation loss = 2.2022  \n",
      "\n",
      "Fold: 1  Epoch: 384  Training loss = 2.7097  Validation loss = 2.1998  \n",
      "\n",
      "Fold: 1  Epoch: 385  Training loss = 2.7098  Validation loss = 2.1999  \n",
      "\n",
      "Fold: 1  Epoch: 386  Training loss = 2.7085  Validation loss = 2.1961  \n",
      "\n",
      "Fold: 1  Epoch: 387  Training loss = 2.7078  Validation loss = 2.1939  \n",
      "\n",
      "Fold: 1  Epoch: 388  Training loss = 2.7073  Validation loss = 2.1922  \n",
      "\n",
      "Fold: 1  Epoch: 389  Training loss = 2.7070  Validation loss = 2.1909  \n",
      "\n",
      "Fold: 1  Epoch: 390  Training loss = 2.7062  Validation loss = 2.1886  \n",
      "\n",
      "Fold: 1  Epoch: 391  Training loss = 2.7057  Validation loss = 2.1867  \n",
      "\n",
      "Fold: 1  Epoch: 392  Training loss = 2.7051  Validation loss = 2.1851  \n",
      "\n",
      "Fold: 1  Epoch: 393  Training loss = 2.7046  Validation loss = 2.1835  \n",
      "\n",
      "Fold: 1  Epoch: 394  Training loss = 2.7042  Validation loss = 2.1825  \n",
      "\n",
      "Fold: 1  Epoch: 395  Training loss = 2.7040  Validation loss = 2.1816  \n",
      "\n",
      "Fold: 1  Epoch: 396  Training loss = 2.7028  Validation loss = 2.1779  \n",
      "\n",
      "Fold: 1  Epoch: 397  Training loss = 2.7020  Validation loss = 2.1751  \n",
      "\n",
      "Fold: 1  Epoch: 398  Training loss = 2.7013  Validation loss = 2.1726  \n",
      "\n",
      "Fold: 1  Epoch: 399  Training loss = 2.7009  Validation loss = 2.1712  \n",
      "\n",
      "Fold: 1  Epoch: 400  Training loss = 2.7002  Validation loss = 2.1688  \n",
      "\n",
      "Fold: 1  Epoch: 401  Training loss = 2.6996  Validation loss = 2.1667  \n",
      "\n",
      "Fold: 1  Epoch: 402  Training loss = 2.6991  Validation loss = 2.1652  \n",
      "\n",
      "Fold: 1  Epoch: 403  Training loss = 2.6984  Validation loss = 2.1633  \n",
      "\n",
      "Fold: 1  Epoch: 404  Training loss = 2.6977  Validation loss = 2.1609  \n",
      "\n",
      "Fold: 1  Epoch: 405  Training loss = 2.6973  Validation loss = 2.1597  \n",
      "\n",
      "Fold: 1  Epoch: 406  Training loss = 2.6971  Validation loss = 2.1589  \n",
      "\n",
      "Fold: 1  Epoch: 407  Training loss = 2.6967  Validation loss = 2.1574  \n",
      "\n",
      "Fold: 1  Epoch: 408  Training loss = 2.6962  Validation loss = 2.1560  \n",
      "\n",
      "Fold: 1  Epoch: 409  Training loss = 2.6957  Validation loss = 2.1545  \n",
      "\n",
      "Fold: 1  Epoch: 410  Training loss = 2.6950  Validation loss = 2.1523  \n",
      "\n",
      "Fold: 1  Epoch: 411  Training loss = 2.6944  Validation loss = 2.1504  \n",
      "\n",
      "Fold: 1  Epoch: 412  Training loss = 2.6933  Validation loss = 2.1471  \n",
      "\n",
      "Fold: 1  Epoch: 413  Training loss = 2.6922  Validation loss = 2.1437  \n",
      "\n",
      "Fold: 1  Epoch: 414  Training loss = 2.6916  Validation loss = 2.1416  \n",
      "\n",
      "Fold: 1  Epoch: 415  Training loss = 2.6909  Validation loss = 2.1393  \n",
      "\n",
      "Fold: 1  Epoch: 416  Training loss = 2.6902  Validation loss = 2.1370  \n",
      "\n",
      "Fold: 1  Epoch: 417  Training loss = 2.6888  Validation loss = 2.1325  \n",
      "\n",
      "Fold: 1  Epoch: 418  Training loss = 2.6884  Validation loss = 2.1308  \n",
      "\n",
      "Fold: 1  Epoch: 419  Training loss = 2.6876  Validation loss = 2.1284  \n",
      "\n",
      "Fold: 1  Epoch: 420  Training loss = 2.6872  Validation loss = 2.1272  \n",
      "\n",
      "Fold: 1  Epoch: 421  Training loss = 2.6870  Validation loss = 2.1265  \n",
      "\n",
      "Fold: 1  Epoch: 422  Training loss = 2.6864  Validation loss = 2.1244  \n",
      "\n",
      "Fold: 1  Epoch: 423  Training loss = 2.6857  Validation loss = 2.1222  \n",
      "\n",
      "Fold: 1  Epoch: 424  Training loss = 2.6853  Validation loss = 2.1208  \n",
      "\n",
      "Fold: 1  Epoch: 425  Training loss = 2.6847  Validation loss = 2.1189  \n",
      "\n",
      "Fold: 1  Epoch: 426  Training loss = 2.6845  Validation loss = 2.1181  \n",
      "\n",
      "Fold: 1  Epoch: 427  Training loss = 2.6839  Validation loss = 2.1162  \n",
      "\n",
      "Fold: 1  Epoch: 428  Training loss = 2.6834  Validation loss = 2.1145  \n",
      "\n",
      "Fold: 1  Epoch: 429  Training loss = 2.6830  Validation loss = 2.1129  \n",
      "\n",
      "Fold: 1  Epoch: 430  Training loss = 2.6827  Validation loss = 2.1123  \n",
      "\n",
      "Fold: 1  Epoch: 431  Training loss = 2.6823  Validation loss = 2.1103  \n",
      "\n",
      "Fold: 1  Epoch: 432  Training loss = 2.6815  Validation loss = 2.1075  \n",
      "\n",
      "Fold: 1  Epoch: 433  Training loss = 2.6806  Validation loss = 2.1043  \n",
      "\n",
      "Fold: 1  Epoch: 434  Training loss = 2.6801  Validation loss = 2.1022  \n",
      "\n",
      "Fold: 1  Epoch: 435  Training loss = 2.6795  Validation loss = 2.1002  \n",
      "\n",
      "Fold: 1  Epoch: 436  Training loss = 2.6790  Validation loss = 2.0985  \n",
      "\n",
      "Fold: 1  Epoch: 437  Training loss = 2.6786  Validation loss = 2.0971  \n",
      "\n",
      "Fold: 1  Epoch: 438  Training loss = 2.6774  Validation loss = 2.0930  \n",
      "\n",
      "Fold: 1  Epoch: 439  Training loss = 2.6770  Validation loss = 2.0917  \n",
      "\n",
      "Fold: 1  Epoch: 440  Training loss = 2.6768  Validation loss = 2.0908  \n",
      "\n",
      "Fold: 1  Epoch: 441  Training loss = 2.6764  Validation loss = 2.0897  \n",
      "\n",
      "Fold: 1  Epoch: 442  Training loss = 2.6761  Validation loss = 2.0884  \n",
      "\n",
      "Fold: 1  Epoch: 443  Training loss = 2.6757  Validation loss = 2.0870  \n",
      "\n",
      "Fold: 1  Epoch: 444  Training loss = 2.6750  Validation loss = 2.0842  \n",
      "\n",
      "Fold: 1  Epoch: 445  Training loss = 2.6749  Validation loss = 2.0841  \n",
      "\n",
      "Fold: 1  Epoch: 446  Training loss = 2.6742  Validation loss = 2.0813  \n",
      "\n",
      "Fold: 1  Epoch: 447  Training loss = 2.6741  Validation loss = 2.0811  \n",
      "\n",
      "Fold: 1  Epoch: 448  Training loss = 2.6732  Validation loss = 2.0778  \n",
      "\n",
      "Fold: 1  Epoch: 449  Training loss = 2.6725  Validation loss = 2.0754  \n",
      "\n",
      "Fold: 1  Epoch: 450  Training loss = 2.6720  Validation loss = 2.0738  \n",
      "\n",
      "Fold: 1  Epoch: 451  Training loss = 2.6716  Validation loss = 2.0723  \n",
      "\n",
      "Fold: 1  Epoch: 452  Training loss = 2.6709  Validation loss = 2.0699  \n",
      "\n",
      "Fold: 1  Epoch: 453  Training loss = 2.6707  Validation loss = 2.0690  \n",
      "\n",
      "Fold: 1  Epoch: 454  Training loss = 2.6700  Validation loss = 2.0664  \n",
      "\n",
      "Fold: 1  Epoch: 455  Training loss = 2.6695  Validation loss = 2.0649  \n",
      "\n",
      "Fold: 1  Epoch: 456  Training loss = 2.6689  Validation loss = 2.0627  \n",
      "\n",
      "Fold: 1  Epoch: 457  Training loss = 2.6686  Validation loss = 2.0617  \n",
      "\n",
      "Fold: 1  Epoch: 458  Training loss = 2.6684  Validation loss = 2.0607  \n",
      "\n",
      "Fold: 1  Epoch: 459  Training loss = 2.6680  Validation loss = 2.0593  \n",
      "\n",
      "Fold: 1  Epoch: 460  Training loss = 2.6674  Validation loss = 2.0571  \n",
      "\n",
      "Fold: 1  Epoch: 461  Training loss = 2.6670  Validation loss = 2.0555  \n",
      "\n",
      "Fold: 1  Epoch: 462  Training loss = 2.6661  Validation loss = 2.0525  \n",
      "\n",
      "Fold: 1  Epoch: 463  Training loss = 2.6659  Validation loss = 2.0513  \n",
      "\n",
      "Fold: 1  Epoch: 464  Training loss = 2.6657  Validation loss = 2.0506  \n",
      "\n",
      "Fold: 1  Epoch: 465  Training loss = 2.6656  Validation loss = 2.0502  \n",
      "\n",
      "Fold: 1  Epoch: 466  Training loss = 2.6654  Validation loss = 2.0499  \n",
      "\n",
      "Fold: 1  Epoch: 467  Training loss = 2.6651  Validation loss = 2.0487  \n",
      "\n",
      "Fold: 1  Epoch: 468  Training loss = 2.6644  Validation loss = 2.0460  \n",
      "\n",
      "Fold: 1  Epoch: 469  Training loss = 2.6639  Validation loss = 2.0437  \n",
      "\n",
      "Fold: 1  Epoch: 470  Training loss = 2.6634  Validation loss = 2.0420  \n",
      "\n",
      "Fold: 1  Epoch: 471  Training loss = 2.6629  Validation loss = 2.0403  \n",
      "\n",
      "Fold: 1  Epoch: 472  Training loss = 2.6624  Validation loss = 2.0386  \n",
      "\n",
      "Fold: 1  Epoch: 473  Training loss = 2.6619  Validation loss = 2.0370  \n",
      "\n",
      "Fold: 1  Epoch: 474  Training loss = 2.6616  Validation loss = 2.0359  \n",
      "\n",
      "Fold: 1  Epoch: 475  Training loss = 2.6614  Validation loss = 2.0353  \n",
      "\n",
      "Fold: 1  Epoch: 476  Training loss = 2.6610  Validation loss = 2.0338  \n",
      "\n",
      "Fold: 1  Epoch: 477  Training loss = 2.6605  Validation loss = 2.0319  \n",
      "\n",
      "Fold: 1  Epoch: 478  Training loss = 2.6603  Validation loss = 2.0315  \n",
      "\n",
      "Fold: 1  Epoch: 479  Training loss = 2.6599  Validation loss = 2.0299  \n",
      "\n",
      "Fold: 1  Epoch: 480  Training loss = 2.6594  Validation loss = 2.0282  \n",
      "\n",
      "Fold: 1  Epoch: 481  Training loss = 2.6591  Validation loss = 2.0274  \n",
      "\n",
      "Fold: 1  Epoch: 482  Training loss = 2.6587  Validation loss = 2.0262  \n",
      "\n",
      "Fold: 1  Epoch: 483  Training loss = 2.6582  Validation loss = 2.0242  \n",
      "\n",
      "Fold: 1  Epoch: 484  Training loss = 2.6579  Validation loss = 2.0232  \n",
      "\n",
      "Fold: 1  Epoch: 485  Training loss = 2.6578  Validation loss = 2.0229  \n",
      "\n",
      "Fold: 1  Epoch: 486  Training loss = 2.6565  Validation loss = 2.0183  \n",
      "\n",
      "Fold: 1  Epoch: 487  Training loss = 2.6560  Validation loss = 2.0164  \n",
      "\n",
      "Fold: 1  Epoch: 488  Training loss = 2.6554  Validation loss = 2.0140  \n",
      "\n",
      "Fold: 1  Epoch: 489  Training loss = 2.6550  Validation loss = 2.0125  \n",
      "\n",
      "Fold: 1  Epoch: 490  Training loss = 2.6547  Validation loss = 2.0112  \n",
      "\n",
      "Fold: 1  Epoch: 491  Training loss = 2.6546  Validation loss = 2.0107  \n",
      "\n",
      "Fold: 1  Epoch: 492  Training loss = 2.6540  Validation loss = 2.0083  \n",
      "\n",
      "Fold: 1  Epoch: 493  Training loss = 2.6535  Validation loss = 2.0066  \n",
      "\n",
      "Fold: 1  Epoch: 494  Training loss = 2.6532  Validation loss = 2.0057  \n",
      "\n",
      "Fold: 1  Epoch: 495  Training loss = 2.6522  Validation loss = 2.0018  \n",
      "\n",
      "Fold: 1  Epoch: 496  Training loss = 2.6519  Validation loss = 2.0006  \n",
      "\n",
      "Fold: 1  Epoch: 497  Training loss = 2.6518  Validation loss = 2.0003  \n",
      "\n",
      "Fold: 1  Epoch: 498  Training loss = 2.6518  Validation loss = 2.0005  \n",
      "\n",
      "Fold: 1  Epoch: 499  Training loss = 2.6514  Validation loss = 1.9990  \n",
      "\n",
      "Fold: 1  Epoch: 500  Training loss = 2.6511  Validation loss = 1.9983  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 500  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.5551  Validation loss = 2.1598  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.5547  Validation loss = 2.1589  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.5542  Validation loss = 2.1578  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.5541  Validation loss = 2.1576  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.5536  Validation loss = 2.1561  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.5529  Validation loss = 2.1546  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.5523  Validation loss = 2.1533  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.5517  Validation loss = 2.1516  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.5510  Validation loss = 2.1501  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.5502  Validation loss = 2.1479  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.5501  Validation loss = 2.1476  \n",
      "\n",
      "Fold: 2  Epoch: 12  Training loss = 2.5493  Validation loss = 2.1457  \n",
      "\n",
      "Fold: 2  Epoch: 13  Training loss = 2.5487  Validation loss = 2.1440  \n",
      "\n",
      "Fold: 2  Epoch: 14  Training loss = 2.5477  Validation loss = 2.1415  \n",
      "\n",
      "Fold: 2  Epoch: 15  Training loss = 2.5472  Validation loss = 2.1406  \n",
      "\n",
      "Fold: 2  Epoch: 16  Training loss = 2.5469  Validation loss = 2.1399  \n",
      "\n",
      "Fold: 2  Epoch: 17  Training loss = 2.5469  Validation loss = 2.1400  \n",
      "\n",
      "Fold: 2  Epoch: 18  Training loss = 2.5463  Validation loss = 2.1388  \n",
      "\n",
      "Fold: 2  Epoch: 19  Training loss = 2.5456  Validation loss = 2.1371  \n",
      "\n",
      "Fold: 2  Epoch: 20  Training loss = 2.5450  Validation loss = 2.1354  \n",
      "\n",
      "Fold: 2  Epoch: 21  Training loss = 2.5444  Validation loss = 2.1338  \n",
      "\n",
      "Fold: 2  Epoch: 22  Training loss = 2.5437  Validation loss = 2.1322  \n",
      "\n",
      "Fold: 2  Epoch: 23  Training loss = 2.5435  Validation loss = 2.1319  \n",
      "\n",
      "Fold: 2  Epoch: 24  Training loss = 2.5429  Validation loss = 2.1301  \n",
      "\n",
      "Fold: 2  Epoch: 25  Training loss = 2.5423  Validation loss = 2.1290  \n",
      "\n",
      "Fold: 2  Epoch: 26  Training loss = 2.5419  Validation loss = 2.1281  \n",
      "\n",
      "Fold: 2  Epoch: 27  Training loss = 2.5419  Validation loss = 2.1279  \n",
      "\n",
      "Fold: 2  Epoch: 28  Training loss = 2.5412  Validation loss = 2.1260  \n",
      "\n",
      "Fold: 2  Epoch: 29  Training loss = 2.5409  Validation loss = 2.1251  \n",
      "\n",
      "Fold: 2  Epoch: 30  Training loss = 2.5403  Validation loss = 2.1234  \n",
      "\n",
      "Fold: 2  Epoch: 31  Training loss = 2.5401  Validation loss = 2.1226  \n",
      "\n",
      "Fold: 2  Epoch: 32  Training loss = 2.5399  Validation loss = 2.1223  \n",
      "\n",
      "Fold: 2  Epoch: 33  Training loss = 2.5399  Validation loss = 2.1223  \n",
      "\n",
      "Fold: 2  Epoch: 34  Training loss = 2.5398  Validation loss = 2.1217  \n",
      "\n",
      "Fold: 2  Epoch: 35  Training loss = 2.5394  Validation loss = 2.1208  \n",
      "\n",
      "Fold: 2  Epoch: 36  Training loss = 2.5388  Validation loss = 2.1196  \n",
      "\n",
      "Fold: 2  Epoch: 37  Training loss = 2.5386  Validation loss = 2.1188  \n",
      "\n",
      "Fold: 2  Epoch: 38  Training loss = 2.5383  Validation loss = 2.1180  \n",
      "\n",
      "Fold: 2  Epoch: 39  Training loss = 2.5376  Validation loss = 2.1159  \n",
      "\n",
      "Fold: 2  Epoch: 40  Training loss = 2.5373  Validation loss = 2.1151  \n",
      "\n",
      "Fold: 2  Epoch: 41  Training loss = 2.5367  Validation loss = 2.1134  \n",
      "\n",
      "Fold: 2  Epoch: 42  Training loss = 2.5360  Validation loss = 2.1118  \n",
      "\n",
      "Fold: 2  Epoch: 43  Training loss = 2.5358  Validation loss = 2.1109  \n",
      "\n",
      "Fold: 2  Epoch: 44  Training loss = 2.5356  Validation loss = 2.1105  \n",
      "\n",
      "Fold: 2  Epoch: 45  Training loss = 2.5352  Validation loss = 2.1093  \n",
      "\n",
      "Fold: 2  Epoch: 46  Training loss = 2.5348  Validation loss = 2.1086  \n",
      "\n",
      "Fold: 2  Epoch: 47  Training loss = 2.5347  Validation loss = 2.1082  \n",
      "\n",
      "Fold: 2  Epoch: 48  Training loss = 2.5345  Validation loss = 2.1077  \n",
      "\n",
      "Fold: 2  Epoch: 49  Training loss = 2.5342  Validation loss = 2.1069  \n",
      "\n",
      "Fold: 2  Epoch: 50  Training loss = 2.5338  Validation loss = 2.1059  \n",
      "\n",
      "Fold: 2  Epoch: 51  Training loss = 2.5337  Validation loss = 2.1055  \n",
      "\n",
      "Fold: 2  Epoch: 52  Training loss = 2.5333  Validation loss = 2.1044  \n",
      "\n",
      "Fold: 2  Epoch: 53  Training loss = 2.5328  Validation loss = 2.1034  \n",
      "\n",
      "Fold: 2  Epoch: 54  Training loss = 2.5325  Validation loss = 2.1022  \n",
      "\n",
      "Fold: 2  Epoch: 55  Training loss = 2.5325  Validation loss = 2.1023  \n",
      "\n",
      "Fold: 2  Epoch: 56  Training loss = 2.5322  Validation loss = 2.1013  \n",
      "\n",
      "Fold: 2  Epoch: 57  Training loss = 2.5313  Validation loss = 2.0991  \n",
      "\n",
      "Fold: 2  Epoch: 58  Training loss = 2.5309  Validation loss = 2.0983  \n",
      "\n",
      "Fold: 2  Epoch: 59  Training loss = 2.5305  Validation loss = 2.0974  \n",
      "\n",
      "Fold: 2  Epoch: 60  Training loss = 2.5299  Validation loss = 2.0957  \n",
      "\n",
      "Fold: 2  Epoch: 61  Training loss = 2.5296  Validation loss = 2.0947  \n",
      "\n",
      "Fold: 2  Epoch: 62  Training loss = 2.5291  Validation loss = 2.0934  \n",
      "\n",
      "Fold: 2  Epoch: 63  Training loss = 2.5286  Validation loss = 2.0920  \n",
      "\n",
      "Fold: 2  Epoch: 64  Training loss = 2.5282  Validation loss = 2.0907  \n",
      "\n",
      "Fold: 2  Epoch: 65  Training loss = 2.5277  Validation loss = 2.0894  \n",
      "\n",
      "Fold: 2  Epoch: 66  Training loss = 2.5275  Validation loss = 2.0891  \n",
      "\n",
      "Fold: 2  Epoch: 67  Training loss = 2.5276  Validation loss = 2.0893  \n",
      "\n",
      "Fold: 2  Epoch: 68  Training loss = 2.5266  Validation loss = 2.0866  \n",
      "\n",
      "Fold: 2  Epoch: 69  Training loss = 2.5262  Validation loss = 2.0856  \n",
      "\n",
      "Fold: 2  Epoch: 70  Training loss = 2.5258  Validation loss = 2.0847  \n",
      "\n",
      "Fold: 2  Epoch: 71  Training loss = 2.5257  Validation loss = 2.0847  \n",
      "\n",
      "Fold: 2  Epoch: 72  Training loss = 2.5254  Validation loss = 2.0841  \n",
      "\n",
      "Fold: 2  Epoch: 73  Training loss = 2.5249  Validation loss = 2.0827  \n",
      "\n",
      "Fold: 2  Epoch: 74  Training loss = 2.5243  Validation loss = 2.0810  \n",
      "\n",
      "Fold: 2  Epoch: 75  Training loss = 2.5240  Validation loss = 2.0800  \n",
      "\n",
      "Fold: 2  Epoch: 76  Training loss = 2.5240  Validation loss = 2.0797  \n",
      "\n",
      "Fold: 2  Epoch: 77  Training loss = 2.5238  Validation loss = 2.0789  \n",
      "\n",
      "Fold: 2  Epoch: 78  Training loss = 2.5235  Validation loss = 2.0780  \n",
      "\n",
      "Fold: 2  Epoch: 79  Training loss = 2.5232  Validation loss = 2.0772  \n",
      "\n",
      "Fold: 2  Epoch: 80  Training loss = 2.5229  Validation loss = 2.0757  \n",
      "\n",
      "Fold: 2  Epoch: 81  Training loss = 2.5226  Validation loss = 2.0752  \n",
      "\n",
      "Fold: 2  Epoch: 82  Training loss = 2.5222  Validation loss = 2.0740  \n",
      "\n",
      "Fold: 2  Epoch: 83  Training loss = 2.5218  Validation loss = 2.0732  \n",
      "\n",
      "Fold: 2  Epoch: 84  Training loss = 2.5216  Validation loss = 2.0726  \n",
      "\n",
      "Fold: 2  Epoch: 85  Training loss = 2.5211  Validation loss = 2.0713  \n",
      "\n",
      "Fold: 2  Epoch: 86  Training loss = 2.5207  Validation loss = 2.0700  \n",
      "\n",
      "Fold: 2  Epoch: 87  Training loss = 2.5202  Validation loss = 2.0684  \n",
      "\n",
      "Fold: 2  Epoch: 88  Training loss = 2.5202  Validation loss = 2.0687  \n",
      "\n",
      "Fold: 2  Epoch: 89  Training loss = 2.5199  Validation loss = 2.0681  \n",
      "\n",
      "Fold: 2  Epoch: 90  Training loss = 2.5199  Validation loss = 2.0682  \n",
      "\n",
      "Fold: 2  Epoch: 91  Training loss = 2.5199  Validation loss = 2.0681  \n",
      "\n",
      "Fold: 2  Epoch: 92  Training loss = 2.5196  Validation loss = 2.0673  \n",
      "\n",
      "Fold: 2  Epoch: 93  Training loss = 2.5195  Validation loss = 2.0667  \n",
      "\n",
      "Fold: 2  Epoch: 94  Training loss = 2.5192  Validation loss = 2.0660  \n",
      "\n",
      "Fold: 2  Epoch: 95  Training loss = 2.5191  Validation loss = 2.0656  \n",
      "\n",
      "Fold: 2  Epoch: 96  Training loss = 2.5189  Validation loss = 2.0652  \n",
      "\n",
      "Fold: 2  Epoch: 97  Training loss = 2.5189  Validation loss = 2.0651  \n",
      "\n",
      "Fold: 2  Epoch: 98  Training loss = 2.5186  Validation loss = 2.0646  \n",
      "\n",
      "Fold: 2  Epoch: 99  Training loss = 2.5183  Validation loss = 2.0638  \n",
      "\n",
      "Fold: 2  Epoch: 100  Training loss = 2.5184  Validation loss = 2.0642  \n",
      "\n",
      "Fold: 2  Epoch: 101  Training loss = 2.5182  Validation loss = 2.0638  \n",
      "\n",
      "Fold: 2  Epoch: 102  Training loss = 2.5179  Validation loss = 2.0624  \n",
      "\n",
      "Fold: 2  Epoch: 103  Training loss = 2.5178  Validation loss = 2.0623  \n",
      "\n",
      "Fold: 2  Epoch: 104  Training loss = 2.5175  Validation loss = 2.0616  \n",
      "\n",
      "Fold: 2  Epoch: 105  Training loss = 2.5172  Validation loss = 2.0608  \n",
      "\n",
      "Fold: 2  Epoch: 106  Training loss = 2.5171  Validation loss = 2.0605  \n",
      "\n",
      "Fold: 2  Epoch: 107  Training loss = 2.5169  Validation loss = 2.0596  \n",
      "\n",
      "Fold: 2  Epoch: 108  Training loss = 2.5169  Validation loss = 2.0594  \n",
      "\n",
      "Fold: 2  Epoch: 109  Training loss = 2.5166  Validation loss = 2.0588  \n",
      "\n",
      "Fold: 2  Epoch: 110  Training loss = 2.5163  Validation loss = 2.0576  \n",
      "\n",
      "Fold: 2  Epoch: 111  Training loss = 2.5163  Validation loss = 2.0576  \n",
      "\n",
      "Fold: 2  Epoch: 112  Training loss = 2.5158  Validation loss = 2.0561  \n",
      "\n",
      "Fold: 2  Epoch: 113  Training loss = 2.5158  Validation loss = 2.0559  \n",
      "\n",
      "Fold: 2  Epoch: 114  Training loss = 2.5156  Validation loss = 2.0556  \n",
      "\n",
      "Fold: 2  Epoch: 115  Training loss = 2.5154  Validation loss = 2.0552  \n",
      "\n",
      "Fold: 2  Epoch: 116  Training loss = 2.5155  Validation loss = 2.0555  \n",
      "\n",
      "Fold: 2  Epoch: 117  Training loss = 2.5154  Validation loss = 2.0556  \n",
      "\n",
      "Fold: 2  Epoch: 118  Training loss = 2.5150  Validation loss = 2.0545  \n",
      "\n",
      "Fold: 2  Epoch: 119  Training loss = 2.5150  Validation loss = 2.0544  \n",
      "\n",
      "Fold: 2  Epoch: 120  Training loss = 2.5149  Validation loss = 2.0541  \n",
      "\n",
      "Fold: 2  Epoch: 121  Training loss = 2.5147  Validation loss = 2.0536  \n",
      "\n",
      "Fold: 2  Epoch: 122  Training loss = 2.5147  Validation loss = 2.0534  \n",
      "\n",
      "Fold: 2  Epoch: 123  Training loss = 2.5145  Validation loss = 2.0528  \n",
      "\n",
      "Fold: 2  Epoch: 124  Training loss = 2.5142  Validation loss = 2.0521  \n",
      "\n",
      "Fold: 2  Epoch: 125  Training loss = 2.5140  Validation loss = 2.0515  \n",
      "\n",
      "Fold: 2  Epoch: 126  Training loss = 2.5141  Validation loss = 2.0520  \n",
      "\n",
      "Fold: 2  Epoch: 127  Training loss = 2.5138  Validation loss = 2.0513  \n",
      "\n",
      "Fold: 2  Epoch: 128  Training loss = 2.5136  Validation loss = 2.0508  \n",
      "\n",
      "Fold: 2  Epoch: 129  Training loss = 2.5133  Validation loss = 2.0499  \n",
      "\n",
      "Fold: 2  Epoch: 130  Training loss = 2.5129  Validation loss = 2.0485  \n",
      "\n",
      "Fold: 2  Epoch: 131  Training loss = 2.5128  Validation loss = 2.0481  \n",
      "\n",
      "Fold: 2  Epoch: 132  Training loss = 2.5127  Validation loss = 2.0477  \n",
      "\n",
      "Fold: 2  Epoch: 133  Training loss = 2.5127  Validation loss = 2.0475  \n",
      "\n",
      "Fold: 2  Epoch: 134  Training loss = 2.5126  Validation loss = 2.0473  \n",
      "\n",
      "Fold: 2  Epoch: 135  Training loss = 2.5125  Validation loss = 2.0471  \n",
      "\n",
      "Fold: 2  Epoch: 136  Training loss = 2.5123  Validation loss = 2.0469  \n",
      "\n",
      "Fold: 2  Epoch: 137  Training loss = 2.5121  Validation loss = 2.0464  \n",
      "\n",
      "Fold: 2  Epoch: 138  Training loss = 2.5118  Validation loss = 2.0453  \n",
      "\n",
      "Fold: 2  Epoch: 139  Training loss = 2.5116  Validation loss = 2.0450  \n",
      "\n",
      "Fold: 2  Epoch: 140  Training loss = 2.5115  Validation loss = 2.0447  \n",
      "\n",
      "Fold: 2  Epoch: 141  Training loss = 2.5116  Validation loss = 2.0447  \n",
      "\n",
      "Fold: 2  Epoch: 142  Training loss = 2.5111  Validation loss = 2.0435  \n",
      "\n",
      "Fold: 2  Epoch: 143  Training loss = 2.5112  Validation loss = 2.0441  \n",
      "\n",
      "Fold: 2  Epoch: 144  Training loss = 2.5110  Validation loss = 2.0433  \n",
      "\n",
      "Fold: 2  Epoch: 145  Training loss = 2.5103  Validation loss = 2.0416  \n",
      "\n",
      "Fold: 2  Epoch: 146  Training loss = 2.5102  Validation loss = 2.0415  \n",
      "\n",
      "Fold: 2  Epoch: 147  Training loss = 2.5100  Validation loss = 2.0410  \n",
      "\n",
      "Fold: 2  Epoch: 148  Training loss = 2.5100  Validation loss = 2.0410  \n",
      "\n",
      "Fold: 2  Epoch: 149  Training loss = 2.5098  Validation loss = 2.0402  \n",
      "\n",
      "Fold: 2  Epoch: 150  Training loss = 2.5095  Validation loss = 2.0395  \n",
      "\n",
      "Fold: 2  Epoch: 151  Training loss = 2.5093  Validation loss = 2.0394  \n",
      "\n",
      "Fold: 2  Epoch: 152  Training loss = 2.5091  Validation loss = 2.0388  \n",
      "\n",
      "Fold: 2  Epoch: 153  Training loss = 2.5090  Validation loss = 2.0388  \n",
      "\n",
      "Fold: 2  Epoch: 154  Training loss = 2.5088  Validation loss = 2.0383  \n",
      "\n",
      "Fold: 2  Epoch: 155  Training loss = 2.5083  Validation loss = 2.0369  \n",
      "\n",
      "Fold: 2  Epoch: 156  Training loss = 2.5082  Validation loss = 2.0365  \n",
      "\n",
      "Fold: 2  Epoch: 157  Training loss = 2.5081  Validation loss = 2.0365  \n",
      "\n",
      "Fold: 2  Epoch: 158  Training loss = 2.5077  Validation loss = 2.0355  \n",
      "\n",
      "Fold: 2  Epoch: 159  Training loss = 2.5075  Validation loss = 2.0352  \n",
      "\n",
      "Fold: 2  Epoch: 160  Training loss = 2.5075  Validation loss = 2.0352  \n",
      "\n",
      "Fold: 2  Epoch: 161  Training loss = 2.5072  Validation loss = 2.0345  \n",
      "\n",
      "Fold: 2  Epoch: 162  Training loss = 2.5066  Validation loss = 2.0324  \n",
      "\n",
      "Fold: 2  Epoch: 163  Training loss = 2.5063  Validation loss = 2.0314  \n",
      "\n",
      "Fold: 2  Epoch: 164  Training loss = 2.5061  Validation loss = 2.0305  \n",
      "\n",
      "Fold: 2  Epoch: 165  Training loss = 2.5056  Validation loss = 2.0290  \n",
      "\n",
      "Fold: 2  Epoch: 166  Training loss = 2.5051  Validation loss = 2.0275  \n",
      "\n",
      "Fold: 2  Epoch: 167  Training loss = 2.5047  Validation loss = 2.0263  \n",
      "\n",
      "Fold: 2  Epoch: 168  Training loss = 2.5047  Validation loss = 2.0263  \n",
      "\n",
      "Fold: 2  Epoch: 169  Training loss = 2.5047  Validation loss = 2.0266  \n",
      "\n",
      "Fold: 2  Epoch: 170  Training loss = 2.5046  Validation loss = 2.0267  \n",
      "\n",
      "Fold: 2  Epoch: 171  Training loss = 2.5045  Validation loss = 2.0265  \n",
      "\n",
      "Fold: 2  Epoch: 172  Training loss = 2.5042  Validation loss = 2.0256  \n",
      "\n",
      "Fold: 2  Epoch: 173  Training loss = 2.5037  Validation loss = 2.0243  \n",
      "\n",
      "Fold: 2  Epoch: 174  Training loss = 2.5034  Validation loss = 2.0234  \n",
      "\n",
      "Fold: 2  Epoch: 175  Training loss = 2.5032  Validation loss = 2.0229  \n",
      "\n",
      "Fold: 2  Epoch: 176  Training loss = 2.5031  Validation loss = 2.0225  \n",
      "\n",
      "Fold: 2  Epoch: 177  Training loss = 2.5031  Validation loss = 2.0229  \n",
      "\n",
      "Fold: 2  Epoch: 178  Training loss = 2.5028  Validation loss = 2.0219  \n",
      "\n",
      "Fold: 2  Epoch: 179  Training loss = 2.5028  Validation loss = 2.0220  \n",
      "\n",
      "Fold: 2  Epoch: 180  Training loss = 2.5027  Validation loss = 2.0218  \n",
      "\n",
      "Fold: 2  Epoch: 181  Training loss = 2.5027  Validation loss = 2.0217  \n",
      "\n",
      "Fold: 2  Epoch: 182  Training loss = 2.5026  Validation loss = 2.0213  \n",
      "\n",
      "Fold: 2  Epoch: 183  Training loss = 2.5024  Validation loss = 2.0211  \n",
      "\n",
      "Fold: 2  Epoch: 184  Training loss = 2.5023  Validation loss = 2.0210  \n",
      "\n",
      "Fold: 2  Epoch: 185  Training loss = 2.5020  Validation loss = 2.0203  \n",
      "\n",
      "Fold: 2  Epoch: 186  Training loss = 2.5018  Validation loss = 2.0197  \n",
      "\n",
      "Fold: 2  Epoch: 187  Training loss = 2.5018  Validation loss = 2.0197  \n",
      "\n",
      "Fold: 2  Epoch: 188  Training loss = 2.5016  Validation loss = 2.0192  \n",
      "\n",
      "Fold: 2  Epoch: 189  Training loss = 2.5014  Validation loss = 2.0184  \n",
      "\n",
      "Fold: 2  Epoch: 190  Training loss = 2.5013  Validation loss = 2.0184  \n",
      "\n",
      "Fold: 2  Epoch: 191  Training loss = 2.5012  Validation loss = 2.0181  \n",
      "\n",
      "Fold: 2  Epoch: 192  Training loss = 2.5009  Validation loss = 2.0175  \n",
      "\n",
      "Fold: 2  Epoch: 193  Training loss = 2.5008  Validation loss = 2.0172  \n",
      "\n",
      "Fold: 2  Epoch: 194  Training loss = 2.5006  Validation loss = 2.0164  \n",
      "\n",
      "Fold: 2  Epoch: 195  Training loss = 2.5002  Validation loss = 2.0153  \n",
      "\n",
      "Fold: 2  Epoch: 196  Training loss = 2.4999  Validation loss = 2.0144  \n",
      "\n",
      "Fold: 2  Epoch: 197  Training loss = 2.4997  Validation loss = 2.0133  \n",
      "\n",
      "Fold: 2  Epoch: 198  Training loss = 2.4994  Validation loss = 2.0126  \n",
      "\n",
      "Fold: 2  Epoch: 199  Training loss = 2.4994  Validation loss = 2.0126  \n",
      "\n",
      "Fold: 2  Epoch: 200  Training loss = 2.4990  Validation loss = 2.0116  \n",
      "\n",
      "Fold: 2  Epoch: 201  Training loss = 2.4989  Validation loss = 2.0116  \n",
      "\n",
      "Fold: 2  Epoch: 202  Training loss = 2.4988  Validation loss = 2.0110  \n",
      "\n",
      "Fold: 2  Epoch: 203  Training loss = 2.4988  Validation loss = 2.0108  \n",
      "\n",
      "Fold: 2  Epoch: 204  Training loss = 2.4987  Validation loss = 2.0108  \n",
      "\n",
      "Fold: 2  Epoch: 205  Training loss = 2.4987  Validation loss = 2.0105  \n",
      "\n",
      "Fold: 2  Epoch: 206  Training loss = 2.4985  Validation loss = 2.0101  \n",
      "\n",
      "Fold: 2  Epoch: 207  Training loss = 2.4981  Validation loss = 2.0085  \n",
      "\n",
      "Fold: 2  Epoch: 208  Training loss = 2.4979  Validation loss = 2.0078  \n",
      "\n",
      "Fold: 2  Epoch: 209  Training loss = 2.4976  Validation loss = 2.0069  \n",
      "\n",
      "Fold: 2  Epoch: 210  Training loss = 2.4973  Validation loss = 2.0056  \n",
      "\n",
      "Fold: 2  Epoch: 211  Training loss = 2.4970  Validation loss = 2.0050  \n",
      "\n",
      "Fold: 2  Epoch: 212  Training loss = 2.4970  Validation loss = 2.0045  \n",
      "\n",
      "Fold: 2  Epoch: 213  Training loss = 2.4966  Validation loss = 2.0033  \n",
      "\n",
      "Fold: 2  Epoch: 214  Training loss = 2.4963  Validation loss = 2.0023  \n",
      "\n",
      "Fold: 2  Epoch: 215  Training loss = 2.4961  Validation loss = 2.0022  \n",
      "\n",
      "Fold: 2  Epoch: 216  Training loss = 2.4962  Validation loss = 2.0023  \n",
      "\n",
      "Fold: 2  Epoch: 217  Training loss = 2.4960  Validation loss = 2.0020  \n",
      "\n",
      "Fold: 2  Epoch: 218  Training loss = 2.4958  Validation loss = 2.0011  \n",
      "\n",
      "Fold: 2  Epoch: 219  Training loss = 2.4956  Validation loss = 2.0005  \n",
      "\n",
      "Fold: 2  Epoch: 220  Training loss = 2.4953  Validation loss = 1.9997  \n",
      "\n",
      "Fold: 2  Epoch: 221  Training loss = 2.4953  Validation loss = 1.9999  \n",
      "\n",
      "Fold: 2  Epoch: 222  Training loss = 2.4950  Validation loss = 1.9989  \n",
      "\n",
      "Fold: 2  Epoch: 223  Training loss = 2.4948  Validation loss = 1.9982  \n",
      "\n",
      "Fold: 2  Epoch: 224  Training loss = 2.4947  Validation loss = 1.9975  \n",
      "\n",
      "Fold: 2  Epoch: 225  Training loss = 2.4946  Validation loss = 1.9973  \n",
      "\n",
      "Fold: 2  Epoch: 226  Training loss = 2.4946  Validation loss = 1.9975  \n",
      "\n",
      "Fold: 2  Epoch: 227  Training loss = 2.4945  Validation loss = 1.9972  \n",
      "\n",
      "Fold: 2  Epoch: 228  Training loss = 2.4944  Validation loss = 1.9966  \n",
      "\n",
      "Fold: 2  Epoch: 229  Training loss = 2.4943  Validation loss = 1.9965  \n",
      "\n",
      "Fold: 2  Epoch: 230  Training loss = 2.4941  Validation loss = 1.9960  \n",
      "\n",
      "Fold: 2  Epoch: 231  Training loss = 2.4939  Validation loss = 1.9956  \n",
      "\n",
      "Fold: 2  Epoch: 232  Training loss = 2.4938  Validation loss = 1.9951  \n",
      "\n",
      "Fold: 2  Epoch: 233  Training loss = 2.4935  Validation loss = 1.9943  \n",
      "\n",
      "Fold: 2  Epoch: 234  Training loss = 2.4934  Validation loss = 1.9943  \n",
      "\n",
      "Fold: 2  Epoch: 235  Training loss = 2.4933  Validation loss = 1.9936  \n",
      "\n",
      "Fold: 2  Epoch: 236  Training loss = 2.4931  Validation loss = 1.9934  \n",
      "\n",
      "Fold: 2  Epoch: 237  Training loss = 2.4931  Validation loss = 1.9934  \n",
      "\n",
      "Fold: 2  Epoch: 238  Training loss = 2.4930  Validation loss = 1.9932  \n",
      "\n",
      "Fold: 2  Epoch: 239  Training loss = 2.4928  Validation loss = 1.9923  \n",
      "\n",
      "Fold: 2  Epoch: 240  Training loss = 2.4927  Validation loss = 1.9920  \n",
      "\n",
      "Fold: 2  Epoch: 241  Training loss = 2.4927  Validation loss = 1.9922  \n",
      "\n",
      "Fold: 2  Epoch: 242  Training loss = 2.4927  Validation loss = 1.9923  \n",
      "\n",
      "Fold: 2  Epoch: 243  Training loss = 2.4924  Validation loss = 1.9918  \n",
      "\n",
      "Fold: 2  Epoch: 244  Training loss = 2.4924  Validation loss = 1.9919  \n",
      "\n",
      "Fold: 2  Epoch: 245  Training loss = 2.4924  Validation loss = 1.9923  \n",
      "\n",
      "Fold: 2  Epoch: 246  Training loss = 2.4924  Validation loss = 1.9922  \n",
      "\n",
      "Fold: 2  Epoch: 247  Training loss = 2.4923  Validation loss = 1.9919  \n",
      "\n",
      "Fold: 2  Epoch: 248  Training loss = 2.4920  Validation loss = 1.9911  \n",
      "\n",
      "Fold: 2  Epoch: 249  Training loss = 2.4917  Validation loss = 1.9901  \n",
      "\n",
      "Fold: 2  Epoch: 250  Training loss = 2.4915  Validation loss = 1.9893  \n",
      "\n",
      "Fold: 2  Epoch: 251  Training loss = 2.4913  Validation loss = 1.9885  \n",
      "\n",
      "Fold: 2  Epoch: 252  Training loss = 2.4912  Validation loss = 1.9882  \n",
      "\n",
      "Fold: 2  Epoch: 253  Training loss = 2.4911  Validation loss = 1.9876  \n",
      "\n",
      "Fold: 2  Epoch: 254  Training loss = 2.4910  Validation loss = 1.9875  \n",
      "\n",
      "Fold: 2  Epoch: 255  Training loss = 2.4908  Validation loss = 1.9869  \n",
      "\n",
      "Fold: 2  Epoch: 256  Training loss = 2.4909  Validation loss = 1.9876  \n",
      "\n",
      "Fold: 2  Epoch: 257  Training loss = 2.4905  Validation loss = 1.9862  \n",
      "\n",
      "Fold: 2  Epoch: 258  Training loss = 2.4905  Validation loss = 1.9862  \n",
      "\n",
      "Fold: 2  Epoch: 259  Training loss = 2.4904  Validation loss = 1.9857  \n",
      "\n",
      "Fold: 2  Epoch: 260  Training loss = 2.4903  Validation loss = 1.9860  \n",
      "\n",
      "Fold: 2  Epoch: 261  Training loss = 2.4902  Validation loss = 1.9854  \n",
      "\n",
      "Fold: 2  Epoch: 262  Training loss = 2.4901  Validation loss = 1.9854  \n",
      "\n",
      "Fold: 2  Epoch: 263  Training loss = 2.4900  Validation loss = 1.9851  \n",
      "\n",
      "Fold: 2  Epoch: 264  Training loss = 2.4898  Validation loss = 1.9845  \n",
      "\n",
      "Fold: 2  Epoch: 265  Training loss = 2.4895  Validation loss = 1.9835  \n",
      "\n",
      "Fold: 2  Epoch: 266  Training loss = 2.4894  Validation loss = 1.9832  \n",
      "\n",
      "Fold: 2  Epoch: 267  Training loss = 2.4894  Validation loss = 1.9836  \n",
      "\n",
      "Fold: 2  Epoch: 268  Training loss = 2.4891  Validation loss = 1.9826  \n",
      "\n",
      "Fold: 2  Epoch: 269  Training loss = 2.4890  Validation loss = 1.9825  \n",
      "\n",
      "Fold: 2  Epoch: 270  Training loss = 2.4890  Validation loss = 1.9828  \n",
      "\n",
      "Fold: 2  Epoch: 271  Training loss = 2.4888  Validation loss = 1.9823  \n",
      "\n",
      "Fold: 2  Epoch: 272  Training loss = 2.4886  Validation loss = 1.9813  \n",
      "\n",
      "Fold: 2  Epoch: 273  Training loss = 2.4886  Validation loss = 1.9811  \n",
      "\n",
      "Fold: 2  Epoch: 274  Training loss = 2.4886  Validation loss = 1.9810  \n",
      "\n",
      "Fold: 2  Epoch: 275  Training loss = 2.4885  Validation loss = 1.9806  \n",
      "\n",
      "Fold: 2  Epoch: 276  Training loss = 2.4883  Validation loss = 1.9801  \n",
      "\n",
      "Fold: 2  Epoch: 277  Training loss = 2.4883  Validation loss = 1.9800  \n",
      "\n",
      "Fold: 2  Epoch: 278  Training loss = 2.4879  Validation loss = 1.9786  \n",
      "\n",
      "Fold: 2  Epoch: 279  Training loss = 2.4878  Validation loss = 1.9787  \n",
      "\n",
      "Fold: 2  Epoch: 280  Training loss = 2.4879  Validation loss = 1.9792  \n",
      "\n",
      "Fold: 2  Epoch: 281  Training loss = 2.4879  Validation loss = 1.9800  \n",
      "\n",
      "Fold: 2  Epoch: 282  Training loss = 2.4877  Validation loss = 1.9795  \n",
      "\n",
      "Fold: 2  Epoch: 283  Training loss = 2.4875  Validation loss = 1.9786  \n",
      "\n",
      "Fold: 2  Epoch: 284  Training loss = 2.4874  Validation loss = 1.9779  \n",
      "\n",
      "Fold: 2  Epoch: 285  Training loss = 2.4873  Validation loss = 1.9778  \n",
      "\n",
      "Fold: 2  Epoch: 286  Training loss = 2.4871  Validation loss = 1.9771  \n",
      "\n",
      "Fold: 2  Epoch: 287  Training loss = 2.4868  Validation loss = 1.9760  \n",
      "\n",
      "Fold: 2  Epoch: 288  Training loss = 2.4869  Validation loss = 1.9764  \n",
      "\n",
      "Fold: 2  Epoch: 289  Training loss = 2.4864  Validation loss = 1.9748  \n",
      "\n",
      "Fold: 2  Epoch: 290  Training loss = 2.4863  Validation loss = 1.9741  \n",
      "\n",
      "Fold: 2  Epoch: 291  Training loss = 2.4859  Validation loss = 1.9726  \n",
      "\n",
      "Fold: 2  Epoch: 292  Training loss = 2.4857  Validation loss = 1.9719  \n",
      "\n",
      "Fold: 2  Epoch: 293  Training loss = 2.4856  Validation loss = 1.9715  \n",
      "\n",
      "Fold: 2  Epoch: 294  Training loss = 2.4856  Validation loss = 1.9716  \n",
      "\n",
      "Fold: 2  Epoch: 295  Training loss = 2.4854  Validation loss = 1.9708  \n",
      "\n",
      "Fold: 2  Epoch: 296  Training loss = 2.4852  Validation loss = 1.9704  \n",
      "\n",
      "Fold: 2  Epoch: 297  Training loss = 2.4850  Validation loss = 1.9699  \n",
      "\n",
      "Fold: 2  Epoch: 298  Training loss = 2.4849  Validation loss = 1.9698  \n",
      "\n",
      "Fold: 2  Epoch: 299  Training loss = 2.4849  Validation loss = 1.9703  \n",
      "\n",
      "Fold: 2  Epoch: 300  Training loss = 2.4846  Validation loss = 1.9692  \n",
      "\n",
      "Fold: 2  Epoch: 301  Training loss = 2.4845  Validation loss = 1.9686  \n",
      "\n",
      "Fold: 2  Epoch: 302  Training loss = 2.4844  Validation loss = 1.9683  \n",
      "\n",
      "Fold: 2  Epoch: 303  Training loss = 2.4844  Validation loss = 1.9683  \n",
      "\n",
      "Fold: 2  Epoch: 304  Training loss = 2.4842  Validation loss = 1.9673  \n",
      "\n",
      "Fold: 2  Epoch: 305  Training loss = 2.4841  Validation loss = 1.9672  \n",
      "\n",
      "Fold: 2  Epoch: 306  Training loss = 2.4840  Validation loss = 1.9666  \n",
      "\n",
      "Fold: 2  Epoch: 307  Training loss = 2.4836  Validation loss = 1.9656  \n",
      "\n",
      "Fold: 2  Epoch: 308  Training loss = 2.4837  Validation loss = 1.9662  \n",
      "\n",
      "Fold: 2  Epoch: 309  Training loss = 2.4835  Validation loss = 1.9658  \n",
      "\n",
      "Fold: 2  Epoch: 310  Training loss = 2.4833  Validation loss = 1.9654  \n",
      "\n",
      "Fold: 2  Epoch: 311  Training loss = 2.4832  Validation loss = 1.9648  \n",
      "\n",
      "Fold: 2  Epoch: 312  Training loss = 2.4829  Validation loss = 1.9636  \n",
      "\n",
      "Fold: 2  Epoch: 313  Training loss = 2.4829  Validation loss = 1.9635  \n",
      "\n",
      "Fold: 2  Epoch: 314  Training loss = 2.4827  Validation loss = 1.9619  \n",
      "\n",
      "Fold: 2  Epoch: 315  Training loss = 2.4826  Validation loss = 1.9621  \n",
      "\n",
      "Fold: 2  Epoch: 316  Training loss = 2.4825  Validation loss = 1.9618  \n",
      "\n",
      "Fold: 2  Epoch: 317  Training loss = 2.4823  Validation loss = 1.9610  \n",
      "\n",
      "Fold: 2  Epoch: 318  Training loss = 2.4821  Validation loss = 1.9605  \n",
      "\n",
      "Fold: 2  Epoch: 319  Training loss = 2.4820  Validation loss = 1.9600  \n",
      "\n",
      "Fold: 2  Epoch: 320  Training loss = 2.4820  Validation loss = 1.9603  \n",
      "\n",
      "Fold: 2  Epoch: 321  Training loss = 2.4820  Validation loss = 1.9604  \n",
      "\n",
      "Fold: 2  Epoch: 322  Training loss = 2.4820  Validation loss = 1.9605  \n",
      "\n",
      "Fold: 2  Epoch: 323  Training loss = 2.4820  Validation loss = 1.9608  \n",
      "\n",
      "Fold: 2  Epoch: 324  Training loss = 2.4820  Validation loss = 1.9604  \n",
      "\n",
      "Fold: 2  Epoch: 325  Training loss = 2.4818  Validation loss = 1.9599  \n",
      "\n",
      "Fold: 2  Epoch: 326  Training loss = 2.4816  Validation loss = 1.9593  \n",
      "\n",
      "Fold: 2  Epoch: 327  Training loss = 2.4815  Validation loss = 1.9588  \n",
      "\n",
      "Fold: 2  Epoch: 328  Training loss = 2.4812  Validation loss = 1.9580  \n",
      "\n",
      "Fold: 2  Epoch: 329  Training loss = 2.4808  Validation loss = 1.9561  \n",
      "\n",
      "Fold: 2  Epoch: 330  Training loss = 2.4807  Validation loss = 1.9559  \n",
      "\n",
      "Fold: 2  Epoch: 331  Training loss = 2.4808  Validation loss = 1.9565  \n",
      "\n",
      "Fold: 2  Epoch: 332  Training loss = 2.4806  Validation loss = 1.9559  \n",
      "\n",
      "Fold: 2  Epoch: 333  Training loss = 2.4805  Validation loss = 1.9556  \n",
      "\n",
      "Fold: 2  Epoch: 334  Training loss = 2.4804  Validation loss = 1.9557  \n",
      "\n",
      "Fold: 2  Epoch: 335  Training loss = 2.4804  Validation loss = 1.9553  \n",
      "\n",
      "Fold: 2  Epoch: 336  Training loss = 2.4804  Validation loss = 1.9555  \n",
      "\n",
      "Fold: 2  Epoch: 337  Training loss = 2.4803  Validation loss = 1.9548  \n",
      "\n",
      "Fold: 2  Epoch: 338  Training loss = 2.4803  Validation loss = 1.9546  \n",
      "\n",
      "Fold: 2  Epoch: 339  Training loss = 2.4802  Validation loss = 1.9543  \n",
      "\n",
      "Fold: 2  Epoch: 340  Training loss = 2.4800  Validation loss = 1.9534  \n",
      "\n",
      "Fold: 2  Epoch: 341  Training loss = 2.4799  Validation loss = 1.9533  \n",
      "\n",
      "Fold: 2  Epoch: 342  Training loss = 2.4798  Validation loss = 1.9529  \n",
      "\n",
      "Fold: 2  Epoch: 343  Training loss = 2.4797  Validation loss = 1.9528  \n",
      "\n",
      "Fold: 2  Epoch: 344  Training loss = 2.4795  Validation loss = 1.9523  \n",
      "\n",
      "Fold: 2  Epoch: 345  Training loss = 2.4795  Validation loss = 1.9525  \n",
      "\n",
      "Fold: 2  Epoch: 346  Training loss = 2.4796  Validation loss = 1.9534  \n",
      "\n",
      "Fold: 2  Epoch: 347  Training loss = 2.4797  Validation loss = 1.9539  \n",
      "\n",
      "Fold: 2  Epoch: 348  Training loss = 2.4795  Validation loss = 1.9533  \n",
      "\n",
      "Fold: 2  Epoch: 349  Training loss = 2.4793  Validation loss = 1.9523  \n",
      "\n",
      "Fold: 2  Epoch: 350  Training loss = 2.4793  Validation loss = 1.9526  \n",
      "\n",
      "Fold: 2  Epoch: 351  Training loss = 2.4792  Validation loss = 1.9521  \n",
      "\n",
      "Fold: 2  Epoch: 352  Training loss = 2.4791  Validation loss = 1.9515  \n",
      "\n",
      "Fold: 2  Epoch: 353  Training loss = 2.4789  Validation loss = 1.9510  \n",
      "\n",
      "Fold: 2  Epoch: 354  Training loss = 2.4789  Validation loss = 1.9513  \n",
      "\n",
      "Fold: 2  Epoch: 355  Training loss = 2.4788  Validation loss = 1.9517  \n",
      "\n",
      "Fold: 2  Epoch: 356  Training loss = 2.4788  Validation loss = 1.9523  \n",
      "\n",
      "Fold: 2  Epoch: 357  Training loss = 2.4789  Validation loss = 1.9527  \n",
      "\n",
      "Fold: 2  Epoch: 358  Training loss = 2.4788  Validation loss = 1.9518  \n",
      "\n",
      "Fold: 2  Epoch: 359  Training loss = 2.4786  Validation loss = 1.9516  \n",
      "\n",
      "Fold: 2  Epoch: 360  Training loss = 2.4787  Validation loss = 1.9520  \n",
      "\n",
      "Fold: 2  Epoch: 361  Training loss = 2.4786  Validation loss = 1.9516  \n",
      "\n",
      "Fold: 2  Epoch: 362  Training loss = 2.4784  Validation loss = 1.9507  \n",
      "\n",
      "Fold: 2  Epoch: 363  Training loss = 2.4784  Validation loss = 1.9509  \n",
      "\n",
      "Fold: 2  Epoch: 364  Training loss = 2.4781  Validation loss = 1.9497  \n",
      "\n",
      "Fold: 2  Epoch: 365  Training loss = 2.4781  Validation loss = 1.9501  \n",
      "\n",
      "Fold: 2  Epoch: 366  Training loss = 2.4780  Validation loss = 1.9496  \n",
      "\n",
      "Fold: 2  Epoch: 367  Training loss = 2.4780  Validation loss = 1.9500  \n",
      "\n",
      "Fold: 2  Epoch: 368  Training loss = 2.4780  Validation loss = 1.9504  \n",
      "\n",
      "Fold: 2  Epoch: 369  Training loss = 2.4779  Validation loss = 1.9504  \n",
      "\n",
      "Fold: 2  Epoch: 370  Training loss = 2.4779  Validation loss = 1.9505  \n",
      "\n",
      "Fold: 2  Epoch: 371  Training loss = 2.4778  Validation loss = 1.9505  \n",
      "\n",
      "Fold: 2  Epoch: 372  Training loss = 2.4778  Validation loss = 1.9510  \n",
      "\n",
      "Fold: 2  Epoch: 373  Training loss = 2.4778  Validation loss = 1.9515  \n",
      "\n",
      "Fold: 2  Epoch: 374  Training loss = 2.4776  Validation loss = 1.9507  \n",
      "\n",
      "Fold: 2  Epoch: 375  Training loss = 2.4775  Validation loss = 1.9505  \n",
      "\n",
      "Fold: 2  Epoch: 376  Training loss = 2.4774  Validation loss = 1.9498  \n",
      "\n",
      "Fold: 2  Epoch: 377  Training loss = 2.4772  Validation loss = 1.9491  \n",
      "\n",
      "Fold: 2  Epoch: 378  Training loss = 2.4772  Validation loss = 1.9490  \n",
      "\n",
      "Fold: 2  Epoch: 379  Training loss = 2.4771  Validation loss = 1.9485  \n",
      "\n",
      "Fold: 2  Epoch: 380  Training loss = 2.4769  Validation loss = 1.9482  \n",
      "\n",
      "Fold: 2  Epoch: 381  Training loss = 2.4769  Validation loss = 1.9484  \n",
      "\n",
      "Fold: 2  Epoch: 382  Training loss = 2.4768  Validation loss = 1.9477  \n",
      "\n",
      "Fold: 2  Epoch: 383  Training loss = 2.4767  Validation loss = 1.9473  \n",
      "\n",
      "Fold: 2  Epoch: 384  Training loss = 2.4767  Validation loss = 1.9480  \n",
      "\n",
      "Fold: 2  Epoch: 385  Training loss = 2.4768  Validation loss = 1.9485  \n",
      "\n",
      "Fold: 2  Epoch: 386  Training loss = 2.4767  Validation loss = 1.9485  \n",
      "\n",
      "Fold: 2  Epoch: 387  Training loss = 2.4766  Validation loss = 1.9484  \n",
      "\n",
      "Fold: 2  Epoch: 388  Training loss = 2.4766  Validation loss = 1.9486  \n",
      "\n",
      "Fold: 2  Epoch: 389  Training loss = 2.4766  Validation loss = 1.9484  \n",
      "\n",
      "Fold: 2  Epoch: 390  Training loss = 2.4765  Validation loss = 1.9487  \n",
      "\n",
      "Fold: 2  Epoch: 391  Training loss = 2.4764  Validation loss = 1.9483  \n",
      "\n",
      "Fold: 2  Epoch: 392  Training loss = 2.4763  Validation loss = 1.9481  \n",
      "\n",
      "Fold: 2  Epoch: 393  Training loss = 2.4762  Validation loss = 1.9478  \n",
      "\n",
      "Fold: 2  Epoch: 394  Training loss = 2.4763  Validation loss = 1.9486  \n",
      "\n",
      "Fold: 2  Epoch: 395  Training loss = 2.4761  Validation loss = 1.9473  \n",
      "\n",
      "Fold: 2  Epoch: 396  Training loss = 2.4759  Validation loss = 1.9468  \n",
      "\n",
      "Fold: 2  Epoch: 397  Training loss = 2.4758  Validation loss = 1.9463  \n",
      "\n",
      "Fold: 2  Epoch: 398  Training loss = 2.4756  Validation loss = 1.9456  \n",
      "\n",
      "Fold: 2  Epoch: 399  Training loss = 2.4755  Validation loss = 1.9449  \n",
      "\n",
      "Fold: 2  Epoch: 400  Training loss = 2.4754  Validation loss = 1.9447  \n",
      "\n",
      "Fold: 2  Epoch: 401  Training loss = 2.4754  Validation loss = 1.9449  \n",
      "\n",
      "Fold: 2  Epoch: 402  Training loss = 2.4752  Validation loss = 1.9440  \n",
      "\n",
      "Fold: 2  Epoch: 403  Training loss = 2.4752  Validation loss = 1.9443  \n",
      "\n",
      "Fold: 2  Epoch: 404  Training loss = 2.4752  Validation loss = 1.9446  \n",
      "\n",
      "Fold: 2  Epoch: 405  Training loss = 2.4751  Validation loss = 1.9446  \n",
      "\n",
      "Fold: 2  Epoch: 406  Training loss = 2.4750  Validation loss = 1.9441  \n",
      "\n",
      "Fold: 2  Epoch: 407  Training loss = 2.4750  Validation loss = 1.9443  \n",
      "\n",
      "Fold: 2  Epoch: 408  Training loss = 2.4747  Validation loss = 1.9429  \n",
      "\n",
      "Fold: 2  Epoch: 409  Training loss = 2.4746  Validation loss = 1.9423  \n",
      "\n",
      "Fold: 2  Epoch: 410  Training loss = 2.4746  Validation loss = 1.9424  \n",
      "\n",
      "Fold: 2  Epoch: 411  Training loss = 2.4747  Validation loss = 1.9436  \n",
      "\n",
      "Fold: 2  Epoch: 412  Training loss = 2.4746  Validation loss = 1.9432  \n",
      "\n",
      "Fold: 2  Epoch: 413  Training loss = 2.4742  Validation loss = 1.9411  \n",
      "\n",
      "Fold: 2  Epoch: 414  Training loss = 2.4742  Validation loss = 1.9413  \n",
      "\n",
      "Fold: 2  Epoch: 415  Training loss = 2.4741  Validation loss = 1.9412  \n",
      "\n",
      "Fold: 2  Epoch: 416  Training loss = 2.4739  Validation loss = 1.9401  \n",
      "\n",
      "Fold: 2  Epoch: 417  Training loss = 2.4738  Validation loss = 1.9399  \n",
      "\n",
      "Fold: 2  Epoch: 418  Training loss = 2.4738  Validation loss = 1.9402  \n",
      "\n",
      "Fold: 2  Epoch: 419  Training loss = 2.4738  Validation loss = 1.9405  \n",
      "\n",
      "Fold: 2  Epoch: 420  Training loss = 2.4737  Validation loss = 1.9400  \n",
      "\n",
      "Fold: 2  Epoch: 421  Training loss = 2.4737  Validation loss = 1.9404  \n",
      "\n",
      "Fold: 2  Epoch: 422  Training loss = 2.4737  Validation loss = 1.9404  \n",
      "\n",
      "Fold: 2  Epoch: 423  Training loss = 2.4737  Validation loss = 1.9411  \n",
      "\n",
      "Fold: 2  Epoch: 424  Training loss = 2.4737  Validation loss = 1.9416  \n",
      "\n",
      "Fold: 2  Epoch: 425  Training loss = 2.4736  Validation loss = 1.9417  \n",
      "\n",
      "Fold: 2  Epoch: 426  Training loss = 2.4734  Validation loss = 1.9408  \n",
      "\n",
      "Fold: 2  Epoch: 427  Training loss = 2.4733  Validation loss = 1.9402  \n",
      "\n",
      "Fold: 2  Epoch: 428  Training loss = 2.4734  Validation loss = 1.9407  \n",
      "\n",
      "Fold: 2  Epoch: 429  Training loss = 2.4733  Validation loss = 1.9409  \n",
      "\n",
      "Fold: 2  Epoch: 430  Training loss = 2.4733  Validation loss = 1.9411  \n",
      "\n",
      "Fold: 2  Epoch: 431  Training loss = 2.4731  Validation loss = 1.9402  \n",
      "\n",
      "Fold: 2  Epoch: 432  Training loss = 2.4729  Validation loss = 1.9398  \n",
      "\n",
      "Fold: 2  Epoch: 433  Training loss = 2.4729  Validation loss = 1.9400  \n",
      "\n",
      "Fold: 2  Epoch: 434  Training loss = 2.4729  Validation loss = 1.9397  \n",
      "\n",
      "Fold: 2  Epoch: 435  Training loss = 2.4730  Validation loss = 1.9406  \n",
      "\n",
      "Fold: 2  Epoch: 436  Training loss = 2.4729  Validation loss = 1.9406  \n",
      "\n",
      "Fold: 2  Epoch: 437  Training loss = 2.4729  Validation loss = 1.9408  \n",
      "\n",
      "Fold: 2  Epoch: 438  Training loss = 2.4728  Validation loss = 1.9409  \n",
      "\n",
      "Fold: 2  Epoch: 439  Training loss = 2.4728  Validation loss = 1.9410  \n",
      "\n",
      "Fold: 2  Epoch: 440  Training loss = 2.4726  Validation loss = 1.9401  \n",
      "\n",
      "Fold: 2  Epoch: 441  Training loss = 2.4725  Validation loss = 1.9399  \n",
      "\n",
      "Fold: 2  Epoch: 442  Training loss = 2.4724  Validation loss = 1.9399  \n",
      "\n",
      "Fold: 2  Epoch: 443  Training loss = 2.4724  Validation loss = 1.9392  \n",
      "\n",
      "Fold: 2  Epoch: 444  Training loss = 2.4724  Validation loss = 1.9394  \n",
      "\n",
      "Fold: 2  Epoch: 445  Training loss = 2.4724  Validation loss = 1.9395  \n",
      "\n",
      "Fold: 2  Epoch: 446  Training loss = 2.4724  Validation loss = 1.9397  \n",
      "\n",
      "Fold: 2  Epoch: 447  Training loss = 2.4723  Validation loss = 1.9391  \n",
      "\n",
      "Fold: 2  Epoch: 448  Training loss = 2.4723  Validation loss = 1.9394  \n",
      "\n",
      "Fold: 2  Epoch: 449  Training loss = 2.4724  Validation loss = 1.9402  \n",
      "\n",
      "Fold: 2  Epoch: 450  Training loss = 2.4722  Validation loss = 1.9390  \n",
      "\n",
      "Fold: 2  Epoch: 451  Training loss = 2.4722  Validation loss = 1.9395  \n",
      "\n",
      "Fold: 2  Epoch: 452  Training loss = 2.4722  Validation loss = 1.9400  \n",
      "\n",
      "Fold: 2  Epoch: 453  Training loss = 2.4720  Validation loss = 1.9389  \n",
      "\n",
      "Fold: 2  Epoch: 454  Training loss = 2.4721  Validation loss = 1.9403  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 453  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.4784  Validation loss = 3.1951  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.4783  Validation loss = 3.1955  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.4783  Validation loss = 3.1950  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.4782  Validation loss = 3.1956  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.4782  Validation loss = 3.1960  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.4781  Validation loss = 3.1966  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.4780  Validation loss = 3.1967  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.4780  Validation loss = 3.1970  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.4779  Validation loss = 3.1977  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.4778  Validation loss = 3.1992  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.4777  Validation loss = 3.1992  \n",
      "\n",
      "Fold: 3  Epoch: 12  Training loss = 1.4777  Validation loss = 3.1992  \n",
      "\n",
      "Fold: 3  Epoch: 13  Training loss = 1.4776  Validation loss = 3.1999  \n",
      "\n",
      "Fold: 3  Epoch: 14  Training loss = 1.4776  Validation loss = 3.2002  \n",
      "\n",
      "Fold: 3  Epoch: 15  Training loss = 1.4775  Validation loss = 3.2009  \n",
      "\n",
      "Fold: 3  Epoch: 16  Training loss = 1.4775  Validation loss = 3.2007  \n",
      "\n",
      "Fold: 3  Epoch: 17  Training loss = 1.4774  Validation loss = 3.2012  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 3  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.5853  Validation loss = 4.3530  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.5852  Validation loss = 4.3527  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.5852  Validation loss = 4.3529  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.5851  Validation loss = 4.3530  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.5851  Validation loss = 4.3537  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.5850  Validation loss = 4.3531  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.5850  Validation loss = 4.3527  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.5850  Validation loss = 4.3524  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.5849  Validation loss = 4.3525  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 1.5849  Validation loss = 4.3516  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.5849  Validation loss = 4.3519  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 1.5848  Validation loss = 4.3517  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.5847  Validation loss = 4.3514  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 1.5847  Validation loss = 4.3503  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.5846  Validation loss = 4.3506  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 1.5846  Validation loss = 4.3507  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 1.5846  Validation loss = 4.3512  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 1.5845  Validation loss = 4.3520  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 1.5845  Validation loss = 4.3517  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 1.5844  Validation loss = 4.3514  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 1.5844  Validation loss = 4.3508  \n",
      "\n",
      "Fold: 4  Epoch: 22  Training loss = 1.5843  Validation loss = 4.3510  \n",
      "\n",
      "Fold: 4  Epoch: 23  Training loss = 1.5843  Validation loss = 4.3507  \n",
      "\n",
      "Fold: 4  Epoch: 24  Training loss = 1.5842  Validation loss = 4.3507  \n",
      "\n",
      "Fold: 4  Epoch: 25  Training loss = 1.5841  Validation loss = 4.3494  \n",
      "\n",
      "Fold: 4  Epoch: 26  Training loss = 1.5841  Validation loss = 4.3489  \n",
      "\n",
      "Fold: 4  Epoch: 27  Training loss = 1.5840  Validation loss = 4.3490  \n",
      "\n",
      "Fold: 4  Epoch: 28  Training loss = 1.5840  Validation loss = 4.3486  \n",
      "\n",
      "Fold: 4  Epoch: 29  Training loss = 1.5839  Validation loss = 4.3482  \n",
      "\n",
      "Fold: 4  Epoch: 30  Training loss = 1.5839  Validation loss = 4.3468  \n",
      "\n",
      "Fold: 4  Epoch: 31  Training loss = 1.5838  Validation loss = 4.3455  \n",
      "\n",
      "Fold: 4  Epoch: 32  Training loss = 1.5838  Validation loss = 4.3457  \n",
      "\n",
      "Fold: 4  Epoch: 33  Training loss = 1.5837  Validation loss = 4.3466  \n",
      "\n",
      "Fold: 4  Epoch: 34  Training loss = 1.5836  Validation loss = 4.3471  \n",
      "\n",
      "Fold: 4  Epoch: 35  Training loss = 1.5836  Validation loss = 4.3483  \n",
      "\n",
      "Fold: 4  Epoch: 36  Training loss = 1.5836  Validation loss = 4.3485  \n",
      "\n",
      "Fold: 4  Epoch: 37  Training loss = 1.5835  Validation loss = 4.3487  \n",
      "\n",
      "Fold: 4  Epoch: 38  Training loss = 1.5834  Validation loss = 4.3485  \n",
      "\n",
      "Fold: 4  Epoch: 39  Training loss = 1.5834  Validation loss = 4.3475  \n",
      "\n",
      "Fold: 4  Epoch: 40  Training loss = 1.5833  Validation loss = 4.3482  \n",
      "\n",
      "Fold: 4  Epoch: 41  Training loss = 1.5833  Validation loss = 4.3488  \n",
      "\n",
      "Fold: 4  Epoch: 42  Training loss = 1.5832  Validation loss = 4.3469  \n",
      "\n",
      "Fold: 4  Epoch: 43  Training loss = 1.5831  Validation loss = 4.3449  \n",
      "\n",
      "Fold: 4  Epoch: 44  Training loss = 1.5830  Validation loss = 4.3452  \n",
      "\n",
      "Fold: 4  Epoch: 45  Training loss = 1.5830  Validation loss = 4.3447  \n",
      "\n",
      "Fold: 4  Epoch: 46  Training loss = 1.5830  Validation loss = 4.3439  \n",
      "\n",
      "Fold: 4  Epoch: 47  Training loss = 1.5829  Validation loss = 4.3437  \n",
      "\n",
      "Fold: 4  Epoch: 48  Training loss = 1.5828  Validation loss = 4.3425  \n",
      "\n",
      "Fold: 4  Epoch: 49  Training loss = 1.5828  Validation loss = 4.3422  \n",
      "\n",
      "Fold: 4  Epoch: 50  Training loss = 1.5827  Validation loss = 4.3412  \n",
      "\n",
      "Fold: 4  Epoch: 51  Training loss = 1.5827  Validation loss = 4.3417  \n",
      "\n",
      "Fold: 4  Epoch: 52  Training loss = 1.5826  Validation loss = 4.3417  \n",
      "\n",
      "Fold: 4  Epoch: 53  Training loss = 1.5826  Validation loss = 4.3415  \n",
      "\n",
      "Fold: 4  Epoch: 54  Training loss = 1.5825  Validation loss = 4.3415  \n",
      "\n",
      "Fold: 4  Epoch: 55  Training loss = 1.5825  Validation loss = 4.3412  \n",
      "\n",
      "Fold: 4  Epoch: 56  Training loss = 1.5824  Validation loss = 4.3403  \n",
      "\n",
      "Fold: 4  Epoch: 57  Training loss = 1.5824  Validation loss = 4.3407  \n",
      "\n",
      "Fold: 4  Epoch: 58  Training loss = 1.5823  Validation loss = 4.3408  \n",
      "\n",
      "Fold: 4  Epoch: 59  Training loss = 1.5823  Validation loss = 4.3420  \n",
      "\n",
      "Fold: 4  Epoch: 60  Training loss = 1.5823  Validation loss = 4.3415  \n",
      "\n",
      "Fold: 4  Epoch: 61  Training loss = 1.5822  Validation loss = 4.3419  \n",
      "\n",
      "Fold: 4  Epoch: 62  Training loss = 1.5822  Validation loss = 4.3416  \n",
      "\n",
      "Fold: 4  Epoch: 63  Training loss = 1.5821  Validation loss = 4.3415  \n",
      "\n",
      "Fold: 4  Epoch: 64  Training loss = 1.5821  Validation loss = 4.3418  \n",
      "\n",
      "Fold: 4  Epoch: 65  Training loss = 1.5820  Validation loss = 4.3420  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 56  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.8732  Validation loss = 4.6019  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.8731  Validation loss = 4.6015  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.8727  Validation loss = 4.5991  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.8723  Validation loss = 4.5972  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.8718  Validation loss = 4.5947  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 1.8712  Validation loss = 4.5914  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 1.8708  Validation loss = 4.5892  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.8704  Validation loss = 4.5869  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 1.8702  Validation loss = 4.5862  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.8698  Validation loss = 4.5839  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 1.8695  Validation loss = 4.5824  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 1.8690  Validation loss = 4.5799  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 1.8686  Validation loss = 4.5778  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.8684  Validation loss = 4.5767  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 1.8682  Validation loss = 4.5756  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 1.8680  Validation loss = 4.5745  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 1.8676  Validation loss = 4.5727  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 1.8673  Validation loss = 4.5713  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 1.8668  Validation loss = 4.5687  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 1.8662  Validation loss = 4.5653  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 1.8658  Validation loss = 4.5629  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 1.8654  Validation loss = 4.5606  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 1.8651  Validation loss = 4.5593  \n",
      "\n",
      "Fold: 5  Epoch: 24  Training loss = 1.8649  Validation loss = 4.5585  \n",
      "\n",
      "Fold: 5  Epoch: 25  Training loss = 1.8647  Validation loss = 4.5577  \n",
      "\n",
      "Fold: 5  Epoch: 26  Training loss = 1.8644  Validation loss = 4.5560  \n",
      "\n",
      "Fold: 5  Epoch: 27  Training loss = 1.8640  Validation loss = 4.5539  \n",
      "\n",
      "Fold: 5  Epoch: 28  Training loss = 1.8638  Validation loss = 4.5533  \n",
      "\n",
      "Fold: 5  Epoch: 29  Training loss = 1.8638  Validation loss = 4.5534  \n",
      "\n",
      "Fold: 5  Epoch: 30  Training loss = 1.8634  Validation loss = 4.5516  \n",
      "\n",
      "Fold: 5  Epoch: 31  Training loss = 1.8631  Validation loss = 4.5497  \n",
      "\n",
      "Fold: 5  Epoch: 32  Training loss = 1.8629  Validation loss = 4.5487  \n",
      "\n",
      "Fold: 5  Epoch: 33  Training loss = 1.8626  Validation loss = 4.5472  \n",
      "\n",
      "Fold: 5  Epoch: 34  Training loss = 1.8623  Validation loss = 4.5456  \n",
      "\n",
      "Fold: 5  Epoch: 35  Training loss = 1.8621  Validation loss = 4.5450  \n",
      "\n",
      "Fold: 5  Epoch: 36  Training loss = 1.8619  Validation loss = 4.5437  \n",
      "\n",
      "Fold: 5  Epoch: 37  Training loss = 1.8616  Validation loss = 4.5422  \n",
      "\n",
      "Fold: 5  Epoch: 38  Training loss = 1.8615  Validation loss = 4.5416  \n",
      "\n",
      "Fold: 5  Epoch: 39  Training loss = 1.8612  Validation loss = 4.5396  \n",
      "\n",
      "Fold: 5  Epoch: 40  Training loss = 1.8607  Validation loss = 4.5367  \n",
      "\n",
      "Fold: 5  Epoch: 41  Training loss = 1.8603  Validation loss = 4.5349  \n",
      "\n",
      "Fold: 5  Epoch: 42  Training loss = 1.8600  Validation loss = 4.5332  \n",
      "\n",
      "Fold: 5  Epoch: 43  Training loss = 1.8598  Validation loss = 4.5320  \n",
      "\n",
      "Fold: 5  Epoch: 44  Training loss = 1.8592  Validation loss = 4.5291  \n",
      "\n",
      "Fold: 5  Epoch: 45  Training loss = 1.8590  Validation loss = 4.5277  \n",
      "\n",
      "Fold: 5  Epoch: 46  Training loss = 1.8586  Validation loss = 4.5249  \n",
      "\n",
      "Fold: 5  Epoch: 47  Training loss = 1.8583  Validation loss = 4.5233  \n",
      "\n",
      "Fold: 5  Epoch: 48  Training loss = 1.8582  Validation loss = 4.5224  \n",
      "\n",
      "Fold: 5  Epoch: 49  Training loss = 1.8578  Validation loss = 4.5202  \n",
      "\n",
      "Fold: 5  Epoch: 50  Training loss = 1.8574  Validation loss = 4.5181  \n",
      "\n",
      "Fold: 5  Epoch: 51  Training loss = 1.8572  Validation loss = 4.5168  \n",
      "\n",
      "Fold: 5  Epoch: 52  Training loss = 1.8570  Validation loss = 4.5158  \n",
      "\n",
      "Fold: 5  Epoch: 53  Training loss = 1.8568  Validation loss = 4.5147  \n",
      "\n",
      "Fold: 5  Epoch: 54  Training loss = 1.8564  Validation loss = 4.5123  \n",
      "\n",
      "Fold: 5  Epoch: 55  Training loss = 1.8560  Validation loss = 4.5099  \n",
      "\n",
      "Fold: 5  Epoch: 56  Training loss = 1.8558  Validation loss = 4.5086  \n",
      "\n",
      "Fold: 5  Epoch: 57  Training loss = 1.8557  Validation loss = 4.5083  \n",
      "\n",
      "Fold: 5  Epoch: 58  Training loss = 1.8556  Validation loss = 4.5084  \n",
      "\n",
      "Fold: 5  Epoch: 59  Training loss = 1.8553  Validation loss = 4.5068  \n",
      "\n",
      "Fold: 5  Epoch: 60  Training loss = 1.8553  Validation loss = 4.5067  \n",
      "\n",
      "Fold: 5  Epoch: 61  Training loss = 1.8549  Validation loss = 4.5045  \n",
      "\n",
      "Fold: 5  Epoch: 62  Training loss = 1.8546  Validation loss = 4.5029  \n",
      "\n",
      "Fold: 5  Epoch: 63  Training loss = 1.8543  Validation loss = 4.5014  \n",
      "\n",
      "Fold: 5  Epoch: 64  Training loss = 1.8540  Validation loss = 4.4993  \n",
      "\n",
      "Fold: 5  Epoch: 65  Training loss = 1.8537  Validation loss = 4.4979  \n",
      "\n",
      "Fold: 5  Epoch: 66  Training loss = 1.8535  Validation loss = 4.4973  \n",
      "\n",
      "Fold: 5  Epoch: 67  Training loss = 1.8534  Validation loss = 4.4969  \n",
      "\n",
      "Fold: 5  Epoch: 68  Training loss = 1.8532  Validation loss = 4.4956  \n",
      "\n",
      "Fold: 5  Epoch: 69  Training loss = 1.8529  Validation loss = 4.4939  \n",
      "\n",
      "Fold: 5  Epoch: 70  Training loss = 1.8527  Validation loss = 4.4922  \n",
      "\n",
      "Fold: 5  Epoch: 71  Training loss = 1.8524  Validation loss = 4.4901  \n",
      "\n",
      "Fold: 5  Epoch: 72  Training loss = 1.8520  Validation loss = 4.4875  \n",
      "\n",
      "Fold: 5  Epoch: 73  Training loss = 1.8517  Validation loss = 4.4860  \n",
      "\n",
      "Fold: 5  Epoch: 74  Training loss = 1.8516  Validation loss = 4.4861  \n",
      "\n",
      "Fold: 5  Epoch: 75  Training loss = 1.8513  Validation loss = 4.4841  \n",
      "\n",
      "Fold: 5  Epoch: 76  Training loss = 1.8512  Validation loss = 4.4837  \n",
      "\n",
      "Fold: 5  Epoch: 77  Training loss = 1.8510  Validation loss = 4.4832  \n",
      "\n",
      "Fold: 5  Epoch: 78  Training loss = 1.8509  Validation loss = 4.4822  \n",
      "\n",
      "Fold: 5  Epoch: 79  Training loss = 1.8506  Validation loss = 4.4808  \n",
      "\n",
      "Fold: 5  Epoch: 80  Training loss = 1.8503  Validation loss = 4.4792  \n",
      "\n",
      "Fold: 5  Epoch: 81  Training loss = 1.8498  Validation loss = 4.4757  \n",
      "\n",
      "Fold: 5  Epoch: 82  Training loss = 1.8493  Validation loss = 4.4734  \n",
      "\n",
      "Fold: 5  Epoch: 83  Training loss = 1.8492  Validation loss = 4.4729  \n",
      "\n",
      "Fold: 5  Epoch: 84  Training loss = 1.8490  Validation loss = 4.4710  \n",
      "\n",
      "Fold: 5  Epoch: 85  Training loss = 1.8487  Validation loss = 4.4695  \n",
      "\n",
      "Fold: 5  Epoch: 86  Training loss = 1.8483  Validation loss = 4.4674  \n",
      "\n",
      "Fold: 5  Epoch: 87  Training loss = 1.8483  Validation loss = 4.4676  \n",
      "\n",
      "Fold: 5  Epoch: 88  Training loss = 1.8479  Validation loss = 4.4654  \n",
      "\n",
      "Fold: 5  Epoch: 89  Training loss = 1.8478  Validation loss = 4.4649  \n",
      "\n",
      "Fold: 5  Epoch: 90  Training loss = 1.8476  Validation loss = 4.4636  \n",
      "\n",
      "Fold: 5  Epoch: 91  Training loss = 1.8474  Validation loss = 4.4631  \n",
      "\n",
      "Fold: 5  Epoch: 92  Training loss = 1.8471  Validation loss = 4.4610  \n",
      "\n",
      "Fold: 5  Epoch: 93  Training loss = 1.8469  Validation loss = 4.4600  \n",
      "\n",
      "Fold: 5  Epoch: 94  Training loss = 1.8466  Validation loss = 4.4583  \n",
      "\n",
      "Fold: 5  Epoch: 95  Training loss = 1.8461  Validation loss = 4.4556  \n",
      "\n",
      "Fold: 5  Epoch: 96  Training loss = 1.8458  Validation loss = 4.4533  \n",
      "\n",
      "Fold: 5  Epoch: 97  Training loss = 1.8456  Validation loss = 4.4522  \n",
      "\n",
      "Fold: 5  Epoch: 98  Training loss = 1.8454  Validation loss = 4.4513  \n",
      "\n",
      "Fold: 5  Epoch: 99  Training loss = 1.8453  Validation loss = 4.4509  \n",
      "\n",
      "Fold: 5  Epoch: 100  Training loss = 1.8452  Validation loss = 4.4513  \n",
      "\n",
      "Fold: 5  Epoch: 101  Training loss = 1.8450  Validation loss = 4.4497  \n",
      "\n",
      "Fold: 5  Epoch: 102  Training loss = 1.8449  Validation loss = 4.4491  \n",
      "\n",
      "Fold: 5  Epoch: 103  Training loss = 1.8446  Validation loss = 4.4476  \n",
      "\n",
      "Fold: 5  Epoch: 104  Training loss = 1.8441  Validation loss = 4.4447  \n",
      "\n",
      "Fold: 5  Epoch: 105  Training loss = 1.8440  Validation loss = 4.4437  \n",
      "\n",
      "Fold: 5  Epoch: 106  Training loss = 1.8437  Validation loss = 4.4419  \n",
      "\n",
      "Fold: 5  Epoch: 107  Training loss = 1.8435  Validation loss = 4.4414  \n",
      "\n",
      "Fold: 5  Epoch: 108  Training loss = 1.8434  Validation loss = 4.4409  \n",
      "\n",
      "Fold: 5  Epoch: 109  Training loss = 1.8432  Validation loss = 4.4399  \n",
      "\n",
      "Fold: 5  Epoch: 110  Training loss = 1.8429  Validation loss = 4.4383  \n",
      "\n",
      "Fold: 5  Epoch: 111  Training loss = 1.8425  Validation loss = 4.4361  \n",
      "\n",
      "Fold: 5  Epoch: 112  Training loss = 1.8424  Validation loss = 4.4357  \n",
      "\n",
      "Fold: 5  Epoch: 113  Training loss = 1.8421  Validation loss = 4.4339  \n",
      "\n",
      "Fold: 5  Epoch: 114  Training loss = 1.8419  Validation loss = 4.4325  \n",
      "\n",
      "Fold: 5  Epoch: 115  Training loss = 1.8417  Validation loss = 4.4313  \n",
      "\n",
      "Fold: 5  Epoch: 116  Training loss = 1.8414  Validation loss = 4.4293  \n",
      "\n",
      "Fold: 5  Epoch: 117  Training loss = 1.8413  Validation loss = 4.4289  \n",
      "\n",
      "Fold: 5  Epoch: 118  Training loss = 1.8409  Validation loss = 4.4260  \n",
      "\n",
      "Fold: 5  Epoch: 119  Training loss = 1.8407  Validation loss = 4.4252  \n",
      "\n",
      "Fold: 5  Epoch: 120  Training loss = 1.8404  Validation loss = 4.4230  \n",
      "\n",
      "Fold: 5  Epoch: 121  Training loss = 1.8400  Validation loss = 4.4200  \n",
      "\n",
      "Fold: 5  Epoch: 122  Training loss = 1.8397  Validation loss = 4.4183  \n",
      "\n",
      "Fold: 5  Epoch: 123  Training loss = 1.8395  Validation loss = 4.4174  \n",
      "\n",
      "Fold: 5  Epoch: 124  Training loss = 1.8394  Validation loss = 4.4169  \n",
      "\n",
      "Fold: 5  Epoch: 125  Training loss = 1.8394  Validation loss = 4.4177  \n",
      "\n",
      "Fold: 5  Epoch: 126  Training loss = 1.8391  Validation loss = 4.4164  \n",
      "\n",
      "Fold: 5  Epoch: 127  Training loss = 1.8391  Validation loss = 4.4160  \n",
      "\n",
      "Fold: 5  Epoch: 128  Training loss = 1.8388  Validation loss = 4.4145  \n",
      "\n",
      "Fold: 5  Epoch: 129  Training loss = 1.8385  Validation loss = 4.4124  \n",
      "\n",
      "Fold: 5  Epoch: 130  Training loss = 1.8381  Validation loss = 4.4101  \n",
      "\n",
      "Fold: 5  Epoch: 131  Training loss = 1.8380  Validation loss = 4.4103  \n",
      "\n",
      "Fold: 5  Epoch: 132  Training loss = 1.8377  Validation loss = 4.4084  \n",
      "\n",
      "Fold: 5  Epoch: 133  Training loss = 1.8374  Validation loss = 4.4069  \n",
      "\n",
      "Fold: 5  Epoch: 134  Training loss = 1.8371  Validation loss = 4.4045  \n",
      "\n",
      "Fold: 5  Epoch: 135  Training loss = 1.8368  Validation loss = 4.4026  \n",
      "\n",
      "Fold: 5  Epoch: 136  Training loss = 1.8366  Validation loss = 4.4011  \n",
      "\n",
      "Fold: 5  Epoch: 137  Training loss = 1.8363  Validation loss = 4.3998  \n",
      "\n",
      "Fold: 5  Epoch: 138  Training loss = 1.8361  Validation loss = 4.3984  \n",
      "\n",
      "Fold: 5  Epoch: 139  Training loss = 1.8358  Validation loss = 4.3960  \n",
      "\n",
      "Fold: 5  Epoch: 140  Training loss = 1.8355  Validation loss = 4.3943  \n",
      "\n",
      "Fold: 5  Epoch: 141  Training loss = 1.8352  Validation loss = 4.3927  \n",
      "\n",
      "Fold: 5  Epoch: 142  Training loss = 1.8348  Validation loss = 4.3901  \n",
      "\n",
      "Fold: 5  Epoch: 143  Training loss = 1.8346  Validation loss = 4.3886  \n",
      "\n",
      "Fold: 5  Epoch: 144  Training loss = 1.8343  Validation loss = 4.3869  \n",
      "\n",
      "Fold: 5  Epoch: 145  Training loss = 1.8339  Validation loss = 4.3846  \n",
      "\n",
      "Fold: 5  Epoch: 146  Training loss = 1.8335  Validation loss = 4.3818  \n",
      "\n",
      "Fold: 5  Epoch: 147  Training loss = 1.8333  Validation loss = 4.3800  \n",
      "\n",
      "Fold: 5  Epoch: 148  Training loss = 1.8330  Validation loss = 4.3779  \n",
      "\n",
      "Fold: 5  Epoch: 149  Training loss = 1.8327  Validation loss = 4.3764  \n",
      "\n",
      "Fold: 5  Epoch: 150  Training loss = 1.8322  Validation loss = 4.3722  \n",
      "\n",
      "Fold: 5  Epoch: 151  Training loss = 1.8320  Validation loss = 4.3710  \n",
      "\n",
      "Fold: 5  Epoch: 152  Training loss = 1.8318  Validation loss = 4.3701  \n",
      "\n",
      "Fold: 5  Epoch: 153  Training loss = 1.8315  Validation loss = 4.3674  \n",
      "\n",
      "Fold: 5  Epoch: 154  Training loss = 1.8313  Validation loss = 4.3668  \n",
      "\n",
      "Fold: 5  Epoch: 155  Training loss = 1.8312  Validation loss = 4.3670  \n",
      "\n",
      "Fold: 5  Epoch: 156  Training loss = 1.8309  Validation loss = 4.3652  \n",
      "\n",
      "Fold: 5  Epoch: 157  Training loss = 1.8308  Validation loss = 4.3646  \n",
      "\n",
      "Fold: 5  Epoch: 158  Training loss = 1.8305  Validation loss = 4.3624  \n",
      "\n",
      "Fold: 5  Epoch: 159  Training loss = 1.8301  Validation loss = 4.3601  \n",
      "\n",
      "Fold: 5  Epoch: 160  Training loss = 1.8301  Validation loss = 4.3601  \n",
      "\n",
      "Fold: 5  Epoch: 161  Training loss = 1.8299  Validation loss = 4.3593  \n",
      "\n",
      "Fold: 5  Epoch: 162  Training loss = 1.8295  Validation loss = 4.3570  \n",
      "\n",
      "Fold: 5  Epoch: 163  Training loss = 1.8291  Validation loss = 4.3533  \n",
      "\n",
      "Fold: 5  Epoch: 164  Training loss = 1.8289  Validation loss = 4.3522  \n",
      "\n",
      "Fold: 5  Epoch: 165  Training loss = 1.8286  Validation loss = 4.3505  \n",
      "\n",
      "Fold: 5  Epoch: 166  Training loss = 1.8285  Validation loss = 4.3504  \n",
      "\n",
      "Fold: 5  Epoch: 167  Training loss = 1.8283  Validation loss = 4.3491  \n",
      "\n",
      "Fold: 5  Epoch: 168  Training loss = 1.8281  Validation loss = 4.3484  \n",
      "\n",
      "Fold: 5  Epoch: 169  Training loss = 1.8280  Validation loss = 4.3481  \n",
      "\n",
      "Fold: 5  Epoch: 170  Training loss = 1.8278  Validation loss = 4.3472  \n",
      "\n",
      "Fold: 5  Epoch: 171  Training loss = 1.8278  Validation loss = 4.3480  \n",
      "\n",
      "Fold: 5  Epoch: 172  Training loss = 1.8278  Validation loss = 4.3491  \n",
      "\n",
      "Fold: 5  Epoch: 173  Training loss = 1.8276  Validation loss = 4.3476  \n",
      "\n",
      "Fold: 5  Epoch: 174  Training loss = 1.8273  Validation loss = 4.3462  \n",
      "\n",
      "Fold: 5  Epoch: 175  Training loss = 1.8271  Validation loss = 4.3452  \n",
      "\n",
      "Fold: 5  Epoch: 176  Training loss = 1.8268  Validation loss = 4.3432  \n",
      "\n",
      "Fold: 5  Epoch: 177  Training loss = 1.8266  Validation loss = 4.3418  \n",
      "\n",
      "Fold: 5  Epoch: 178  Training loss = 1.8266  Validation loss = 4.3425  \n",
      "\n",
      "Fold: 5  Epoch: 179  Training loss = 1.8264  Validation loss = 4.3412  \n",
      "\n",
      "Fold: 5  Epoch: 180  Training loss = 1.8262  Validation loss = 4.3404  \n",
      "\n",
      "Fold: 5  Epoch: 181  Training loss = 1.8261  Validation loss = 4.3407  \n",
      "\n",
      "Fold: 5  Epoch: 182  Training loss = 1.8258  Validation loss = 4.3382  \n",
      "\n",
      "Fold: 5  Epoch: 183  Training loss = 1.8256  Validation loss = 4.3371  \n",
      "\n",
      "Fold: 5  Epoch: 184  Training loss = 1.8254  Validation loss = 4.3356  \n",
      "\n",
      "Fold: 5  Epoch: 185  Training loss = 1.8252  Validation loss = 4.3341  \n",
      "\n",
      "Fold: 5  Epoch: 186  Training loss = 1.8249  Validation loss = 4.3329  \n",
      "\n",
      "Fold: 5  Epoch: 187  Training loss = 1.8247  Validation loss = 4.3311  \n",
      "\n",
      "Fold: 5  Epoch: 188  Training loss = 1.8246  Validation loss = 4.3311  \n",
      "\n",
      "Fold: 5  Epoch: 189  Training loss = 1.8245  Validation loss = 4.3318  \n",
      "\n",
      "Fold: 5  Epoch: 190  Training loss = 1.8243  Validation loss = 4.3301  \n",
      "\n",
      "Fold: 5  Epoch: 191  Training loss = 1.8241  Validation loss = 4.3287  \n",
      "\n",
      "Fold: 5  Epoch: 192  Training loss = 1.8239  Validation loss = 4.3270  \n",
      "\n",
      "Fold: 5  Epoch: 193  Training loss = 1.8236  Validation loss = 4.3253  \n",
      "\n",
      "Fold: 5  Epoch: 194  Training loss = 1.8235  Validation loss = 4.3245  \n",
      "\n",
      "Fold: 5  Epoch: 195  Training loss = 1.8233  Validation loss = 4.3239  \n",
      "\n",
      "Fold: 5  Epoch: 196  Training loss = 1.8231  Validation loss = 4.3227  \n",
      "\n",
      "Fold: 5  Epoch: 197  Training loss = 1.8227  Validation loss = 4.3193  \n",
      "\n",
      "Fold: 5  Epoch: 198  Training loss = 1.8226  Validation loss = 4.3195  \n",
      "\n",
      "Fold: 5  Epoch: 199  Training loss = 1.8225  Validation loss = 4.3188  \n",
      "\n",
      "Fold: 5  Epoch: 200  Training loss = 1.8222  Validation loss = 4.3169  \n",
      "\n",
      "Fold: 5  Epoch: 201  Training loss = 1.8221  Validation loss = 4.3163  \n",
      "\n",
      "Fold: 5  Epoch: 202  Training loss = 1.8219  Validation loss = 4.3147  \n",
      "\n",
      "Fold: 5  Epoch: 203  Training loss = 1.8218  Validation loss = 4.3157  \n",
      "\n",
      "Fold: 5  Epoch: 204  Training loss = 1.8217  Validation loss = 4.3157  \n",
      "\n",
      "Fold: 5  Epoch: 205  Training loss = 1.8215  Validation loss = 4.3145  \n",
      "\n",
      "Fold: 5  Epoch: 206  Training loss = 1.8213  Validation loss = 4.3129  \n",
      "\n",
      "Fold: 5  Epoch: 207  Training loss = 1.8212  Validation loss = 4.3132  \n",
      "\n",
      "Fold: 5  Epoch: 208  Training loss = 1.8209  Validation loss = 4.3109  \n",
      "\n",
      "Fold: 5  Epoch: 209  Training loss = 1.8207  Validation loss = 4.3094  \n",
      "\n",
      "Fold: 5  Epoch: 210  Training loss = 1.8206  Validation loss = 4.3087  \n",
      "\n",
      "Fold: 5  Epoch: 211  Training loss = 1.8203  Validation loss = 4.3073  \n",
      "\n",
      "Fold: 5  Epoch: 212  Training loss = 1.8200  Validation loss = 4.3051  \n",
      "\n",
      "Fold: 5  Epoch: 213  Training loss = 1.8200  Validation loss = 4.3050  \n",
      "\n",
      "Fold: 5  Epoch: 214  Training loss = 1.8197  Validation loss = 4.3028  \n",
      "\n",
      "Fold: 5  Epoch: 215  Training loss = 1.8196  Validation loss = 4.3032  \n",
      "\n",
      "Fold: 5  Epoch: 216  Training loss = 1.8194  Validation loss = 4.3014  \n",
      "\n",
      "Fold: 5  Epoch: 217  Training loss = 1.8192  Validation loss = 4.3001  \n",
      "\n",
      "Fold: 5  Epoch: 218  Training loss = 1.8190  Validation loss = 4.2988  \n",
      "\n",
      "Fold: 5  Epoch: 219  Training loss = 1.8188  Validation loss = 4.2980  \n",
      "\n",
      "Fold: 5  Epoch: 220  Training loss = 1.8186  Validation loss = 4.2972  \n",
      "\n",
      "Fold: 5  Epoch: 221  Training loss = 1.8185  Validation loss = 4.2963  \n",
      "\n",
      "Fold: 5  Epoch: 222  Training loss = 1.8182  Validation loss = 4.2944  \n",
      "\n",
      "Fold: 5  Epoch: 223  Training loss = 1.8180  Validation loss = 4.2929  \n",
      "\n",
      "Fold: 5  Epoch: 224  Training loss = 1.8178  Validation loss = 4.2912  \n",
      "\n",
      "Fold: 5  Epoch: 225  Training loss = 1.8176  Validation loss = 4.2898  \n",
      "\n",
      "Fold: 5  Epoch: 226  Training loss = 1.8175  Validation loss = 4.2900  \n",
      "\n",
      "Fold: 5  Epoch: 227  Training loss = 1.8173  Validation loss = 4.2883  \n",
      "\n",
      "Fold: 5  Epoch: 228  Training loss = 1.8171  Validation loss = 4.2872  \n",
      "\n",
      "Fold: 5  Epoch: 229  Training loss = 1.8169  Validation loss = 4.2868  \n",
      "\n",
      "Fold: 5  Epoch: 230  Training loss = 1.8168  Validation loss = 4.2866  \n",
      "\n",
      "Fold: 5  Epoch: 231  Training loss = 1.8165  Validation loss = 4.2842  \n",
      "\n",
      "Fold: 5  Epoch: 232  Training loss = 1.8163  Validation loss = 4.2830  \n",
      "\n",
      "Fold: 5  Epoch: 233  Training loss = 1.8162  Validation loss = 4.2826  \n",
      "\n",
      "Fold: 5  Epoch: 234  Training loss = 1.8160  Validation loss = 4.2821  \n",
      "\n",
      "Fold: 5  Epoch: 235  Training loss = 1.8159  Validation loss = 4.2820  \n",
      "\n",
      "Fold: 5  Epoch: 236  Training loss = 1.8159  Validation loss = 4.2821  \n",
      "\n",
      "Fold: 5  Epoch: 237  Training loss = 1.8156  Validation loss = 4.2807  \n",
      "\n",
      "Fold: 5  Epoch: 238  Training loss = 1.8153  Validation loss = 4.2782  \n",
      "\n",
      "Fold: 5  Epoch: 239  Training loss = 1.8151  Validation loss = 4.2758  \n",
      "\n",
      "Fold: 5  Epoch: 240  Training loss = 1.8150  Validation loss = 4.2759  \n",
      "\n",
      "Fold: 5  Epoch: 241  Training loss = 1.8147  Validation loss = 4.2738  \n",
      "\n",
      "Fold: 5  Epoch: 242  Training loss = 1.8145  Validation loss = 4.2735  \n",
      "\n",
      "Fold: 5  Epoch: 243  Training loss = 1.8143  Validation loss = 4.2717  \n",
      "\n",
      "Fold: 5  Epoch: 244  Training loss = 1.8142  Validation loss = 4.2713  \n",
      "\n",
      "Fold: 5  Epoch: 245  Training loss = 1.8140  Validation loss = 4.2699  \n",
      "\n",
      "Fold: 5  Epoch: 246  Training loss = 1.8139  Validation loss = 4.2696  \n",
      "\n",
      "Fold: 5  Epoch: 247  Training loss = 1.8138  Validation loss = 4.2692  \n",
      "\n",
      "Fold: 5  Epoch: 248  Training loss = 1.8135  Validation loss = 4.2670  \n",
      "\n",
      "Fold: 5  Epoch: 249  Training loss = 1.8133  Validation loss = 4.2659  \n",
      "\n",
      "Fold: 5  Epoch: 250  Training loss = 1.8131  Validation loss = 4.2654  \n",
      "\n",
      "Fold: 5  Epoch: 251  Training loss = 1.8130  Validation loss = 4.2650  \n",
      "\n",
      "Fold: 5  Epoch: 252  Training loss = 1.8128  Validation loss = 4.2638  \n",
      "\n",
      "Fold: 5  Epoch: 253  Training loss = 1.8126  Validation loss = 4.2618  \n",
      "\n",
      "Fold: 5  Epoch: 254  Training loss = 1.8124  Validation loss = 4.2615  \n",
      "\n",
      "Fold: 5  Epoch: 255  Training loss = 1.8123  Validation loss = 4.2615  \n",
      "\n",
      "Fold: 5  Epoch: 256  Training loss = 1.8122  Validation loss = 4.2604  \n",
      "\n",
      "Fold: 5  Epoch: 257  Training loss = 1.8119  Validation loss = 4.2585  \n",
      "\n",
      "Fold: 5  Epoch: 258  Training loss = 1.8118  Validation loss = 4.2574  \n",
      "\n",
      "Fold: 5  Epoch: 259  Training loss = 1.8116  Validation loss = 4.2561  \n",
      "\n",
      "Fold: 5  Epoch: 260  Training loss = 1.8115  Validation loss = 4.2558  \n",
      "\n",
      "Fold: 5  Epoch: 261  Training loss = 1.8114  Validation loss = 4.2555  \n",
      "\n",
      "Fold: 5  Epoch: 262  Training loss = 1.8113  Validation loss = 4.2554  \n",
      "\n",
      "Fold: 5  Epoch: 263  Training loss = 1.8111  Validation loss = 4.2542  \n",
      "\n",
      "Fold: 5  Epoch: 264  Training loss = 1.8108  Validation loss = 4.2517  \n",
      "\n",
      "Fold: 5  Epoch: 265  Training loss = 1.8106  Validation loss = 4.2504  \n",
      "\n",
      "Fold: 5  Epoch: 266  Training loss = 1.8104  Validation loss = 4.2481  \n",
      "\n",
      "Fold: 5  Epoch: 267  Training loss = 1.8102  Validation loss = 4.2466  \n",
      "\n",
      "Fold: 5  Epoch: 268  Training loss = 1.8100  Validation loss = 4.2452  \n",
      "\n",
      "Fold: 5  Epoch: 269  Training loss = 1.8099  Validation loss = 4.2450  \n",
      "\n",
      "Fold: 5  Epoch: 270  Training loss = 1.8096  Validation loss = 4.2428  \n",
      "\n",
      "Fold: 5  Epoch: 271  Training loss = 1.8094  Validation loss = 4.2414  \n",
      "\n",
      "Fold: 5  Epoch: 272  Training loss = 1.8094  Validation loss = 4.2417  \n",
      "\n",
      "Fold: 5  Epoch: 273  Training loss = 1.8092  Validation loss = 4.2412  \n",
      "\n",
      "Fold: 5  Epoch: 274  Training loss = 1.8091  Validation loss = 4.2406  \n",
      "\n",
      "Fold: 5  Epoch: 275  Training loss = 1.8089  Validation loss = 4.2399  \n",
      "\n",
      "Fold: 5  Epoch: 276  Training loss = 1.8088  Validation loss = 4.2392  \n",
      "\n",
      "Fold: 5  Epoch: 277  Training loss = 1.8086  Validation loss = 4.2388  \n",
      "\n",
      "Fold: 5  Epoch: 278  Training loss = 1.8085  Validation loss = 4.2390  \n",
      "\n",
      "Fold: 5  Epoch: 279  Training loss = 1.8083  Validation loss = 4.2372  \n",
      "\n",
      "Fold: 5  Epoch: 280  Training loss = 1.8081  Validation loss = 4.2363  \n",
      "\n",
      "Fold: 5  Epoch: 281  Training loss = 1.8079  Validation loss = 4.2343  \n",
      "\n",
      "Fold: 5  Epoch: 282  Training loss = 1.8077  Validation loss = 4.2332  \n",
      "\n",
      "Fold: 5  Epoch: 283  Training loss = 1.8076  Validation loss = 4.2328  \n",
      "\n",
      "Fold: 5  Epoch: 284  Training loss = 1.8074  Validation loss = 4.2318  \n",
      "\n",
      "Fold: 5  Epoch: 285  Training loss = 1.8072  Validation loss = 4.2311  \n",
      "\n",
      "Fold: 5  Epoch: 286  Training loss = 1.8070  Validation loss = 4.2289  \n",
      "\n",
      "Fold: 5  Epoch: 287  Training loss = 1.8068  Validation loss = 4.2267  \n",
      "\n",
      "Fold: 5  Epoch: 288  Training loss = 1.8065  Validation loss = 4.2250  \n",
      "\n",
      "Fold: 5  Epoch: 289  Training loss = 1.8064  Validation loss = 4.2252  \n",
      "\n",
      "Fold: 5  Epoch: 290  Training loss = 1.8063  Validation loss = 4.2249  \n",
      "\n",
      "Fold: 5  Epoch: 291  Training loss = 1.8060  Validation loss = 4.2231  \n",
      "\n",
      "Fold: 5  Epoch: 292  Training loss = 1.8059  Validation loss = 4.2222  \n",
      "\n",
      "Fold: 5  Epoch: 293  Training loss = 1.8056  Validation loss = 4.2206  \n",
      "\n",
      "Fold: 5  Epoch: 294  Training loss = 1.8054  Validation loss = 4.2187  \n",
      "\n",
      "Fold: 5  Epoch: 295  Training loss = 1.8053  Validation loss = 4.2183  \n",
      "\n",
      "Fold: 5  Epoch: 296  Training loss = 1.8050  Validation loss = 4.2158  \n",
      "\n",
      "Fold: 5  Epoch: 297  Training loss = 1.8049  Validation loss = 4.2161  \n",
      "\n",
      "Fold: 5  Epoch: 298  Training loss = 1.8047  Validation loss = 4.2148  \n",
      "\n",
      "Fold: 5  Epoch: 299  Training loss = 1.8046  Validation loss = 4.2135  \n",
      "\n",
      "Fold: 5  Epoch: 300  Training loss = 1.8044  Validation loss = 4.2123  \n",
      "\n",
      "Fold: 5  Epoch: 301  Training loss = 1.8043  Validation loss = 4.2121  \n",
      "\n",
      "Fold: 5  Epoch: 302  Training loss = 1.8043  Validation loss = 4.2128  \n",
      "\n",
      "Fold: 5  Epoch: 303  Training loss = 1.8041  Validation loss = 4.2123  \n",
      "\n",
      "Fold: 5  Epoch: 304  Training loss = 1.8040  Validation loss = 4.2123  \n",
      "\n",
      "Fold: 5  Epoch: 305  Training loss = 1.8039  Validation loss = 4.2118  \n",
      "\n",
      "Fold: 5  Epoch: 306  Training loss = 1.8037  Validation loss = 4.2114  \n",
      "\n",
      "Fold: 5  Epoch: 307  Training loss = 1.8036  Validation loss = 4.2104  \n",
      "\n",
      "Fold: 5  Epoch: 308  Training loss = 1.8034  Validation loss = 4.2095  \n",
      "\n",
      "Fold: 5  Epoch: 309  Training loss = 1.8032  Validation loss = 4.2087  \n",
      "\n",
      "Fold: 5  Epoch: 310  Training loss = 1.8031  Validation loss = 4.2086  \n",
      "\n",
      "Fold: 5  Epoch: 311  Training loss = 1.8030  Validation loss = 4.2076  \n",
      "\n",
      "Fold: 5  Epoch: 312  Training loss = 1.8029  Validation loss = 4.2081  \n",
      "\n",
      "Fold: 5  Epoch: 313  Training loss = 1.8028  Validation loss = 4.2077  \n",
      "\n",
      "Fold: 5  Epoch: 314  Training loss = 1.8026  Validation loss = 4.2059  \n",
      "\n",
      "Fold: 5  Epoch: 315  Training loss = 1.8024  Validation loss = 4.2050  \n",
      "\n",
      "Fold: 5  Epoch: 316  Training loss = 1.8023  Validation loss = 4.2042  \n",
      "\n",
      "Fold: 5  Epoch: 317  Training loss = 1.8021  Validation loss = 4.2035  \n",
      "\n",
      "Fold: 5  Epoch: 318  Training loss = 1.8019  Validation loss = 4.2023  \n",
      "\n",
      "Fold: 5  Epoch: 319  Training loss = 1.8019  Validation loss = 4.2024  \n",
      "\n",
      "Fold: 5  Epoch: 320  Training loss = 1.8017  Validation loss = 4.2015  \n",
      "\n",
      "Fold: 5  Epoch: 321  Training loss = 1.8016  Validation loss = 4.2013  \n",
      "\n",
      "Fold: 5  Epoch: 322  Training loss = 1.8014  Validation loss = 4.2001  \n",
      "\n",
      "Fold: 5  Epoch: 323  Training loss = 1.8012  Validation loss = 4.1991  \n",
      "\n",
      "Fold: 5  Epoch: 324  Training loss = 1.8012  Validation loss = 4.2000  \n",
      "\n",
      "Fold: 5  Epoch: 325  Training loss = 1.8010  Validation loss = 4.1987  \n",
      "\n",
      "Fold: 5  Epoch: 326  Training loss = 1.8008  Validation loss = 4.1972  \n",
      "\n",
      "Fold: 5  Epoch: 327  Training loss = 1.8006  Validation loss = 4.1960  \n",
      "\n",
      "Fold: 5  Epoch: 328  Training loss = 1.8005  Validation loss = 4.1958  \n",
      "\n",
      "Fold: 5  Epoch: 329  Training loss = 1.8004  Validation loss = 4.1956  \n",
      "\n",
      "Fold: 5  Epoch: 330  Training loss = 1.8003  Validation loss = 4.1955  \n",
      "\n",
      "Fold: 5  Epoch: 331  Training loss = 1.8001  Validation loss = 4.1941  \n",
      "\n",
      "Fold: 5  Epoch: 332  Training loss = 1.7999  Validation loss = 4.1932  \n",
      "\n",
      "Fold: 5  Epoch: 333  Training loss = 1.7998  Validation loss = 4.1922  \n",
      "\n",
      "Fold: 5  Epoch: 334  Training loss = 1.7996  Validation loss = 4.1916  \n",
      "\n",
      "Fold: 5  Epoch: 335  Training loss = 1.7995  Validation loss = 4.1905  \n",
      "\n",
      "Fold: 5  Epoch: 336  Training loss = 1.7994  Validation loss = 4.1898  \n",
      "\n",
      "Fold: 5  Epoch: 337  Training loss = 1.7993  Validation loss = 4.1897  \n",
      "\n",
      "Fold: 5  Epoch: 338  Training loss = 1.7991  Validation loss = 4.1892  \n",
      "\n",
      "Fold: 5  Epoch: 339  Training loss = 1.7990  Validation loss = 4.1887  \n",
      "\n",
      "Fold: 5  Epoch: 340  Training loss = 1.7988  Validation loss = 4.1880  \n",
      "\n",
      "Fold: 5  Epoch: 341  Training loss = 1.7988  Validation loss = 4.1879  \n",
      "\n",
      "Fold: 5  Epoch: 342  Training loss = 1.7986  Validation loss = 4.1878  \n",
      "\n",
      "Fold: 5  Epoch: 343  Training loss = 1.7985  Validation loss = 4.1874  \n",
      "\n",
      "Fold: 5  Epoch: 344  Training loss = 1.7983  Validation loss = 4.1858  \n",
      "\n",
      "Fold: 5  Epoch: 345  Training loss = 1.7981  Validation loss = 4.1838  \n",
      "\n",
      "Fold: 5  Epoch: 346  Training loss = 1.7979  Validation loss = 4.1828  \n",
      "\n",
      "Fold: 5  Epoch: 347  Training loss = 1.7977  Validation loss = 4.1815  \n",
      "\n",
      "Fold: 5  Epoch: 348  Training loss = 1.7976  Validation loss = 4.1810  \n",
      "\n",
      "Fold: 5  Epoch: 349  Training loss = 1.7974  Validation loss = 4.1790  \n",
      "\n",
      "Fold: 5  Epoch: 350  Training loss = 1.7973  Validation loss = 4.1796  \n",
      "\n",
      "Fold: 5  Epoch: 351  Training loss = 1.7971  Validation loss = 4.1783  \n",
      "\n",
      "Fold: 5  Epoch: 352  Training loss = 1.7969  Validation loss = 4.1776  \n",
      "\n",
      "Fold: 5  Epoch: 353  Training loss = 1.7969  Validation loss = 4.1776  \n",
      "\n",
      "Fold: 5  Epoch: 354  Training loss = 1.7967  Validation loss = 4.1760  \n",
      "\n",
      "Fold: 5  Epoch: 355  Training loss = 1.7965  Validation loss = 4.1751  \n",
      "\n",
      "Fold: 5  Epoch: 356  Training loss = 1.7963  Validation loss = 4.1735  \n",
      "\n",
      "Fold: 5  Epoch: 357  Training loss = 1.7962  Validation loss = 4.1731  \n",
      "\n",
      "Fold: 5  Epoch: 358  Training loss = 1.7961  Validation loss = 4.1719  \n",
      "\n",
      "Fold: 5  Epoch: 359  Training loss = 1.7960  Validation loss = 4.1729  \n",
      "\n",
      "Fold: 5  Epoch: 360  Training loss = 1.7960  Validation loss = 4.1741  \n",
      "\n",
      "Fold: 5  Epoch: 361  Training loss = 1.7959  Validation loss = 4.1742  \n",
      "\n",
      "Fold: 5  Epoch: 362  Training loss = 1.7958  Validation loss = 4.1737  \n",
      "\n",
      "Fold: 5  Epoch: 363  Training loss = 1.7956  Validation loss = 4.1723  \n",
      "\n",
      "Fold: 5  Epoch: 364  Training loss = 1.7955  Validation loss = 4.1730  \n",
      "\n",
      "Fold: 5  Epoch: 365  Training loss = 1.7952  Validation loss = 4.1713  \n",
      "\n",
      "Fold: 5  Epoch: 366  Training loss = 1.7951  Validation loss = 4.1704  \n",
      "\n",
      "Fold: 5  Epoch: 367  Training loss = 1.7949  Validation loss = 4.1694  \n",
      "\n",
      "Fold: 5  Epoch: 368  Training loss = 1.7948  Validation loss = 4.1690  \n",
      "\n",
      "Fold: 5  Epoch: 369  Training loss = 1.7948  Validation loss = 4.1692  \n",
      "\n",
      "Fold: 5  Epoch: 370  Training loss = 1.7946  Validation loss = 4.1681  \n",
      "\n",
      "Fold: 5  Epoch: 371  Training loss = 1.7943  Validation loss = 4.1661  \n",
      "\n",
      "Fold: 5  Epoch: 372  Training loss = 1.7941  Validation loss = 4.1642  \n",
      "\n",
      "Fold: 5  Epoch: 373  Training loss = 1.7940  Validation loss = 4.1642  \n",
      "\n",
      "Fold: 5  Epoch: 374  Training loss = 1.7939  Validation loss = 4.1636  \n",
      "\n",
      "Fold: 5  Epoch: 375  Training loss = 1.7939  Validation loss = 4.1652  \n",
      "\n",
      "Fold: 5  Epoch: 376  Training loss = 1.7939  Validation loss = 4.1658  \n",
      "\n",
      "Fold: 5  Epoch: 377  Training loss = 1.7938  Validation loss = 4.1664  \n",
      "\n",
      "Fold: 5  Epoch: 378  Training loss = 1.7936  Validation loss = 4.1646  \n",
      "\n",
      "Fold: 5  Epoch: 379  Training loss = 1.7934  Validation loss = 4.1635  \n",
      "\n",
      "Fold: 5  Epoch: 380  Training loss = 1.7933  Validation loss = 4.1624  \n",
      "\n",
      "Fold: 5  Epoch: 381  Training loss = 1.7931  Validation loss = 4.1605  \n",
      "\n",
      "Fold: 5  Epoch: 382  Training loss = 1.7928  Validation loss = 4.1582  \n",
      "\n",
      "Fold: 5  Epoch: 383  Training loss = 1.7927  Validation loss = 4.1587  \n",
      "\n",
      "Fold: 5  Epoch: 384  Training loss = 1.7926  Validation loss = 4.1579  \n",
      "\n",
      "Fold: 5  Epoch: 385  Training loss = 1.7924  Validation loss = 4.1562  \n",
      "\n",
      "Fold: 5  Epoch: 386  Training loss = 1.7923  Validation loss = 4.1568  \n",
      "\n",
      "Fold: 5  Epoch: 387  Training loss = 1.7921  Validation loss = 4.1553  \n",
      "\n",
      "Fold: 5  Epoch: 388  Training loss = 1.7919  Validation loss = 4.1546  \n",
      "\n",
      "Fold: 5  Epoch: 389  Training loss = 1.7919  Validation loss = 4.1553  \n",
      "\n",
      "Fold: 5  Epoch: 390  Training loss = 1.7917  Validation loss = 4.1531  \n",
      "\n",
      "Fold: 5  Epoch: 391  Training loss = 1.7916  Validation loss = 4.1532  \n",
      "\n",
      "Fold: 5  Epoch: 392  Training loss = 1.7916  Validation loss = 4.1540  \n",
      "\n",
      "Fold: 5  Epoch: 393  Training loss = 1.7914  Validation loss = 4.1535  \n",
      "\n",
      "Fold: 5  Epoch: 394  Training loss = 1.7913  Validation loss = 4.1528  \n",
      "\n",
      "Fold: 5  Epoch: 395  Training loss = 1.7911  Validation loss = 4.1513  \n",
      "\n",
      "Fold: 5  Epoch: 396  Training loss = 1.7910  Validation loss = 4.1513  \n",
      "\n",
      "Fold: 5  Epoch: 397  Training loss = 1.7909  Validation loss = 4.1512  \n",
      "\n",
      "Fold: 5  Epoch: 398  Training loss = 1.7907  Validation loss = 4.1496  \n",
      "\n",
      "Fold: 5  Epoch: 399  Training loss = 1.7906  Validation loss = 4.1487  \n",
      "\n",
      "Fold: 5  Epoch: 400  Training loss = 1.7905  Validation loss = 4.1486  \n",
      "\n",
      "Fold: 5  Epoch: 401  Training loss = 1.7904  Validation loss = 4.1493  \n",
      "\n",
      "Fold: 5  Epoch: 402  Training loss = 1.7902  Validation loss = 4.1478  \n",
      "\n",
      "Fold: 5  Epoch: 403  Training loss = 1.7900  Validation loss = 4.1468  \n",
      "\n",
      "Fold: 5  Epoch: 404  Training loss = 1.7899  Validation loss = 4.1467  \n",
      "\n",
      "Fold: 5  Epoch: 405  Training loss = 1.7897  Validation loss = 4.1464  \n",
      "\n",
      "Fold: 5  Epoch: 406  Training loss = 1.7895  Validation loss = 4.1455  \n",
      "\n",
      "Fold: 5  Epoch: 407  Training loss = 1.7894  Validation loss = 4.1460  \n",
      "\n",
      "Fold: 5  Epoch: 408  Training loss = 1.7893  Validation loss = 4.1456  \n",
      "\n",
      "Fold: 5  Epoch: 409  Training loss = 1.7893  Validation loss = 4.1456  \n",
      "\n",
      "Fold: 5  Epoch: 410  Training loss = 1.7891  Validation loss = 4.1446  \n",
      "\n",
      "Fold: 5  Epoch: 411  Training loss = 1.7889  Validation loss = 4.1437  \n",
      "\n",
      "Fold: 5  Epoch: 412  Training loss = 1.7888  Validation loss = 4.1428  \n",
      "\n",
      "Fold: 5  Epoch: 413  Training loss = 1.7886  Validation loss = 4.1415  \n",
      "\n",
      "Fold: 5  Epoch: 414  Training loss = 1.7886  Validation loss = 4.1423  \n",
      "\n",
      "Fold: 5  Epoch: 415  Training loss = 1.7885  Validation loss = 4.1422  \n",
      "\n",
      "Fold: 5  Epoch: 416  Training loss = 1.7884  Validation loss = 4.1431  \n",
      "\n",
      "Fold: 5  Epoch: 417  Training loss = 1.7883  Validation loss = 4.1423  \n",
      "\n",
      "Fold: 5  Epoch: 418  Training loss = 1.7881  Validation loss = 4.1414  \n",
      "\n",
      "Fold: 5  Epoch: 419  Training loss = 1.7881  Validation loss = 4.1423  \n",
      "\n",
      "Fold: 5  Epoch: 420  Training loss = 1.7879  Validation loss = 4.1403  \n",
      "\n",
      "Fold: 5  Epoch: 421  Training loss = 1.7876  Validation loss = 4.1390  \n",
      "\n",
      "Fold: 5  Epoch: 422  Training loss = 1.7874  Validation loss = 4.1376  \n",
      "\n",
      "Fold: 5  Epoch: 423  Training loss = 1.7873  Validation loss = 4.1374  \n",
      "\n",
      "Fold: 5  Epoch: 424  Training loss = 1.7873  Validation loss = 4.1376  \n",
      "\n",
      "Fold: 5  Epoch: 425  Training loss = 1.7871  Validation loss = 4.1366  \n",
      "\n",
      "Fold: 5  Epoch: 426  Training loss = 1.7870  Validation loss = 4.1365  \n",
      "\n",
      "Fold: 5  Epoch: 427  Training loss = 1.7870  Validation loss = 4.1367  \n",
      "\n",
      "Fold: 5  Epoch: 428  Training loss = 1.7869  Validation loss = 4.1370  \n",
      "\n",
      "Fold: 5  Epoch: 429  Training loss = 1.7867  Validation loss = 4.1359  \n",
      "\n",
      "Fold: 5  Epoch: 430  Training loss = 1.7866  Validation loss = 4.1353  \n",
      "\n",
      "Fold: 5  Epoch: 431  Training loss = 1.7864  Validation loss = 4.1347  \n",
      "\n",
      "Fold: 5  Epoch: 432  Training loss = 1.7863  Validation loss = 4.1333  \n",
      "\n",
      "Fold: 5  Epoch: 433  Training loss = 1.7861  Validation loss = 4.1326  \n",
      "\n",
      "Fold: 5  Epoch: 434  Training loss = 1.7859  Validation loss = 4.1315  \n",
      "\n",
      "Fold: 5  Epoch: 435  Training loss = 1.7857  Validation loss = 4.1300  \n",
      "\n",
      "Fold: 5  Epoch: 436  Training loss = 1.7856  Validation loss = 4.1297  \n",
      "\n",
      "Fold: 5  Epoch: 437  Training loss = 1.7855  Validation loss = 4.1297  \n",
      "\n",
      "Fold: 5  Epoch: 438  Training loss = 1.7854  Validation loss = 4.1308  \n",
      "\n",
      "Fold: 5  Epoch: 439  Training loss = 1.7854  Validation loss = 4.1321  \n",
      "\n",
      "Fold: 5  Epoch: 440  Training loss = 1.7852  Validation loss = 4.1304  \n",
      "\n",
      "Fold: 5  Epoch: 441  Training loss = 1.7851  Validation loss = 4.1300  \n",
      "\n",
      "Fold: 5  Epoch: 442  Training loss = 1.7850  Validation loss = 4.1301  \n",
      "\n",
      "Fold: 5  Epoch: 443  Training loss = 1.7847  Validation loss = 4.1280  \n",
      "\n",
      "Fold: 5  Epoch: 444  Training loss = 1.7846  Validation loss = 4.1279  \n",
      "\n",
      "Fold: 5  Epoch: 445  Training loss = 1.7846  Validation loss = 4.1285  \n",
      "\n",
      "Fold: 5  Epoch: 446  Training loss = 1.7844  Validation loss = 4.1278  \n",
      "\n",
      "Fold: 5  Epoch: 447  Training loss = 1.7843  Validation loss = 4.1265  \n",
      "\n",
      "Fold: 5  Epoch: 448  Training loss = 1.7841  Validation loss = 4.1243  \n",
      "\n",
      "Fold: 5  Epoch: 449  Training loss = 1.7840  Validation loss = 4.1246  \n",
      "\n",
      "Fold: 5  Epoch: 450  Training loss = 1.7838  Validation loss = 4.1240  \n",
      "\n",
      "Fold: 5  Epoch: 451  Training loss = 1.7836  Validation loss = 4.1223  \n",
      "\n",
      "Fold: 5  Epoch: 452  Training loss = 1.7835  Validation loss = 4.1224  \n",
      "\n",
      "Fold: 5  Epoch: 453  Training loss = 1.7833  Validation loss = 4.1204  \n",
      "\n",
      "Fold: 5  Epoch: 454  Training loss = 1.7832  Validation loss = 4.1205  \n",
      "\n",
      "Fold: 5  Epoch: 455  Training loss = 1.7830  Validation loss = 4.1188  \n",
      "\n",
      "Fold: 5  Epoch: 456  Training loss = 1.7829  Validation loss = 4.1176  \n",
      "\n",
      "Fold: 5  Epoch: 457  Training loss = 1.7828  Validation loss = 4.1172  \n",
      "\n",
      "Fold: 5  Epoch: 458  Training loss = 1.7826  Validation loss = 4.1166  \n",
      "\n",
      "Fold: 5  Epoch: 459  Training loss = 1.7825  Validation loss = 4.1167  \n",
      "\n",
      "Fold: 5  Epoch: 460  Training loss = 1.7824  Validation loss = 4.1171  \n",
      "\n",
      "Fold: 5  Epoch: 461  Training loss = 1.7823  Validation loss = 4.1167  \n",
      "\n",
      "Fold: 5  Epoch: 462  Training loss = 1.7822  Validation loss = 4.1165  \n",
      "\n",
      "Fold: 5  Epoch: 463  Training loss = 1.7822  Validation loss = 4.1171  \n",
      "\n",
      "Fold: 5  Epoch: 464  Training loss = 1.7819  Validation loss = 4.1137  \n",
      "\n",
      "Fold: 5  Epoch: 465  Training loss = 1.7818  Validation loss = 4.1134  \n",
      "\n",
      "Fold: 5  Epoch: 466  Training loss = 1.7817  Validation loss = 4.1135  \n",
      "\n",
      "Fold: 5  Epoch: 467  Training loss = 1.7817  Validation loss = 4.1146  \n",
      "\n",
      "Fold: 5  Epoch: 468  Training loss = 1.7815  Validation loss = 4.1142  \n",
      "\n",
      "Fold: 5  Epoch: 469  Training loss = 1.7814  Validation loss = 4.1133  \n",
      "\n",
      "Fold: 5  Epoch: 470  Training loss = 1.7813  Validation loss = 4.1129  \n",
      "\n",
      "Fold: 5  Epoch: 471  Training loss = 1.7812  Validation loss = 4.1125  \n",
      "\n",
      "Fold: 5  Epoch: 472  Training loss = 1.7810  Validation loss = 4.1122  \n",
      "\n",
      "Fold: 5  Epoch: 473  Training loss = 1.7810  Validation loss = 4.1130  \n",
      "\n",
      "Fold: 5  Epoch: 474  Training loss = 1.7809  Validation loss = 4.1125  \n",
      "\n",
      "Fold: 5  Epoch: 475  Training loss = 1.7807  Validation loss = 4.1118  \n",
      "\n",
      "Fold: 5  Epoch: 476  Training loss = 1.7805  Validation loss = 4.1098  \n",
      "\n",
      "Fold: 5  Epoch: 477  Training loss = 1.7804  Validation loss = 4.1098  \n",
      "\n",
      "Fold: 5  Epoch: 478  Training loss = 1.7803  Validation loss = 4.1091  \n",
      "\n",
      "Fold: 5  Epoch: 479  Training loss = 1.7801  Validation loss = 4.1080  \n",
      "\n",
      "Fold: 5  Epoch: 480  Training loss = 1.7801  Validation loss = 4.1093  \n",
      "\n",
      "Fold: 5  Epoch: 481  Training loss = 1.7799  Validation loss = 4.1085  \n",
      "\n",
      "Fold: 5  Epoch: 482  Training loss = 1.7799  Validation loss = 4.1091  \n",
      "\n",
      "Fold: 5  Epoch: 483  Training loss = 1.7797  Validation loss = 4.1084  \n",
      "\n",
      "Fold: 5  Epoch: 484  Training loss = 1.7796  Validation loss = 4.1087  \n",
      "\n",
      "Fold: 5  Epoch: 485  Training loss = 1.7795  Validation loss = 4.1082  \n",
      "\n",
      "Fold: 5  Epoch: 486  Training loss = 1.7793  Validation loss = 4.1075  \n",
      "\n",
      "Fold: 5  Epoch: 487  Training loss = 1.7792  Validation loss = 4.1078  \n",
      "\n",
      "Fold: 5  Epoch: 488  Training loss = 1.7791  Validation loss = 4.1075  \n",
      "\n",
      "Fold: 5  Epoch: 489  Training loss = 1.7791  Validation loss = 4.1085  \n",
      "\n",
      "Fold: 5  Epoch: 490  Training loss = 1.7789  Validation loss = 4.1075  \n",
      "\n",
      "Fold: 5  Epoch: 491  Training loss = 1.7789  Validation loss = 4.1080  \n",
      "\n",
      "Fold: 5  Epoch: 492  Training loss = 1.7787  Validation loss = 4.1068  \n",
      "\n",
      "Fold: 5  Epoch: 493  Training loss = 1.7786  Validation loss = 4.1062  \n",
      "\n",
      "Fold: 5  Epoch: 494  Training loss = 1.7784  Validation loss = 4.1043  \n",
      "\n",
      "Fold: 5  Epoch: 495  Training loss = 1.7782  Validation loss = 4.1032  \n",
      "\n",
      "Fold: 5  Epoch: 496  Training loss = 1.7781  Validation loss = 4.1024  \n",
      "\n",
      "Fold: 5  Epoch: 497  Training loss = 1.7779  Validation loss = 4.1012  \n",
      "\n",
      "Fold: 5  Epoch: 498  Training loss = 1.7778  Validation loss = 4.1000  \n",
      "\n",
      "Fold: 5  Epoch: 499  Training loss = 1.7777  Validation loss = 4.1001  \n",
      "\n",
      "Fold: 5  Epoch: 500  Training loss = 1.7777  Validation loss = 4.1008  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 498  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 2.0143  Validation loss = 1.7389  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 2.0138  Validation loss = 1.7374  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 2.0129  Validation loss = 1.7337  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 2.0123  Validation loss = 1.7314  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 2.0119  Validation loss = 1.7306  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 2.0113  Validation loss = 1.7282  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 2.0107  Validation loss = 1.7256  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 2.0102  Validation loss = 1.7239  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 2.0095  Validation loss = 1.7209  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 2.0092  Validation loss = 1.7201  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 2.0087  Validation loss = 1.7178  \n",
      "\n",
      "Fold: 6  Epoch: 12  Training loss = 2.0077  Validation loss = 1.7141  \n",
      "\n",
      "Fold: 6  Epoch: 13  Training loss = 2.0070  Validation loss = 1.7115  \n",
      "\n",
      "Fold: 6  Epoch: 14  Training loss = 2.0066  Validation loss = 1.7099  \n",
      "\n",
      "Fold: 6  Epoch: 15  Training loss = 2.0058  Validation loss = 1.7068  \n",
      "\n",
      "Fold: 6  Epoch: 16  Training loss = 2.0046  Validation loss = 1.7019  \n",
      "\n",
      "Fold: 6  Epoch: 17  Training loss = 2.0039  Validation loss = 1.6987  \n",
      "\n",
      "Fold: 6  Epoch: 18  Training loss = 2.0033  Validation loss = 1.6964  \n",
      "\n",
      "Fold: 6  Epoch: 19  Training loss = 2.0027  Validation loss = 1.6941  \n",
      "\n",
      "Fold: 6  Epoch: 20  Training loss = 2.0018  Validation loss = 1.6903  \n",
      "\n",
      "Fold: 6  Epoch: 21  Training loss = 2.0013  Validation loss = 1.6885  \n",
      "\n",
      "Fold: 6  Epoch: 22  Training loss = 2.0007  Validation loss = 1.6860  \n",
      "\n",
      "Fold: 6  Epoch: 23  Training loss = 2.0001  Validation loss = 1.6837  \n",
      "\n",
      "Fold: 6  Epoch: 24  Training loss = 1.9993  Validation loss = 1.6802  \n",
      "\n",
      "Fold: 6  Epoch: 25  Training loss = 1.9988  Validation loss = 1.6785  \n",
      "\n",
      "Fold: 6  Epoch: 26  Training loss = 1.9984  Validation loss = 1.6766  \n",
      "\n",
      "Fold: 6  Epoch: 27  Training loss = 1.9978  Validation loss = 1.6744  \n",
      "\n",
      "Fold: 6  Epoch: 28  Training loss = 1.9969  Validation loss = 1.6706  \n",
      "\n",
      "Fold: 6  Epoch: 29  Training loss = 1.9967  Validation loss = 1.6701  \n",
      "\n",
      "Fold: 6  Epoch: 30  Training loss = 1.9962  Validation loss = 1.6681  \n",
      "\n",
      "Fold: 6  Epoch: 31  Training loss = 1.9956  Validation loss = 1.6659  \n",
      "\n",
      "Fold: 6  Epoch: 32  Training loss = 1.9954  Validation loss = 1.6650  \n",
      "\n",
      "Fold: 6  Epoch: 33  Training loss = 1.9947  Validation loss = 1.6623  \n",
      "\n",
      "Fold: 6  Epoch: 34  Training loss = 1.9939  Validation loss = 1.6588  \n",
      "\n",
      "Fold: 6  Epoch: 35  Training loss = 1.9931  Validation loss = 1.6557  \n",
      "\n",
      "Fold: 6  Epoch: 36  Training loss = 1.9923  Validation loss = 1.6521  \n",
      "\n",
      "Fold: 6  Epoch: 37  Training loss = 1.9913  Validation loss = 1.6478  \n",
      "\n",
      "Fold: 6  Epoch: 38  Training loss = 1.9908  Validation loss = 1.6456  \n",
      "\n",
      "Fold: 6  Epoch: 39  Training loss = 1.9905  Validation loss = 1.6444  \n",
      "\n",
      "Fold: 6  Epoch: 40  Training loss = 1.9899  Validation loss = 1.6416  \n",
      "\n",
      "Fold: 6  Epoch: 41  Training loss = 1.9892  Validation loss = 1.6389  \n",
      "\n",
      "Fold: 6  Epoch: 42  Training loss = 1.9891  Validation loss = 1.6391  \n",
      "\n",
      "Fold: 6  Epoch: 43  Training loss = 1.9884  Validation loss = 1.6361  \n",
      "\n",
      "Fold: 6  Epoch: 44  Training loss = 1.9880  Validation loss = 1.6343  \n",
      "\n",
      "Fold: 6  Epoch: 45  Training loss = 1.9876  Validation loss = 1.6331  \n",
      "\n",
      "Fold: 6  Epoch: 46  Training loss = 1.9873  Validation loss = 1.6321  \n",
      "\n",
      "Fold: 6  Epoch: 47  Training loss = 1.9868  Validation loss = 1.6297  \n",
      "\n",
      "Fold: 6  Epoch: 48  Training loss = 1.9861  Validation loss = 1.6266  \n",
      "\n",
      "Fold: 6  Epoch: 49  Training loss = 1.9857  Validation loss = 1.6252  \n",
      "\n",
      "Fold: 6  Epoch: 50  Training loss = 1.9850  Validation loss = 1.6217  \n",
      "\n",
      "Fold: 6  Epoch: 51  Training loss = 1.9847  Validation loss = 1.6208  \n",
      "\n",
      "Fold: 6  Epoch: 52  Training loss = 1.9840  Validation loss = 1.6177  \n",
      "\n",
      "Fold: 6  Epoch: 53  Training loss = 1.9834  Validation loss = 1.6153  \n",
      "\n",
      "Fold: 6  Epoch: 54  Training loss = 1.9829  Validation loss = 1.6130  \n",
      "\n",
      "Fold: 6  Epoch: 55  Training loss = 1.9824  Validation loss = 1.6106  \n",
      "\n",
      "Fold: 6  Epoch: 56  Training loss = 1.9820  Validation loss = 1.6090  \n",
      "\n",
      "Fold: 6  Epoch: 57  Training loss = 1.9812  Validation loss = 1.6051  \n",
      "\n",
      "Fold: 6  Epoch: 58  Training loss = 1.9807  Validation loss = 1.6034  \n",
      "\n",
      "Fold: 6  Epoch: 59  Training loss = 1.9800  Validation loss = 1.6002  \n",
      "\n",
      "Fold: 6  Epoch: 60  Training loss = 1.9798  Validation loss = 1.5994  \n",
      "\n",
      "Fold: 6  Epoch: 61  Training loss = 1.9790  Validation loss = 1.5960  \n",
      "\n",
      "Fold: 6  Epoch: 62  Training loss = 1.9786  Validation loss = 1.5944  \n",
      "\n",
      "Fold: 6  Epoch: 63  Training loss = 1.9784  Validation loss = 1.5933  \n",
      "\n",
      "Fold: 6  Epoch: 64  Training loss = 1.9780  Validation loss = 1.5921  \n",
      "\n",
      "Fold: 6  Epoch: 65  Training loss = 1.9776  Validation loss = 1.5901  \n",
      "\n",
      "Fold: 6  Epoch: 66  Training loss = 1.9769  Validation loss = 1.5872  \n",
      "\n",
      "Fold: 6  Epoch: 67  Training loss = 1.9764  Validation loss = 1.5854  \n",
      "\n",
      "Fold: 6  Epoch: 68  Training loss = 1.9760  Validation loss = 1.5832  \n",
      "\n",
      "Fold: 6  Epoch: 69  Training loss = 1.9759  Validation loss = 1.5834  \n",
      "\n",
      "Fold: 6  Epoch: 70  Training loss = 1.9756  Validation loss = 1.5821  \n",
      "\n",
      "Fold: 6  Epoch: 71  Training loss = 1.9750  Validation loss = 1.5795  \n",
      "\n",
      "Fold: 6  Epoch: 72  Training loss = 1.9745  Validation loss = 1.5777  \n",
      "\n",
      "Fold: 6  Epoch: 73  Training loss = 1.9736  Validation loss = 1.5733  \n",
      "\n",
      "Fold: 6  Epoch: 74  Training loss = 1.9732  Validation loss = 1.5718  \n",
      "\n",
      "Fold: 6  Epoch: 75  Training loss = 1.9728  Validation loss = 1.5703  \n",
      "\n",
      "Fold: 6  Epoch: 76  Training loss = 1.9723  Validation loss = 1.5676  \n",
      "\n",
      "Fold: 6  Epoch: 77  Training loss = 1.9718  Validation loss = 1.5655  \n",
      "\n",
      "Fold: 6  Epoch: 78  Training loss = 1.9715  Validation loss = 1.5642  \n",
      "\n",
      "Fold: 6  Epoch: 79  Training loss = 1.9710  Validation loss = 1.5618  \n",
      "\n",
      "Fold: 6  Epoch: 80  Training loss = 1.9705  Validation loss = 1.5594  \n",
      "\n",
      "Fold: 6  Epoch: 81  Training loss = 1.9700  Validation loss = 1.5574  \n",
      "\n",
      "Fold: 6  Epoch: 82  Training loss = 1.9694  Validation loss = 1.5545  \n",
      "\n",
      "Fold: 6  Epoch: 83  Training loss = 1.9690  Validation loss = 1.5529  \n",
      "\n",
      "Fold: 6  Epoch: 84  Training loss = 1.9689  Validation loss = 1.5532  \n",
      "\n",
      "Fold: 6  Epoch: 85  Training loss = 1.9681  Validation loss = 1.5492  \n",
      "\n",
      "Fold: 6  Epoch: 86  Training loss = 1.9677  Validation loss = 1.5476  \n",
      "\n",
      "Fold: 6  Epoch: 87  Training loss = 1.9671  Validation loss = 1.5447  \n",
      "\n",
      "Fold: 6  Epoch: 88  Training loss = 1.9664  Validation loss = 1.5416  \n",
      "\n",
      "Fold: 6  Epoch: 89  Training loss = 1.9659  Validation loss = 1.5397  \n",
      "\n",
      "Fold: 6  Epoch: 90  Training loss = 1.9657  Validation loss = 1.5389  \n",
      "\n",
      "Fold: 6  Epoch: 91  Training loss = 1.9651  Validation loss = 1.5363  \n",
      "\n",
      "Fold: 6  Epoch: 92  Training loss = 1.9647  Validation loss = 1.5346  \n",
      "\n",
      "Fold: 6  Epoch: 93  Training loss = 1.9646  Validation loss = 1.5344  \n",
      "\n",
      "Fold: 6  Epoch: 94  Training loss = 1.9641  Validation loss = 1.5321  \n",
      "\n",
      "Fold: 6  Epoch: 95  Training loss = 1.9634  Validation loss = 1.5289  \n",
      "\n",
      "Fold: 6  Epoch: 96  Training loss = 1.9629  Validation loss = 1.5266  \n",
      "\n",
      "Fold: 6  Epoch: 97  Training loss = 1.9625  Validation loss = 1.5244  \n",
      "\n",
      "Fold: 6  Epoch: 98  Training loss = 1.9617  Validation loss = 1.5205  \n",
      "\n",
      "Fold: 6  Epoch: 99  Training loss = 1.9611  Validation loss = 1.5177  \n",
      "\n",
      "Fold: 6  Epoch: 100  Training loss = 1.9606  Validation loss = 1.5158  \n",
      "\n",
      "Fold: 6  Epoch: 101  Training loss = 1.9603  Validation loss = 1.5143  \n",
      "\n",
      "Fold: 6  Epoch: 102  Training loss = 1.9598  Validation loss = 1.5124  \n",
      "\n",
      "Fold: 6  Epoch: 103  Training loss = 1.9596  Validation loss = 1.5115  \n",
      "\n",
      "Fold: 6  Epoch: 104  Training loss = 1.9591  Validation loss = 1.5090  \n",
      "\n",
      "Fold: 6  Epoch: 105  Training loss = 1.9587  Validation loss = 1.5077  \n",
      "\n",
      "Fold: 6  Epoch: 106  Training loss = 1.9586  Validation loss = 1.5073  \n",
      "\n",
      "Fold: 6  Epoch: 107  Training loss = 1.9583  Validation loss = 1.5066  \n",
      "\n",
      "Fold: 6  Epoch: 108  Training loss = 1.9581  Validation loss = 1.5061  \n",
      "\n",
      "Fold: 6  Epoch: 109  Training loss = 1.9580  Validation loss = 1.5058  \n",
      "\n",
      "Fold: 6  Epoch: 110  Training loss = 1.9576  Validation loss = 1.5040  \n",
      "\n",
      "Fold: 6  Epoch: 111  Training loss = 1.9572  Validation loss = 1.5020  \n",
      "\n",
      "Fold: 6  Epoch: 112  Training loss = 1.9566  Validation loss = 1.4995  \n",
      "\n",
      "Fold: 6  Epoch: 113  Training loss = 1.9560  Validation loss = 1.4964  \n",
      "\n",
      "Fold: 6  Epoch: 114  Training loss = 1.9558  Validation loss = 1.4956  \n",
      "\n",
      "Fold: 6  Epoch: 115  Training loss = 1.9553  Validation loss = 1.4936  \n",
      "\n",
      "Fold: 6  Epoch: 116  Training loss = 1.9551  Validation loss = 1.4928  \n",
      "\n",
      "Fold: 6  Epoch: 117  Training loss = 1.9543  Validation loss = 1.4882  \n",
      "\n",
      "Fold: 6  Epoch: 118  Training loss = 1.9541  Validation loss = 1.4878  \n",
      "\n",
      "Fold: 6  Epoch: 119  Training loss = 1.9539  Validation loss = 1.4870  \n",
      "\n",
      "Fold: 6  Epoch: 120  Training loss = 1.9535  Validation loss = 1.4852  \n",
      "\n",
      "Fold: 6  Epoch: 121  Training loss = 1.9532  Validation loss = 1.4842  \n",
      "\n",
      "Fold: 6  Epoch: 122  Training loss = 1.9527  Validation loss = 1.4819  \n",
      "\n",
      "Fold: 6  Epoch: 123  Training loss = 1.9524  Validation loss = 1.4807  \n",
      "\n",
      "Fold: 6  Epoch: 124  Training loss = 1.9519  Validation loss = 1.4782  \n",
      "\n",
      "Fold: 6  Epoch: 125  Training loss = 1.9516  Validation loss = 1.4775  \n",
      "\n",
      "Fold: 6  Epoch: 126  Training loss = 1.9512  Validation loss = 1.4758  \n",
      "\n",
      "Fold: 6  Epoch: 127  Training loss = 1.9507  Validation loss = 1.4737  \n",
      "\n",
      "Fold: 6  Epoch: 128  Training loss = 1.9505  Validation loss = 1.4727  \n",
      "\n",
      "Fold: 6  Epoch: 129  Training loss = 1.9498  Validation loss = 1.4691  \n",
      "\n",
      "Fold: 6  Epoch: 130  Training loss = 1.9494  Validation loss = 1.4667  \n",
      "\n",
      "Fold: 6  Epoch: 131  Training loss = 1.9491  Validation loss = 1.4661  \n",
      "\n",
      "Fold: 6  Epoch: 132  Training loss = 1.9484  Validation loss = 1.4625  \n",
      "\n",
      "Fold: 6  Epoch: 133  Training loss = 1.9482  Validation loss = 1.4618  \n",
      "\n",
      "Fold: 6  Epoch: 134  Training loss = 1.9478  Validation loss = 1.4596  \n",
      "\n",
      "Fold: 6  Epoch: 135  Training loss = 1.9474  Validation loss = 1.4581  \n",
      "\n",
      "Fold: 6  Epoch: 136  Training loss = 1.9468  Validation loss = 1.4551  \n",
      "\n",
      "Fold: 6  Epoch: 137  Training loss = 1.9467  Validation loss = 1.4547  \n",
      "\n",
      "Fold: 6  Epoch: 138  Training loss = 1.9467  Validation loss = 1.4557  \n",
      "\n",
      "Fold: 6  Epoch: 139  Training loss = 1.9463  Validation loss = 1.4542  \n",
      "\n",
      "Fold: 6  Epoch: 140  Training loss = 1.9456  Validation loss = 1.4501  \n",
      "\n",
      "Fold: 6  Epoch: 141  Training loss = 1.9453  Validation loss = 1.4486  \n",
      "\n",
      "Fold: 6  Epoch: 142  Training loss = 1.9448  Validation loss = 1.4462  \n",
      "\n",
      "Fold: 6  Epoch: 143  Training loss = 1.9445  Validation loss = 1.4454  \n",
      "\n",
      "Fold: 6  Epoch: 144  Training loss = 1.9442  Validation loss = 1.4438  \n",
      "\n",
      "Fold: 6  Epoch: 145  Training loss = 1.9439  Validation loss = 1.4428  \n",
      "\n",
      "Fold: 6  Epoch: 146  Training loss = 1.9435  Validation loss = 1.4406  \n",
      "\n",
      "Fold: 6  Epoch: 147  Training loss = 1.9432  Validation loss = 1.4395  \n",
      "\n",
      "Fold: 6  Epoch: 148  Training loss = 1.9427  Validation loss = 1.4372  \n",
      "\n",
      "Fold: 6  Epoch: 149  Training loss = 1.9423  Validation loss = 1.4355  \n",
      "\n",
      "Fold: 6  Epoch: 150  Training loss = 1.9422  Validation loss = 1.4355  \n",
      "\n",
      "Fold: 6  Epoch: 151  Training loss = 1.9415  Validation loss = 1.4319  \n",
      "\n",
      "Fold: 6  Epoch: 152  Training loss = 1.9413  Validation loss = 1.4315  \n",
      "\n",
      "Fold: 6  Epoch: 153  Training loss = 1.9409  Validation loss = 1.4297  \n",
      "\n",
      "Fold: 6  Epoch: 154  Training loss = 1.9402  Validation loss = 1.4248  \n",
      "\n",
      "Fold: 6  Epoch: 155  Training loss = 1.9399  Validation loss = 1.4238  \n",
      "\n",
      "Fold: 6  Epoch: 156  Training loss = 1.9395  Validation loss = 1.4219  \n",
      "\n",
      "Fold: 6  Epoch: 157  Training loss = 1.9390  Validation loss = 1.4187  \n",
      "\n",
      "Fold: 6  Epoch: 158  Training loss = 1.9388  Validation loss = 1.4182  \n",
      "\n",
      "Fold: 6  Epoch: 159  Training loss = 1.9385  Validation loss = 1.4172  \n",
      "\n",
      "Fold: 6  Epoch: 160  Training loss = 1.9384  Validation loss = 1.4168  \n",
      "\n",
      "Fold: 6  Epoch: 161  Training loss = 1.9382  Validation loss = 1.4158  \n",
      "\n",
      "Fold: 6  Epoch: 162  Training loss = 1.9378  Validation loss = 1.4146  \n",
      "\n",
      "Fold: 6  Epoch: 163  Training loss = 1.9378  Validation loss = 1.4152  \n",
      "\n",
      "Fold: 6  Epoch: 164  Training loss = 1.9373  Validation loss = 1.4128  \n",
      "\n",
      "Fold: 6  Epoch: 165  Training loss = 1.9370  Validation loss = 1.4113  \n",
      "\n",
      "Fold: 6  Epoch: 166  Training loss = 1.9365  Validation loss = 1.4088  \n",
      "\n",
      "Fold: 6  Epoch: 167  Training loss = 1.9361  Validation loss = 1.4066  \n",
      "\n",
      "Fold: 6  Epoch: 168  Training loss = 1.9356  Validation loss = 1.4040  \n",
      "\n",
      "Fold: 6  Epoch: 169  Training loss = 1.9352  Validation loss = 1.4016  \n",
      "\n",
      "Fold: 6  Epoch: 170  Training loss = 1.9350  Validation loss = 1.4014  \n",
      "\n",
      "Fold: 6  Epoch: 171  Training loss = 1.9345  Validation loss = 1.3984  \n",
      "\n",
      "Fold: 6  Epoch: 172  Training loss = 1.9343  Validation loss = 1.3975  \n",
      "\n",
      "Fold: 6  Epoch: 173  Training loss = 1.9339  Validation loss = 1.3959  \n",
      "\n",
      "Fold: 6  Epoch: 174  Training loss = 1.9338  Validation loss = 1.3957  \n",
      "\n",
      "Fold: 6  Epoch: 175  Training loss = 1.9336  Validation loss = 1.3956  \n",
      "\n",
      "Fold: 6  Epoch: 176  Training loss = 1.9334  Validation loss = 1.3949  \n",
      "\n",
      "Fold: 6  Epoch: 177  Training loss = 1.9332  Validation loss = 1.3946  \n",
      "\n",
      "Fold: 6  Epoch: 178  Training loss = 1.9330  Validation loss = 1.3942  \n",
      "\n",
      "Fold: 6  Epoch: 179  Training loss = 1.9325  Validation loss = 1.3911  \n",
      "\n",
      "Fold: 6  Epoch: 180  Training loss = 1.9323  Validation loss = 1.3909  \n",
      "\n",
      "Fold: 6  Epoch: 181  Training loss = 1.9321  Validation loss = 1.3899  \n",
      "\n",
      "Fold: 6  Epoch: 182  Training loss = 1.9320  Validation loss = 1.3898  \n",
      "\n",
      "Fold: 6  Epoch: 183  Training loss = 1.9320  Validation loss = 1.3908  \n",
      "\n",
      "Fold: 6  Epoch: 184  Training loss = 1.9318  Validation loss = 1.3904  \n",
      "\n",
      "Fold: 6  Epoch: 185  Training loss = 1.9315  Validation loss = 1.3888  \n",
      "\n",
      "Fold: 6  Epoch: 186  Training loss = 1.9311  Validation loss = 1.3872  \n",
      "\n",
      "Fold: 6  Epoch: 187  Training loss = 1.9310  Validation loss = 1.3872  \n",
      "\n",
      "Fold: 6  Epoch: 188  Training loss = 1.9306  Validation loss = 1.3848  \n",
      "\n",
      "Fold: 6  Epoch: 189  Training loss = 1.9301  Validation loss = 1.3819  \n",
      "\n",
      "Fold: 6  Epoch: 190  Training loss = 1.9296  Validation loss = 1.3792  \n",
      "\n",
      "Fold: 6  Epoch: 191  Training loss = 1.9295  Validation loss = 1.3790  \n",
      "\n",
      "Fold: 6  Epoch: 192  Training loss = 1.9291  Validation loss = 1.3772  \n",
      "\n",
      "Fold: 6  Epoch: 193  Training loss = 1.9288  Validation loss = 1.3753  \n",
      "\n",
      "Fold: 6  Epoch: 194  Training loss = 1.9285  Validation loss = 1.3749  \n",
      "\n",
      "Fold: 6  Epoch: 195  Training loss = 1.9283  Validation loss = 1.3740  \n",
      "\n",
      "Fold: 6  Epoch: 196  Training loss = 1.9280  Validation loss = 1.3727  \n",
      "\n",
      "Fold: 6  Epoch: 197  Training loss = 1.9276  Validation loss = 1.3703  \n",
      "\n",
      "Fold: 6  Epoch: 198  Training loss = 1.9274  Validation loss = 1.3694  \n",
      "\n",
      "Fold: 6  Epoch: 199  Training loss = 1.9271  Validation loss = 1.3686  \n",
      "\n",
      "Fold: 6  Epoch: 200  Training loss = 1.9271  Validation loss = 1.3688  \n",
      "\n",
      "Fold: 6  Epoch: 201  Training loss = 1.9269  Validation loss = 1.3680  \n",
      "\n",
      "Fold: 6  Epoch: 202  Training loss = 1.9264  Validation loss = 1.3652  \n",
      "\n",
      "Fold: 6  Epoch: 203  Training loss = 1.9261  Validation loss = 1.3640  \n",
      "\n",
      "Fold: 6  Epoch: 204  Training loss = 1.9258  Validation loss = 1.3626  \n",
      "\n",
      "Fold: 6  Epoch: 205  Training loss = 1.9254  Validation loss = 1.3614  \n",
      "\n",
      "Fold: 6  Epoch: 206  Training loss = 1.9252  Validation loss = 1.3599  \n",
      "\n",
      "Fold: 6  Epoch: 207  Training loss = 1.9248  Validation loss = 1.3579  \n",
      "\n",
      "Fold: 6  Epoch: 208  Training loss = 1.9244  Validation loss = 1.3557  \n",
      "\n",
      "Fold: 6  Epoch: 209  Training loss = 1.9242  Validation loss = 1.3546  \n",
      "\n",
      "Fold: 6  Epoch: 210  Training loss = 1.9237  Validation loss = 1.3514  \n",
      "\n",
      "Fold: 6  Epoch: 211  Training loss = 1.9236  Validation loss = 1.3518  \n",
      "\n",
      "Fold: 6  Epoch: 212  Training loss = 1.9232  Validation loss = 1.3494  \n",
      "\n",
      "Fold: 6  Epoch: 213  Training loss = 1.9227  Validation loss = 1.3466  \n",
      "\n",
      "Fold: 6  Epoch: 214  Training loss = 1.9223  Validation loss = 1.3439  \n",
      "\n",
      "Fold: 6  Epoch: 215  Training loss = 1.9218  Validation loss = 1.3413  \n",
      "\n",
      "Fold: 6  Epoch: 216  Training loss = 1.9215  Validation loss = 1.3400  \n",
      "\n",
      "Fold: 6  Epoch: 217  Training loss = 1.9215  Validation loss = 1.3404  \n",
      "\n",
      "Fold: 6  Epoch: 218  Training loss = 1.9213  Validation loss = 1.3402  \n",
      "\n",
      "Fold: 6  Epoch: 219  Training loss = 1.9210  Validation loss = 1.3390  \n",
      "\n",
      "Fold: 6  Epoch: 220  Training loss = 1.9207  Validation loss = 1.3373  \n",
      "\n",
      "Fold: 6  Epoch: 221  Training loss = 1.9205  Validation loss = 1.3359  \n",
      "\n",
      "Fold: 6  Epoch: 222  Training loss = 1.9203  Validation loss = 1.3347  \n",
      "\n",
      "Fold: 6  Epoch: 223  Training loss = 1.9199  Validation loss = 1.3328  \n",
      "\n",
      "Fold: 6  Epoch: 224  Training loss = 1.9197  Validation loss = 1.3321  \n",
      "\n",
      "Fold: 6  Epoch: 225  Training loss = 1.9195  Validation loss = 1.3315  \n",
      "\n",
      "Fold: 6  Epoch: 226  Training loss = 1.9192  Validation loss = 1.3300  \n",
      "\n",
      "Fold: 6  Epoch: 227  Training loss = 1.9191  Validation loss = 1.3298  \n",
      "\n",
      "Fold: 6  Epoch: 228  Training loss = 1.9189  Validation loss = 1.3293  \n",
      "\n",
      "Fold: 6  Epoch: 229  Training loss = 1.9184  Validation loss = 1.3266  \n",
      "\n",
      "Fold: 6  Epoch: 230  Training loss = 1.9180  Validation loss = 1.3247  \n",
      "\n",
      "Fold: 6  Epoch: 231  Training loss = 1.9176  Validation loss = 1.3230  \n",
      "\n",
      "Fold: 6  Epoch: 232  Training loss = 1.9174  Validation loss = 1.3220  \n",
      "\n",
      "Fold: 6  Epoch: 233  Training loss = 1.9170  Validation loss = 1.3201  \n",
      "\n",
      "Fold: 6  Epoch: 234  Training loss = 1.9167  Validation loss = 1.3186  \n",
      "\n",
      "Fold: 6  Epoch: 235  Training loss = 1.9165  Validation loss = 1.3178  \n",
      "\n",
      "Fold: 6  Epoch: 236  Training loss = 1.9163  Validation loss = 1.3174  \n",
      "\n",
      "Fold: 6  Epoch: 237  Training loss = 1.9161  Validation loss = 1.3169  \n",
      "\n",
      "Fold: 6  Epoch: 238  Training loss = 1.9158  Validation loss = 1.3160  \n",
      "\n",
      "Fold: 6  Epoch: 239  Training loss = 1.9157  Validation loss = 1.3171  \n",
      "\n",
      "Fold: 6  Epoch: 240  Training loss = 1.9154  Validation loss = 1.3156  \n",
      "\n",
      "Fold: 6  Epoch: 241  Training loss = 1.9151  Validation loss = 1.3140  \n",
      "\n",
      "Fold: 6  Epoch: 242  Training loss = 1.9150  Validation loss = 1.3131  \n",
      "\n",
      "Fold: 6  Epoch: 243  Training loss = 1.9148  Validation loss = 1.3124  \n",
      "\n",
      "Fold: 6  Epoch: 244  Training loss = 1.9144  Validation loss = 1.3098  \n",
      "\n",
      "Fold: 6  Epoch: 245  Training loss = 1.9140  Validation loss = 1.3078  \n",
      "\n",
      "Fold: 6  Epoch: 246  Training loss = 1.9136  Validation loss = 1.3057  \n",
      "\n",
      "Fold: 6  Epoch: 247  Training loss = 1.9133  Validation loss = 1.3036  \n",
      "\n",
      "Fold: 6  Epoch: 248  Training loss = 1.9130  Validation loss = 1.3030  \n",
      "\n",
      "Fold: 6  Epoch: 249  Training loss = 1.9125  Validation loss = 1.2996  \n",
      "\n",
      "Fold: 6  Epoch: 250  Training loss = 1.9124  Validation loss = 1.2995  \n",
      "\n",
      "Fold: 6  Epoch: 251  Training loss = 1.9121  Validation loss = 1.2986  \n",
      "\n",
      "Fold: 6  Epoch: 252  Training loss = 1.9119  Validation loss = 1.2973  \n",
      "\n",
      "Fold: 6  Epoch: 253  Training loss = 1.9114  Validation loss = 1.2946  \n",
      "\n",
      "Fold: 6  Epoch: 254  Training loss = 1.9112  Validation loss = 1.2939  \n",
      "\n",
      "Fold: 6  Epoch: 255  Training loss = 1.9109  Validation loss = 1.2923  \n",
      "\n",
      "Fold: 6  Epoch: 256  Training loss = 1.9105  Validation loss = 1.2899  \n",
      "\n",
      "Fold: 6  Epoch: 257  Training loss = 1.9101  Validation loss = 1.2875  \n",
      "\n",
      "Fold: 6  Epoch: 258  Training loss = 1.9100  Validation loss = 1.2877  \n",
      "\n",
      "Fold: 6  Epoch: 259  Training loss = 1.9098  Validation loss = 1.2874  \n",
      "\n",
      "Fold: 6  Epoch: 260  Training loss = 1.9097  Validation loss = 1.2870  \n",
      "\n",
      "Fold: 6  Epoch: 261  Training loss = 1.9094  Validation loss = 1.2859  \n",
      "\n",
      "Fold: 6  Epoch: 262  Training loss = 1.9090  Validation loss = 1.2841  \n",
      "\n",
      "Fold: 6  Epoch: 263  Training loss = 1.9087  Validation loss = 1.2828  \n",
      "\n",
      "Fold: 6  Epoch: 264  Training loss = 1.9086  Validation loss = 1.2824  \n",
      "\n",
      "Fold: 6  Epoch: 265  Training loss = 1.9083  Validation loss = 1.2814  \n",
      "\n",
      "Fold: 6  Epoch: 266  Training loss = 1.9080  Validation loss = 1.2799  \n",
      "\n",
      "Fold: 6  Epoch: 267  Training loss = 1.9078  Validation loss = 1.2785  \n",
      "\n",
      "Fold: 6  Epoch: 268  Training loss = 1.9074  Validation loss = 1.2766  \n",
      "\n",
      "Fold: 6  Epoch: 269  Training loss = 1.9070  Validation loss = 1.2741  \n",
      "\n",
      "Fold: 6  Epoch: 270  Training loss = 1.9067  Validation loss = 1.2725  \n",
      "\n",
      "Fold: 6  Epoch: 271  Training loss = 1.9064  Validation loss = 1.2705  \n",
      "\n",
      "Fold: 6  Epoch: 272  Training loss = 1.9062  Validation loss = 1.2701  \n",
      "\n",
      "Fold: 6  Epoch: 273  Training loss = 1.9058  Validation loss = 1.2675  \n",
      "\n",
      "Fold: 6  Epoch: 274  Training loss = 1.9056  Validation loss = 1.2669  \n",
      "\n",
      "Fold: 6  Epoch: 275  Training loss = 1.9053  Validation loss = 1.2654  \n",
      "\n",
      "Fold: 6  Epoch: 276  Training loss = 1.9050  Validation loss = 1.2638  \n",
      "\n",
      "Fold: 6  Epoch: 277  Training loss = 1.9049  Validation loss = 1.2639  \n",
      "\n",
      "Fold: 6  Epoch: 278  Training loss = 1.9045  Validation loss = 1.2617  \n",
      "\n",
      "Fold: 6  Epoch: 279  Training loss = 1.9042  Validation loss = 1.2605  \n",
      "\n",
      "Fold: 6  Epoch: 280  Training loss = 1.9040  Validation loss = 1.2596  \n",
      "\n",
      "Fold: 6  Epoch: 281  Training loss = 1.9038  Validation loss = 1.2595  \n",
      "\n",
      "Fold: 6  Epoch: 282  Training loss = 1.9037  Validation loss = 1.2602  \n",
      "\n",
      "Fold: 6  Epoch: 283  Training loss = 1.9034  Validation loss = 1.2583  \n",
      "\n",
      "Fold: 6  Epoch: 284  Training loss = 1.9030  Validation loss = 1.2564  \n",
      "\n",
      "Fold: 6  Epoch: 285  Training loss = 1.9028  Validation loss = 1.2552  \n",
      "\n",
      "Fold: 6  Epoch: 286  Training loss = 1.9024  Validation loss = 1.2533  \n",
      "\n",
      "Fold: 6  Epoch: 287  Training loss = 1.9021  Validation loss = 1.2517  \n",
      "\n",
      "Fold: 6  Epoch: 288  Training loss = 1.9018  Validation loss = 1.2501  \n",
      "\n",
      "Fold: 6  Epoch: 289  Training loss = 1.9015  Validation loss = 1.2487  \n",
      "\n",
      "Fold: 6  Epoch: 290  Training loss = 1.9014  Validation loss = 1.2490  \n",
      "\n",
      "Fold: 6  Epoch: 291  Training loss = 1.9011  Validation loss = 1.2473  \n",
      "\n",
      "Fold: 6  Epoch: 292  Training loss = 1.9007  Validation loss = 1.2447  \n",
      "\n",
      "Fold: 6  Epoch: 293  Training loss = 1.9006  Validation loss = 1.2442  \n",
      "\n",
      "Fold: 6  Epoch: 294  Training loss = 1.9004  Validation loss = 1.2439  \n",
      "\n",
      "Fold: 6  Epoch: 295  Training loss = 1.9002  Validation loss = 1.2435  \n",
      "\n",
      "Fold: 6  Epoch: 296  Training loss = 1.8998  Validation loss = 1.2411  \n",
      "\n",
      "Fold: 6  Epoch: 297  Training loss = 1.8997  Validation loss = 1.2412  \n",
      "\n",
      "Fold: 6  Epoch: 298  Training loss = 1.8992  Validation loss = 1.2378  \n",
      "\n",
      "Fold: 6  Epoch: 299  Training loss = 1.8989  Validation loss = 1.2360  \n",
      "\n",
      "Fold: 6  Epoch: 300  Training loss = 1.8987  Validation loss = 1.2352  \n",
      "\n",
      "Fold: 6  Epoch: 301  Training loss = 1.8985  Validation loss = 1.2350  \n",
      "\n",
      "Fold: 6  Epoch: 302  Training loss = 1.8981  Validation loss = 1.2320  \n",
      "\n",
      "Fold: 6  Epoch: 303  Training loss = 1.8980  Validation loss = 1.2329  \n",
      "\n",
      "Fold: 6  Epoch: 304  Training loss = 1.8978  Validation loss = 1.2320  \n",
      "\n",
      "Fold: 6  Epoch: 305  Training loss = 1.8977  Validation loss = 1.2321  \n",
      "\n",
      "Fold: 6  Epoch: 306  Training loss = 1.8974  Validation loss = 1.2310  \n",
      "\n",
      "Fold: 6  Epoch: 307  Training loss = 1.8973  Validation loss = 1.2312  \n",
      "\n",
      "Fold: 6  Epoch: 308  Training loss = 1.8971  Validation loss = 1.2306  \n",
      "\n",
      "Fold: 6  Epoch: 309  Training loss = 1.8968  Validation loss = 1.2285  \n",
      "\n",
      "Fold: 6  Epoch: 310  Training loss = 1.8964  Validation loss = 1.2263  \n",
      "\n",
      "Fold: 6  Epoch: 311  Training loss = 1.8964  Validation loss = 1.2265  \n",
      "\n",
      "Fold: 6  Epoch: 312  Training loss = 1.8960  Validation loss = 1.2244  \n",
      "\n",
      "Fold: 6  Epoch: 313  Training loss = 1.8958  Validation loss = 1.2244  \n",
      "\n",
      "Fold: 6  Epoch: 314  Training loss = 1.8957  Validation loss = 1.2240  \n",
      "\n",
      "Fold: 6  Epoch: 315  Training loss = 1.8953  Validation loss = 1.2205  \n",
      "\n",
      "Fold: 6  Epoch: 316  Training loss = 1.8951  Validation loss = 1.2198  \n",
      "\n",
      "Fold: 6  Epoch: 317  Training loss = 1.8948  Validation loss = 1.2182  \n",
      "\n",
      "Fold: 6  Epoch: 318  Training loss = 1.8944  Validation loss = 1.2161  \n",
      "\n",
      "Fold: 6  Epoch: 319  Training loss = 1.8942  Validation loss = 1.2156  \n",
      "\n",
      "Fold: 6  Epoch: 320  Training loss = 1.8940  Validation loss = 1.2146  \n",
      "\n",
      "Fold: 6  Epoch: 321  Training loss = 1.8937  Validation loss = 1.2135  \n",
      "\n",
      "Fold: 6  Epoch: 322  Training loss = 1.8935  Validation loss = 1.2126  \n",
      "\n",
      "Fold: 6  Epoch: 323  Training loss = 1.8932  Validation loss = 1.2110  \n",
      "\n",
      "Fold: 6  Epoch: 324  Training loss = 1.8930  Validation loss = 1.2105  \n",
      "\n",
      "Fold: 6  Epoch: 325  Training loss = 1.8927  Validation loss = 1.2078  \n",
      "\n",
      "Fold: 6  Epoch: 326  Training loss = 1.8926  Validation loss = 1.2081  \n",
      "\n",
      "Fold: 6  Epoch: 327  Training loss = 1.8924  Validation loss = 1.2072  \n",
      "\n",
      "Fold: 6  Epoch: 328  Training loss = 1.8920  Validation loss = 1.2059  \n",
      "\n",
      "Fold: 6  Epoch: 329  Training loss = 1.8918  Validation loss = 1.2050  \n",
      "\n",
      "Fold: 6  Epoch: 330  Training loss = 1.8915  Validation loss = 1.2035  \n",
      "\n",
      "Fold: 6  Epoch: 331  Training loss = 1.8914  Validation loss = 1.2039  \n",
      "\n",
      "Fold: 6  Epoch: 332  Training loss = 1.8911  Validation loss = 1.2022  \n",
      "\n",
      "Fold: 6  Epoch: 333  Training loss = 1.8908  Validation loss = 1.1999  \n",
      "\n",
      "Fold: 6  Epoch: 334  Training loss = 1.8906  Validation loss = 1.1997  \n",
      "\n",
      "Fold: 6  Epoch: 335  Training loss = 1.8904  Validation loss = 1.1991  \n",
      "\n",
      "Fold: 6  Epoch: 336  Training loss = 1.8903  Validation loss = 1.1996  \n",
      "\n",
      "Fold: 6  Epoch: 337  Training loss = 1.8901  Validation loss = 1.1995  \n",
      "\n",
      "Fold: 6  Epoch: 338  Training loss = 1.8898  Validation loss = 1.1981  \n",
      "\n",
      "Fold: 6  Epoch: 339  Training loss = 1.8897  Validation loss = 1.1979  \n",
      "\n",
      "Fold: 6  Epoch: 340  Training loss = 1.8895  Validation loss = 1.1981  \n",
      "\n",
      "Fold: 6  Epoch: 341  Training loss = 1.8894  Validation loss = 1.1978  \n",
      "\n",
      "Fold: 6  Epoch: 342  Training loss = 1.8891  Validation loss = 1.1960  \n",
      "\n",
      "Fold: 6  Epoch: 343  Training loss = 1.8890  Validation loss = 1.1963  \n",
      "\n",
      "Fold: 6  Epoch: 344  Training loss = 1.8888  Validation loss = 1.1955  \n",
      "\n",
      "Fold: 6  Epoch: 345  Training loss = 1.8886  Validation loss = 1.1945  \n",
      "\n",
      "Fold: 6  Epoch: 346  Training loss = 1.8884  Validation loss = 1.1944  \n",
      "\n",
      "Fold: 6  Epoch: 347  Training loss = 1.8884  Validation loss = 1.1956  \n",
      "\n",
      "Fold: 6  Epoch: 348  Training loss = 1.8881  Validation loss = 1.1950  \n",
      "\n",
      "Fold: 6  Epoch: 349  Training loss = 1.8879  Validation loss = 1.1941  \n",
      "\n",
      "Fold: 6  Epoch: 350  Training loss = 1.8877  Validation loss = 1.1940  \n",
      "\n",
      "Fold: 6  Epoch: 351  Training loss = 1.8877  Validation loss = 1.1948  \n",
      "\n",
      "Fold: 6  Epoch: 352  Training loss = 1.8874  Validation loss = 1.1935  \n",
      "\n",
      "Fold: 6  Epoch: 353  Training loss = 1.8871  Validation loss = 1.1918  \n",
      "\n",
      "Fold: 6  Epoch: 354  Training loss = 1.8869  Validation loss = 1.1907  \n",
      "\n",
      "Fold: 6  Epoch: 355  Training loss = 1.8866  Validation loss = 1.1897  \n",
      "\n",
      "Fold: 6  Epoch: 356  Training loss = 1.8863  Validation loss = 1.1868  \n",
      "\n",
      "Fold: 6  Epoch: 357  Training loss = 1.8860  Validation loss = 1.1848  \n",
      "\n",
      "Fold: 6  Epoch: 358  Training loss = 1.8858  Validation loss = 1.1841  \n",
      "\n",
      "Fold: 6  Epoch: 359  Training loss = 1.8856  Validation loss = 1.1831  \n",
      "\n",
      "Fold: 6  Epoch: 360  Training loss = 1.8853  Validation loss = 1.1819  \n",
      "\n",
      "Fold: 6  Epoch: 361  Training loss = 1.8850  Validation loss = 1.1800  \n",
      "\n",
      "Fold: 6  Epoch: 362  Training loss = 1.8849  Validation loss = 1.1803  \n",
      "\n",
      "Fold: 6  Epoch: 363  Training loss = 1.8845  Validation loss = 1.1777  \n",
      "\n",
      "Fold: 6  Epoch: 364  Training loss = 1.8841  Validation loss = 1.1753  \n",
      "\n",
      "Fold: 6  Epoch: 365  Training loss = 1.8839  Validation loss = 1.1735  \n",
      "\n",
      "Fold: 6  Epoch: 366  Training loss = 1.8836  Validation loss = 1.1723  \n",
      "\n",
      "Fold: 6  Epoch: 367  Training loss = 1.8835  Validation loss = 1.1723  \n",
      "\n",
      "Fold: 6  Epoch: 368  Training loss = 1.8833  Validation loss = 1.1716  \n",
      "\n",
      "Fold: 6  Epoch: 369  Training loss = 1.8831  Validation loss = 1.1713  \n",
      "\n",
      "Fold: 6  Epoch: 370  Training loss = 1.8830  Validation loss = 1.1714  \n",
      "\n",
      "Fold: 6  Epoch: 371  Training loss = 1.8828  Validation loss = 1.1711  \n",
      "\n",
      "Fold: 6  Epoch: 372  Training loss = 1.8826  Validation loss = 1.1706  \n",
      "\n",
      "Fold: 6  Epoch: 373  Training loss = 1.8825  Validation loss = 1.1703  \n",
      "\n",
      "Fold: 6  Epoch: 374  Training loss = 1.8823  Validation loss = 1.1707  \n",
      "\n",
      "Fold: 6  Epoch: 375  Training loss = 1.8822  Validation loss = 1.1704  \n",
      "\n",
      "Fold: 6  Epoch: 376  Training loss = 1.8820  Validation loss = 1.1698  \n",
      "\n",
      "Fold: 6  Epoch: 377  Training loss = 1.8818  Validation loss = 1.1693  \n",
      "\n",
      "Fold: 6  Epoch: 378  Training loss = 1.8815  Validation loss = 1.1680  \n",
      "\n",
      "Fold: 6  Epoch: 379  Training loss = 1.8814  Validation loss = 1.1679  \n",
      "\n",
      "Fold: 6  Epoch: 380  Training loss = 1.8810  Validation loss = 1.1653  \n",
      "\n",
      "Fold: 6  Epoch: 381  Training loss = 1.8809  Validation loss = 1.1652  \n",
      "\n",
      "Fold: 6  Epoch: 382  Training loss = 1.8806  Validation loss = 1.1646  \n",
      "\n",
      "Fold: 6  Epoch: 383  Training loss = 1.8804  Validation loss = 1.1630  \n",
      "\n",
      "Fold: 6  Epoch: 384  Training loss = 1.8801  Validation loss = 1.1617  \n",
      "\n",
      "Fold: 6  Epoch: 385  Training loss = 1.8798  Validation loss = 1.1601  \n",
      "\n",
      "Fold: 6  Epoch: 386  Training loss = 1.8795  Validation loss = 1.1581  \n",
      "\n",
      "Fold: 6  Epoch: 387  Training loss = 1.8793  Validation loss = 1.1565  \n",
      "\n",
      "Fold: 6  Epoch: 388  Training loss = 1.8791  Validation loss = 1.1558  \n",
      "\n",
      "Fold: 6  Epoch: 389  Training loss = 1.8788  Validation loss = 1.1548  \n",
      "\n",
      "Fold: 6  Epoch: 390  Training loss = 1.8787  Validation loss = 1.1552  \n",
      "\n",
      "Fold: 6  Epoch: 391  Training loss = 1.8787  Validation loss = 1.1577  \n",
      "\n",
      "Fold: 6  Epoch: 392  Training loss = 1.8785  Validation loss = 1.1570  \n",
      "\n",
      "Fold: 6  Epoch: 393  Training loss = 1.8784  Validation loss = 1.1572  \n",
      "\n",
      "Fold: 6  Epoch: 394  Training loss = 1.8780  Validation loss = 1.1551  \n",
      "\n",
      "Fold: 6  Epoch: 395  Training loss = 1.8778  Validation loss = 1.1544  \n",
      "\n",
      "Fold: 6  Epoch: 396  Training loss = 1.8774  Validation loss = 1.1514  \n",
      "\n",
      "Fold: 6  Epoch: 397  Training loss = 1.8772  Validation loss = 1.1502  \n",
      "\n",
      "Fold: 6  Epoch: 398  Training loss = 1.8770  Validation loss = 1.1493  \n",
      "\n",
      "Fold: 6  Epoch: 399  Training loss = 1.8768  Validation loss = 1.1488  \n",
      "\n",
      "Fold: 6  Epoch: 400  Training loss = 1.8766  Validation loss = 1.1483  \n",
      "\n",
      "Fold: 6  Epoch: 401  Training loss = 1.8764  Validation loss = 1.1475  \n",
      "\n",
      "Fold: 6  Epoch: 402  Training loss = 1.8762  Validation loss = 1.1470  \n",
      "\n",
      "Fold: 6  Epoch: 403  Training loss = 1.8760  Validation loss = 1.1473  \n",
      "\n",
      "Fold: 6  Epoch: 404  Training loss = 1.8758  Validation loss = 1.1474  \n",
      "\n",
      "Fold: 6  Epoch: 405  Training loss = 1.8756  Validation loss = 1.1471  \n",
      "\n",
      "Fold: 6  Epoch: 406  Training loss = 1.8755  Validation loss = 1.1473  \n",
      "\n",
      "Fold: 6  Epoch: 407  Training loss = 1.8752  Validation loss = 1.1462  \n",
      "\n",
      "Fold: 6  Epoch: 408  Training loss = 1.8749  Validation loss = 1.1446  \n",
      "\n",
      "Fold: 6  Epoch: 409  Training loss = 1.8747  Validation loss = 1.1437  \n",
      "\n",
      "Fold: 6  Epoch: 410  Training loss = 1.8745  Validation loss = 1.1424  \n",
      "\n",
      "Fold: 6  Epoch: 411  Training loss = 1.8742  Validation loss = 1.1415  \n",
      "\n",
      "Fold: 6  Epoch: 412  Training loss = 1.8739  Validation loss = 1.1392  \n",
      "\n",
      "Fold: 6  Epoch: 413  Training loss = 1.8737  Validation loss = 1.1380  \n",
      "\n",
      "Fold: 6  Epoch: 414  Training loss = 1.8736  Validation loss = 1.1378  \n",
      "\n",
      "Fold: 6  Epoch: 415  Training loss = 1.8734  Validation loss = 1.1363  \n",
      "\n",
      "Fold: 6  Epoch: 416  Training loss = 1.8732  Validation loss = 1.1362  \n",
      "\n",
      "Fold: 6  Epoch: 417  Training loss = 1.8730  Validation loss = 1.1356  \n",
      "\n",
      "Fold: 6  Epoch: 418  Training loss = 1.8728  Validation loss = 1.1345  \n",
      "\n",
      "Fold: 6  Epoch: 419  Training loss = 1.8727  Validation loss = 1.1337  \n",
      "\n",
      "Fold: 6  Epoch: 420  Training loss = 1.8724  Validation loss = 1.1321  \n",
      "\n",
      "Fold: 6  Epoch: 421  Training loss = 1.8722  Validation loss = 1.1313  \n",
      "\n",
      "Fold: 6  Epoch: 422  Training loss = 1.8720  Validation loss = 1.1304  \n",
      "\n",
      "Fold: 6  Epoch: 423  Training loss = 1.8717  Validation loss = 1.1284  \n",
      "\n",
      "Fold: 6  Epoch: 424  Training loss = 1.8715  Validation loss = 1.1273  \n",
      "\n",
      "Fold: 6  Epoch: 425  Training loss = 1.8713  Validation loss = 1.1274  \n",
      "\n",
      "Fold: 6  Epoch: 426  Training loss = 1.8712  Validation loss = 1.1276  \n",
      "\n",
      "Fold: 6  Epoch: 427  Training loss = 1.8711  Validation loss = 1.1283  \n",
      "\n",
      "Fold: 6  Epoch: 428  Training loss = 1.8709  Validation loss = 1.1276  \n",
      "\n",
      "Fold: 6  Epoch: 429  Training loss = 1.8707  Validation loss = 1.1266  \n",
      "\n",
      "Fold: 6  Epoch: 430  Training loss = 1.8704  Validation loss = 1.1247  \n",
      "\n",
      "Fold: 6  Epoch: 431  Training loss = 1.8702  Validation loss = 1.1243  \n",
      "\n",
      "Fold: 6  Epoch: 432  Training loss = 1.8698  Validation loss = 1.1217  \n",
      "\n",
      "Fold: 6  Epoch: 433  Training loss = 1.8695  Validation loss = 1.1199  \n",
      "\n",
      "Fold: 6  Epoch: 434  Training loss = 1.8692  Validation loss = 1.1184  \n",
      "\n",
      "Fold: 6  Epoch: 435  Training loss = 1.8690  Validation loss = 1.1185  \n",
      "\n",
      "Fold: 6  Epoch: 436  Training loss = 1.8688  Validation loss = 1.1186  \n",
      "\n",
      "Fold: 6  Epoch: 437  Training loss = 1.8687  Validation loss = 1.1170  \n",
      "\n",
      "Fold: 6  Epoch: 438  Training loss = 1.8685  Validation loss = 1.1166  \n",
      "\n",
      "Fold: 6  Epoch: 439  Training loss = 1.8682  Validation loss = 1.1158  \n",
      "\n",
      "Fold: 6  Epoch: 440  Training loss = 1.8680  Validation loss = 1.1150  \n",
      "\n",
      "Fold: 6  Epoch: 441  Training loss = 1.8679  Validation loss = 1.1157  \n",
      "\n",
      "Fold: 6  Epoch: 442  Training loss = 1.8676  Validation loss = 1.1134  \n",
      "\n",
      "Fold: 6  Epoch: 443  Training loss = 1.8673  Validation loss = 1.1120  \n",
      "\n",
      "Fold: 6  Epoch: 444  Training loss = 1.8670  Validation loss = 1.1105  \n",
      "\n",
      "Fold: 6  Epoch: 445  Training loss = 1.8668  Validation loss = 1.1097  \n",
      "\n",
      "Fold: 6  Epoch: 446  Training loss = 1.8665  Validation loss = 1.1075  \n",
      "\n",
      "Fold: 6  Epoch: 447  Training loss = 1.8663  Validation loss = 1.1061  \n",
      "\n",
      "Fold: 6  Epoch: 448  Training loss = 1.8660  Validation loss = 1.1042  \n",
      "\n",
      "Fold: 6  Epoch: 449  Training loss = 1.8660  Validation loss = 1.1056  \n",
      "\n",
      "Fold: 6  Epoch: 450  Training loss = 1.8658  Validation loss = 1.1051  \n",
      "\n",
      "Fold: 6  Epoch: 451  Training loss = 1.8656  Validation loss = 1.1050  \n",
      "\n",
      "Fold: 6  Epoch: 452  Training loss = 1.8654  Validation loss = 1.1041  \n",
      "\n",
      "Fold: 6  Epoch: 453  Training loss = 1.8652  Validation loss = 1.1043  \n",
      "\n",
      "Fold: 6  Epoch: 454  Training loss = 1.8649  Validation loss = 1.1018  \n",
      "\n",
      "Fold: 6  Epoch: 455  Training loss = 1.8648  Validation loss = 1.1020  \n",
      "\n",
      "Fold: 6  Epoch: 456  Training loss = 1.8646  Validation loss = 1.1013  \n",
      "\n",
      "Fold: 6  Epoch: 457  Training loss = 1.8644  Validation loss = 1.1006  \n",
      "\n",
      "Fold: 6  Epoch: 458  Training loss = 1.8643  Validation loss = 1.1007  \n",
      "\n",
      "Fold: 6  Epoch: 459  Training loss = 1.8642  Validation loss = 1.1019  \n",
      "\n",
      "Fold: 6  Epoch: 460  Training loss = 1.8639  Validation loss = 1.1003  \n",
      "\n",
      "Fold: 6  Epoch: 461  Training loss = 1.8635  Validation loss = 1.0977  \n",
      "\n",
      "Fold: 6  Epoch: 462  Training loss = 1.8633  Validation loss = 1.0961  \n",
      "\n",
      "Fold: 6  Epoch: 463  Training loss = 1.8631  Validation loss = 1.0970  \n",
      "\n",
      "Fold: 6  Epoch: 464  Training loss = 1.8630  Validation loss = 1.0972  \n",
      "\n",
      "Fold: 6  Epoch: 465  Training loss = 1.8628  Validation loss = 1.0964  \n",
      "\n",
      "Fold: 6  Epoch: 466  Training loss = 1.8625  Validation loss = 1.0954  \n",
      "\n",
      "Fold: 6  Epoch: 467  Training loss = 1.8624  Validation loss = 1.0960  \n",
      "\n",
      "Fold: 6  Epoch: 468  Training loss = 1.8623  Validation loss = 1.0960  \n",
      "\n",
      "Fold: 6  Epoch: 469  Training loss = 1.8620  Validation loss = 1.0942  \n",
      "\n",
      "Fold: 6  Epoch: 470  Training loss = 1.8619  Validation loss = 1.0939  \n",
      "\n",
      "Fold: 6  Epoch: 471  Training loss = 1.8617  Validation loss = 1.0932  \n",
      "\n",
      "Fold: 6  Epoch: 472  Training loss = 1.8615  Validation loss = 1.0936  \n",
      "\n",
      "Fold: 6  Epoch: 473  Training loss = 1.8615  Validation loss = 1.0948  \n",
      "\n",
      "Fold: 6  Epoch: 474  Training loss = 1.8613  Validation loss = 1.0935  \n",
      "\n",
      "Fold: 6  Epoch: 475  Training loss = 1.8610  Validation loss = 1.0924  \n",
      "\n",
      "Fold: 6  Epoch: 476  Training loss = 1.8608  Validation loss = 1.0903  \n",
      "\n",
      "Fold: 6  Epoch: 477  Training loss = 1.8606  Validation loss = 1.0905  \n",
      "\n",
      "Fold: 6  Epoch: 478  Training loss = 1.8604  Validation loss = 1.0910  \n",
      "\n",
      "Fold: 6  Epoch: 479  Training loss = 1.8603  Validation loss = 1.0907  \n",
      "\n",
      "Fold: 6  Epoch: 480  Training loss = 1.8600  Validation loss = 1.0896  \n",
      "\n",
      "Fold: 6  Epoch: 481  Training loss = 1.8599  Validation loss = 1.0901  \n",
      "\n",
      "Fold: 6  Epoch: 482  Training loss = 1.8597  Validation loss = 1.0901  \n",
      "\n",
      "Fold: 6  Epoch: 483  Training loss = 1.8595  Validation loss = 1.0899  \n",
      "\n",
      "Fold: 6  Epoch: 484  Training loss = 1.8591  Validation loss = 1.0871  \n",
      "\n",
      "Fold: 6  Epoch: 485  Training loss = 1.8590  Validation loss = 1.0871  \n",
      "\n",
      "Fold: 6  Epoch: 486  Training loss = 1.8588  Validation loss = 1.0869  \n",
      "\n",
      "Fold: 6  Epoch: 487  Training loss = 1.8586  Validation loss = 1.0865  \n",
      "\n",
      "Fold: 6  Epoch: 488  Training loss = 1.8585  Validation loss = 1.0861  \n",
      "\n",
      "Fold: 6  Epoch: 489  Training loss = 1.8583  Validation loss = 1.0857  \n",
      "\n",
      "Fold: 6  Epoch: 490  Training loss = 1.8581  Validation loss = 1.0852  \n",
      "\n",
      "Fold: 6  Epoch: 491  Training loss = 1.8578  Validation loss = 1.0842  \n",
      "\n",
      "Fold: 6  Epoch: 492  Training loss = 1.8576  Validation loss = 1.0843  \n",
      "\n",
      "Fold: 6  Epoch: 493  Training loss = 1.8574  Validation loss = 1.0829  \n",
      "\n",
      "Fold: 6  Epoch: 494  Training loss = 1.8572  Validation loss = 1.0828  \n",
      "\n",
      "Fold: 6  Epoch: 495  Training loss = 1.8571  Validation loss = 1.0833  \n",
      "\n",
      "Fold: 6  Epoch: 496  Training loss = 1.8568  Validation loss = 1.0813  \n",
      "\n",
      "Fold: 6  Epoch: 497  Training loss = 1.8566  Validation loss = 1.0808  \n",
      "\n",
      "Fold: 6  Epoch: 498  Training loss = 1.8563  Validation loss = 1.0783  \n",
      "\n",
      "Fold: 6  Epoch: 499  Training loss = 1.8562  Validation loss = 1.0777  \n",
      "\n",
      "Fold: 6  Epoch: 500  Training loss = 1.8561  Validation loss = 1.0786  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 499  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 1.8184  Validation loss = 0.9215  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 1.8178  Validation loss = 0.9189  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 1.8175  Validation loss = 0.9176  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.8170  Validation loss = 0.9163  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 1.8167  Validation loss = 0.9153  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 1.8163  Validation loss = 0.9140  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 1.8156  Validation loss = 0.9106  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 1.8151  Validation loss = 0.9083  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 1.8148  Validation loss = 0.9075  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 1.8145  Validation loss = 0.9062  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 1.8140  Validation loss = 0.9045  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 1.8137  Validation loss = 0.9038  \n",
      "\n",
      "Fold: 7  Epoch: 13  Training loss = 1.8135  Validation loss = 0.9039  \n",
      "\n",
      "Fold: 7  Epoch: 14  Training loss = 1.8133  Validation loss = 0.9032  \n",
      "\n",
      "Fold: 7  Epoch: 15  Training loss = 1.8126  Validation loss = 0.9005  \n",
      "\n",
      "Fold: 7  Epoch: 16  Training loss = 1.8122  Validation loss = 0.8993  \n",
      "\n",
      "Fold: 7  Epoch: 17  Training loss = 1.8120  Validation loss = 0.8989  \n",
      "\n",
      "Fold: 7  Epoch: 18  Training loss = 1.8118  Validation loss = 0.8986  \n",
      "\n",
      "Fold: 7  Epoch: 19  Training loss = 1.8115  Validation loss = 0.8979  \n",
      "\n",
      "Fold: 7  Epoch: 20  Training loss = 1.8111  Validation loss = 0.8958  \n",
      "\n",
      "Fold: 7  Epoch: 21  Training loss = 1.8107  Validation loss = 0.8947  \n",
      "\n",
      "Fold: 7  Epoch: 22  Training loss = 1.8100  Validation loss = 0.8915  \n",
      "\n",
      "Fold: 7  Epoch: 23  Training loss = 1.8098  Validation loss = 0.8913  \n",
      "\n",
      "Fold: 7  Epoch: 24  Training loss = 1.8091  Validation loss = 0.8884  \n",
      "\n",
      "Fold: 7  Epoch: 25  Training loss = 1.8088  Validation loss = 0.8874  \n",
      "\n",
      "Fold: 7  Epoch: 26  Training loss = 1.8085  Validation loss = 0.8866  \n",
      "\n",
      "Fold: 7  Epoch: 27  Training loss = 1.8082  Validation loss = 0.8857  \n",
      "\n",
      "Fold: 7  Epoch: 28  Training loss = 1.8079  Validation loss = 0.8850  \n",
      "\n",
      "Fold: 7  Epoch: 29  Training loss = 1.8076  Validation loss = 0.8839  \n",
      "\n",
      "Fold: 7  Epoch: 30  Training loss = 1.8072  Validation loss = 0.8822  \n",
      "\n",
      "Fold: 7  Epoch: 31  Training loss = 1.8071  Validation loss = 0.8826  \n",
      "\n",
      "Fold: 7  Epoch: 32  Training loss = 1.8068  Validation loss = 0.8822  \n",
      "\n",
      "Fold: 7  Epoch: 33  Training loss = 1.8062  Validation loss = 0.8796  \n",
      "\n",
      "Fold: 7  Epoch: 34  Training loss = 1.8059  Validation loss = 0.8785  \n",
      "\n",
      "Fold: 7  Epoch: 35  Training loss = 1.8056  Validation loss = 0.8778  \n",
      "\n",
      "Fold: 7  Epoch: 36  Training loss = 1.8052  Validation loss = 0.8764  \n",
      "\n",
      "Fold: 7  Epoch: 37  Training loss = 1.8051  Validation loss = 0.8768  \n",
      "\n",
      "Fold: 7  Epoch: 38  Training loss = 1.8050  Validation loss = 0.8769  \n",
      "\n",
      "Fold: 7  Epoch: 39  Training loss = 1.8045  Validation loss = 0.8749  \n",
      "\n",
      "Fold: 7  Epoch: 40  Training loss = 1.8043  Validation loss = 0.8743  \n",
      "\n",
      "Fold: 7  Epoch: 41  Training loss = 1.8038  Validation loss = 0.8720  \n",
      "\n",
      "Fold: 7  Epoch: 42  Training loss = 1.8034  Validation loss = 0.8707  \n",
      "\n",
      "Fold: 7  Epoch: 43  Training loss = 1.8029  Validation loss = 0.8686  \n",
      "\n",
      "Fold: 7  Epoch: 44  Training loss = 1.8025  Validation loss = 0.8670  \n",
      "\n",
      "Fold: 7  Epoch: 45  Training loss = 1.8020  Validation loss = 0.8653  \n",
      "\n",
      "Fold: 7  Epoch: 46  Training loss = 1.8019  Validation loss = 0.8657  \n",
      "\n",
      "Fold: 7  Epoch: 47  Training loss = 1.8016  Validation loss = 0.8646  \n",
      "\n",
      "Fold: 7  Epoch: 48  Training loss = 1.8011  Validation loss = 0.8622  \n",
      "\n",
      "Fold: 7  Epoch: 49  Training loss = 1.8005  Validation loss = 0.8593  \n",
      "\n",
      "Fold: 7  Epoch: 50  Training loss = 1.8002  Validation loss = 0.8587  \n",
      "\n",
      "Fold: 7  Epoch: 51  Training loss = 1.7999  Validation loss = 0.8573  \n",
      "\n",
      "Fold: 7  Epoch: 52  Training loss = 1.7997  Validation loss = 0.8574  \n",
      "\n",
      "Fold: 7  Epoch: 53  Training loss = 1.7995  Validation loss = 0.8568  \n",
      "\n",
      "Fold: 7  Epoch: 54  Training loss = 1.7990  Validation loss = 0.8550  \n",
      "\n",
      "Fold: 7  Epoch: 55  Training loss = 1.7985  Validation loss = 0.8526  \n",
      "\n",
      "Fold: 7  Epoch: 56  Training loss = 1.7980  Validation loss = 0.8506  \n",
      "\n",
      "Fold: 7  Epoch: 57  Training loss = 1.7977  Validation loss = 0.8496  \n",
      "\n",
      "Fold: 7  Epoch: 58  Training loss = 1.7972  Validation loss = 0.8475  \n",
      "\n",
      "Fold: 7  Epoch: 59  Training loss = 1.7969  Validation loss = 0.8467  \n",
      "\n",
      "Fold: 7  Epoch: 60  Training loss = 1.7965  Validation loss = 0.8455  \n",
      "\n",
      "Fold: 7  Epoch: 61  Training loss = 1.7965  Validation loss = 0.8465  \n",
      "\n",
      "Fold: 7  Epoch: 62  Training loss = 1.7960  Validation loss = 0.8435  \n",
      "\n",
      "Fold: 7  Epoch: 63  Training loss = 1.7958  Validation loss = 0.8431  \n",
      "\n",
      "Fold: 7  Epoch: 64  Training loss = 1.7956  Validation loss = 0.8424  \n",
      "\n",
      "Fold: 7  Epoch: 65  Training loss = 1.7954  Validation loss = 0.8419  \n",
      "\n",
      "Fold: 7  Epoch: 66  Training loss = 1.7951  Validation loss = 0.8417  \n",
      "\n",
      "Fold: 7  Epoch: 67  Training loss = 1.7948  Validation loss = 0.8402  \n",
      "\n",
      "Fold: 7  Epoch: 68  Training loss = 1.7948  Validation loss = 0.8415  \n",
      "\n",
      "Fold: 7  Epoch: 69  Training loss = 1.7945  Validation loss = 0.8406  \n",
      "\n",
      "Fold: 7  Epoch: 70  Training loss = 1.7941  Validation loss = 0.8397  \n",
      "\n",
      "Fold: 7  Epoch: 71  Training loss = 1.7938  Validation loss = 0.8385  \n",
      "\n",
      "Fold: 7  Epoch: 72  Training loss = 1.7933  Validation loss = 0.8362  \n",
      "\n",
      "Fold: 7  Epoch: 73  Training loss = 1.7929  Validation loss = 0.8354  \n",
      "\n",
      "Fold: 7  Epoch: 74  Training loss = 1.7926  Validation loss = 0.8344  \n",
      "\n",
      "Fold: 7  Epoch: 75  Training loss = 1.7923  Validation loss = 0.8336  \n",
      "\n",
      "Fold: 7  Epoch: 76  Training loss = 1.7920  Validation loss = 0.8327  \n",
      "\n",
      "Fold: 7  Epoch: 77  Training loss = 1.7917  Validation loss = 0.8319  \n",
      "\n",
      "Fold: 7  Epoch: 78  Training loss = 1.7912  Validation loss = 0.8303  \n",
      "\n",
      "Fold: 7  Epoch: 79  Training loss = 1.7906  Validation loss = 0.8277  \n",
      "\n",
      "Fold: 7  Epoch: 80  Training loss = 1.7903  Validation loss = 0.8268  \n",
      "\n",
      "Fold: 7  Epoch: 81  Training loss = 1.7898  Validation loss = 0.8243  \n",
      "\n",
      "Fold: 7  Epoch: 82  Training loss = 1.7894  Validation loss = 0.8230  \n",
      "\n",
      "Fold: 7  Epoch: 83  Training loss = 1.7889  Validation loss = 0.8207  \n",
      "\n",
      "Fold: 7  Epoch: 84  Training loss = 1.7885  Validation loss = 0.8190  \n",
      "\n",
      "Fold: 7  Epoch: 85  Training loss = 1.7882  Validation loss = 0.8181  \n",
      "\n",
      "Fold: 7  Epoch: 86  Training loss = 1.7878  Validation loss = 0.8161  \n",
      "\n",
      "Fold: 7  Epoch: 87  Training loss = 1.7875  Validation loss = 0.8152  \n",
      "\n",
      "Fold: 7  Epoch: 88  Training loss = 1.7870  Validation loss = 0.8131  \n",
      "\n",
      "Fold: 7  Epoch: 89  Training loss = 1.7867  Validation loss = 0.8121  \n",
      "\n",
      "Fold: 7  Epoch: 90  Training loss = 1.7865  Validation loss = 0.8113  \n",
      "\n",
      "Fold: 7  Epoch: 91  Training loss = 1.7862  Validation loss = 0.8105  \n",
      "\n",
      "Fold: 7  Epoch: 92  Training loss = 1.7861  Validation loss = 0.8105  \n",
      "\n",
      "Fold: 7  Epoch: 93  Training loss = 1.7858  Validation loss = 0.8099  \n",
      "\n",
      "Fold: 7  Epoch: 94  Training loss = 1.7854  Validation loss = 0.8080  \n",
      "\n",
      "Fold: 7  Epoch: 95  Training loss = 1.7852  Validation loss = 0.8077  \n",
      "\n",
      "Fold: 7  Epoch: 96  Training loss = 1.7847  Validation loss = 0.8057  \n",
      "\n",
      "Fold: 7  Epoch: 97  Training loss = 1.7843  Validation loss = 0.8040  \n",
      "\n",
      "Fold: 7  Epoch: 98  Training loss = 1.7841  Validation loss = 0.8041  \n",
      "\n",
      "Fold: 7  Epoch: 99  Training loss = 1.7838  Validation loss = 0.8031  \n",
      "\n",
      "Fold: 7  Epoch: 100  Training loss = 1.7836  Validation loss = 0.8026  \n",
      "\n",
      "Fold: 7  Epoch: 101  Training loss = 1.7833  Validation loss = 0.8023  \n",
      "\n",
      "Fold: 7  Epoch: 102  Training loss = 1.7830  Validation loss = 0.8016  \n",
      "\n",
      "Fold: 7  Epoch: 103  Training loss = 1.7828  Validation loss = 0.8009  \n",
      "\n",
      "Fold: 7  Epoch: 104  Training loss = 1.7824  Validation loss = 0.7996  \n",
      "\n",
      "Fold: 7  Epoch: 105  Training loss = 1.7822  Validation loss = 0.7990  \n",
      "\n",
      "Fold: 7  Epoch: 106  Training loss = 1.7819  Validation loss = 0.7984  \n",
      "\n",
      "Fold: 7  Epoch: 107  Training loss = 1.7813  Validation loss = 0.7958  \n",
      "\n",
      "Fold: 7  Epoch: 108  Training loss = 1.7810  Validation loss = 0.7949  \n",
      "\n",
      "Fold: 7  Epoch: 109  Training loss = 1.7807  Validation loss = 0.7941  \n",
      "\n",
      "Fold: 7  Epoch: 110  Training loss = 1.7804  Validation loss = 0.7934  \n",
      "\n",
      "Fold: 7  Epoch: 111  Training loss = 1.7800  Validation loss = 0.7919  \n",
      "\n",
      "Fold: 7  Epoch: 112  Training loss = 1.7795  Validation loss = 0.7891  \n",
      "\n",
      "Fold: 7  Epoch: 113  Training loss = 1.7789  Validation loss = 0.7862  \n",
      "\n",
      "Fold: 7  Epoch: 114  Training loss = 1.7786  Validation loss = 0.7851  \n",
      "\n",
      "Fold: 7  Epoch: 115  Training loss = 1.7782  Validation loss = 0.7832  \n",
      "\n",
      "Fold: 7  Epoch: 116  Training loss = 1.7777  Validation loss = 0.7812  \n",
      "\n",
      "Fold: 7  Epoch: 117  Training loss = 1.7774  Validation loss = 0.7802  \n",
      "\n",
      "Fold: 7  Epoch: 118  Training loss = 1.7771  Validation loss = 0.7790  \n",
      "\n",
      "Fold: 7  Epoch: 119  Training loss = 1.7768  Validation loss = 0.7777  \n",
      "\n",
      "Fold: 7  Epoch: 120  Training loss = 1.7765  Validation loss = 0.7766  \n",
      "\n",
      "Fold: 7  Epoch: 121  Training loss = 1.7763  Validation loss = 0.7761  \n",
      "\n",
      "Fold: 7  Epoch: 122  Training loss = 1.7761  Validation loss = 0.7759  \n",
      "\n",
      "Fold: 7  Epoch: 123  Training loss = 1.7759  Validation loss = 0.7758  \n",
      "\n",
      "Fold: 7  Epoch: 124  Training loss = 1.7757  Validation loss = 0.7753  \n",
      "\n",
      "Fold: 7  Epoch: 125  Training loss = 1.7755  Validation loss = 0.7748  \n",
      "\n",
      "Fold: 7  Epoch: 126  Training loss = 1.7752  Validation loss = 0.7732  \n",
      "\n",
      "Fold: 7  Epoch: 127  Training loss = 1.7749  Validation loss = 0.7717  \n",
      "\n",
      "Fold: 7  Epoch: 128  Training loss = 1.7747  Validation loss = 0.7713  \n",
      "\n",
      "Fold: 7  Epoch: 129  Training loss = 1.7745  Validation loss = 0.7713  \n",
      "\n",
      "Fold: 7  Epoch: 130  Training loss = 1.7743  Validation loss = 0.7707  \n",
      "\n",
      "Fold: 7  Epoch: 131  Training loss = 1.7739  Validation loss = 0.7689  \n",
      "\n",
      "Fold: 7  Epoch: 132  Training loss = 1.7735  Validation loss = 0.7680  \n",
      "\n",
      "Fold: 7  Epoch: 133  Training loss = 1.7733  Validation loss = 0.7669  \n",
      "\n",
      "Fold: 7  Epoch: 134  Training loss = 1.7730  Validation loss = 0.7658  \n",
      "\n",
      "Fold: 7  Epoch: 135  Training loss = 1.7725  Validation loss = 0.7638  \n",
      "\n",
      "Fold: 7  Epoch: 136  Training loss = 1.7722  Validation loss = 0.7630  \n",
      "\n",
      "Fold: 7  Epoch: 137  Training loss = 1.7718  Validation loss = 0.7621  \n",
      "\n",
      "Fold: 7  Epoch: 138  Training loss = 1.7716  Validation loss = 0.7613  \n",
      "\n",
      "Fold: 7  Epoch: 139  Training loss = 1.7712  Validation loss = 0.7598  \n",
      "\n",
      "Fold: 7  Epoch: 140  Training loss = 1.7710  Validation loss = 0.7596  \n",
      "\n",
      "Fold: 7  Epoch: 141  Training loss = 1.7707  Validation loss = 0.7595  \n",
      "\n",
      "Fold: 7  Epoch: 142  Training loss = 1.7706  Validation loss = 0.7601  \n",
      "\n",
      "Fold: 7  Epoch: 143  Training loss = 1.7703  Validation loss = 0.7590  \n",
      "\n",
      "Fold: 7  Epoch: 144  Training loss = 1.7701  Validation loss = 0.7586  \n",
      "\n",
      "Fold: 7  Epoch: 145  Training loss = 1.7700  Validation loss = 0.7587  \n",
      "\n",
      "Fold: 7  Epoch: 146  Training loss = 1.7697  Validation loss = 0.7581  \n",
      "\n",
      "Fold: 7  Epoch: 147  Training loss = 1.7695  Validation loss = 0.7578  \n",
      "\n",
      "Fold: 7  Epoch: 148  Training loss = 1.7693  Validation loss = 0.7576  \n",
      "\n",
      "Fold: 7  Epoch: 149  Training loss = 1.7691  Validation loss = 0.7572  \n",
      "\n",
      "Fold: 7  Epoch: 150  Training loss = 1.7687  Validation loss = 0.7562  \n",
      "\n",
      "Fold: 7  Epoch: 151  Training loss = 1.7684  Validation loss = 0.7552  \n",
      "\n",
      "Fold: 7  Epoch: 152  Training loss = 1.7681  Validation loss = 0.7535  \n",
      "\n",
      "Fold: 7  Epoch: 153  Training loss = 1.7680  Validation loss = 0.7536  \n",
      "\n",
      "Fold: 7  Epoch: 154  Training loss = 1.7675  Validation loss = 0.7512  \n",
      "\n",
      "Fold: 7  Epoch: 155  Training loss = 1.7674  Validation loss = 0.7517  \n",
      "\n",
      "Fold: 7  Epoch: 156  Training loss = 1.7670  Validation loss = 0.7502  \n",
      "\n",
      "Fold: 7  Epoch: 157  Training loss = 1.7666  Validation loss = 0.7488  \n",
      "\n",
      "Fold: 7  Epoch: 158  Training loss = 1.7664  Validation loss = 0.7483  \n",
      "\n",
      "Fold: 7  Epoch: 159  Training loss = 1.7660  Validation loss = 0.7465  \n",
      "\n",
      "Fold: 7  Epoch: 160  Training loss = 1.7658  Validation loss = 0.7464  \n",
      "\n",
      "Fold: 7  Epoch: 161  Training loss = 1.7656  Validation loss = 0.7465  \n",
      "\n",
      "Fold: 7  Epoch: 162  Training loss = 1.7653  Validation loss = 0.7450  \n",
      "\n",
      "Fold: 7  Epoch: 163  Training loss = 1.7648  Validation loss = 0.7431  \n",
      "\n",
      "Fold: 7  Epoch: 164  Training loss = 1.7646  Validation loss = 0.7428  \n",
      "\n",
      "Fold: 7  Epoch: 165  Training loss = 1.7644  Validation loss = 0.7429  \n",
      "\n",
      "Fold: 7  Epoch: 166  Training loss = 1.7642  Validation loss = 0.7427  \n",
      "\n",
      "Fold: 7  Epoch: 167  Training loss = 1.7640  Validation loss = 0.7425  \n",
      "\n",
      "Fold: 7  Epoch: 168  Training loss = 1.7636  Validation loss = 0.7410  \n",
      "\n",
      "Fold: 7  Epoch: 169  Training loss = 1.7634  Validation loss = 0.7400  \n",
      "\n",
      "Fold: 7  Epoch: 170  Training loss = 1.7632  Validation loss = 0.7398  \n",
      "\n",
      "Fold: 7  Epoch: 171  Training loss = 1.7629  Validation loss = 0.7391  \n",
      "\n",
      "Fold: 7  Epoch: 172  Training loss = 1.7627  Validation loss = 0.7388  \n",
      "\n",
      "Fold: 7  Epoch: 173  Training loss = 1.7624  Validation loss = 0.7385  \n",
      "\n",
      "Fold: 7  Epoch: 174  Training loss = 1.7623  Validation loss = 0.7384  \n",
      "\n",
      "Fold: 7  Epoch: 175  Training loss = 1.7621  Validation loss = 0.7380  \n",
      "\n",
      "Fold: 7  Epoch: 176  Training loss = 1.7617  Validation loss = 0.7361  \n",
      "\n",
      "Fold: 7  Epoch: 177  Training loss = 1.7614  Validation loss = 0.7354  \n",
      "\n",
      "Fold: 7  Epoch: 178  Training loss = 1.7612  Validation loss = 0.7349  \n",
      "\n",
      "Fold: 7  Epoch: 179  Training loss = 1.7610  Validation loss = 0.7349  \n",
      "\n",
      "Fold: 7  Epoch: 180  Training loss = 1.7607  Validation loss = 0.7342  \n",
      "\n",
      "Fold: 7  Epoch: 181  Training loss = 1.7603  Validation loss = 0.7327  \n",
      "\n",
      "Fold: 7  Epoch: 182  Training loss = 1.7601  Validation loss = 0.7317  \n",
      "\n",
      "Fold: 7  Epoch: 183  Training loss = 1.7597  Validation loss = 0.7310  \n",
      "\n",
      "Fold: 7  Epoch: 184  Training loss = 1.7595  Validation loss = 0.7303  \n",
      "\n",
      "Fold: 7  Epoch: 185  Training loss = 1.7593  Validation loss = 0.7303  \n",
      "\n",
      "Fold: 7  Epoch: 186  Training loss = 1.7588  Validation loss = 0.7285  \n",
      "\n",
      "Fold: 7  Epoch: 187  Training loss = 1.7589  Validation loss = 0.7303  \n",
      "\n",
      "Fold: 7  Epoch: 188  Training loss = 1.7585  Validation loss = 0.7288  \n",
      "\n",
      "Fold: 7  Epoch: 189  Training loss = 1.7583  Validation loss = 0.7283  \n",
      "\n",
      "Fold: 7  Epoch: 190  Training loss = 1.7581  Validation loss = 0.7284  \n",
      "\n",
      "Fold: 7  Epoch: 191  Training loss = 1.7578  Validation loss = 0.7272  \n",
      "\n",
      "Fold: 7  Epoch: 192  Training loss = 1.7575  Validation loss = 0.7267  \n",
      "\n",
      "Fold: 7  Epoch: 193  Training loss = 1.7571  Validation loss = 0.7250  \n",
      "\n",
      "Fold: 7  Epoch: 194  Training loss = 1.7569  Validation loss = 0.7238  \n",
      "\n",
      "Fold: 7  Epoch: 195  Training loss = 1.7567  Validation loss = 0.7232  \n",
      "\n",
      "Fold: 7  Epoch: 196  Training loss = 1.7565  Validation loss = 0.7230  \n",
      "\n",
      "Fold: 7  Epoch: 197  Training loss = 1.7562  Validation loss = 0.7216  \n",
      "\n",
      "Fold: 7  Epoch: 198  Training loss = 1.7559  Validation loss = 0.7201  \n",
      "\n",
      "Fold: 7  Epoch: 199  Training loss = 1.7556  Validation loss = 0.7196  \n",
      "\n",
      "Fold: 7  Epoch: 200  Training loss = 1.7554  Validation loss = 0.7195  \n",
      "\n",
      "Fold: 7  Epoch: 201  Training loss = 1.7551  Validation loss = 0.7188  \n",
      "\n",
      "Fold: 7  Epoch: 202  Training loss = 1.7549  Validation loss = 0.7187  \n",
      "\n",
      "Fold: 7  Epoch: 203  Training loss = 1.7546  Validation loss = 0.7179  \n",
      "\n",
      "Fold: 7  Epoch: 204  Training loss = 1.7542  Validation loss = 0.7168  \n",
      "\n",
      "Fold: 7  Epoch: 205  Training loss = 1.7539  Validation loss = 0.7154  \n",
      "\n",
      "Fold: 7  Epoch: 206  Training loss = 1.7537  Validation loss = 0.7149  \n",
      "\n",
      "Fold: 7  Epoch: 207  Training loss = 1.7535  Validation loss = 0.7148  \n",
      "\n",
      "Fold: 7  Epoch: 208  Training loss = 1.7532  Validation loss = 0.7140  \n",
      "\n",
      "Fold: 7  Epoch: 209  Training loss = 1.7529  Validation loss = 0.7131  \n",
      "\n",
      "Fold: 7  Epoch: 210  Training loss = 1.7529  Validation loss = 0.7145  \n",
      "\n",
      "Fold: 7  Epoch: 211  Training loss = 1.7528  Validation loss = 0.7147  \n",
      "\n",
      "Fold: 7  Epoch: 212  Training loss = 1.7527  Validation loss = 0.7149  \n",
      "\n",
      "Fold: 7  Epoch: 213  Training loss = 1.7523  Validation loss = 0.7130  \n",
      "\n",
      "Fold: 7  Epoch: 214  Training loss = 1.7521  Validation loss = 0.7122  \n",
      "\n",
      "Fold: 7  Epoch: 215  Training loss = 1.7519  Validation loss = 0.7120  \n",
      "\n",
      "Fold: 7  Epoch: 216  Training loss = 1.7517  Validation loss = 0.7111  \n",
      "\n",
      "Fold: 7  Epoch: 217  Training loss = 1.7513  Validation loss = 0.7095  \n",
      "\n",
      "Fold: 7  Epoch: 218  Training loss = 1.7511  Validation loss = 0.7095  \n",
      "\n",
      "Fold: 7  Epoch: 219  Training loss = 1.7508  Validation loss = 0.7094  \n",
      "\n",
      "Fold: 7  Epoch: 220  Training loss = 1.7506  Validation loss = 0.7090  \n",
      "\n",
      "Fold: 7  Epoch: 221  Training loss = 1.7503  Validation loss = 0.7079  \n",
      "\n",
      "Fold: 7  Epoch: 222  Training loss = 1.7500  Validation loss = 0.7070  \n",
      "\n",
      "Fold: 7  Epoch: 223  Training loss = 1.7499  Validation loss = 0.7073  \n",
      "\n",
      "Fold: 7  Epoch: 224  Training loss = 1.7496  Validation loss = 0.7060  \n",
      "\n",
      "Fold: 7  Epoch: 225  Training loss = 1.7493  Validation loss = 0.7052  \n",
      "\n",
      "Fold: 7  Epoch: 226  Training loss = 1.7491  Validation loss = 0.7044  \n",
      "\n",
      "Fold: 7  Epoch: 227  Training loss = 1.7488  Validation loss = 0.7034  \n",
      "\n",
      "Fold: 7  Epoch: 228  Training loss = 1.7487  Validation loss = 0.7036  \n",
      "\n",
      "Fold: 7  Epoch: 229  Training loss = 1.7484  Validation loss = 0.7031  \n",
      "\n",
      "Fold: 7  Epoch: 230  Training loss = 1.7482  Validation loss = 0.7026  \n",
      "\n",
      "Fold: 7  Epoch: 231  Training loss = 1.7480  Validation loss = 0.7027  \n",
      "\n",
      "Fold: 7  Epoch: 232  Training loss = 1.7477  Validation loss = 0.7018  \n",
      "\n",
      "Fold: 7  Epoch: 233  Training loss = 1.7473  Validation loss = 0.7005  \n",
      "\n",
      "Fold: 7  Epoch: 234  Training loss = 1.7471  Validation loss = 0.6997  \n",
      "\n",
      "Fold: 7  Epoch: 235  Training loss = 1.7469  Validation loss = 0.6990  \n",
      "\n",
      "Fold: 7  Epoch: 236  Training loss = 1.7466  Validation loss = 0.6990  \n",
      "\n",
      "Fold: 7  Epoch: 237  Training loss = 1.7465  Validation loss = 0.6992  \n",
      "\n",
      "Fold: 7  Epoch: 238  Training loss = 1.7461  Validation loss = 0.6978  \n",
      "\n",
      "Fold: 7  Epoch: 239  Training loss = 1.7458  Validation loss = 0.6971  \n",
      "\n",
      "Fold: 7  Epoch: 240  Training loss = 1.7456  Validation loss = 0.6964  \n",
      "\n",
      "Fold: 7  Epoch: 241  Training loss = 1.7454  Validation loss = 0.6961  \n",
      "\n",
      "Fold: 7  Epoch: 242  Training loss = 1.7452  Validation loss = 0.6952  \n",
      "\n",
      "Fold: 7  Epoch: 243  Training loss = 1.7450  Validation loss = 0.6947  \n",
      "\n",
      "Fold: 7  Epoch: 244  Training loss = 1.7447  Validation loss = 0.6940  \n",
      "\n",
      "Fold: 7  Epoch: 245  Training loss = 1.7446  Validation loss = 0.6939  \n",
      "\n",
      "Fold: 7  Epoch: 246  Training loss = 1.7444  Validation loss = 0.6939  \n",
      "\n",
      "Fold: 7  Epoch: 247  Training loss = 1.7441  Validation loss = 0.6930  \n",
      "\n",
      "Fold: 7  Epoch: 248  Training loss = 1.7438  Validation loss = 0.6923  \n",
      "\n",
      "Fold: 7  Epoch: 249  Training loss = 1.7435  Validation loss = 0.6919  \n",
      "\n",
      "Fold: 7  Epoch: 250  Training loss = 1.7432  Validation loss = 0.6910  \n",
      "\n",
      "Fold: 7  Epoch: 251  Training loss = 1.7430  Validation loss = 0.6894  \n",
      "\n",
      "Fold: 7  Epoch: 252  Training loss = 1.7427  Validation loss = 0.6883  \n",
      "\n",
      "Fold: 7  Epoch: 253  Training loss = 1.7424  Validation loss = 0.6873  \n",
      "\n",
      "Fold: 7  Epoch: 254  Training loss = 1.7423  Validation loss = 0.6873  \n",
      "\n",
      "Fold: 7  Epoch: 255  Training loss = 1.7421  Validation loss = 0.6874  \n",
      "\n",
      "Fold: 7  Epoch: 256  Training loss = 1.7418  Validation loss = 0.6867  \n",
      "\n",
      "Fold: 7  Epoch: 257  Training loss = 1.7415  Validation loss = 0.6857  \n",
      "\n",
      "Fold: 7  Epoch: 258  Training loss = 1.7413  Validation loss = 0.6855  \n",
      "\n",
      "Fold: 7  Epoch: 259  Training loss = 1.7410  Validation loss = 0.6845  \n",
      "\n",
      "Fold: 7  Epoch: 260  Training loss = 1.7408  Validation loss = 0.6833  \n",
      "\n",
      "Fold: 7  Epoch: 261  Training loss = 1.7406  Validation loss = 0.6828  \n",
      "\n",
      "Fold: 7  Epoch: 262  Training loss = 1.7404  Validation loss = 0.6824  \n",
      "\n",
      "Fold: 7  Epoch: 263  Training loss = 1.7401  Validation loss = 0.6817  \n",
      "\n",
      "Fold: 7  Epoch: 264  Training loss = 1.7399  Validation loss = 0.6815  \n",
      "\n",
      "Fold: 7  Epoch: 265  Training loss = 1.7398  Validation loss = 0.6816  \n",
      "\n",
      "Fold: 7  Epoch: 266  Training loss = 1.7395  Validation loss = 0.6809  \n",
      "\n",
      "Fold: 7  Epoch: 267  Training loss = 1.7393  Validation loss = 0.6806  \n",
      "\n",
      "Fold: 7  Epoch: 268  Training loss = 1.7390  Validation loss = 0.6798  \n",
      "\n",
      "Fold: 7  Epoch: 269  Training loss = 1.7389  Validation loss = 0.6806  \n",
      "\n",
      "Fold: 7  Epoch: 270  Training loss = 1.7386  Validation loss = 0.6795  \n",
      "\n",
      "Fold: 7  Epoch: 271  Training loss = 1.7383  Validation loss = 0.6792  \n",
      "\n",
      "Fold: 7  Epoch: 272  Training loss = 1.7381  Validation loss = 0.6785  \n",
      "\n",
      "Fold: 7  Epoch: 273  Training loss = 1.7379  Validation loss = 0.6783  \n",
      "\n",
      "Fold: 7  Epoch: 274  Training loss = 1.7377  Validation loss = 0.6783  \n",
      "\n",
      "Fold: 7  Epoch: 275  Training loss = 1.7375  Validation loss = 0.6775  \n",
      "\n",
      "Fold: 7  Epoch: 276  Training loss = 1.7372  Validation loss = 0.6767  \n",
      "\n",
      "Fold: 7  Epoch: 277  Training loss = 1.7369  Validation loss = 0.6759  \n",
      "\n",
      "Fold: 7  Epoch: 278  Training loss = 1.7368  Validation loss = 0.6767  \n",
      "\n",
      "Fold: 7  Epoch: 279  Training loss = 1.7365  Validation loss = 0.6766  \n",
      "\n",
      "Fold: 7  Epoch: 280  Training loss = 1.7363  Validation loss = 0.6763  \n",
      "\n",
      "Fold: 7  Epoch: 281  Training loss = 1.7361  Validation loss = 0.6758  \n",
      "\n",
      "Fold: 7  Epoch: 282  Training loss = 1.7359  Validation loss = 0.6762  \n",
      "\n",
      "Fold: 7  Epoch: 283  Training loss = 1.7357  Validation loss = 0.6757  \n",
      "\n",
      "Fold: 7  Epoch: 284  Training loss = 1.7355  Validation loss = 0.6751  \n",
      "\n",
      "Fold: 7  Epoch: 285  Training loss = 1.7350  Validation loss = 0.6727  \n",
      "\n",
      "Fold: 7  Epoch: 286  Training loss = 1.7347  Validation loss = 0.6720  \n",
      "\n",
      "Fold: 7  Epoch: 287  Training loss = 1.7346  Validation loss = 0.6725  \n",
      "\n",
      "Fold: 7  Epoch: 288  Training loss = 1.7344  Validation loss = 0.6722  \n",
      "\n",
      "Fold: 7  Epoch: 289  Training loss = 1.7343  Validation loss = 0.6722  \n",
      "\n",
      "Fold: 7  Epoch: 290  Training loss = 1.7341  Validation loss = 0.6724  \n",
      "\n",
      "Fold: 7  Epoch: 291  Training loss = 1.7338  Validation loss = 0.6716  \n",
      "\n",
      "Fold: 7  Epoch: 292  Training loss = 1.7335  Validation loss = 0.6705  \n",
      "\n",
      "Fold: 7  Epoch: 293  Training loss = 1.7334  Validation loss = 0.6701  \n",
      "\n",
      "Fold: 7  Epoch: 294  Training loss = 1.7331  Validation loss = 0.6688  \n",
      "\n",
      "Fold: 7  Epoch: 295  Training loss = 1.7329  Validation loss = 0.6684  \n",
      "\n",
      "Fold: 7  Epoch: 296  Training loss = 1.7327  Validation loss = 0.6684  \n",
      "\n",
      "Fold: 7  Epoch: 297  Training loss = 1.7325  Validation loss = 0.6677  \n",
      "\n",
      "Fold: 7  Epoch: 298  Training loss = 1.7323  Validation loss = 0.6673  \n",
      "\n",
      "Fold: 7  Epoch: 299  Training loss = 1.7319  Validation loss = 0.6654  \n",
      "\n",
      "Fold: 7  Epoch: 300  Training loss = 1.7318  Validation loss = 0.6659  \n",
      "\n",
      "Fold: 7  Epoch: 301  Training loss = 1.7315  Validation loss = 0.6651  \n",
      "\n",
      "Fold: 7  Epoch: 302  Training loss = 1.7313  Validation loss = 0.6642  \n",
      "\n",
      "Fold: 7  Epoch: 303  Training loss = 1.7310  Validation loss = 0.6630  \n",
      "\n",
      "Fold: 7  Epoch: 304  Training loss = 1.7309  Validation loss = 0.6631  \n",
      "\n",
      "Fold: 7  Epoch: 305  Training loss = 1.7306  Validation loss = 0.6628  \n",
      "\n",
      "Fold: 7  Epoch: 306  Training loss = 1.7304  Validation loss = 0.6627  \n",
      "\n",
      "Fold: 7  Epoch: 307  Training loss = 1.7303  Validation loss = 0.6628  \n",
      "\n",
      "Fold: 7  Epoch: 308  Training loss = 1.7302  Validation loss = 0.6625  \n",
      "\n",
      "Fold: 7  Epoch: 309  Training loss = 1.7299  Validation loss = 0.6619  \n",
      "\n",
      "Fold: 7  Epoch: 310  Training loss = 1.7297  Validation loss = 0.6619  \n",
      "\n",
      "Fold: 7  Epoch: 311  Training loss = 1.7296  Validation loss = 0.6615  \n",
      "\n",
      "Fold: 7  Epoch: 312  Training loss = 1.7294  Validation loss = 0.6602  \n",
      "\n",
      "Fold: 7  Epoch: 313  Training loss = 1.7292  Validation loss = 0.6601  \n",
      "\n",
      "Fold: 7  Epoch: 314  Training loss = 1.7291  Validation loss = 0.6610  \n",
      "\n",
      "Fold: 7  Epoch: 315  Training loss = 1.7289  Validation loss = 0.6609  \n",
      "\n",
      "Fold: 7  Epoch: 316  Training loss = 1.7288  Validation loss = 0.6613  \n",
      "\n",
      "Fold: 7  Epoch: 317  Training loss = 1.7287  Validation loss = 0.6616  \n",
      "\n",
      "Fold: 7  Epoch: 318  Training loss = 1.7285  Validation loss = 0.6608  \n",
      "\n",
      "Fold: 7  Epoch: 319  Training loss = 1.7282  Validation loss = 0.6603  \n",
      "\n",
      "Fold: 7  Epoch: 320  Training loss = 1.7279  Validation loss = 0.6590  \n",
      "\n",
      "Fold: 7  Epoch: 321  Training loss = 1.7278  Validation loss = 0.6585  \n",
      "\n",
      "Fold: 7  Epoch: 322  Training loss = 1.7276  Validation loss = 0.6583  \n",
      "\n",
      "Fold: 7  Epoch: 323  Training loss = 1.7272  Validation loss = 0.6573  \n",
      "\n",
      "Fold: 7  Epoch: 324  Training loss = 1.7270  Validation loss = 0.6568  \n",
      "\n",
      "Fold: 7  Epoch: 325  Training loss = 1.7268  Validation loss = 0.6565  \n",
      "\n",
      "Fold: 7  Epoch: 326  Training loss = 1.7265  Validation loss = 0.6553  \n",
      "\n",
      "Fold: 7  Epoch: 327  Training loss = 1.7264  Validation loss = 0.6557  \n",
      "\n",
      "Fold: 7  Epoch: 328  Training loss = 1.7262  Validation loss = 0.6558  \n",
      "\n",
      "Fold: 7  Epoch: 329  Training loss = 1.7261  Validation loss = 0.6561  \n",
      "\n",
      "Fold: 7  Epoch: 330  Training loss = 1.7259  Validation loss = 0.6559  \n",
      "\n",
      "Fold: 7  Epoch: 331  Training loss = 1.7255  Validation loss = 0.6543  \n",
      "\n",
      "Fold: 7  Epoch: 332  Training loss = 1.7253  Validation loss = 0.6539  \n",
      "\n",
      "Fold: 7  Epoch: 333  Training loss = 1.7249  Validation loss = 0.6523  \n",
      "\n",
      "Fold: 7  Epoch: 334  Training loss = 1.7247  Validation loss = 0.6515  \n",
      "\n",
      "Fold: 7  Epoch: 335  Training loss = 1.7243  Validation loss = 0.6505  \n",
      "\n",
      "Fold: 7  Epoch: 336  Training loss = 1.7241  Validation loss = 0.6503  \n",
      "\n",
      "Fold: 7  Epoch: 337  Training loss = 1.7238  Validation loss = 0.6488  \n",
      "\n",
      "Fold: 7  Epoch: 338  Training loss = 1.7235  Validation loss = 0.6481  \n",
      "\n",
      "Fold: 7  Epoch: 339  Training loss = 1.7234  Validation loss = 0.6482  \n",
      "\n",
      "Fold: 7  Epoch: 340  Training loss = 1.7232  Validation loss = 0.6480  \n",
      "\n",
      "Fold: 7  Epoch: 341  Training loss = 1.7231  Validation loss = 0.6482  \n",
      "\n",
      "Fold: 7  Epoch: 342  Training loss = 1.7230  Validation loss = 0.6487  \n",
      "\n",
      "Fold: 7  Epoch: 343  Training loss = 1.7228  Validation loss = 0.6482  \n",
      "\n",
      "Fold: 7  Epoch: 344  Training loss = 1.7226  Validation loss = 0.6475  \n",
      "\n",
      "Fold: 7  Epoch: 345  Training loss = 1.7223  Validation loss = 0.6469  \n",
      "\n",
      "Fold: 7  Epoch: 346  Training loss = 1.7221  Validation loss = 0.6467  \n",
      "\n",
      "Fold: 7  Epoch: 347  Training loss = 1.7219  Validation loss = 0.6461  \n",
      "\n",
      "Fold: 7  Epoch: 348  Training loss = 1.7216  Validation loss = 0.6456  \n",
      "\n",
      "Fold: 7  Epoch: 349  Training loss = 1.7214  Validation loss = 0.6454  \n",
      "\n",
      "Fold: 7  Epoch: 350  Training loss = 1.7212  Validation loss = 0.6459  \n",
      "\n",
      "Fold: 7  Epoch: 351  Training loss = 1.7210  Validation loss = 0.6447  \n",
      "\n",
      "Fold: 7  Epoch: 352  Training loss = 1.7209  Validation loss = 0.6450  \n",
      "\n",
      "Fold: 7  Epoch: 353  Training loss = 1.7206  Validation loss = 0.6443  \n",
      "\n",
      "Fold: 7  Epoch: 354  Training loss = 1.7205  Validation loss = 0.6443  \n",
      "\n",
      "Fold: 7  Epoch: 355  Training loss = 1.7202  Validation loss = 0.6435  \n",
      "\n",
      "Fold: 7  Epoch: 356  Training loss = 1.7201  Validation loss = 0.6432  \n",
      "\n",
      "Fold: 7  Epoch: 357  Training loss = 1.7199  Validation loss = 0.6429  \n",
      "\n",
      "Fold: 7  Epoch: 358  Training loss = 1.7198  Validation loss = 0.6426  \n",
      "\n",
      "Fold: 7  Epoch: 359  Training loss = 1.7195  Validation loss = 0.6422  \n",
      "\n",
      "Fold: 7  Epoch: 360  Training loss = 1.7193  Validation loss = 0.6421  \n",
      "\n",
      "Fold: 7  Epoch: 361  Training loss = 1.7191  Validation loss = 0.6419  \n",
      "\n",
      "Fold: 7  Epoch: 362  Training loss = 1.7190  Validation loss = 0.6420  \n",
      "\n",
      "Fold: 7  Epoch: 363  Training loss = 1.7188  Validation loss = 0.6421  \n",
      "\n",
      "Fold: 7  Epoch: 364  Training loss = 1.7187  Validation loss = 0.6422  \n",
      "\n",
      "Fold: 7  Epoch: 365  Training loss = 1.7185  Validation loss = 0.6423  \n",
      "\n",
      "Fold: 7  Epoch: 366  Training loss = 1.7183  Validation loss = 0.6420  \n",
      "\n",
      "Fold: 7  Epoch: 367  Training loss = 1.7182  Validation loss = 0.6428  \n",
      "\n",
      "Fold: 7  Epoch: 368  Training loss = 1.7180  Validation loss = 0.6422  \n",
      "\n",
      "Fold: 7  Epoch: 369  Training loss = 1.7178  Validation loss = 0.6422  \n",
      "\n",
      "Fold: 7  Epoch: 370  Training loss = 1.7177  Validation loss = 0.6418  \n",
      "\n",
      "Fold: 7  Epoch: 371  Training loss = 1.7175  Validation loss = 0.6418  \n",
      "\n",
      "Fold: 7  Epoch: 372  Training loss = 1.7173  Validation loss = 0.6418  \n",
      "\n",
      "Fold: 7  Epoch: 373  Training loss = 1.7170  Validation loss = 0.6407  \n",
      "\n",
      "Fold: 7  Epoch: 374  Training loss = 1.7168  Validation loss = 0.6400  \n",
      "\n",
      "Fold: 7  Epoch: 375  Training loss = 1.7165  Validation loss = 0.6388  \n",
      "\n",
      "Fold: 7  Epoch: 376  Training loss = 1.7163  Validation loss = 0.6390  \n",
      "\n",
      "Fold: 7  Epoch: 377  Training loss = 1.7162  Validation loss = 0.6383  \n",
      "\n",
      "Fold: 7  Epoch: 378  Training loss = 1.7160  Validation loss = 0.6379  \n",
      "\n",
      "Fold: 7  Epoch: 379  Training loss = 1.7157  Validation loss = 0.6364  \n",
      "\n",
      "Fold: 7  Epoch: 380  Training loss = 1.7155  Validation loss = 0.6353  \n",
      "\n",
      "Fold: 7  Epoch: 381  Training loss = 1.7154  Validation loss = 0.6356  \n",
      "\n",
      "Fold: 7  Epoch: 382  Training loss = 1.7151  Validation loss = 0.6353  \n",
      "\n",
      "Fold: 7  Epoch: 383  Training loss = 1.7149  Validation loss = 0.6353  \n",
      "\n",
      "Fold: 7  Epoch: 384  Training loss = 1.7147  Validation loss = 0.6349  \n",
      "\n",
      "Fold: 7  Epoch: 385  Training loss = 1.7145  Validation loss = 0.6345  \n",
      "\n",
      "Fold: 7  Epoch: 386  Training loss = 1.7142  Validation loss = 0.6336  \n",
      "\n",
      "Fold: 7  Epoch: 387  Training loss = 1.7140  Validation loss = 0.6331  \n",
      "\n",
      "Fold: 7  Epoch: 388  Training loss = 1.7137  Validation loss = 0.6323  \n",
      "\n",
      "Fold: 7  Epoch: 389  Training loss = 1.7135  Validation loss = 0.6322  \n",
      "\n",
      "Fold: 7  Epoch: 390  Training loss = 1.7134  Validation loss = 0.6318  \n",
      "\n",
      "Fold: 7  Epoch: 391  Training loss = 1.7132  Validation loss = 0.6320  \n",
      "\n",
      "Fold: 7  Epoch: 392  Training loss = 1.7130  Validation loss = 0.6311  \n",
      "\n",
      "Fold: 7  Epoch: 393  Training loss = 1.7129  Validation loss = 0.6311  \n",
      "\n",
      "Fold: 7  Epoch: 394  Training loss = 1.7126  Validation loss = 0.6308  \n",
      "\n",
      "Fold: 7  Epoch: 395  Training loss = 1.7125  Validation loss = 0.6311  \n",
      "\n",
      "Fold: 7  Epoch: 396  Training loss = 1.7123  Validation loss = 0.6304  \n",
      "\n",
      "Fold: 7  Epoch: 397  Training loss = 1.7121  Validation loss = 0.6300  \n",
      "\n",
      "Fold: 7  Epoch: 398  Training loss = 1.7119  Validation loss = 0.6292  \n",
      "\n",
      "Fold: 7  Epoch: 399  Training loss = 1.7117  Validation loss = 0.6295  \n",
      "\n",
      "Fold: 7  Epoch: 400  Training loss = 1.7115  Validation loss = 0.6292  \n",
      "\n",
      "Fold: 7  Epoch: 401  Training loss = 1.7114  Validation loss = 0.6298  \n",
      "\n",
      "Fold: 7  Epoch: 402  Training loss = 1.7113  Validation loss = 0.6301  \n",
      "\n",
      "Fold: 7  Epoch: 403  Training loss = 1.7111  Validation loss = 0.6297  \n",
      "\n",
      "Fold: 7  Epoch: 404  Training loss = 1.7108  Validation loss = 0.6291  \n",
      "\n",
      "Fold: 7  Epoch: 405  Training loss = 1.7105  Validation loss = 0.6282  \n",
      "\n",
      "Fold: 7  Epoch: 406  Training loss = 1.7104  Validation loss = 0.6278  \n",
      "\n",
      "Fold: 7  Epoch: 407  Training loss = 1.7102  Validation loss = 0.6277  \n",
      "\n",
      "Fold: 7  Epoch: 408  Training loss = 1.7101  Validation loss = 0.6275  \n",
      "\n",
      "Fold: 7  Epoch: 409  Training loss = 1.7099  Validation loss = 0.6275  \n",
      "\n",
      "Fold: 7  Epoch: 410  Training loss = 1.7097  Validation loss = 0.6274  \n",
      "\n",
      "Fold: 7  Epoch: 411  Training loss = 1.7095  Validation loss = 0.6273  \n",
      "\n",
      "Fold: 7  Epoch: 412  Training loss = 1.7093  Validation loss = 0.6269  \n",
      "\n",
      "Fold: 7  Epoch: 413  Training loss = 1.7091  Validation loss = 0.6266  \n",
      "\n",
      "Fold: 7  Epoch: 414  Training loss = 1.7089  Validation loss = 0.6255  \n",
      "\n",
      "Fold: 7  Epoch: 415  Training loss = 1.7088  Validation loss = 0.6256  \n",
      "\n",
      "Fold: 7  Epoch: 416  Training loss = 1.7086  Validation loss = 0.6255  \n",
      "\n",
      "Fold: 7  Epoch: 417  Training loss = 1.7084  Validation loss = 0.6249  \n",
      "\n",
      "Fold: 7  Epoch: 418  Training loss = 1.7082  Validation loss = 0.6248  \n",
      "\n",
      "Fold: 7  Epoch: 419  Training loss = 1.7080  Validation loss = 0.6238  \n",
      "\n",
      "Fold: 7  Epoch: 420  Training loss = 1.7077  Validation loss = 0.6233  \n",
      "\n",
      "Fold: 7  Epoch: 421  Training loss = 1.7076  Validation loss = 0.6232  \n",
      "\n",
      "Fold: 7  Epoch: 422  Training loss = 1.7073  Validation loss = 0.6234  \n",
      "\n",
      "Fold: 7  Epoch: 423  Training loss = 1.7072  Validation loss = 0.6233  \n",
      "\n",
      "Fold: 7  Epoch: 424  Training loss = 1.7070  Validation loss = 0.6234  \n",
      "\n",
      "Fold: 7  Epoch: 425  Training loss = 1.7068  Validation loss = 0.6230  \n",
      "\n",
      "Fold: 7  Epoch: 426  Training loss = 1.7066  Validation loss = 0.6221  \n",
      "\n",
      "Fold: 7  Epoch: 427  Training loss = 1.7063  Validation loss = 0.6214  \n",
      "\n",
      "Fold: 7  Epoch: 428  Training loss = 1.7061  Validation loss = 0.6210  \n",
      "\n",
      "Fold: 7  Epoch: 429  Training loss = 1.7059  Validation loss = 0.6206  \n",
      "\n",
      "Fold: 7  Epoch: 430  Training loss = 1.7057  Validation loss = 0.6198  \n",
      "\n",
      "Fold: 7  Epoch: 431  Training loss = 1.7056  Validation loss = 0.6199  \n",
      "\n",
      "Fold: 7  Epoch: 432  Training loss = 1.7055  Validation loss = 0.6201  \n",
      "\n",
      "Fold: 7  Epoch: 433  Training loss = 1.7053  Validation loss = 0.6200  \n",
      "\n",
      "Fold: 7  Epoch: 434  Training loss = 1.7052  Validation loss = 0.6203  \n",
      "\n",
      "Fold: 7  Epoch: 435  Training loss = 1.7051  Validation loss = 0.6206  \n",
      "\n",
      "Fold: 7  Epoch: 436  Training loss = 1.7049  Validation loss = 0.6201  \n",
      "\n",
      "Fold: 7  Epoch: 437  Training loss = 1.7047  Validation loss = 0.6196  \n",
      "\n",
      "Fold: 7  Epoch: 438  Training loss = 1.7045  Validation loss = 0.6200  \n",
      "\n",
      "Fold: 7  Epoch: 439  Training loss = 1.7044  Validation loss = 0.6203  \n",
      "\n",
      "Fold: 7  Epoch: 440  Training loss = 1.7042  Validation loss = 0.6196  \n",
      "\n",
      "Fold: 7  Epoch: 441  Training loss = 1.7039  Validation loss = 0.6195  \n",
      "\n",
      "Fold: 7  Epoch: 442  Training loss = 1.7037  Validation loss = 0.6188  \n",
      "\n",
      "Fold: 7  Epoch: 443  Training loss = 1.7035  Validation loss = 0.6183  \n",
      "\n",
      "Fold: 7  Epoch: 444  Training loss = 1.7033  Validation loss = 0.6181  \n",
      "\n",
      "Fold: 7  Epoch: 445  Training loss = 1.7031  Validation loss = 0.6178  \n",
      "\n",
      "Fold: 7  Epoch: 446  Training loss = 1.7028  Validation loss = 0.6167  \n",
      "\n",
      "Fold: 7  Epoch: 447  Training loss = 1.7026  Validation loss = 0.6166  \n",
      "\n",
      "Fold: 7  Epoch: 448  Training loss = 1.7023  Validation loss = 0.6154  \n",
      "\n",
      "Fold: 7  Epoch: 449  Training loss = 1.7022  Validation loss = 0.6154  \n",
      "\n",
      "Fold: 7  Epoch: 450  Training loss = 1.7021  Validation loss = 0.6162  \n",
      "\n",
      "Fold: 7  Epoch: 451  Training loss = 1.7019  Validation loss = 0.6150  \n",
      "\n",
      "Fold: 7  Epoch: 452  Training loss = 1.7017  Validation loss = 0.6143  \n",
      "\n",
      "Fold: 7  Epoch: 453  Training loss = 1.7014  Validation loss = 0.6136  \n",
      "\n",
      "Fold: 7  Epoch: 454  Training loss = 1.7012  Validation loss = 0.6128  \n",
      "\n",
      "Fold: 7  Epoch: 455  Training loss = 1.7009  Validation loss = 0.6120  \n",
      "\n",
      "Fold: 7  Epoch: 456  Training loss = 1.7007  Validation loss = 0.6120  \n",
      "\n",
      "Fold: 7  Epoch: 457  Training loss = 1.7005  Validation loss = 0.6119  \n",
      "\n",
      "Fold: 7  Epoch: 458  Training loss = 1.7003  Validation loss = 0.6112  \n",
      "\n",
      "Fold: 7  Epoch: 459  Training loss = 1.7001  Validation loss = 0.6108  \n",
      "\n",
      "Fold: 7  Epoch: 460  Training loss = 1.6999  Validation loss = 0.6106  \n",
      "\n",
      "Fold: 7  Epoch: 461  Training loss = 1.6997  Validation loss = 0.6104  \n",
      "\n",
      "Fold: 7  Epoch: 462  Training loss = 1.6994  Validation loss = 0.6093  \n",
      "\n",
      "Fold: 7  Epoch: 463  Training loss = 1.6992  Validation loss = 0.6094  \n",
      "\n",
      "Fold: 7  Epoch: 464  Training loss = 1.6990  Validation loss = 0.6091  \n",
      "\n",
      "Fold: 7  Epoch: 465  Training loss = 1.6989  Validation loss = 0.6088  \n",
      "\n",
      "Fold: 7  Epoch: 466  Training loss = 1.6987  Validation loss = 0.6087  \n",
      "\n",
      "Fold: 7  Epoch: 467  Training loss = 1.6984  Validation loss = 0.6085  \n",
      "\n",
      "Fold: 7  Epoch: 468  Training loss = 1.6983  Validation loss = 0.6082  \n",
      "\n",
      "Fold: 7  Epoch: 469  Training loss = 1.6981  Validation loss = 0.6081  \n",
      "\n",
      "Fold: 7  Epoch: 470  Training loss = 1.6979  Validation loss = 0.6085  \n",
      "\n",
      "Fold: 7  Epoch: 471  Training loss = 1.6977  Validation loss = 0.6078  \n",
      "\n",
      "Fold: 7  Epoch: 472  Training loss = 1.6976  Validation loss = 0.6080  \n",
      "\n",
      "Fold: 7  Epoch: 473  Training loss = 1.6974  Validation loss = 0.6082  \n",
      "\n",
      "Fold: 7  Epoch: 474  Training loss = 1.6972  Validation loss = 0.6085  \n",
      "\n",
      "Fold: 7  Epoch: 475  Training loss = 1.6970  Validation loss = 0.6082  \n",
      "\n",
      "Fold: 7  Epoch: 476  Training loss = 1.6969  Validation loss = 0.6082  \n",
      "\n",
      "Fold: 7  Epoch: 477  Training loss = 1.6968  Validation loss = 0.6091  \n",
      "\n",
      "Fold: 7  Epoch: 478  Training loss = 1.6967  Validation loss = 0.6098  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 471  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 1.6469  Validation loss = 5.6816  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 1.6467  Validation loss = 5.6818  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 1.6465  Validation loss = 5.6815  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 1.6463  Validation loss = 5.6820  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 1.6461  Validation loss = 5.6817  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 1.6458  Validation loss = 5.6813  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 1.6457  Validation loss = 5.6820  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 1.6456  Validation loss = 5.6825  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 1.6454  Validation loss = 5.6825  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 1.6452  Validation loss = 5.6843  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 1.6451  Validation loss = 5.6846  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 1.6449  Validation loss = 5.6842  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 1.6447  Validation loss = 5.6838  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 1.6445  Validation loss = 5.6843  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 1.6443  Validation loss = 5.6835  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 1.6440  Validation loss = 5.6833  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 1.6437  Validation loss = 5.6828  \n",
      "\n",
      "Fold: 8  Epoch: 18  Training loss = 1.6434  Validation loss = 5.6829  \n",
      "\n",
      "Fold: 8  Epoch: 19  Training loss = 1.6432  Validation loss = 5.6821  \n",
      "\n",
      "Fold: 8  Epoch: 20  Training loss = 1.6431  Validation loss = 5.6822  \n",
      "\n",
      "Fold: 8  Epoch: 21  Training loss = 1.6430  Validation loss = 5.6807  \n",
      "\n",
      "Fold: 8  Epoch: 22  Training loss = 1.6429  Validation loss = 5.6825  \n",
      "\n",
      "Fold: 8  Epoch: 23  Training loss = 1.6428  Validation loss = 5.6840  \n",
      "\n",
      "Fold: 8  Epoch: 24  Training loss = 1.6425  Validation loss = 5.6819  \n",
      "\n",
      "Fold: 8  Epoch: 25  Training loss = 1.6424  Validation loss = 5.6831  \n",
      "\n",
      "Fold: 8  Epoch: 26  Training loss = 1.6422  Validation loss = 5.6821  \n",
      "\n",
      "Fold: 8  Epoch: 27  Training loss = 1.6421  Validation loss = 5.6828  \n",
      "\n",
      "Fold: 8  Epoch: 28  Training loss = 1.6419  Validation loss = 5.6825  \n",
      "\n",
      "Fold: 8  Epoch: 29  Training loss = 1.6418  Validation loss = 5.6831  \n",
      "\n",
      "Fold: 8  Epoch: 30  Training loss = 1.6417  Validation loss = 5.6834  \n",
      "\n",
      "Fold: 8  Epoch: 31  Training loss = 1.6415  Validation loss = 5.6819  \n",
      "\n",
      "Fold: 8  Epoch: 32  Training loss = 1.6412  Validation loss = 5.6816  \n",
      "\n",
      "Fold: 8  Epoch: 33  Training loss = 1.6411  Validation loss = 5.6816  \n",
      "\n",
      "Fold: 8  Epoch: 34  Training loss = 1.6409  Validation loss = 5.6812  \n",
      "\n",
      "Fold: 8  Epoch: 35  Training loss = 1.6407  Validation loss = 5.6828  \n",
      "\n",
      "Fold: 8  Epoch: 36  Training loss = 1.6405  Validation loss = 5.6817  \n",
      "\n",
      "Fold: 8  Epoch: 37  Training loss = 1.6403  Validation loss = 5.6828  \n",
      "\n",
      "Fold: 8  Epoch: 38  Training loss = 1.6400  Validation loss = 5.6802  \n",
      "\n",
      "Fold: 8  Epoch: 39  Training loss = 1.6397  Validation loss = 5.6790  \n",
      "\n",
      "Fold: 8  Epoch: 40  Training loss = 1.6395  Validation loss = 5.6784  \n",
      "\n",
      "Fold: 8  Epoch: 41  Training loss = 1.6393  Validation loss = 5.6792  \n",
      "\n",
      "Fold: 8  Epoch: 42  Training loss = 1.6392  Validation loss = 5.6798  \n",
      "\n",
      "Fold: 8  Epoch: 43  Training loss = 1.6390  Validation loss = 5.6797  \n",
      "\n",
      "Fold: 8  Epoch: 44  Training loss = 1.6388  Validation loss = 5.6796  \n",
      "\n",
      "Fold: 8  Epoch: 45  Training loss = 1.6387  Validation loss = 5.6803  \n",
      "\n",
      "Fold: 8  Epoch: 46  Training loss = 1.6384  Validation loss = 5.6792  \n",
      "\n",
      "Fold: 8  Epoch: 47  Training loss = 1.6382  Validation loss = 5.6776  \n",
      "\n",
      "Fold: 8  Epoch: 48  Training loss = 1.6380  Validation loss = 5.6790  \n",
      "\n",
      "Fold: 8  Epoch: 49  Training loss = 1.6378  Validation loss = 5.6787  \n",
      "\n",
      "Fold: 8  Epoch: 50  Training loss = 1.6375  Validation loss = 5.6778  \n",
      "\n",
      "Fold: 8  Epoch: 51  Training loss = 1.6373  Validation loss = 5.6780  \n",
      "\n",
      "Fold: 8  Epoch: 52  Training loss = 1.6372  Validation loss = 5.6779  \n",
      "\n",
      "Fold: 8  Epoch: 53  Training loss = 1.6370  Validation loss = 5.6768  \n",
      "\n",
      "Fold: 8  Epoch: 54  Training loss = 1.6368  Validation loss = 5.6765  \n",
      "\n",
      "Fold: 8  Epoch: 55  Training loss = 1.6366  Validation loss = 5.6761  \n",
      "\n",
      "Fold: 8  Epoch: 56  Training loss = 1.6365  Validation loss = 5.6771  \n",
      "\n",
      "Fold: 8  Epoch: 57  Training loss = 1.6363  Validation loss = 5.6762  \n",
      "\n",
      "Fold: 8  Epoch: 58  Training loss = 1.6360  Validation loss = 5.6761  \n",
      "\n",
      "Fold: 8  Epoch: 59  Training loss = 1.6359  Validation loss = 5.6763  \n",
      "\n",
      "Fold: 8  Epoch: 60  Training loss = 1.6357  Validation loss = 5.6768  \n",
      "\n",
      "Fold: 8  Epoch: 61  Training loss = 1.6355  Validation loss = 5.6771  \n",
      "\n",
      "Fold: 8  Epoch: 62  Training loss = 1.6353  Validation loss = 5.6764  \n",
      "\n",
      "Fold: 8  Epoch: 63  Training loss = 1.6351  Validation loss = 5.6766  \n",
      "\n",
      "Fold: 8  Epoch: 64  Training loss = 1.6348  Validation loss = 5.6755  \n",
      "\n",
      "Fold: 8  Epoch: 65  Training loss = 1.6347  Validation loss = 5.6768  \n",
      "\n",
      "Fold: 8  Epoch: 66  Training loss = 1.6346  Validation loss = 5.6772  \n",
      "\n",
      "Fold: 8  Epoch: 67  Training loss = 1.6344  Validation loss = 5.6771  \n",
      "\n",
      "Fold: 8  Epoch: 68  Training loss = 1.6342  Validation loss = 5.6760  \n",
      "\n",
      "Fold: 8  Epoch: 69  Training loss = 1.6339  Validation loss = 5.6752  \n",
      "\n",
      "Fold: 8  Epoch: 70  Training loss = 1.6338  Validation loss = 5.6751  \n",
      "\n",
      "Fold: 8  Epoch: 71  Training loss = 1.6337  Validation loss = 5.6750  \n",
      "\n",
      "Fold: 8  Epoch: 72  Training loss = 1.6334  Validation loss = 5.6740  \n",
      "\n",
      "Fold: 8  Epoch: 73  Training loss = 1.6333  Validation loss = 5.6741  \n",
      "\n",
      "Fold: 8  Epoch: 74  Training loss = 1.6331  Validation loss = 5.6741  \n",
      "\n",
      "Fold: 8  Epoch: 75  Training loss = 1.6328  Validation loss = 5.6736  \n",
      "\n",
      "Fold: 8  Epoch: 76  Training loss = 1.6325  Validation loss = 5.6730  \n",
      "\n",
      "Fold: 8  Epoch: 77  Training loss = 1.6324  Validation loss = 5.6717  \n",
      "\n",
      "Fold: 8  Epoch: 78  Training loss = 1.6323  Validation loss = 5.6739  \n",
      "\n",
      "Fold: 8  Epoch: 79  Training loss = 1.6321  Validation loss = 5.6736  \n",
      "\n",
      "Fold: 8  Epoch: 80  Training loss = 1.6319  Validation loss = 5.6732  \n",
      "\n",
      "Fold: 8  Epoch: 81  Training loss = 1.6317  Validation loss = 5.6740  \n",
      "\n",
      "Fold: 8  Epoch: 82  Training loss = 1.6315  Validation loss = 5.6746  \n",
      "\n",
      "Fold: 8  Epoch: 83  Training loss = 1.6313  Validation loss = 5.6725  \n",
      "\n",
      "Fold: 8  Epoch: 84  Training loss = 1.6311  Validation loss = 5.6729  \n",
      "\n",
      "Fold: 8  Epoch: 85  Training loss = 1.6309  Validation loss = 5.6711  \n",
      "\n",
      "Fold: 8  Epoch: 86  Training loss = 1.6308  Validation loss = 5.6692  \n",
      "\n",
      "Fold: 8  Epoch: 87  Training loss = 1.6307  Validation loss = 5.6708  \n",
      "\n",
      "Fold: 8  Epoch: 88  Training loss = 1.6306  Validation loss = 5.6701  \n",
      "\n",
      "Fold: 8  Epoch: 89  Training loss = 1.6304  Validation loss = 5.6684  \n",
      "\n",
      "Fold: 8  Epoch: 90  Training loss = 1.6303  Validation loss = 5.6687  \n",
      "\n",
      "Fold: 8  Epoch: 91  Training loss = 1.6300  Validation loss = 5.6685  \n",
      "\n",
      "Fold: 8  Epoch: 92  Training loss = 1.6299  Validation loss = 5.6684  \n",
      "\n",
      "Fold: 8  Epoch: 93  Training loss = 1.6297  Validation loss = 5.6668  \n",
      "\n",
      "Fold: 8  Epoch: 94  Training loss = 1.6295  Validation loss = 5.6673  \n",
      "\n",
      "Fold: 8  Epoch: 95  Training loss = 1.6294  Validation loss = 5.6673  \n",
      "\n",
      "Fold: 8  Epoch: 96  Training loss = 1.6293  Validation loss = 5.6673  \n",
      "\n",
      "Fold: 8  Epoch: 97  Training loss = 1.6290  Validation loss = 5.6657  \n",
      "\n",
      "Fold: 8  Epoch: 98  Training loss = 1.6289  Validation loss = 5.6660  \n",
      "\n",
      "Fold: 8  Epoch: 99  Training loss = 1.6287  Validation loss = 5.6680  \n",
      "\n",
      "Fold: 8  Epoch: 100  Training loss = 1.6286  Validation loss = 5.6692  \n",
      "\n",
      "Fold: 8  Epoch: 101  Training loss = 1.6284  Validation loss = 5.6684  \n",
      "\n",
      "Fold: 8  Epoch: 102  Training loss = 1.6283  Validation loss = 5.6677  \n",
      "\n",
      "Fold: 8  Epoch: 103  Training loss = 1.6281  Validation loss = 5.6687  \n",
      "\n",
      "Fold: 8  Epoch: 104  Training loss = 1.6279  Validation loss = 5.6677  \n",
      "\n",
      "Fold: 8  Epoch: 105  Training loss = 1.6276  Validation loss = 5.6663  \n",
      "\n",
      "Fold: 8  Epoch: 106  Training loss = 1.6274  Validation loss = 5.6658  \n",
      "\n",
      "Fold: 8  Epoch: 107  Training loss = 1.6273  Validation loss = 5.6664  \n",
      "\n",
      "Fold: 8  Epoch: 108  Training loss = 1.6272  Validation loss = 5.6662  \n",
      "\n",
      "Fold: 8  Epoch: 109  Training loss = 1.6269  Validation loss = 5.6666  \n",
      "\n",
      "Fold: 8  Epoch: 110  Training loss = 1.6268  Validation loss = 5.6668  \n",
      "\n",
      "Fold: 8  Epoch: 111  Training loss = 1.6266  Validation loss = 5.6674  \n",
      "\n",
      "Fold: 8  Epoch: 112  Training loss = 1.6264  Validation loss = 5.6666  \n",
      "\n",
      "Fold: 8  Epoch: 113  Training loss = 1.6263  Validation loss = 5.6659  \n",
      "\n",
      "Fold: 8  Epoch: 114  Training loss = 1.6261  Validation loss = 5.6664  \n",
      "\n",
      "Fold: 8  Epoch: 115  Training loss = 1.6260  Validation loss = 5.6680  \n",
      "\n",
      "Fold: 8  Epoch: 116  Training loss = 1.6257  Validation loss = 5.6664  \n",
      "\n",
      "Fold: 8  Epoch: 117  Training loss = 1.6255  Validation loss = 5.6668  \n",
      "\n",
      "Fold: 8  Epoch: 118  Training loss = 1.6253  Validation loss = 5.6658  \n",
      "\n",
      "Fold: 8  Epoch: 119  Training loss = 1.6250  Validation loss = 5.6640  \n",
      "\n",
      "Fold: 8  Epoch: 120  Training loss = 1.6247  Validation loss = 5.6632  \n",
      "\n",
      "Fold: 8  Epoch: 121  Training loss = 1.6246  Validation loss = 5.6649  \n",
      "\n",
      "Fold: 8  Epoch: 122  Training loss = 1.6245  Validation loss = 5.6652  \n",
      "\n",
      "Fold: 8  Epoch: 123  Training loss = 1.6243  Validation loss = 5.6655  \n",
      "\n",
      "Fold: 8  Epoch: 124  Training loss = 1.6241  Validation loss = 5.6654  \n",
      "\n",
      "Fold: 8  Epoch: 125  Training loss = 1.6240  Validation loss = 5.6679  \n",
      "\n",
      "Fold: 8  Epoch: 126  Training loss = 1.6238  Validation loss = 5.6679  \n",
      "\n",
      "Fold: 8  Epoch: 127  Training loss = 1.6236  Validation loss = 5.6685  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 120  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 2.1202  Validation loss = 8.9213  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 2.1198  Validation loss = 8.9196  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 2.1197  Validation loss = 8.9194  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 2.1196  Validation loss = 8.9192  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 2.1191  Validation loss = 8.9167  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 2.1188  Validation loss = 8.9154  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 2.1189  Validation loss = 8.9158  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 2.1187  Validation loss = 8.9151  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 2.1183  Validation loss = 8.9134  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 2.1179  Validation loss = 8.9107  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 2.1176  Validation loss = 8.9092  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 2.1173  Validation loss = 8.9079  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 2.1169  Validation loss = 8.9059  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 2.1167  Validation loss = 8.9048  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 2.1160  Validation loss = 8.9010  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 2.1156  Validation loss = 8.8992  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 2.1156  Validation loss = 8.8994  \n",
      "\n",
      "Fold: 9  Epoch: 18  Training loss = 2.1156  Validation loss = 8.9004  \n",
      "\n",
      "Fold: 9  Epoch: 19  Training loss = 2.1153  Validation loss = 8.8992  \n",
      "\n",
      "Fold: 9  Epoch: 20  Training loss = 2.1148  Validation loss = 8.8960  \n",
      "\n",
      "Fold: 9  Epoch: 21  Training loss = 2.1144  Validation loss = 8.8938  \n",
      "\n",
      "Fold: 9  Epoch: 22  Training loss = 2.1142  Validation loss = 8.8933  \n",
      "\n",
      "Fold: 9  Epoch: 23  Training loss = 2.1143  Validation loss = 8.8942  \n",
      "\n",
      "Fold: 9  Epoch: 24  Training loss = 2.1141  Validation loss = 8.8933  \n",
      "\n",
      "Fold: 9  Epoch: 25  Training loss = 2.1138  Validation loss = 8.8923  \n",
      "\n",
      "Fold: 9  Epoch: 26  Training loss = 2.1137  Validation loss = 8.8918  \n",
      "\n",
      "Fold: 9  Epoch: 27  Training loss = 2.1133  Validation loss = 8.8899  \n",
      "\n",
      "Fold: 9  Epoch: 28  Training loss = 2.1132  Validation loss = 8.8895  \n",
      "\n",
      "Fold: 9  Epoch: 29  Training loss = 2.1131  Validation loss = 8.8893  \n",
      "\n",
      "Fold: 9  Epoch: 30  Training loss = 2.1128  Validation loss = 8.8880  \n",
      "\n",
      "Fold: 9  Epoch: 31  Training loss = 2.1122  Validation loss = 8.8843  \n",
      "\n",
      "Fold: 9  Epoch: 32  Training loss = 2.1122  Validation loss = 8.8851  \n",
      "\n",
      "Fold: 9  Epoch: 33  Training loss = 2.1118  Validation loss = 8.8831  \n",
      "\n",
      "Fold: 9  Epoch: 34  Training loss = 2.1116  Validation loss = 8.8821  \n",
      "\n",
      "Fold: 9  Epoch: 35  Training loss = 2.1112  Validation loss = 8.8802  \n",
      "\n",
      "Fold: 9  Epoch: 36  Training loss = 2.1105  Validation loss = 8.8762  \n",
      "\n",
      "Fold: 9  Epoch: 37  Training loss = 2.1105  Validation loss = 8.8766  \n",
      "\n",
      "Fold: 9  Epoch: 38  Training loss = 2.1103  Validation loss = 8.8763  \n",
      "\n",
      "Fold: 9  Epoch: 39  Training loss = 2.1099  Validation loss = 8.8739  \n",
      "\n",
      "Fold: 9  Epoch: 40  Training loss = 2.1095  Validation loss = 8.8714  \n",
      "\n",
      "Fold: 9  Epoch: 41  Training loss = 2.1090  Validation loss = 8.8689  \n",
      "\n",
      "Fold: 9  Epoch: 42  Training loss = 2.1087  Validation loss = 8.8672  \n",
      "\n",
      "Fold: 9  Epoch: 43  Training loss = 2.1081  Validation loss = 8.8635  \n",
      "\n",
      "Fold: 9  Epoch: 44  Training loss = 2.1078  Validation loss = 8.8618  \n",
      "\n",
      "Fold: 9  Epoch: 45  Training loss = 2.1069  Validation loss = 8.8560  \n",
      "\n",
      "Fold: 9  Epoch: 46  Training loss = 2.1064  Validation loss = 8.8532  \n",
      "\n",
      "Fold: 9  Epoch: 47  Training loss = 2.1062  Validation loss = 8.8522  \n",
      "\n",
      "Fold: 9  Epoch: 48  Training loss = 2.1054  Validation loss = 8.8476  \n",
      "\n",
      "Fold: 9  Epoch: 49  Training loss = 2.1051  Validation loss = 8.8461  \n",
      "\n",
      "Fold: 9  Epoch: 50  Training loss = 2.1048  Validation loss = 8.8440  \n",
      "\n",
      "Fold: 9  Epoch: 51  Training loss = 2.1043  Validation loss = 8.8413  \n",
      "\n",
      "Fold: 9  Epoch: 52  Training loss = 2.1037  Validation loss = 8.8378  \n",
      "\n",
      "Fold: 9  Epoch: 53  Training loss = 2.1035  Validation loss = 8.8366  \n",
      "\n",
      "Fold: 9  Epoch: 54  Training loss = 2.1033  Validation loss = 8.8362  \n",
      "\n",
      "Fold: 9  Epoch: 55  Training loss = 2.1030  Validation loss = 8.8348  \n",
      "\n",
      "Fold: 9  Epoch: 56  Training loss = 2.1029  Validation loss = 8.8344  \n",
      "\n",
      "Fold: 9  Epoch: 57  Training loss = 2.1025  Validation loss = 8.8323  \n",
      "\n",
      "Fold: 9  Epoch: 58  Training loss = 2.1024  Validation loss = 8.8319  \n",
      "\n",
      "Fold: 9  Epoch: 59  Training loss = 2.1023  Validation loss = 8.8321  \n",
      "\n",
      "Fold: 9  Epoch: 60  Training loss = 2.1014  Validation loss = 8.8260  \n",
      "\n",
      "Fold: 9  Epoch: 61  Training loss = 2.1012  Validation loss = 8.8254  \n",
      "\n",
      "Fold: 9  Epoch: 62  Training loss = 2.1009  Validation loss = 8.8230  \n",
      "\n",
      "Fold: 9  Epoch: 63  Training loss = 2.1007  Validation loss = 8.8220  \n",
      "\n",
      "Fold: 9  Epoch: 64  Training loss = 2.1004  Validation loss = 8.8206  \n",
      "\n",
      "Fold: 9  Epoch: 65  Training loss = 2.1002  Validation loss = 8.8197  \n",
      "\n",
      "Fold: 9  Epoch: 66  Training loss = 2.0997  Validation loss = 8.8165  \n",
      "\n",
      "Fold: 9  Epoch: 67  Training loss = 2.0994  Validation loss = 8.8146  \n",
      "\n",
      "Fold: 9  Epoch: 68  Training loss = 2.0991  Validation loss = 8.8126  \n",
      "\n",
      "Fold: 9  Epoch: 69  Training loss = 2.0985  Validation loss = 8.8090  \n",
      "\n",
      "Fold: 9  Epoch: 70  Training loss = 2.0980  Validation loss = 8.8053  \n",
      "\n",
      "Fold: 9  Epoch: 71  Training loss = 2.0978  Validation loss = 8.8047  \n",
      "\n",
      "Fold: 9  Epoch: 72  Training loss = 2.0977  Validation loss = 8.8040  \n",
      "\n",
      "Fold: 9  Epoch: 73  Training loss = 2.0974  Validation loss = 8.8028  \n",
      "\n",
      "Fold: 9  Epoch: 74  Training loss = 2.0973  Validation loss = 8.8023  \n",
      "\n",
      "Fold: 9  Epoch: 75  Training loss = 2.0970  Validation loss = 8.8009  \n",
      "\n",
      "Fold: 9  Epoch: 76  Training loss = 2.0968  Validation loss = 8.7994  \n",
      "\n",
      "Fold: 9  Epoch: 77  Training loss = 2.0964  Validation loss = 8.7975  \n",
      "\n",
      "Fold: 9  Epoch: 78  Training loss = 2.0960  Validation loss = 8.7948  \n",
      "\n",
      "Fold: 9  Epoch: 79  Training loss = 2.0957  Validation loss = 8.7933  \n",
      "\n",
      "Fold: 9  Epoch: 80  Training loss = 2.0953  Validation loss = 8.7908  \n",
      "\n",
      "Fold: 9  Epoch: 81  Training loss = 2.0950  Validation loss = 8.7888  \n",
      "\n",
      "Fold: 9  Epoch: 82  Training loss = 2.0948  Validation loss = 8.7877  \n",
      "\n",
      "Fold: 9  Epoch: 83  Training loss = 2.0946  Validation loss = 8.7870  \n",
      "\n",
      "Fold: 9  Epoch: 84  Training loss = 2.0944  Validation loss = 8.7862  \n",
      "\n",
      "Fold: 9  Epoch: 85  Training loss = 2.0943  Validation loss = 8.7868  \n",
      "\n",
      "Fold: 9  Epoch: 86  Training loss = 2.0940  Validation loss = 8.7848  \n",
      "\n",
      "Fold: 9  Epoch: 87  Training loss = 2.0937  Validation loss = 8.7831  \n",
      "\n",
      "Fold: 9  Epoch: 88  Training loss = 2.0935  Validation loss = 8.7825  \n",
      "\n",
      "Fold: 9  Epoch: 89  Training loss = 2.0930  Validation loss = 8.7789  \n",
      "\n",
      "Fold: 9  Epoch: 90  Training loss = 2.0928  Validation loss = 8.7778  \n",
      "\n",
      "Fold: 9  Epoch: 91  Training loss = 2.0927  Validation loss = 8.7778  \n",
      "\n",
      "Fold: 9  Epoch: 92  Training loss = 2.0926  Validation loss = 8.7772  \n",
      "\n",
      "Fold: 9  Epoch: 93  Training loss = 2.0922  Validation loss = 8.7752  \n",
      "\n",
      "Fold: 9  Epoch: 94  Training loss = 2.0917  Validation loss = 8.7711  \n",
      "\n",
      "Fold: 9  Epoch: 95  Training loss = 2.0915  Validation loss = 8.7706  \n",
      "\n",
      "Fold: 9  Epoch: 96  Training loss = 2.0910  Validation loss = 8.7671  \n",
      "\n",
      "Fold: 9  Epoch: 97  Training loss = 2.0908  Validation loss = 8.7657  \n",
      "\n",
      "Fold: 9  Epoch: 98  Training loss = 2.0905  Validation loss = 8.7633  \n",
      "\n",
      "Fold: 9  Epoch: 99  Training loss = 2.0899  Validation loss = 8.7593  \n",
      "\n",
      "Fold: 9  Epoch: 100  Training loss = 2.0897  Validation loss = 8.7587  \n",
      "\n",
      "Fold: 9  Epoch: 101  Training loss = 2.0895  Validation loss = 8.7573  \n",
      "\n",
      "Fold: 9  Epoch: 102  Training loss = 2.0893  Validation loss = 8.7572  \n",
      "\n",
      "Fold: 9  Epoch: 103  Training loss = 2.0891  Validation loss = 8.7563  \n",
      "\n",
      "Fold: 9  Epoch: 104  Training loss = 2.0889  Validation loss = 8.7552  \n",
      "\n",
      "Fold: 9  Epoch: 105  Training loss = 2.0885  Validation loss = 8.7527  \n",
      "\n",
      "Fold: 9  Epoch: 106  Training loss = 2.0884  Validation loss = 8.7528  \n",
      "\n",
      "Fold: 9  Epoch: 107  Training loss = 2.0883  Validation loss = 8.7527  \n",
      "\n",
      "Fold: 9  Epoch: 108  Training loss = 2.0882  Validation loss = 8.7527  \n",
      "\n",
      "Fold: 9  Epoch: 109  Training loss = 2.0880  Validation loss = 8.7522  \n",
      "\n",
      "Fold: 9  Epoch: 110  Training loss = 2.0875  Validation loss = 8.7488  \n",
      "\n",
      "Fold: 9  Epoch: 111  Training loss = 2.0874  Validation loss = 8.7481  \n",
      "\n",
      "Fold: 9  Epoch: 112  Training loss = 2.0873  Validation loss = 8.7485  \n",
      "\n",
      "Fold: 9  Epoch: 113  Training loss = 2.0871  Validation loss = 8.7477  \n",
      "\n",
      "Fold: 9  Epoch: 114  Training loss = 2.0868  Validation loss = 8.7457  \n",
      "\n",
      "Fold: 9  Epoch: 115  Training loss = 2.0865  Validation loss = 8.7444  \n",
      "\n",
      "Fold: 9  Epoch: 116  Training loss = 2.0862  Validation loss = 8.7422  \n",
      "\n",
      "Fold: 9  Epoch: 117  Training loss = 2.0863  Validation loss = 8.7442  \n",
      "\n",
      "Fold: 9  Epoch: 118  Training loss = 2.0862  Validation loss = 8.7444  \n",
      "\n",
      "Fold: 9  Epoch: 119  Training loss = 2.0860  Validation loss = 8.7427  \n",
      "\n",
      "Fold: 9  Epoch: 120  Training loss = 2.0855  Validation loss = 8.7398  \n",
      "\n",
      "Fold: 9  Epoch: 121  Training loss = 2.0854  Validation loss = 8.7391  \n",
      "\n",
      "Fold: 9  Epoch: 122  Training loss = 2.0850  Validation loss = 8.7371  \n",
      "\n",
      "Fold: 9  Epoch: 123  Training loss = 2.0847  Validation loss = 8.7353  \n",
      "\n",
      "Fold: 9  Epoch: 124  Training loss = 2.0844  Validation loss = 8.7333  \n",
      "\n",
      "Fold: 9  Epoch: 125  Training loss = 2.0842  Validation loss = 8.7315  \n",
      "\n",
      "Fold: 9  Epoch: 126  Training loss = 2.0839  Validation loss = 8.7292  \n",
      "\n",
      "Fold: 9  Epoch: 127  Training loss = 2.0838  Validation loss = 8.7291  \n",
      "\n",
      "Fold: 9  Epoch: 128  Training loss = 2.0835  Validation loss = 8.7268  \n",
      "\n",
      "Fold: 9  Epoch: 129  Training loss = 2.0831  Validation loss = 8.7244  \n",
      "\n",
      "Fold: 9  Epoch: 130  Training loss = 2.0827  Validation loss = 8.7211  \n",
      "\n",
      "Fold: 9  Epoch: 131  Training loss = 2.0825  Validation loss = 8.7195  \n",
      "\n",
      "Fold: 9  Epoch: 132  Training loss = 2.0823  Validation loss = 8.7195  \n",
      "\n",
      "Fold: 9  Epoch: 133  Training loss = 2.0822  Validation loss = 8.7192  \n",
      "\n",
      "Fold: 9  Epoch: 134  Training loss = 2.0818  Validation loss = 8.7161  \n",
      "\n",
      "Fold: 9  Epoch: 135  Training loss = 2.0816  Validation loss = 8.7153  \n",
      "\n",
      "Fold: 9  Epoch: 136  Training loss = 2.0815  Validation loss = 8.7157  \n",
      "\n",
      "Fold: 9  Epoch: 137  Training loss = 2.0813  Validation loss = 8.7145  \n",
      "\n",
      "Fold: 9  Epoch: 138  Training loss = 2.0810  Validation loss = 8.7127  \n",
      "\n",
      "Fold: 9  Epoch: 139  Training loss = 2.0809  Validation loss = 8.7125  \n",
      "\n",
      "Fold: 9  Epoch: 140  Training loss = 2.0807  Validation loss = 8.7118  \n",
      "\n",
      "Fold: 9  Epoch: 141  Training loss = 2.0805  Validation loss = 8.7112  \n",
      "\n",
      "Fold: 9  Epoch: 142  Training loss = 2.0803  Validation loss = 8.7098  \n",
      "\n",
      "Fold: 9  Epoch: 143  Training loss = 2.0802  Validation loss = 8.7105  \n",
      "\n",
      "Fold: 9  Epoch: 144  Training loss = 2.0800  Validation loss = 8.7091  \n",
      "\n",
      "Fold: 9  Epoch: 145  Training loss = 2.0797  Validation loss = 8.7070  \n",
      "\n",
      "Fold: 9  Epoch: 146  Training loss = 2.0795  Validation loss = 8.7059  \n",
      "\n",
      "Fold: 9  Epoch: 147  Training loss = 2.0793  Validation loss = 8.7042  \n",
      "\n",
      "Fold: 9  Epoch: 148  Training loss = 2.0791  Validation loss = 8.7032  \n",
      "\n",
      "Fold: 9  Epoch: 149  Training loss = 2.0790  Validation loss = 8.7037  \n",
      "\n",
      "Fold: 9  Epoch: 150  Training loss = 2.0789  Validation loss = 8.7038  \n",
      "\n",
      "Fold: 9  Epoch: 151  Training loss = 2.0787  Validation loss = 8.7022  \n",
      "\n",
      "Fold: 9  Epoch: 152  Training loss = 2.0785  Validation loss = 8.7015  \n",
      "\n",
      "Fold: 9  Epoch: 153  Training loss = 2.0781  Validation loss = 8.6981  \n",
      "\n",
      "Fold: 9  Epoch: 154  Training loss = 2.0779  Validation loss = 8.6968  \n",
      "\n",
      "Fold: 9  Epoch: 155  Training loss = 2.0777  Validation loss = 8.6966  \n",
      "\n",
      "Fold: 9  Epoch: 156  Training loss = 2.0775  Validation loss = 8.6959  \n",
      "\n",
      "Fold: 9  Epoch: 157  Training loss = 2.0774  Validation loss = 8.6955  \n",
      "\n",
      "Fold: 9  Epoch: 158  Training loss = 2.0773  Validation loss = 8.6965  \n",
      "\n",
      "Fold: 9  Epoch: 159  Training loss = 2.0771  Validation loss = 8.6947  \n",
      "\n",
      "Fold: 9  Epoch: 160  Training loss = 2.0767  Validation loss = 8.6917  \n",
      "\n",
      "Fold: 9  Epoch: 161  Training loss = 2.0763  Validation loss = 8.6884  \n",
      "\n",
      "Fold: 9  Epoch: 162  Training loss = 2.0762  Validation loss = 8.6885  \n",
      "\n",
      "Fold: 9  Epoch: 163  Training loss = 2.0760  Validation loss = 8.6872  \n",
      "\n",
      "Fold: 9  Epoch: 164  Training loss = 2.0758  Validation loss = 8.6851  \n",
      "\n",
      "Fold: 9  Epoch: 165  Training loss = 2.0757  Validation loss = 8.6860  \n",
      "\n",
      "Fold: 9  Epoch: 166  Training loss = 2.0756  Validation loss = 8.6868  \n",
      "\n",
      "Fold: 9  Epoch: 167  Training loss = 2.0753  Validation loss = 8.6834  \n",
      "\n",
      "Fold: 9  Epoch: 168  Training loss = 2.0749  Validation loss = 8.6800  \n",
      "\n",
      "Fold: 9  Epoch: 169  Training loss = 2.0747  Validation loss = 8.6796  \n",
      "\n",
      "Fold: 9  Epoch: 170  Training loss = 2.0745  Validation loss = 8.6785  \n",
      "\n",
      "Fold: 9  Epoch: 171  Training loss = 2.0741  Validation loss = 8.6746  \n",
      "\n",
      "Fold: 9  Epoch: 172  Training loss = 2.0738  Validation loss = 8.6715  \n",
      "\n",
      "Fold: 9  Epoch: 173  Training loss = 2.0736  Validation loss = 8.6705  \n",
      "\n",
      "Fold: 9  Epoch: 174  Training loss = 2.0733  Validation loss = 8.6679  \n",
      "\n",
      "Fold: 9  Epoch: 175  Training loss = 2.0729  Validation loss = 8.6650  \n",
      "\n",
      "Fold: 9  Epoch: 176  Training loss = 2.0728  Validation loss = 8.6646  \n",
      "\n",
      "Fold: 9  Epoch: 177  Training loss = 2.0726  Validation loss = 8.6637  \n",
      "\n",
      "Fold: 9  Epoch: 178  Training loss = 2.0723  Validation loss = 8.6614  \n",
      "\n",
      "Fold: 9  Epoch: 179  Training loss = 2.0720  Validation loss = 8.6599  \n",
      "\n",
      "Fold: 9  Epoch: 180  Training loss = 2.0719  Validation loss = 8.6599  \n",
      "\n",
      "Fold: 9  Epoch: 181  Training loss = 2.0717  Validation loss = 8.6582  \n",
      "\n",
      "Fold: 9  Epoch: 182  Training loss = 2.0715  Validation loss = 8.6584  \n",
      "\n",
      "Fold: 9  Epoch: 183  Training loss = 2.0712  Validation loss = 8.6555  \n",
      "\n",
      "Fold: 9  Epoch: 184  Training loss = 2.0710  Validation loss = 8.6538  \n",
      "\n",
      "Fold: 9  Epoch: 185  Training loss = 2.0707  Validation loss = 8.6520  \n",
      "\n",
      "Fold: 9  Epoch: 186  Training loss = 2.0706  Validation loss = 8.6517  \n",
      "\n",
      "Fold: 9  Epoch: 187  Training loss = 2.0705  Validation loss = 8.6516  \n",
      "\n",
      "Fold: 9  Epoch: 188  Training loss = 2.0702  Validation loss = 8.6488  \n",
      "\n",
      "Fold: 9  Epoch: 189  Training loss = 2.0700  Validation loss = 8.6484  \n",
      "\n",
      "Fold: 9  Epoch: 190  Training loss = 2.0700  Validation loss = 8.6487  \n",
      "\n",
      "Fold: 9  Epoch: 191  Training loss = 2.0698  Validation loss = 8.6472  \n",
      "\n",
      "Fold: 9  Epoch: 192  Training loss = 2.0697  Validation loss = 8.6483  \n",
      "\n",
      "Fold: 9  Epoch: 193  Training loss = 2.0695  Validation loss = 8.6464  \n",
      "\n",
      "Fold: 9  Epoch: 194  Training loss = 2.0692  Validation loss = 8.6437  \n",
      "\n",
      "Fold: 9  Epoch: 195  Training loss = 2.0689  Validation loss = 8.6404  \n",
      "\n",
      "Fold: 9  Epoch: 196  Training loss = 2.0689  Validation loss = 8.6414  \n",
      "\n",
      "Fold: 9  Epoch: 197  Training loss = 2.0688  Validation loss = 8.6430  \n",
      "\n",
      "Fold: 9  Epoch: 198  Training loss = 2.0688  Validation loss = 8.6442  \n",
      "\n",
      "Fold: 9  Epoch: 199  Training loss = 2.0687  Validation loss = 8.6452  \n",
      "\n",
      "Fold: 9  Epoch: 200  Training loss = 2.0684  Validation loss = 8.6417  \n",
      "\n",
      "Fold: 9  Epoch: 201  Training loss = 2.0683  Validation loss = 8.6425  \n",
      "\n",
      "Fold: 9  Epoch: 202  Training loss = 2.0680  Validation loss = 8.6405  \n",
      "\n",
      "Fold: 9  Epoch: 203  Training loss = 2.0680  Validation loss = 8.6415  \n",
      "\n",
      "Fold: 9  Epoch: 204  Training loss = 2.0677  Validation loss = 8.6393  \n",
      "\n",
      "Fold: 9  Epoch: 205  Training loss = 2.0674  Validation loss = 8.6367  \n",
      "\n",
      "Fold: 9  Epoch: 206  Training loss = 2.0672  Validation loss = 8.6360  \n",
      "\n",
      "Fold: 9  Epoch: 207  Training loss = 2.0671  Validation loss = 8.6359  \n",
      "\n",
      "Fold: 9  Epoch: 208  Training loss = 2.0668  Validation loss = 8.6327  \n",
      "\n",
      "Fold: 9  Epoch: 209  Training loss = 2.0665  Validation loss = 8.6305  \n",
      "\n",
      "Fold: 9  Epoch: 210  Training loss = 2.0663  Validation loss = 8.6286  \n",
      "\n",
      "Fold: 9  Epoch: 211  Training loss = 2.0661  Validation loss = 8.6269  \n",
      "\n",
      "Fold: 9  Epoch: 212  Training loss = 2.0658  Validation loss = 8.6248  \n",
      "\n",
      "Fold: 9  Epoch: 213  Training loss = 2.0656  Validation loss = 8.6229  \n",
      "\n",
      "Fold: 9  Epoch: 214  Training loss = 2.0654  Validation loss = 8.6230  \n",
      "\n",
      "Fold: 9  Epoch: 215  Training loss = 2.0653  Validation loss = 8.6223  \n",
      "\n",
      "Fold: 9  Epoch: 216  Training loss = 2.0651  Validation loss = 8.6221  \n",
      "\n",
      "Fold: 9  Epoch: 217  Training loss = 2.0649  Validation loss = 8.6213  \n",
      "\n",
      "Fold: 9  Epoch: 218  Training loss = 2.0647  Validation loss = 8.6197  \n",
      "\n",
      "Fold: 9  Epoch: 219  Training loss = 2.0644  Validation loss = 8.6169  \n",
      "\n",
      "Fold: 9  Epoch: 220  Training loss = 2.0642  Validation loss = 8.6142  \n",
      "\n",
      "Fold: 9  Epoch: 221  Training loss = 2.0640  Validation loss = 8.6133  \n",
      "\n",
      "Fold: 9  Epoch: 222  Training loss = 2.0638  Validation loss = 8.6113  \n",
      "\n",
      "Fold: 9  Epoch: 223  Training loss = 2.0636  Validation loss = 8.6100  \n",
      "\n",
      "Fold: 9  Epoch: 224  Training loss = 2.0633  Validation loss = 8.6077  \n",
      "\n",
      "Fold: 9  Epoch: 225  Training loss = 2.0632  Validation loss = 8.6069  \n",
      "\n",
      "Fold: 9  Epoch: 226  Training loss = 2.0630  Validation loss = 8.6071  \n",
      "\n",
      "Fold: 9  Epoch: 227  Training loss = 2.0630  Validation loss = 8.6081  \n",
      "\n",
      "Fold: 9  Epoch: 228  Training loss = 2.0629  Validation loss = 8.6088  \n",
      "\n",
      "Fold: 9  Epoch: 229  Training loss = 2.0627  Validation loss = 8.6072  \n",
      "\n",
      "Fold: 9  Epoch: 230  Training loss = 2.0625  Validation loss = 8.6066  \n",
      "\n",
      "Fold: 9  Epoch: 231  Training loss = 2.0624  Validation loss = 8.6055  \n",
      "\n",
      "Fold: 9  Epoch: 232  Training loss = 2.0622  Validation loss = 8.6038  \n",
      "\n",
      "Fold: 9  Epoch: 233  Training loss = 2.0621  Validation loss = 8.6041  \n",
      "\n",
      "Fold: 9  Epoch: 234  Training loss = 2.0618  Validation loss = 8.6020  \n",
      "\n",
      "Fold: 9  Epoch: 235  Training loss = 2.0617  Validation loss = 8.6011  \n",
      "\n",
      "Fold: 9  Epoch: 236  Training loss = 2.0616  Validation loss = 8.6010  \n",
      "\n",
      "Fold: 9  Epoch: 237  Training loss = 2.0614  Validation loss = 8.6006  \n",
      "\n",
      "Fold: 9  Epoch: 238  Training loss = 2.0612  Validation loss = 8.5991  \n",
      "\n",
      "Fold: 9  Epoch: 239  Training loss = 2.0610  Validation loss = 8.5979  \n",
      "\n",
      "Fold: 9  Epoch: 240  Training loss = 2.0609  Validation loss = 8.5984  \n",
      "\n",
      "Fold: 9  Epoch: 241  Training loss = 2.0606  Validation loss = 8.5958  \n",
      "\n",
      "Fold: 9  Epoch: 242  Training loss = 2.0604  Validation loss = 8.5941  \n",
      "\n",
      "Fold: 9  Epoch: 243  Training loss = 2.0603  Validation loss = 8.5948  \n",
      "\n",
      "Fold: 9  Epoch: 244  Training loss = 2.0600  Validation loss = 8.5913  \n",
      "\n",
      "Fold: 9  Epoch: 245  Training loss = 2.0599  Validation loss = 8.5909  \n",
      "\n",
      "Fold: 9  Epoch: 246  Training loss = 2.0597  Validation loss = 8.5903  \n",
      "\n",
      "Fold: 9  Epoch: 247  Training loss = 2.0596  Validation loss = 8.5910  \n",
      "\n",
      "Fold: 9  Epoch: 248  Training loss = 2.0594  Validation loss = 8.5899  \n",
      "\n",
      "Fold: 9  Epoch: 249  Training loss = 2.0592  Validation loss = 8.5883  \n",
      "\n",
      "Fold: 9  Epoch: 250  Training loss = 2.0590  Validation loss = 8.5878  \n",
      "\n",
      "Fold: 9  Epoch: 251  Training loss = 2.0589  Validation loss = 8.5878  \n",
      "\n",
      "Fold: 9  Epoch: 252  Training loss = 2.0587  Validation loss = 8.5875  \n",
      "\n",
      "Fold: 9  Epoch: 253  Training loss = 2.0587  Validation loss = 8.5896  \n",
      "\n",
      "Fold: 9  Epoch: 254  Training loss = 2.0587  Validation loss = 8.5918  \n",
      "\n",
      "Fold: 9  Epoch: 255  Training loss = 2.0586  Validation loss = 8.5917  \n",
      "\n",
      "Fold: 9  Epoch: 256  Training loss = 2.0583  Validation loss = 8.5886  \n",
      "\n",
      "Fold: 9  Epoch: 257  Training loss = 2.0582  Validation loss = 8.5886  \n",
      "\n",
      "Fold: 9  Epoch: 258  Training loss = 2.0581  Validation loss = 8.5889  \n",
      "\n",
      "Fold: 9  Epoch: 259  Training loss = 2.0580  Validation loss = 8.5888  \n",
      "\n",
      "Fold: 9  Epoch: 260  Training loss = 2.0578  Validation loss = 8.5888  \n",
      "\n",
      "Fold: 9  Epoch: 261  Training loss = 2.0576  Validation loss = 8.5878  \n",
      "\n",
      "Fold: 9  Epoch: 262  Training loss = 2.0574  Validation loss = 8.5852  \n",
      "\n",
      "Fold: 9  Epoch: 263  Training loss = 2.0572  Validation loss = 8.5834  \n",
      "\n",
      "Fold: 9  Epoch: 264  Training loss = 2.0570  Validation loss = 8.5821  \n",
      "\n",
      "Fold: 9  Epoch: 265  Training loss = 2.0569  Validation loss = 8.5825  \n",
      "\n",
      "Fold: 9  Epoch: 266  Training loss = 2.0566  Validation loss = 8.5798  \n",
      "\n",
      "Fold: 9  Epoch: 267  Training loss = 2.0565  Validation loss = 8.5790  \n",
      "\n",
      "Fold: 9  Epoch: 268  Training loss = 2.0564  Validation loss = 8.5804  \n",
      "\n",
      "Fold: 9  Epoch: 269  Training loss = 2.0561  Validation loss = 8.5782  \n",
      "\n",
      "Fold: 9  Epoch: 270  Training loss = 2.0560  Validation loss = 8.5768  \n",
      "\n",
      "Fold: 9  Epoch: 271  Training loss = 2.0559  Validation loss = 8.5776  \n",
      "\n",
      "Fold: 9  Epoch: 272  Training loss = 2.0557  Validation loss = 8.5772  \n",
      "\n",
      "Fold: 9  Epoch: 273  Training loss = 2.0555  Validation loss = 8.5767  \n",
      "\n",
      "Fold: 9  Epoch: 274  Training loss = 2.0553  Validation loss = 8.5755  \n",
      "\n",
      "Fold: 9  Epoch: 275  Training loss = 2.0550  Validation loss = 8.5724  \n",
      "\n",
      "Fold: 9  Epoch: 276  Training loss = 2.0548  Validation loss = 8.5698  \n",
      "\n",
      "Fold: 9  Epoch: 277  Training loss = 2.0546  Validation loss = 8.5685  \n",
      "\n",
      "Fold: 9  Epoch: 278  Training loss = 2.0545  Validation loss = 8.5697  \n",
      "\n",
      "Fold: 9  Epoch: 279  Training loss = 2.0544  Validation loss = 8.5702  \n",
      "\n",
      "Fold: 9  Epoch: 280  Training loss = 2.0541  Validation loss = 8.5666  \n",
      "\n",
      "Fold: 9  Epoch: 281  Training loss = 2.0540  Validation loss = 8.5665  \n",
      "\n",
      "Fold: 9  Epoch: 282  Training loss = 2.0538  Validation loss = 8.5651  \n",
      "\n",
      "Fold: 9  Epoch: 283  Training loss = 2.0536  Validation loss = 8.5628  \n",
      "\n",
      "Fold: 9  Epoch: 284  Training loss = 2.0533  Validation loss = 8.5611  \n",
      "\n",
      "Fold: 9  Epoch: 285  Training loss = 2.0532  Validation loss = 8.5615  \n",
      "\n",
      "Fold: 9  Epoch: 286  Training loss = 2.0530  Validation loss = 8.5600  \n",
      "\n",
      "Fold: 9  Epoch: 287  Training loss = 2.0530  Validation loss = 8.5609  \n",
      "\n",
      "Fold: 9  Epoch: 288  Training loss = 2.0529  Validation loss = 8.5611  \n",
      "\n",
      "Fold: 9  Epoch: 289  Training loss = 2.0526  Validation loss = 8.5586  \n",
      "\n",
      "Fold: 9  Epoch: 290  Training loss = 2.0525  Validation loss = 8.5588  \n",
      "\n",
      "Fold: 9  Epoch: 291  Training loss = 2.0525  Validation loss = 8.5603  \n",
      "\n",
      "Fold: 9  Epoch: 292  Training loss = 2.0524  Validation loss = 8.5602  \n",
      "\n",
      "Fold: 9  Epoch: 293  Training loss = 2.0521  Validation loss = 8.5571  \n",
      "\n",
      "Fold: 9  Epoch: 294  Training loss = 2.0519  Validation loss = 8.5547  \n",
      "\n",
      "Fold: 9  Epoch: 295  Training loss = 2.0517  Validation loss = 8.5537  \n",
      "\n",
      "Fold: 9  Epoch: 296  Training loss = 2.0515  Validation loss = 8.5525  \n",
      "\n",
      "Fold: 9  Epoch: 297  Training loss = 2.0514  Validation loss = 8.5522  \n",
      "\n",
      "Fold: 9  Epoch: 298  Training loss = 2.0511  Validation loss = 8.5495  \n",
      "\n",
      "Fold: 9  Epoch: 299  Training loss = 2.0511  Validation loss = 8.5515  \n",
      "\n",
      "Fold: 9  Epoch: 300  Training loss = 2.0510  Validation loss = 8.5533  \n",
      "\n",
      "Fold: 9  Epoch: 301  Training loss = 2.0509  Validation loss = 8.5532  \n",
      "\n",
      "Fold: 9  Epoch: 302  Training loss = 2.0506  Validation loss = 8.5508  \n",
      "\n",
      "Fold: 9  Epoch: 303  Training loss = 2.0505  Validation loss = 8.5499  \n",
      "\n",
      "Fold: 9  Epoch: 304  Training loss = 2.0503  Validation loss = 8.5493  \n",
      "\n",
      "Fold: 9  Epoch: 305  Training loss = 2.0501  Validation loss = 8.5486  \n",
      "\n",
      "Fold: 9  Epoch: 306  Training loss = 2.0499  Validation loss = 8.5461  \n",
      "\n",
      "Fold: 9  Epoch: 307  Training loss = 2.0498  Validation loss = 8.5453  \n",
      "\n",
      "Fold: 9  Epoch: 308  Training loss = 2.0496  Validation loss = 8.5458  \n",
      "\n",
      "Fold: 9  Epoch: 309  Training loss = 2.0495  Validation loss = 8.5452  \n",
      "\n",
      "Fold: 9  Epoch: 310  Training loss = 2.0493  Validation loss = 8.5431  \n",
      "\n",
      "Fold: 9  Epoch: 311  Training loss = 2.0490  Validation loss = 8.5399  \n",
      "\n",
      "Fold: 9  Epoch: 312  Training loss = 2.0489  Validation loss = 8.5399  \n",
      "\n",
      "Fold: 9  Epoch: 313  Training loss = 2.0487  Validation loss = 8.5391  \n",
      "\n",
      "Fold: 9  Epoch: 314  Training loss = 2.0486  Validation loss = 8.5382  \n",
      "\n",
      "Fold: 9  Epoch: 315  Training loss = 2.0485  Validation loss = 8.5394  \n",
      "\n",
      "Fold: 9  Epoch: 316  Training loss = 2.0483  Validation loss = 8.5387  \n",
      "\n",
      "Fold: 9  Epoch: 317  Training loss = 2.0482  Validation loss = 8.5391  \n",
      "\n",
      "Fold: 9  Epoch: 318  Training loss = 2.0481  Validation loss = 8.5375  \n",
      "\n",
      "Fold: 9  Epoch: 319  Training loss = 2.0478  Validation loss = 8.5341  \n",
      "\n",
      "Fold: 9  Epoch: 320  Training loss = 2.0476  Validation loss = 8.5315  \n",
      "\n",
      "Fold: 9  Epoch: 321  Training loss = 2.0475  Validation loss = 8.5335  \n",
      "\n",
      "Fold: 9  Epoch: 322  Training loss = 2.0475  Validation loss = 8.5356  \n",
      "\n",
      "Fold: 9  Epoch: 323  Training loss = 2.0473  Validation loss = 8.5354  \n",
      "\n",
      "Fold: 9  Epoch: 324  Training loss = 2.0472  Validation loss = 8.5346  \n",
      "\n",
      "Fold: 9  Epoch: 325  Training loss = 2.0470  Validation loss = 8.5336  \n",
      "\n",
      "Fold: 9  Epoch: 326  Training loss = 2.0469  Validation loss = 8.5332  \n",
      "\n",
      "Fold: 9  Epoch: 327  Training loss = 2.0467  Validation loss = 8.5312  \n",
      "\n",
      "Fold: 9  Epoch: 328  Training loss = 2.0465  Validation loss = 8.5322  \n",
      "\n",
      "Fold: 9  Epoch: 329  Training loss = 2.0463  Validation loss = 8.5302  \n",
      "\n",
      "Fold: 9  Epoch: 330  Training loss = 2.0461  Validation loss = 8.5298  \n",
      "\n",
      "Fold: 9  Epoch: 331  Training loss = 2.0459  Validation loss = 8.5284  \n",
      "\n",
      "Fold: 9  Epoch: 332  Training loss = 2.0458  Validation loss = 8.5276  \n",
      "\n",
      "Fold: 9  Epoch: 333  Training loss = 2.0456  Validation loss = 8.5270  \n",
      "\n",
      "Fold: 9  Epoch: 334  Training loss = 2.0454  Validation loss = 8.5252  \n",
      "\n",
      "Fold: 9  Epoch: 335  Training loss = 2.0453  Validation loss = 8.5247  \n",
      "\n",
      "Fold: 9  Epoch: 336  Training loss = 2.0451  Validation loss = 8.5239  \n",
      "\n",
      "Fold: 9  Epoch: 337  Training loss = 2.0450  Validation loss = 8.5238  \n",
      "\n",
      "Fold: 9  Epoch: 338  Training loss = 2.0447  Validation loss = 8.5208  \n",
      "\n",
      "Fold: 9  Epoch: 339  Training loss = 2.0445  Validation loss = 8.5189  \n",
      "\n",
      "Fold: 9  Epoch: 340  Training loss = 2.0443  Validation loss = 8.5175  \n",
      "\n",
      "Fold: 9  Epoch: 341  Training loss = 2.0442  Validation loss = 8.5176  \n",
      "\n",
      "Fold: 9  Epoch: 342  Training loss = 2.0441  Validation loss = 8.5192  \n",
      "\n",
      "Fold: 9  Epoch: 343  Training loss = 2.0439  Validation loss = 8.5163  \n",
      "\n",
      "Fold: 9  Epoch: 344  Training loss = 2.0437  Validation loss = 8.5160  \n",
      "\n",
      "Fold: 9  Epoch: 345  Training loss = 2.0435  Validation loss = 8.5131  \n",
      "\n",
      "Fold: 9  Epoch: 346  Training loss = 2.0433  Validation loss = 8.5127  \n",
      "\n",
      "Fold: 9  Epoch: 347  Training loss = 2.0431  Validation loss = 8.5096  \n",
      "\n",
      "Fold: 9  Epoch: 348  Training loss = 2.0430  Validation loss = 8.5104  \n",
      "\n",
      "Fold: 9  Epoch: 349  Training loss = 2.0428  Validation loss = 8.5079  \n",
      "\n",
      "Fold: 9  Epoch: 350  Training loss = 2.0427  Validation loss = 8.5084  \n",
      "\n",
      "Fold: 9  Epoch: 351  Training loss = 2.0426  Validation loss = 8.5095  \n",
      "\n",
      "Fold: 9  Epoch: 352  Training loss = 2.0424  Validation loss = 8.5095  \n",
      "\n",
      "Fold: 9  Epoch: 353  Training loss = 2.0423  Validation loss = 8.5104  \n",
      "\n",
      "Fold: 9  Epoch: 354  Training loss = 2.0421  Validation loss = 8.5085  \n",
      "\n",
      "Fold: 9  Epoch: 355  Training loss = 2.0420  Validation loss = 8.5077  \n",
      "\n",
      "Fold: 9  Epoch: 356  Training loss = 2.0418  Validation loss = 8.5065  \n",
      "\n",
      "Fold: 9  Epoch: 357  Training loss = 2.0417  Validation loss = 8.5079  \n",
      "\n",
      "Fold: 9  Epoch: 358  Training loss = 2.0416  Validation loss = 8.5074  \n",
      "\n",
      "Fold: 9  Epoch: 359  Training loss = 2.0414  Validation loss = 8.5065  \n",
      "\n",
      "Fold: 9  Epoch: 360  Training loss = 2.0413  Validation loss = 8.5056  \n",
      "\n",
      "Fold: 9  Epoch: 361  Training loss = 2.0411  Validation loss = 8.5035  \n",
      "\n",
      "Fold: 9  Epoch: 362  Training loss = 2.0409  Validation loss = 8.5021  \n",
      "\n",
      "Fold: 9  Epoch: 363  Training loss = 2.0408  Validation loss = 8.5022  \n",
      "\n",
      "Fold: 9  Epoch: 364  Training loss = 2.0406  Validation loss = 8.5026  \n",
      "\n",
      "Fold: 9  Epoch: 365  Training loss = 2.0405  Validation loss = 8.5024  \n",
      "\n",
      "Fold: 9  Epoch: 366  Training loss = 2.0403  Validation loss = 8.5009  \n",
      "\n",
      "Fold: 9  Epoch: 367  Training loss = 2.0402  Validation loss = 8.5006  \n",
      "\n",
      "Fold: 9  Epoch: 368  Training loss = 2.0400  Validation loss = 8.4978  \n",
      "\n",
      "Fold: 9  Epoch: 369  Training loss = 2.0398  Validation loss = 8.4979  \n",
      "\n",
      "Fold: 9  Epoch: 370  Training loss = 2.0397  Validation loss = 8.4973  \n",
      "\n",
      "Fold: 9  Epoch: 371  Training loss = 2.0395  Validation loss = 8.4966  \n",
      "\n",
      "Fold: 9  Epoch: 372  Training loss = 2.0394  Validation loss = 8.4964  \n",
      "\n",
      "Fold: 9  Epoch: 373  Training loss = 2.0392  Validation loss = 8.4957  \n",
      "\n",
      "Fold: 9  Epoch: 374  Training loss = 2.0391  Validation loss = 8.4950  \n",
      "\n",
      "Fold: 9  Epoch: 375  Training loss = 2.0389  Validation loss = 8.4929  \n",
      "\n",
      "Fold: 9  Epoch: 376  Training loss = 2.0388  Validation loss = 8.4935  \n",
      "\n",
      "Fold: 9  Epoch: 377  Training loss = 2.0386  Validation loss = 8.4917  \n",
      "\n",
      "Fold: 9  Epoch: 378  Training loss = 2.0384  Validation loss = 8.4914  \n",
      "\n",
      "Fold: 9  Epoch: 379  Training loss = 2.0383  Validation loss = 8.4907  \n",
      "\n",
      "Fold: 9  Epoch: 380  Training loss = 2.0382  Validation loss = 8.4920  \n",
      "\n",
      "Fold: 9  Epoch: 381  Training loss = 2.0381  Validation loss = 8.4920  \n",
      "\n",
      "Fold: 9  Epoch: 382  Training loss = 2.0380  Validation loss = 8.4934  \n",
      "\n",
      "Fold: 9  Epoch: 383  Training loss = 2.0379  Validation loss = 8.4937  \n",
      "\n",
      "Fold: 9  Epoch: 384  Training loss = 2.0377  Validation loss = 8.4919  \n",
      "\n",
      "Fold: 9  Epoch: 385  Training loss = 2.0376  Validation loss = 8.4911  \n",
      "\n",
      "Fold: 9  Epoch: 386  Training loss = 2.0373  Validation loss = 8.4878  \n",
      "\n",
      "Fold: 9  Epoch: 387  Training loss = 2.0372  Validation loss = 8.4882  \n",
      "\n",
      "Fold: 9  Epoch: 388  Training loss = 2.0371  Validation loss = 8.4899  \n",
      "\n",
      "Fold: 9  Epoch: 389  Training loss = 2.0370  Validation loss = 8.4902  \n",
      "\n",
      "Fold: 9  Epoch: 390  Training loss = 2.0370  Validation loss = 8.4933  \n",
      "\n",
      "Fold: 9  Epoch: 391  Training loss = 2.0369  Validation loss = 8.4944  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 386  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 2.8941  Validation loss = 5.3235  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 2.8930  Validation loss = 5.3180  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 2.8926  Validation loss = 5.3150  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 2.8917  Validation loss = 5.3114  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 2.8906  Validation loss = 5.3062  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 2.8900  Validation loss = 5.3047  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 2.8885  Validation loss = 5.2973  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 2.8875  Validation loss = 5.2907  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 2.8862  Validation loss = 5.2851  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 2.8854  Validation loss = 5.2807  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 2.8846  Validation loss = 5.2764  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 2.8826  Validation loss = 5.2636  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 2.8812  Validation loss = 5.2553  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 2.8799  Validation loss = 5.2497  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 2.8794  Validation loss = 5.2467  \n",
      "\n",
      "Fold: 10  Epoch: 16  Training loss = 2.8784  Validation loss = 5.2410  \n",
      "\n",
      "Fold: 10  Epoch: 17  Training loss = 2.8766  Validation loss = 5.2319  \n",
      "\n",
      "Fold: 10  Epoch: 18  Training loss = 2.8751  Validation loss = 5.2237  \n",
      "\n",
      "Fold: 10  Epoch: 19  Training loss = 2.8746  Validation loss = 5.2223  \n",
      "\n",
      "Fold: 10  Epoch: 20  Training loss = 2.8731  Validation loss = 5.2117  \n",
      "\n",
      "Fold: 10  Epoch: 21  Training loss = 2.8724  Validation loss = 5.2075  \n",
      "\n",
      "Fold: 10  Epoch: 22  Training loss = 2.8708  Validation loss = 5.1979  \n",
      "\n",
      "Fold: 10  Epoch: 23  Training loss = 2.8691  Validation loss = 5.1864  \n",
      "\n",
      "Fold: 10  Epoch: 24  Training loss = 2.8677  Validation loss = 5.1781  \n",
      "\n",
      "Fold: 10  Epoch: 25  Training loss = 2.8655  Validation loss = 5.1607  \n",
      "\n",
      "Fold: 10  Epoch: 26  Training loss = 2.8654  Validation loss = 5.1603  \n",
      "\n",
      "Fold: 10  Epoch: 27  Training loss = 2.8645  Validation loss = 5.1551  \n",
      "\n",
      "Fold: 10  Epoch: 28  Training loss = 2.8637  Validation loss = 5.1531  \n",
      "\n",
      "Fold: 10  Epoch: 29  Training loss = 2.8629  Validation loss = 5.1474  \n",
      "\n",
      "Fold: 10  Epoch: 30  Training loss = 2.8619  Validation loss = 5.1412  \n",
      "\n",
      "Fold: 10  Epoch: 31  Training loss = 2.8606  Validation loss = 5.1334  \n",
      "\n",
      "Fold: 10  Epoch: 32  Training loss = 2.8597  Validation loss = 5.1285  \n",
      "\n",
      "Fold: 10  Epoch: 33  Training loss = 2.8587  Validation loss = 5.1228  \n",
      "\n",
      "Fold: 10  Epoch: 34  Training loss = 2.8572  Validation loss = 5.1178  \n",
      "\n",
      "Fold: 10  Epoch: 35  Training loss = 2.8562  Validation loss = 5.1138  \n",
      "\n",
      "Fold: 10  Epoch: 36  Training loss = 2.8560  Validation loss = 5.1125  \n",
      "\n",
      "Fold: 10  Epoch: 37  Training loss = 2.8555  Validation loss = 5.1088  \n",
      "\n",
      "Fold: 10  Epoch: 38  Training loss = 2.8545  Validation loss = 5.1036  \n",
      "\n",
      "Fold: 10  Epoch: 39  Training loss = 2.8536  Validation loss = 5.0995  \n",
      "\n",
      "Fold: 10  Epoch: 40  Training loss = 2.8528  Validation loss = 5.0952  \n",
      "\n",
      "Fold: 10  Epoch: 41  Training loss = 2.8522  Validation loss = 5.0913  \n",
      "\n",
      "Fold: 10  Epoch: 42  Training loss = 2.8509  Validation loss = 5.0858  \n",
      "\n",
      "Fold: 10  Epoch: 43  Training loss = 2.8500  Validation loss = 5.0802  \n",
      "\n",
      "Fold: 10  Epoch: 44  Training loss = 2.8491  Validation loss = 5.0754  \n",
      "\n",
      "Fold: 10  Epoch: 45  Training loss = 2.8486  Validation loss = 5.0728  \n",
      "\n",
      "Fold: 10  Epoch: 46  Training loss = 2.8481  Validation loss = 5.0694  \n",
      "\n",
      "Fold: 10  Epoch: 47  Training loss = 2.8480  Validation loss = 5.0691  \n",
      "\n",
      "Fold: 10  Epoch: 48  Training loss = 2.8474  Validation loss = 5.0661  \n",
      "\n",
      "Fold: 10  Epoch: 49  Training loss = 2.8463  Validation loss = 5.0624  \n",
      "\n",
      "Fold: 10  Epoch: 50  Training loss = 2.8458  Validation loss = 5.0602  \n",
      "\n",
      "Fold: 10  Epoch: 51  Training loss = 2.8451  Validation loss = 5.0572  \n",
      "\n",
      "Fold: 10  Epoch: 52  Training loss = 2.8442  Validation loss = 5.0538  \n",
      "\n",
      "Fold: 10  Epoch: 53  Training loss = 2.8438  Validation loss = 5.0503  \n",
      "\n",
      "Fold: 10  Epoch: 54  Training loss = 2.8424  Validation loss = 5.0412  \n",
      "\n",
      "Fold: 10  Epoch: 55  Training loss = 2.8418  Validation loss = 5.0395  \n",
      "\n",
      "Fold: 10  Epoch: 56  Training loss = 2.8408  Validation loss = 5.0362  \n",
      "\n",
      "Fold: 10  Epoch: 57  Training loss = 2.8398  Validation loss = 5.0302  \n",
      "\n",
      "Fold: 10  Epoch: 58  Training loss = 2.8395  Validation loss = 5.0289  \n",
      "\n",
      "Fold: 10  Epoch: 59  Training loss = 2.8384  Validation loss = 5.0213  \n",
      "\n",
      "Fold: 10  Epoch: 60  Training loss = 2.8378  Validation loss = 5.0186  \n",
      "\n",
      "Fold: 10  Epoch: 61  Training loss = 2.8366  Validation loss = 5.0126  \n",
      "\n",
      "Fold: 10  Epoch: 62  Training loss = 2.8357  Validation loss = 5.0089  \n",
      "\n",
      "Fold: 10  Epoch: 63  Training loss = 2.8345  Validation loss = 5.0043  \n",
      "\n",
      "Fold: 10  Epoch: 64  Training loss = 2.8337  Validation loss = 5.0009  \n",
      "\n",
      "Fold: 10  Epoch: 65  Training loss = 2.8327  Validation loss = 4.9959  \n",
      "\n",
      "Fold: 10  Epoch: 66  Training loss = 2.8319  Validation loss = 4.9920  \n",
      "\n",
      "Fold: 10  Epoch: 67  Training loss = 2.8309  Validation loss = 4.9859  \n",
      "\n",
      "Fold: 10  Epoch: 68  Training loss = 2.8297  Validation loss = 4.9807  \n",
      "\n",
      "Fold: 10  Epoch: 69  Training loss = 2.8292  Validation loss = 4.9777  \n",
      "\n",
      "Fold: 10  Epoch: 70  Training loss = 2.8284  Validation loss = 4.9722  \n",
      "\n",
      "Fold: 10  Epoch: 71  Training loss = 2.8281  Validation loss = 4.9717  \n",
      "\n",
      "Fold: 10  Epoch: 72  Training loss = 2.8274  Validation loss = 4.9680  \n",
      "\n",
      "Fold: 10  Epoch: 73  Training loss = 2.8270  Validation loss = 4.9673  \n",
      "\n",
      "Fold: 10  Epoch: 74  Training loss = 2.8259  Validation loss = 4.9599  \n",
      "\n",
      "Fold: 10  Epoch: 75  Training loss = 2.8257  Validation loss = 4.9578  \n",
      "\n",
      "Fold: 10  Epoch: 76  Training loss = 2.8247  Validation loss = 4.9504  \n",
      "\n",
      "Fold: 10  Epoch: 77  Training loss = 2.8240  Validation loss = 4.9459  \n",
      "\n",
      "Fold: 10  Epoch: 78  Training loss = 2.8236  Validation loss = 4.9442  \n",
      "\n",
      "Fold: 10  Epoch: 79  Training loss = 2.8226  Validation loss = 4.9382  \n",
      "\n",
      "Fold: 10  Epoch: 80  Training loss = 2.8221  Validation loss = 4.9363  \n",
      "\n",
      "Fold: 10  Epoch: 81  Training loss = 2.8212  Validation loss = 4.9319  \n",
      "\n",
      "Fold: 10  Epoch: 82  Training loss = 2.8206  Validation loss = 4.9260  \n",
      "\n",
      "Fold: 10  Epoch: 83  Training loss = 2.8202  Validation loss = 4.9250  \n",
      "\n",
      "Fold: 10  Epoch: 84  Training loss = 2.8193  Validation loss = 4.9193  \n",
      "\n",
      "Fold: 10  Epoch: 85  Training loss = 2.8186  Validation loss = 4.9167  \n",
      "\n",
      "Fold: 10  Epoch: 86  Training loss = 2.8183  Validation loss = 4.9149  \n",
      "\n",
      "Fold: 10  Epoch: 87  Training loss = 2.8172  Validation loss = 4.9063  \n",
      "\n",
      "Fold: 10  Epoch: 88  Training loss = 2.8152  Validation loss = 4.8924  \n",
      "\n",
      "Fold: 10  Epoch: 89  Training loss = 2.8147  Validation loss = 4.8874  \n",
      "\n",
      "Fold: 10  Epoch: 90  Training loss = 2.8142  Validation loss = 4.8820  \n",
      "\n",
      "Fold: 10  Epoch: 91  Training loss = 2.8134  Validation loss = 4.8765  \n",
      "\n",
      "Fold: 10  Epoch: 92  Training loss = 2.8126  Validation loss = 4.8721  \n",
      "\n",
      "Fold: 10  Epoch: 93  Training loss = 2.8120  Validation loss = 4.8665  \n",
      "\n",
      "Fold: 10  Epoch: 94  Training loss = 2.8117  Validation loss = 4.8662  \n",
      "\n",
      "Fold: 10  Epoch: 95  Training loss = 2.8109  Validation loss = 4.8618  \n",
      "\n",
      "Fold: 10  Epoch: 96  Training loss = 2.8102  Validation loss = 4.8564  \n",
      "\n",
      "Fold: 10  Epoch: 97  Training loss = 2.8094  Validation loss = 4.8496  \n",
      "\n",
      "Fold: 10  Epoch: 98  Training loss = 2.8092  Validation loss = 4.8494  \n",
      "\n",
      "Fold: 10  Epoch: 99  Training loss = 2.8087  Validation loss = 4.8460  \n",
      "\n",
      "Fold: 10  Epoch: 100  Training loss = 2.8084  Validation loss = 4.8439  \n",
      "\n",
      "Fold: 10  Epoch: 101  Training loss = 2.8080  Validation loss = 4.8392  \n",
      "\n",
      "Fold: 10  Epoch: 102  Training loss = 2.8074  Validation loss = 4.8368  \n",
      "\n",
      "Fold: 10  Epoch: 103  Training loss = 2.8067  Validation loss = 4.8313  \n",
      "\n",
      "Fold: 10  Epoch: 104  Training loss = 2.8064  Validation loss = 4.8310  \n",
      "\n",
      "Fold: 10  Epoch: 105  Training loss = 2.8057  Validation loss = 4.8278  \n",
      "\n",
      "Fold: 10  Epoch: 106  Training loss = 2.8050  Validation loss = 4.8247  \n",
      "\n",
      "Fold: 10  Epoch: 107  Training loss = 2.8044  Validation loss = 4.8214  \n",
      "\n",
      "Fold: 10  Epoch: 108  Training loss = 2.8040  Validation loss = 4.8195  \n",
      "\n",
      "Fold: 10  Epoch: 109  Training loss = 2.8037  Validation loss = 4.8189  \n",
      "\n",
      "Fold: 10  Epoch: 110  Training loss = 2.8031  Validation loss = 4.8166  \n",
      "\n",
      "Fold: 10  Epoch: 111  Training loss = 2.8021  Validation loss = 4.8101  \n",
      "\n",
      "Fold: 10  Epoch: 112  Training loss = 2.8012  Validation loss = 4.8055  \n",
      "\n",
      "Fold: 10  Epoch: 113  Training loss = 2.8007  Validation loss = 4.8023  \n",
      "\n",
      "Fold: 10  Epoch: 114  Training loss = 2.7998  Validation loss = 4.7974  \n",
      "\n",
      "Fold: 10  Epoch: 115  Training loss = 2.7988  Validation loss = 4.7922  \n",
      "\n",
      "Fold: 10  Epoch: 116  Training loss = 2.7979  Validation loss = 4.7875  \n",
      "\n",
      "Fold: 10  Epoch: 117  Training loss = 2.7976  Validation loss = 4.7859  \n",
      "\n",
      "Fold: 10  Epoch: 118  Training loss = 2.7972  Validation loss = 4.7841  \n",
      "\n",
      "Fold: 10  Epoch: 119  Training loss = 2.7969  Validation loss = 4.7844  \n",
      "\n",
      "Fold: 10  Epoch: 120  Training loss = 2.7966  Validation loss = 4.7832  \n",
      "\n",
      "Fold: 10  Epoch: 121  Training loss = 2.7960  Validation loss = 4.7795  \n",
      "\n",
      "Fold: 10  Epoch: 122  Training loss = 2.7953  Validation loss = 4.7759  \n",
      "\n",
      "Fold: 10  Epoch: 123  Training loss = 2.7949  Validation loss = 4.7734  \n",
      "\n",
      "Fold: 10  Epoch: 124  Training loss = 2.7946  Validation loss = 4.7720  \n",
      "\n",
      "Fold: 10  Epoch: 125  Training loss = 2.7942  Validation loss = 4.7723  \n",
      "\n",
      "Fold: 10  Epoch: 126  Training loss = 2.7943  Validation loss = 4.7740  \n",
      "\n",
      "Fold: 10  Epoch: 127  Training loss = 2.7933  Validation loss = 4.7679  \n",
      "\n",
      "Fold: 10  Epoch: 128  Training loss = 2.7927  Validation loss = 4.7639  \n",
      "\n",
      "Fold: 10  Epoch: 129  Training loss = 2.7917  Validation loss = 4.7583  \n",
      "\n",
      "Fold: 10  Epoch: 130  Training loss = 2.7912  Validation loss = 4.7564  \n",
      "\n",
      "Fold: 10  Epoch: 131  Training loss = 2.7908  Validation loss = 4.7547  \n",
      "\n",
      "Fold: 10  Epoch: 132  Training loss = 2.7897  Validation loss = 4.7484  \n",
      "\n",
      "Fold: 10  Epoch: 133  Training loss = 2.7894  Validation loss = 4.7492  \n",
      "\n",
      "Fold: 10  Epoch: 134  Training loss = 2.7890  Validation loss = 4.7466  \n",
      "\n",
      "Fold: 10  Epoch: 135  Training loss = 2.7880  Validation loss = 4.7412  \n",
      "\n",
      "Fold: 10  Epoch: 136  Training loss = 2.7874  Validation loss = 4.7385  \n",
      "\n",
      "Fold: 10  Epoch: 137  Training loss = 2.7868  Validation loss = 4.7359  \n",
      "\n",
      "Fold: 10  Epoch: 138  Training loss = 2.7865  Validation loss = 4.7349  \n",
      "\n",
      "Fold: 10  Epoch: 139  Training loss = 2.7860  Validation loss = 4.7316  \n",
      "\n",
      "Fold: 10  Epoch: 140  Training loss = 2.7855  Validation loss = 4.7302  \n",
      "\n",
      "Fold: 10  Epoch: 141  Training loss = 2.7849  Validation loss = 4.7260  \n",
      "\n",
      "Fold: 10  Epoch: 142  Training loss = 2.7844  Validation loss = 4.7242  \n",
      "\n",
      "Fold: 10  Epoch: 143  Training loss = 2.7840  Validation loss = 4.7224  \n",
      "\n",
      "Fold: 10  Epoch: 144  Training loss = 2.7832  Validation loss = 4.7183  \n",
      "\n",
      "Fold: 10  Epoch: 145  Training loss = 2.7831  Validation loss = 4.7192  \n",
      "\n",
      "Fold: 10  Epoch: 146  Training loss = 2.7830  Validation loss = 4.7185  \n",
      "\n",
      "Fold: 10  Epoch: 147  Training loss = 2.7828  Validation loss = 4.7183  \n",
      "\n",
      "Fold: 10  Epoch: 148  Training loss = 2.7829  Validation loss = 4.7192  \n",
      "\n",
      "Fold: 10  Epoch: 149  Training loss = 2.7826  Validation loss = 4.7188  \n",
      "\n",
      "Fold: 10  Epoch: 150  Training loss = 2.7818  Validation loss = 4.7146  \n",
      "\n",
      "Fold: 10  Epoch: 151  Training loss = 2.7816  Validation loss = 4.7140  \n",
      "\n",
      "Fold: 10  Epoch: 152  Training loss = 2.7812  Validation loss = 4.7109  \n",
      "\n",
      "Fold: 10  Epoch: 153  Training loss = 2.7805  Validation loss = 4.7072  \n",
      "\n",
      "Fold: 10  Epoch: 154  Training loss = 2.7804  Validation loss = 4.7085  \n",
      "\n",
      "Fold: 10  Epoch: 155  Training loss = 2.7801  Validation loss = 4.7073  \n",
      "\n",
      "Fold: 10  Epoch: 156  Training loss = 2.7799  Validation loss = 4.7073  \n",
      "\n",
      "Fold: 10  Epoch: 157  Training loss = 2.7796  Validation loss = 4.7071  \n",
      "\n",
      "Fold: 10  Epoch: 158  Training loss = 2.7788  Validation loss = 4.7030  \n",
      "\n",
      "Fold: 10  Epoch: 159  Training loss = 2.7787  Validation loss = 4.7035  \n",
      "\n",
      "Fold: 10  Epoch: 160  Training loss = 2.7781  Validation loss = 4.7001  \n",
      "\n",
      "Fold: 10  Epoch: 161  Training loss = 2.7772  Validation loss = 4.6945  \n",
      "\n",
      "Fold: 10  Epoch: 162  Training loss = 2.7764  Validation loss = 4.6914  \n",
      "\n",
      "Fold: 10  Epoch: 163  Training loss = 2.7757  Validation loss = 4.6871  \n",
      "\n",
      "Fold: 10  Epoch: 164  Training loss = 2.7751  Validation loss = 4.6836  \n",
      "\n",
      "Fold: 10  Epoch: 165  Training loss = 2.7749  Validation loss = 4.6812  \n",
      "\n",
      "Fold: 10  Epoch: 166  Training loss = 2.7740  Validation loss = 4.6761  \n",
      "\n",
      "Fold: 10  Epoch: 167  Training loss = 2.7736  Validation loss = 4.6736  \n",
      "\n",
      "Fold: 10  Epoch: 168  Training loss = 2.7736  Validation loss = 4.6755  \n",
      "\n",
      "Fold: 10  Epoch: 169  Training loss = 2.7734  Validation loss = 4.6753  \n",
      "\n",
      "Fold: 10  Epoch: 170  Training loss = 2.7728  Validation loss = 4.6723  \n",
      "\n",
      "Fold: 10  Epoch: 171  Training loss = 2.7723  Validation loss = 4.6709  \n",
      "\n",
      "Fold: 10  Epoch: 172  Training loss = 2.7714  Validation loss = 4.6634  \n",
      "\n",
      "Fold: 10  Epoch: 173  Training loss = 2.7709  Validation loss = 4.6611  \n",
      "\n",
      "Fold: 10  Epoch: 174  Training loss = 2.7704  Validation loss = 4.6587  \n",
      "\n",
      "Fold: 10  Epoch: 175  Training loss = 2.7699  Validation loss = 4.6550  \n",
      "\n",
      "Fold: 10  Epoch: 176  Training loss = 2.7695  Validation loss = 4.6521  \n",
      "\n",
      "Fold: 10  Epoch: 177  Training loss = 2.7689  Validation loss = 4.6473  \n",
      "\n",
      "Fold: 10  Epoch: 178  Training loss = 2.7686  Validation loss = 4.6466  \n",
      "\n",
      "Fold: 10  Epoch: 179  Training loss = 2.7680  Validation loss = 4.6420  \n",
      "\n",
      "Fold: 10  Epoch: 180  Training loss = 2.7673  Validation loss = 4.6356  \n",
      "\n",
      "Fold: 10  Epoch: 181  Training loss = 2.7672  Validation loss = 4.6356  \n",
      "\n",
      "Fold: 10  Epoch: 182  Training loss = 2.7665  Validation loss = 4.6309  \n",
      "\n",
      "Fold: 10  Epoch: 183  Training loss = 2.7661  Validation loss = 4.6248  \n",
      "\n",
      "Fold: 10  Epoch: 184  Training loss = 2.7655  Validation loss = 4.6170  \n",
      "\n",
      "Fold: 10  Epoch: 185  Training loss = 2.7647  Validation loss = 4.6065  \n",
      "\n",
      "Fold: 10  Epoch: 186  Training loss = 2.7644  Validation loss = 4.6090  \n",
      "\n",
      "Fold: 10  Epoch: 187  Training loss = 2.7634  Validation loss = 4.5932  \n",
      "\n",
      "Fold: 10  Epoch: 188  Training loss = 2.7631  Validation loss = 4.5893  \n",
      "\n",
      "Fold: 10  Epoch: 189  Training loss = 2.7624  Validation loss = 4.5801  \n",
      "\n",
      "Fold: 10  Epoch: 190  Training loss = 2.7617  Validation loss = 4.5636  \n",
      "\n",
      "Fold: 10  Epoch: 191  Training loss = 2.7615  Validation loss = 4.5665  \n",
      "\n",
      "Fold: 10  Epoch: 192  Training loss = 2.7611  Validation loss = 4.5612  \n",
      "\n",
      "Fold: 10  Epoch: 193  Training loss = 2.7611  Validation loss = 4.5638  \n",
      "\n",
      "Fold: 10  Epoch: 194  Training loss = 2.7605  Validation loss = 4.5568  \n",
      "\n",
      "Fold: 10  Epoch: 195  Training loss = 2.7602  Validation loss = 4.5573  \n",
      "\n",
      "Fold: 10  Epoch: 196  Training loss = 2.7599  Validation loss = 4.5549  \n",
      "\n",
      "Fold: 10  Epoch: 197  Training loss = 2.7596  Validation loss = 4.5525  \n",
      "\n",
      "Fold: 10  Epoch: 198  Training loss = 2.7590  Validation loss = 4.5476  \n",
      "\n",
      "Fold: 10  Epoch: 199  Training loss = 2.7585  Validation loss = 4.5457  \n",
      "\n",
      "Fold: 10  Epoch: 200  Training loss = 2.7583  Validation loss = 4.5446  \n",
      "\n",
      "Fold: 10  Epoch: 201  Training loss = 2.7579  Validation loss = 4.5426  \n",
      "\n",
      "Fold: 10  Epoch: 202  Training loss = 2.7577  Validation loss = 4.5441  \n",
      "\n",
      "Fold: 10  Epoch: 203  Training loss = 2.7571  Validation loss = 4.5379  \n",
      "\n",
      "Fold: 10  Epoch: 204  Training loss = 2.7569  Validation loss = 4.5366  \n",
      "\n",
      "Fold: 10  Epoch: 205  Training loss = 2.7565  Validation loss = 4.5336  \n",
      "\n",
      "Fold: 10  Epoch: 206  Training loss = 2.7559  Validation loss = 4.5260  \n",
      "\n",
      "Fold: 10  Epoch: 207  Training loss = 2.7550  Validation loss = 4.5195  \n",
      "\n",
      "Fold: 10  Epoch: 208  Training loss = 2.7542  Validation loss = 4.5143  \n",
      "\n",
      "Fold: 10  Epoch: 209  Training loss = 2.7540  Validation loss = 4.5135  \n",
      "\n",
      "Fold: 10  Epoch: 210  Training loss = 2.7534  Validation loss = 4.5104  \n",
      "\n",
      "Fold: 10  Epoch: 211  Training loss = 2.7531  Validation loss = 4.5087  \n",
      "\n",
      "Fold: 10  Epoch: 212  Training loss = 2.7526  Validation loss = 4.5057  \n",
      "\n",
      "Fold: 10  Epoch: 213  Training loss = 2.7522  Validation loss = 4.5048  \n",
      "\n",
      "Fold: 10  Epoch: 214  Training loss = 2.7518  Validation loss = 4.5044  \n",
      "\n",
      "Fold: 10  Epoch: 215  Training loss = 2.7515  Validation loss = 4.5032  \n",
      "\n",
      "Fold: 10  Epoch: 216  Training loss = 2.7507  Validation loss = 4.4962  \n",
      "\n",
      "Fold: 10  Epoch: 217  Training loss = 2.7500  Validation loss = 4.4916  \n",
      "\n",
      "Fold: 10  Epoch: 218  Training loss = 2.7494  Validation loss = 4.4872  \n",
      "\n",
      "Fold: 10  Epoch: 219  Training loss = 2.7486  Validation loss = 4.4797  \n",
      "\n",
      "Fold: 10  Epoch: 220  Training loss = 2.7483  Validation loss = 4.4777  \n",
      "\n",
      "Fold: 10  Epoch: 221  Training loss = 2.7481  Validation loss = 4.4778  \n",
      "\n",
      "Fold: 10  Epoch: 222  Training loss = 2.7475  Validation loss = 4.4719  \n",
      "\n",
      "Fold: 10  Epoch: 223  Training loss = 2.7466  Validation loss = 4.4653  \n",
      "\n",
      "Fold: 10  Epoch: 224  Training loss = 2.7460  Validation loss = 4.4608  \n",
      "\n",
      "Fold: 10  Epoch: 225  Training loss = 2.7458  Validation loss = 4.4596  \n",
      "\n",
      "Fold: 10  Epoch: 226  Training loss = 2.7455  Validation loss = 4.4604  \n",
      "\n",
      "Fold: 10  Epoch: 227  Training loss = 2.7451  Validation loss = 4.4579  \n",
      "\n",
      "Fold: 10  Epoch: 228  Training loss = 2.7446  Validation loss = 4.4532  \n",
      "\n",
      "Fold: 10  Epoch: 229  Training loss = 2.7437  Validation loss = 4.4464  \n",
      "\n",
      "Fold: 10  Epoch: 230  Training loss = 2.7430  Validation loss = 4.4388  \n",
      "\n",
      "Fold: 10  Epoch: 231  Training loss = 2.7421  Validation loss = 4.4317  \n",
      "\n",
      "Fold: 10  Epoch: 232  Training loss = 2.7415  Validation loss = 4.4267  \n",
      "\n",
      "Fold: 10  Epoch: 233  Training loss = 2.7415  Validation loss = 4.4311  \n",
      "\n",
      "Fold: 10  Epoch: 234  Training loss = 2.7412  Validation loss = 4.4305  \n",
      "\n",
      "Fold: 10  Epoch: 235  Training loss = 2.7408  Validation loss = 4.4282  \n",
      "\n",
      "Fold: 10  Epoch: 236  Training loss = 2.7401  Validation loss = 4.4209  \n",
      "\n",
      "Fold: 10  Epoch: 237  Training loss = 2.7392  Validation loss = 4.4142  \n",
      "\n",
      "Fold: 10  Epoch: 238  Training loss = 2.7388  Validation loss = 4.4119  \n",
      "\n",
      "Fold: 10  Epoch: 239  Training loss = 2.7382  Validation loss = 4.4054  \n",
      "\n",
      "Fold: 10  Epoch: 240  Training loss = 2.7379  Validation loss = 4.4062  \n",
      "\n",
      "Fold: 10  Epoch: 241  Training loss = 2.7370  Validation loss = 4.3974  \n",
      "\n",
      "Fold: 10  Epoch: 242  Training loss = 2.7369  Validation loss = 4.3997  \n",
      "\n",
      "Fold: 10  Epoch: 243  Training loss = 2.7368  Validation loss = 4.3999  \n",
      "\n",
      "Fold: 10  Epoch: 244  Training loss = 2.7366  Validation loss = 4.4007  \n",
      "\n",
      "Fold: 10  Epoch: 245  Training loss = 2.7361  Validation loss = 4.3995  \n",
      "\n",
      "Fold: 10  Epoch: 246  Training loss = 2.7357  Validation loss = 4.3972  \n",
      "\n",
      "Fold: 10  Epoch: 247  Training loss = 2.7354  Validation loss = 4.3959  \n",
      "\n",
      "Fold: 10  Epoch: 248  Training loss = 2.7347  Validation loss = 4.3904  \n",
      "\n",
      "Fold: 10  Epoch: 249  Training loss = 2.7341  Validation loss = 4.3853  \n",
      "\n",
      "Fold: 10  Epoch: 250  Training loss = 2.7335  Validation loss = 4.3813  \n",
      "\n",
      "Fold: 10  Epoch: 251  Training loss = 2.7328  Validation loss = 4.3738  \n",
      "\n",
      "Fold: 10  Epoch: 252  Training loss = 2.7319  Validation loss = 4.3675  \n",
      "\n",
      "Fold: 10  Epoch: 253  Training loss = 2.7319  Validation loss = 4.3693  \n",
      "\n",
      "Fold: 10  Epoch: 254  Training loss = 2.7315  Validation loss = 4.3689  \n",
      "\n",
      "Fold: 10  Epoch: 255  Training loss = 2.7313  Validation loss = 4.3668  \n",
      "\n",
      "Fold: 10  Epoch: 256  Training loss = 2.7307  Validation loss = 4.3619  \n",
      "\n",
      "Fold: 10  Epoch: 257  Training loss = 2.7305  Validation loss = 4.3610  \n",
      "\n",
      "Fold: 10  Epoch: 258  Training loss = 2.7299  Validation loss = 4.3560  \n",
      "\n",
      "Fold: 10  Epoch: 259  Training loss = 2.7295  Validation loss = 4.3528  \n",
      "\n",
      "Fold: 10  Epoch: 260  Training loss = 2.7289  Validation loss = 4.3491  \n",
      "\n",
      "Fold: 10  Epoch: 261  Training loss = 2.7286  Validation loss = 4.3500  \n",
      "\n",
      "Fold: 10  Epoch: 262  Training loss = 2.7283  Validation loss = 4.3499  \n",
      "\n",
      "Fold: 10  Epoch: 263  Training loss = 2.7279  Validation loss = 4.3485  \n",
      "\n",
      "Fold: 10  Epoch: 264  Training loss = 2.7275  Validation loss = 4.3450  \n",
      "\n",
      "Fold: 10  Epoch: 265  Training loss = 2.7273  Validation loss = 4.3454  \n",
      "\n",
      "Fold: 10  Epoch: 266  Training loss = 2.7270  Validation loss = 4.3445  \n",
      "\n",
      "Fold: 10  Epoch: 267  Training loss = 2.7265  Validation loss = 4.3416  \n",
      "\n",
      "Fold: 10  Epoch: 268  Training loss = 2.7260  Validation loss = 4.3396  \n",
      "\n",
      "Fold: 10  Epoch: 269  Training loss = 2.7259  Validation loss = 4.3398  \n",
      "\n",
      "Fold: 10  Epoch: 270  Training loss = 2.7256  Validation loss = 4.3390  \n",
      "\n",
      "Fold: 10  Epoch: 271  Training loss = 2.7252  Validation loss = 4.3373  \n",
      "\n",
      "Fold: 10  Epoch: 272  Training loss = 2.7247  Validation loss = 4.3333  \n",
      "\n",
      "Fold: 10  Epoch: 273  Training loss = 2.7244  Validation loss = 4.3330  \n",
      "\n",
      "Fold: 10  Epoch: 274  Training loss = 2.7239  Validation loss = 4.3283  \n",
      "\n",
      "Fold: 10  Epoch: 275  Training loss = 2.7237  Validation loss = 4.3293  \n",
      "\n",
      "Fold: 10  Epoch: 276  Training loss = 2.7235  Validation loss = 4.3291  \n",
      "\n",
      "Fold: 10  Epoch: 277  Training loss = 2.7232  Validation loss = 4.3287  \n",
      "\n",
      "Fold: 10  Epoch: 278  Training loss = 2.7227  Validation loss = 4.3250  \n",
      "\n",
      "Fold: 10  Epoch: 279  Training loss = 2.7223  Validation loss = 4.3238  \n",
      "\n",
      "Fold: 10  Epoch: 280  Training loss = 2.7219  Validation loss = 4.3223  \n",
      "\n",
      "Fold: 10  Epoch: 281  Training loss = 2.7215  Validation loss = 4.3205  \n",
      "\n",
      "Fold: 10  Epoch: 282  Training loss = 2.7212  Validation loss = 4.3205  \n",
      "\n",
      "Fold: 10  Epoch: 283  Training loss = 2.7208  Validation loss = 4.3175  \n",
      "\n",
      "Fold: 10  Epoch: 284  Training loss = 2.7206  Validation loss = 4.3167  \n",
      "\n",
      "Fold: 10  Epoch: 285  Training loss = 2.7204  Validation loss = 4.3176  \n",
      "\n",
      "Fold: 10  Epoch: 286  Training loss = 2.7202  Validation loss = 4.3182  \n",
      "\n",
      "Fold: 10  Epoch: 287  Training loss = 2.7199  Validation loss = 4.3190  \n",
      "\n",
      "Fold: 10  Epoch: 288  Training loss = 2.7193  Validation loss = 4.3147  \n",
      "\n",
      "Fold: 10  Epoch: 289  Training loss = 2.7189  Validation loss = 4.3133  \n",
      "\n",
      "Fold: 10  Epoch: 290  Training loss = 2.7184  Validation loss = 4.3108  \n",
      "\n",
      "Fold: 10  Epoch: 291  Training loss = 2.7181  Validation loss = 4.3102  \n",
      "\n",
      "Fold: 10  Epoch: 292  Training loss = 2.7177  Validation loss = 4.3098  \n",
      "\n",
      "Fold: 10  Epoch: 293  Training loss = 2.7173  Validation loss = 4.3081  \n",
      "\n",
      "Fold: 10  Epoch: 294  Training loss = 2.7168  Validation loss = 4.3045  \n",
      "\n",
      "Fold: 10  Epoch: 295  Training loss = 2.7164  Validation loss = 4.3027  \n",
      "\n",
      "Fold: 10  Epoch: 296  Training loss = 2.7160  Validation loss = 4.3011  \n",
      "\n",
      "Fold: 10  Epoch: 297  Training loss = 2.7156  Validation loss = 4.2995  \n",
      "\n",
      "Fold: 10  Epoch: 298  Training loss = 2.7146  Validation loss = 4.2907  \n",
      "\n",
      "Fold: 10  Epoch: 299  Training loss = 2.7141  Validation loss = 4.2875  \n",
      "\n",
      "Fold: 10  Epoch: 300  Training loss = 2.7136  Validation loss = 4.2853  \n",
      "\n",
      "Fold: 10  Epoch: 301  Training loss = 2.7132  Validation loss = 4.2833  \n",
      "\n",
      "Fold: 10  Epoch: 302  Training loss = 2.7128  Validation loss = 4.2827  \n",
      "\n",
      "Fold: 10  Epoch: 303  Training loss = 2.7125  Validation loss = 4.2839  \n",
      "\n",
      "Fold: 10  Epoch: 304  Training loss = 2.7120  Validation loss = 4.2800  \n",
      "\n",
      "Fold: 10  Epoch: 305  Training loss = 2.7116  Validation loss = 4.2782  \n",
      "\n",
      "Fold: 10  Epoch: 306  Training loss = 2.7112  Validation loss = 4.2779  \n",
      "\n",
      "Fold: 10  Epoch: 307  Training loss = 2.7101  Validation loss = 4.2696  \n",
      "\n",
      "Fold: 10  Epoch: 308  Training loss = 2.7099  Validation loss = 4.2713  \n",
      "\n",
      "Fold: 10  Epoch: 309  Training loss = 2.7096  Validation loss = 4.2698  \n",
      "\n",
      "Fold: 10  Epoch: 310  Training loss = 2.7090  Validation loss = 4.2660  \n",
      "\n",
      "Fold: 10  Epoch: 311  Training loss = 2.7086  Validation loss = 4.2643  \n",
      "\n",
      "Fold: 10  Epoch: 312  Training loss = 2.7082  Validation loss = 4.2650  \n",
      "\n",
      "Fold: 10  Epoch: 313  Training loss = 2.7074  Validation loss = 4.2588  \n",
      "\n",
      "Fold: 10  Epoch: 314  Training loss = 2.7067  Validation loss = 4.2545  \n",
      "\n",
      "Fold: 10  Epoch: 315  Training loss = 2.7062  Validation loss = 4.2522  \n",
      "\n",
      "Fold: 10  Epoch: 316  Training loss = 2.7060  Validation loss = 4.2531  \n",
      "\n",
      "Fold: 10  Epoch: 317  Training loss = 2.7056  Validation loss = 4.2526  \n",
      "\n",
      "Fold: 10  Epoch: 318  Training loss = 2.7053  Validation loss = 4.2505  \n",
      "\n",
      "Fold: 10  Epoch: 319  Training loss = 2.7049  Validation loss = 4.2505  \n",
      "\n",
      "Fold: 10  Epoch: 320  Training loss = 2.7045  Validation loss = 4.2491  \n",
      "\n",
      "Fold: 10  Epoch: 321  Training loss = 2.7041  Validation loss = 4.2467  \n",
      "\n",
      "Fold: 10  Epoch: 322  Training loss = 2.7034  Validation loss = 4.2403  \n",
      "\n",
      "Fold: 10  Epoch: 323  Training loss = 2.7028  Validation loss = 4.2365  \n",
      "\n",
      "Fold: 10  Epoch: 324  Training loss = 2.7027  Validation loss = 4.2378  \n",
      "\n",
      "Fold: 10  Epoch: 325  Training loss = 2.7022  Validation loss = 4.2355  \n",
      "\n",
      "Fold: 10  Epoch: 326  Training loss = 2.7019  Validation loss = 4.2330  \n",
      "\n",
      "Fold: 10  Epoch: 327  Training loss = 2.7015  Validation loss = 4.2325  \n",
      "\n",
      "Fold: 10  Epoch: 328  Training loss = 2.7013  Validation loss = 4.2321  \n",
      "\n",
      "Fold: 10  Epoch: 329  Training loss = 2.7011  Validation loss = 4.2321  \n",
      "\n",
      "Fold: 10  Epoch: 330  Training loss = 2.7007  Validation loss = 4.2308  \n",
      "\n",
      "Fold: 10  Epoch: 331  Training loss = 2.7001  Validation loss = 4.2276  \n",
      "\n",
      "Fold: 10  Epoch: 332  Training loss = 2.6998  Validation loss = 4.2263  \n",
      "\n",
      "Fold: 10  Epoch: 333  Training loss = 2.6995  Validation loss = 4.2263  \n",
      "\n",
      "Fold: 10  Epoch: 334  Training loss = 2.6989  Validation loss = 4.2219  \n",
      "\n",
      "Fold: 10  Epoch: 335  Training loss = 2.6983  Validation loss = 4.2196  \n",
      "\n",
      "Fold: 10  Epoch: 336  Training loss = 2.6978  Validation loss = 4.2176  \n",
      "\n",
      "Fold: 10  Epoch: 337  Training loss = 2.6968  Validation loss = 4.2085  \n",
      "\n",
      "Fold: 10  Epoch: 338  Training loss = 2.6967  Validation loss = 4.2090  \n",
      "\n",
      "Fold: 10  Epoch: 339  Training loss = 2.6961  Validation loss = 4.2065  \n",
      "\n",
      "Fold: 10  Epoch: 340  Training loss = 2.6960  Validation loss = 4.2065  \n",
      "\n",
      "Fold: 10  Epoch: 341  Training loss = 2.6955  Validation loss = 4.2027  \n",
      "\n",
      "Fold: 10  Epoch: 342  Training loss = 2.6951  Validation loss = 4.2022  \n",
      "\n",
      "Fold: 10  Epoch: 343  Training loss = 2.6948  Validation loss = 4.2001  \n",
      "\n",
      "Fold: 10  Epoch: 344  Training loss = 2.6943  Validation loss = 4.1985  \n",
      "\n",
      "Fold: 10  Epoch: 345  Training loss = 2.6938  Validation loss = 4.1954  \n",
      "\n",
      "Fold: 10  Epoch: 346  Training loss = 2.6933  Validation loss = 4.1925  \n",
      "\n",
      "Fold: 10  Epoch: 347  Training loss = 2.6925  Validation loss = 4.1859  \n",
      "\n",
      "Fold: 10  Epoch: 348  Training loss = 2.6921  Validation loss = 4.1848  \n",
      "\n",
      "Fold: 10  Epoch: 349  Training loss = 2.6916  Validation loss = 4.1817  \n",
      "\n",
      "Fold: 10  Epoch: 350  Training loss = 2.6912  Validation loss = 4.1805  \n",
      "\n",
      "Fold: 10  Epoch: 351  Training loss = 2.6908  Validation loss = 4.1792  \n",
      "\n",
      "Fold: 10  Epoch: 352  Training loss = 2.6906  Validation loss = 4.1803  \n",
      "\n",
      "Fold: 10  Epoch: 353  Training loss = 2.6900  Validation loss = 4.1755  \n",
      "\n",
      "Fold: 10  Epoch: 354  Training loss = 2.6894  Validation loss = 4.1702  \n",
      "\n",
      "Fold: 10  Epoch: 355  Training loss = 2.6891  Validation loss = 4.1697  \n",
      "\n",
      "Fold: 10  Epoch: 356  Training loss = 2.6888  Validation loss = 4.1700  \n",
      "\n",
      "Fold: 10  Epoch: 357  Training loss = 2.6883  Validation loss = 4.1681  \n",
      "\n",
      "Fold: 10  Epoch: 358  Training loss = 2.6879  Validation loss = 4.1640  \n",
      "\n",
      "Fold: 10  Epoch: 359  Training loss = 2.6877  Validation loss = 4.1659  \n",
      "\n",
      "Fold: 10  Epoch: 360  Training loss = 2.6873  Validation loss = 4.1649  \n",
      "\n",
      "Fold: 10  Epoch: 361  Training loss = 2.6868  Validation loss = 4.1628  \n",
      "\n",
      "Fold: 10  Epoch: 362  Training loss = 2.6865  Validation loss = 4.1615  \n",
      "\n",
      "Fold: 10  Epoch: 363  Training loss = 2.6863  Validation loss = 4.1619  \n",
      "\n",
      "Fold: 10  Epoch: 364  Training loss = 2.6859  Validation loss = 4.1596  \n",
      "\n",
      "Fold: 10  Epoch: 365  Training loss = 2.6857  Validation loss = 4.1598  \n",
      "\n",
      "Fold: 10  Epoch: 366  Training loss = 2.6852  Validation loss = 4.1572  \n",
      "\n",
      "Fold: 10  Epoch: 367  Training loss = 2.6847  Validation loss = 4.1549  \n",
      "\n",
      "Fold: 10  Epoch: 368  Training loss = 2.6843  Validation loss = 4.1527  \n",
      "\n",
      "Fold: 10  Epoch: 369  Training loss = 2.6836  Validation loss = 4.1482  \n",
      "\n",
      "Fold: 10  Epoch: 370  Training loss = 2.6833  Validation loss = 4.1460  \n",
      "\n",
      "Fold: 10  Epoch: 371  Training loss = 2.6827  Validation loss = 4.1429  \n",
      "\n",
      "Fold: 10  Epoch: 372  Training loss = 2.6826  Validation loss = 4.1431  \n",
      "\n",
      "Fold: 10  Epoch: 373  Training loss = 2.6822  Validation loss = 4.1420  \n",
      "\n",
      "Fold: 10  Epoch: 374  Training loss = 2.6818  Validation loss = 4.1413  \n",
      "\n",
      "Fold: 10  Epoch: 375  Training loss = 2.6815  Validation loss = 4.1424  \n",
      "\n",
      "Fold: 10  Epoch: 376  Training loss = 2.6809  Validation loss = 4.1366  \n",
      "\n",
      "Fold: 10  Epoch: 377  Training loss = 2.6804  Validation loss = 4.1349  \n",
      "\n",
      "Fold: 10  Epoch: 378  Training loss = 2.6801  Validation loss = 4.1331  \n",
      "\n",
      "Fold: 10  Epoch: 379  Training loss = 2.6799  Validation loss = 4.1356  \n",
      "\n",
      "Fold: 10  Epoch: 380  Training loss = 2.6796  Validation loss = 4.1350  \n",
      "\n",
      "Fold: 10  Epoch: 381  Training loss = 2.6791  Validation loss = 4.1321  \n",
      "\n",
      "Fold: 10  Epoch: 382  Training loss = 2.6786  Validation loss = 4.1294  \n",
      "\n",
      "Fold: 10  Epoch: 383  Training loss = 2.6777  Validation loss = 4.1221  \n",
      "\n",
      "Fold: 10  Epoch: 384  Training loss = 2.6774  Validation loss = 4.1238  \n",
      "\n",
      "Fold: 10  Epoch: 385  Training loss = 2.6769  Validation loss = 4.1191  \n",
      "\n",
      "Fold: 10  Epoch: 386  Training loss = 2.6766  Validation loss = 4.1201  \n",
      "\n",
      "Fold: 10  Epoch: 387  Training loss = 2.6764  Validation loss = 4.1191  \n",
      "\n",
      "Fold: 10  Epoch: 388  Training loss = 2.6758  Validation loss = 4.1153  \n",
      "\n",
      "Fold: 10  Epoch: 389  Training loss = 2.6755  Validation loss = 4.1178  \n",
      "\n",
      "Fold: 10  Epoch: 390  Training loss = 2.6751  Validation loss = 4.1149  \n",
      "\n",
      "Fold: 10  Epoch: 391  Training loss = 2.6747  Validation loss = 4.1142  \n",
      "\n",
      "Fold: 10  Epoch: 392  Training loss = 2.6744  Validation loss = 4.1117  \n",
      "\n",
      "Fold: 10  Epoch: 393  Training loss = 2.6740  Validation loss = 4.1094  \n",
      "\n",
      "Fold: 10  Epoch: 394  Training loss = 2.6737  Validation loss = 4.1078  \n",
      "\n",
      "Fold: 10  Epoch: 395  Training loss = 2.6736  Validation loss = 4.1100  \n",
      "\n",
      "Fold: 10  Epoch: 396  Training loss = 2.6733  Validation loss = 4.1084  \n",
      "\n",
      "Fold: 10  Epoch: 397  Training loss = 2.6729  Validation loss = 4.1064  \n",
      "\n",
      "Fold: 10  Epoch: 398  Training loss = 2.6725  Validation loss = 4.1021  \n",
      "\n",
      "Fold: 10  Epoch: 399  Training loss = 2.6722  Validation loss = 4.0992  \n",
      "\n",
      "Fold: 10  Epoch: 400  Training loss = 2.6718  Validation loss = 4.0972  \n",
      "\n",
      "Fold: 10  Epoch: 401  Training loss = 2.6715  Validation loss = 4.0964  \n",
      "\n",
      "Fold: 10  Epoch: 402  Training loss = 2.6710  Validation loss = 4.0925  \n",
      "\n",
      "Fold: 10  Epoch: 403  Training loss = 2.6707  Validation loss = 4.0923  \n",
      "\n",
      "Fold: 10  Epoch: 404  Training loss = 2.6700  Validation loss = 4.0860  \n",
      "\n",
      "Fold: 10  Epoch: 405  Training loss = 2.6694  Validation loss = 4.0796  \n",
      "\n",
      "Fold: 10  Epoch: 406  Training loss = 2.6691  Validation loss = 4.0795  \n",
      "\n",
      "Fold: 10  Epoch: 407  Training loss = 2.6689  Validation loss = 4.0813  \n",
      "\n",
      "Fold: 10  Epoch: 408  Training loss = 2.6685  Validation loss = 4.0760  \n",
      "\n",
      "Fold: 10  Epoch: 409  Training loss = 2.6682  Validation loss = 4.0753  \n",
      "\n",
      "Fold: 10  Epoch: 410  Training loss = 2.6679  Validation loss = 4.0742  \n",
      "\n",
      "Fold: 10  Epoch: 411  Training loss = 2.6676  Validation loss = 4.0716  \n",
      "\n",
      "Fold: 10  Epoch: 412  Training loss = 2.6670  Validation loss = 4.0650  \n",
      "\n",
      "Fold: 10  Epoch: 413  Training loss = 2.6669  Validation loss = 4.0688  \n",
      "\n",
      "Fold: 10  Epoch: 414  Training loss = 2.6665  Validation loss = 4.0657  \n",
      "\n",
      "Fold: 10  Epoch: 415  Training loss = 2.6663  Validation loss = 4.0664  \n",
      "\n",
      "Fold: 10  Epoch: 416  Training loss = 2.6659  Validation loss = 4.0640  \n",
      "\n",
      "Fold: 10  Epoch: 417  Training loss = 2.6657  Validation loss = 4.0668  \n",
      "\n",
      "Fold: 10  Epoch: 418  Training loss = 2.6656  Validation loss = 4.0672  \n",
      "\n",
      "Fold: 10  Epoch: 419  Training loss = 2.6650  Validation loss = 4.0626  \n",
      "\n",
      "Fold: 10  Epoch: 420  Training loss = 2.6646  Validation loss = 4.0563  \n",
      "\n",
      "Fold: 10  Epoch: 421  Training loss = 2.6643  Validation loss = 4.0553  \n",
      "\n",
      "Fold: 10  Epoch: 422  Training loss = 2.6640  Validation loss = 4.0554  \n",
      "\n",
      "Fold: 10  Epoch: 423  Training loss = 2.6637  Validation loss = 4.0534  \n",
      "\n",
      "Fold: 10  Epoch: 424  Training loss = 2.6632  Validation loss = 4.0494  \n",
      "\n",
      "Fold: 10  Epoch: 425  Training loss = 2.6630  Validation loss = 4.0499  \n",
      "\n",
      "Fold: 10  Epoch: 426  Training loss = 2.6626  Validation loss = 4.0459  \n",
      "\n",
      "Fold: 10  Epoch: 427  Training loss = 2.6625  Validation loss = 4.0480  \n",
      "\n",
      "Fold: 10  Epoch: 428  Training loss = 2.6621  Validation loss = 4.0443  \n",
      "\n",
      "Fold: 10  Epoch: 429  Training loss = 2.6617  Validation loss = 4.0437  \n",
      "\n",
      "Fold: 10  Epoch: 430  Training loss = 2.6613  Validation loss = 4.0410  \n",
      "\n",
      "Fold: 10  Epoch: 431  Training loss = 2.6609  Validation loss = 4.0374  \n",
      "\n",
      "Fold: 10  Epoch: 432  Training loss = 2.6606  Validation loss = 4.0367  \n",
      "\n",
      "Fold: 10  Epoch: 433  Training loss = 2.6602  Validation loss = 4.0355  \n",
      "\n",
      "Fold: 10  Epoch: 434  Training loss = 2.6597  Validation loss = 4.0271  \n",
      "\n",
      "Fold: 10  Epoch: 435  Training loss = 2.6591  Validation loss = 4.0210  \n",
      "\n",
      "Fold: 10  Epoch: 436  Training loss = 2.6585  Validation loss = 4.0148  \n",
      "\n",
      "Fold: 10  Epoch: 437  Training loss = 2.6584  Validation loss = 4.0206  \n",
      "\n",
      "Fold: 10  Epoch: 438  Training loss = 2.6582  Validation loss = 4.0203  \n",
      "\n",
      "Fold: 10  Epoch: 439  Training loss = 2.6579  Validation loss = 4.0221  \n",
      "\n",
      "Fold: 10  Epoch: 440  Training loss = 2.6574  Validation loss = 4.0161  \n",
      "\n",
      "Fold: 10  Epoch: 441  Training loss = 2.6569  Validation loss = 4.0151  \n",
      "\n",
      "Fold: 10  Epoch: 442  Training loss = 2.6565  Validation loss = 4.0101  \n",
      "\n",
      "Fold: 10  Epoch: 443  Training loss = 2.6562  Validation loss = 4.0082  \n",
      "\n",
      "Fold: 10  Epoch: 444  Training loss = 2.6559  Validation loss = 4.0058  \n",
      "\n",
      "Fold: 10  Epoch: 445  Training loss = 2.6554  Validation loss = 4.0012  \n",
      "\n",
      "Fold: 10  Epoch: 446  Training loss = 2.6549  Validation loss = 3.9951  \n",
      "\n",
      "Fold: 10  Epoch: 447  Training loss = 2.6546  Validation loss = 3.9965  \n",
      "\n",
      "Fold: 10  Epoch: 448  Training loss = 2.6543  Validation loss = 3.9954  \n",
      "\n",
      "Fold: 10  Epoch: 449  Training loss = 2.6541  Validation loss = 3.9970  \n",
      "\n",
      "Fold: 10  Epoch: 450  Training loss = 2.6536  Validation loss = 3.9921  \n",
      "\n",
      "Fold: 10  Epoch: 451  Training loss = 2.6532  Validation loss = 3.9895  \n",
      "\n",
      "Fold: 10  Epoch: 452  Training loss = 2.6529  Validation loss = 3.9896  \n",
      "\n",
      "Fold: 10  Epoch: 453  Training loss = 2.6526  Validation loss = 3.9888  \n",
      "\n",
      "Fold: 10  Epoch: 454  Training loss = 2.6522  Validation loss = 3.9867  \n",
      "\n",
      "Fold: 10  Epoch: 455  Training loss = 2.6518  Validation loss = 3.9822  \n",
      "\n",
      "Fold: 10  Epoch: 456  Training loss = 2.6517  Validation loss = 3.9893  \n",
      "\n",
      "Fold: 10  Epoch: 457  Training loss = 2.6513  Validation loss = 3.9829  \n",
      "\n",
      "Fold: 10  Epoch: 458  Training loss = 2.6509  Validation loss = 3.9814  \n",
      "\n",
      "Fold: 10  Epoch: 459  Training loss = 2.6507  Validation loss = 3.9872  \n",
      "\n",
      "Fold: 10  Epoch: 460  Training loss = 2.6504  Validation loss = 3.9879  \n",
      "\n",
      "Fold: 10  Epoch: 461  Training loss = 2.6502  Validation loss = 3.9855  \n",
      "\n",
      "Fold: 10  Epoch: 462  Training loss = 2.6499  Validation loss = 3.9822  \n",
      "\n",
      "Fold: 10  Epoch: 463  Training loss = 2.6497  Validation loss = 3.9833  \n",
      "\n",
      "Fold: 10  Epoch: 464  Training loss = 2.6494  Validation loss = 3.9847  \n",
      "\n",
      "Fold: 10  Epoch: 465  Training loss = 2.6488  Validation loss = 3.9786  \n",
      "\n",
      "Fold: 10  Epoch: 466  Training loss = 2.6483  Validation loss = 3.9739  \n",
      "\n",
      "Fold: 10  Epoch: 467  Training loss = 2.6479  Validation loss = 3.9725  \n",
      "\n",
      "Fold: 10  Epoch: 468  Training loss = 2.6475  Validation loss = 3.9729  \n",
      "\n",
      "Fold: 10  Epoch: 469  Training loss = 2.6474  Validation loss = 3.9755  \n",
      "\n",
      "Fold: 10  Epoch: 470  Training loss = 2.6471  Validation loss = 3.9713  \n",
      "\n",
      "Fold: 10  Epoch: 471  Training loss = 2.6467  Validation loss = 3.9691  \n",
      "\n",
      "Fold: 10  Epoch: 472  Training loss = 2.6463  Validation loss = 3.9659  \n",
      "\n",
      "Fold: 10  Epoch: 473  Training loss = 2.6458  Validation loss = 3.9649  \n",
      "\n",
      "Fold: 10  Epoch: 474  Training loss = 2.6456  Validation loss = 3.9635  \n",
      "\n",
      "Fold: 10  Epoch: 475  Training loss = 2.6452  Validation loss = 3.9606  \n",
      "\n",
      "Fold: 10  Epoch: 476  Training loss = 2.6447  Validation loss = 3.9575  \n",
      "\n",
      "Fold: 10  Epoch: 477  Training loss = 2.6439  Validation loss = 3.9500  \n",
      "\n",
      "Fold: 10  Epoch: 478  Training loss = 2.6435  Validation loss = 3.9468  \n",
      "\n",
      "Fold: 10  Epoch: 479  Training loss = 2.6433  Validation loss = 3.9475  \n",
      "\n",
      "Fold: 10  Epoch: 480  Training loss = 2.6431  Validation loss = 3.9489  \n",
      "\n",
      "Fold: 10  Epoch: 481  Training loss = 2.6427  Validation loss = 3.9475  \n",
      "\n",
      "Fold: 10  Epoch: 482  Training loss = 2.6424  Validation loss = 3.9469  \n",
      "\n",
      "Fold: 10  Epoch: 483  Training loss = 2.6419  Validation loss = 3.9431  \n",
      "\n",
      "Fold: 10  Epoch: 484  Training loss = 2.6414  Validation loss = 3.9391  \n",
      "\n",
      "Fold: 10  Epoch: 485  Training loss = 2.6410  Validation loss = 3.9380  \n",
      "\n",
      "Fold: 10  Epoch: 486  Training loss = 2.6406  Validation loss = 3.9344  \n",
      "\n",
      "Fold: 10  Epoch: 487  Training loss = 2.6401  Validation loss = 3.9305  \n",
      "\n",
      "Fold: 10  Epoch: 488  Training loss = 2.6397  Validation loss = 3.9288  \n",
      "\n",
      "Fold: 10  Epoch: 489  Training loss = 2.6395  Validation loss = 3.9290  \n",
      "\n",
      "Fold: 10  Epoch: 490  Training loss = 2.6391  Validation loss = 3.9257  \n",
      "\n",
      "Fold: 10  Epoch: 491  Training loss = 2.6385  Validation loss = 3.9201  \n",
      "\n",
      "Fold: 10  Epoch: 492  Training loss = 2.6380  Validation loss = 3.9166  \n",
      "\n",
      "Fold: 10  Epoch: 493  Training loss = 2.6377  Validation loss = 3.9181  \n",
      "\n",
      "Fold: 10  Epoch: 494  Training loss = 2.6373  Validation loss = 3.9177  \n",
      "\n",
      "Fold: 10  Epoch: 495  Training loss = 2.6370  Validation loss = 3.9192  \n",
      "\n",
      "Fold: 10  Epoch: 496  Training loss = 2.6366  Validation loss = 3.9179  \n",
      "\n",
      "Fold: 10  Epoch: 497  Training loss = 2.6362  Validation loss = 3.9162  \n",
      "\n",
      "Fold: 10  Epoch: 498  Training loss = 2.6357  Validation loss = 3.9140  \n",
      "\n",
      "Fold: 10  Epoch: 499  Training loss = 2.6351  Validation loss = 3.9098  \n",
      "\n",
      "Fold: 10  Epoch: 500  Training loss = 2.6346  Validation loss = 3.9066  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 500  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 2.7145  Validation loss = 1.4286  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 2.7067  Validation loss = 1.4268  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 2.7053  Validation loss = 1.4263  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 2.7042  Validation loss = 1.4264  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 2.7028  Validation loss = 1.4270  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 2.7019  Validation loss = 1.4269  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 2.7015  Validation loss = 1.4264  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 2.7003  Validation loss = 1.4267  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 2.6974  Validation loss = 1.4264  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 2.6963  Validation loss = 1.4255  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 2.6947  Validation loss = 1.4264  \n",
      "\n",
      "Fold: 11  Epoch: 12  Training loss = 2.6936  Validation loss = 1.4267  \n",
      "\n",
      "Fold: 11  Epoch: 13  Training loss = 2.6925  Validation loss = 1.4270  \n",
      "\n",
      "Fold: 11  Epoch: 14  Training loss = 2.6907  Validation loss = 1.4267  \n",
      "\n",
      "Fold: 11  Epoch: 15  Training loss = 2.6867  Validation loss = 1.4263  \n",
      "\n",
      "Fold: 11  Epoch: 16  Training loss = 2.6853  Validation loss = 1.4270  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 10  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 2.6816  Validation loss = 1.6341  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 2.6804  Validation loss = 1.6295  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 2.6787  Validation loss = 1.6216  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 2.6781  Validation loss = 1.6195  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 2.6771  Validation loss = 1.6157  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 2.6759  Validation loss = 1.6097  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 2.6748  Validation loss = 1.6050  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 2.6737  Validation loss = 1.6018  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 2.6714  Validation loss = 1.5949  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 2.6703  Validation loss = 1.5895  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 2.6688  Validation loss = 1.5835  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 2.6683  Validation loss = 1.5829  \n",
      "\n",
      "Fold: 12  Epoch: 13  Training loss = 2.6674  Validation loss = 1.5792  \n",
      "\n",
      "Fold: 12  Epoch: 14  Training loss = 2.6664  Validation loss = 1.5754  \n",
      "\n",
      "Fold: 12  Epoch: 15  Training loss = 2.6652  Validation loss = 1.5712  \n",
      "\n",
      "Fold: 12  Epoch: 16  Training loss = 2.6639  Validation loss = 1.5660  \n",
      "\n",
      "Fold: 12  Epoch: 17  Training loss = 2.6625  Validation loss = 1.5615  \n",
      "\n",
      "Fold: 12  Epoch: 18  Training loss = 2.6600  Validation loss = 1.5550  \n",
      "\n",
      "Fold: 12  Epoch: 19  Training loss = 2.6588  Validation loss = 1.5502  \n",
      "\n",
      "Fold: 12  Epoch: 20  Training loss = 2.6574  Validation loss = 1.5429  \n",
      "\n",
      "Fold: 12  Epoch: 21  Training loss = 2.6565  Validation loss = 1.5397  \n",
      "\n",
      "Fold: 12  Epoch: 22  Training loss = 2.6557  Validation loss = 1.5366  \n",
      "\n",
      "Fold: 12  Epoch: 23  Training loss = 2.6547  Validation loss = 1.5330  \n",
      "\n",
      "Fold: 12  Epoch: 24  Training loss = 2.6538  Validation loss = 1.5291  \n",
      "\n",
      "Fold: 12  Epoch: 25  Training loss = 2.6528  Validation loss = 1.5243  \n",
      "\n",
      "Fold: 12  Epoch: 26  Training loss = 2.6526  Validation loss = 1.5245  \n",
      "\n",
      "Fold: 12  Epoch: 27  Training loss = 2.6518  Validation loss = 1.5211  \n",
      "\n",
      "Fold: 12  Epoch: 28  Training loss = 2.6509  Validation loss = 1.5174  \n",
      "\n",
      "Fold: 12  Epoch: 29  Training loss = 2.6498  Validation loss = 1.5128  \n",
      "\n",
      "Fold: 12  Epoch: 30  Training loss = 2.6451  Validation loss = 1.5100  \n",
      "\n",
      "Fold: 12  Epoch: 31  Training loss = 2.6440  Validation loss = 1.5056  \n",
      "\n",
      "Fold: 12  Epoch: 32  Training loss = 2.6427  Validation loss = 1.4996  \n",
      "\n",
      "Fold: 12  Epoch: 33  Training loss = 2.6420  Validation loss = 1.4979  \n",
      "\n",
      "Fold: 12  Epoch: 34  Training loss = 2.6406  Validation loss = 1.4909  \n",
      "\n",
      "Fold: 12  Epoch: 35  Training loss = 2.6390  Validation loss = 1.4821  \n",
      "\n",
      "Fold: 12  Epoch: 36  Training loss = 2.6377  Validation loss = 1.4758  \n",
      "\n",
      "Fold: 12  Epoch: 37  Training loss = 2.6363  Validation loss = 1.4680  \n",
      "\n",
      "Fold: 12  Epoch: 38  Training loss = 2.6353  Validation loss = 1.4638  \n",
      "\n",
      "Fold: 12  Epoch: 39  Training loss = 2.6342  Validation loss = 1.4587  \n",
      "\n",
      "Fold: 12  Epoch: 40  Training loss = 2.6342  Validation loss = 1.4599  \n",
      "\n",
      "Fold: 12  Epoch: 41  Training loss = 2.6332  Validation loss = 1.4541  \n",
      "\n",
      "Fold: 12  Epoch: 42  Training loss = 2.6320  Validation loss = 1.4531  \n",
      "\n",
      "Fold: 12  Epoch: 43  Training loss = 2.6313  Validation loss = 1.4513  \n",
      "\n",
      "Fold: 12  Epoch: 44  Training loss = 2.6302  Validation loss = 1.4458  \n",
      "\n",
      "Fold: 12  Epoch: 45  Training loss = 2.6296  Validation loss = 1.4436  \n",
      "\n",
      "Fold: 12  Epoch: 46  Training loss = 2.6290  Validation loss = 1.4426  \n",
      "\n",
      "Fold: 12  Epoch: 47  Training loss = 2.6281  Validation loss = 1.4385  \n",
      "\n",
      "Fold: 12  Epoch: 48  Training loss = 2.6272  Validation loss = 1.4345  \n",
      "\n",
      "Fold: 12  Epoch: 49  Training loss = 2.6268  Validation loss = 1.4333  \n",
      "\n",
      "Fold: 12  Epoch: 50  Training loss = 2.6265  Validation loss = 1.4329  \n",
      "\n",
      "Fold: 12  Epoch: 51  Training loss = 2.6251  Validation loss = 1.4261  \n",
      "\n",
      "Fold: 12  Epoch: 52  Training loss = 2.6243  Validation loss = 1.4235  \n",
      "\n",
      "Fold: 12  Epoch: 53  Training loss = 2.6237  Validation loss = 1.4225  \n",
      "\n",
      "Fold: 12  Epoch: 54  Training loss = 2.6224  Validation loss = 1.4155  \n",
      "\n",
      "Fold: 12  Epoch: 55  Training loss = 2.6216  Validation loss = 1.4102  \n",
      "\n",
      "Fold: 12  Epoch: 56  Training loss = 2.6206  Validation loss = 1.4040  \n",
      "\n",
      "Fold: 12  Epoch: 57  Training loss = 2.6198  Validation loss = 1.4006  \n",
      "\n",
      "Fold: 12  Epoch: 58  Training loss = 2.6190  Validation loss = 1.3965  \n",
      "\n",
      "Fold: 12  Epoch: 59  Training loss = 2.6186  Validation loss = 1.3951  \n",
      "\n",
      "Fold: 12  Epoch: 60  Training loss = 2.6180  Validation loss = 1.3919  \n",
      "\n",
      "Fold: 12  Epoch: 61  Training loss = 2.6172  Validation loss = 1.3888  \n",
      "\n",
      "Fold: 12  Epoch: 62  Training loss = 2.6169  Validation loss = 1.3897  \n",
      "\n",
      "Fold: 12  Epoch: 63  Training loss = 2.6157  Validation loss = 1.3829  \n",
      "\n",
      "Fold: 12  Epoch: 64  Training loss = 2.6143  Validation loss = 1.3762  \n",
      "\n",
      "Fold: 12  Epoch: 65  Training loss = 2.6137  Validation loss = 1.3745  \n",
      "\n",
      "Fold: 12  Epoch: 66  Training loss = 2.6135  Validation loss = 1.3758  \n",
      "\n",
      "Fold: 12  Epoch: 67  Training loss = 2.6129  Validation loss = 1.3729  \n",
      "\n",
      "Fold: 12  Epoch: 68  Training loss = 2.6125  Validation loss = 1.3725  \n",
      "\n",
      "Fold: 12  Epoch: 69  Training loss = 2.6119  Validation loss = 1.3688  \n",
      "\n",
      "Fold: 12  Epoch: 70  Training loss = 2.6114  Validation loss = 1.3664  \n",
      "\n",
      "Fold: 12  Epoch: 71  Training loss = 2.6110  Validation loss = 1.3647  \n",
      "\n",
      "Fold: 12  Epoch: 72  Training loss = 2.6103  Validation loss = 1.3612  \n",
      "\n",
      "Fold: 12  Epoch: 73  Training loss = 2.6094  Validation loss = 1.3576  \n",
      "\n",
      "Fold: 12  Epoch: 74  Training loss = 2.6089  Validation loss = 1.3562  \n",
      "\n",
      "Fold: 12  Epoch: 75  Training loss = 2.6081  Validation loss = 1.3526  \n",
      "\n",
      "Fold: 12  Epoch: 76  Training loss = 2.6073  Validation loss = 1.3485  \n",
      "\n",
      "Fold: 12  Epoch: 77  Training loss = 2.6067  Validation loss = 1.3464  \n",
      "\n",
      "Fold: 12  Epoch: 78  Training loss = 2.6063  Validation loss = 1.3467  \n",
      "\n",
      "Fold: 12  Epoch: 79  Training loss = 2.6057  Validation loss = 1.3434  \n",
      "\n",
      "Fold: 12  Epoch: 80  Training loss = 2.6044  Validation loss = 1.3358  \n",
      "\n",
      "Fold: 12  Epoch: 81  Training loss = 2.6037  Validation loss = 1.3326  \n",
      "\n",
      "Fold: 12  Epoch: 82  Training loss = 2.6032  Validation loss = 1.3321  \n",
      "\n",
      "Fold: 12  Epoch: 83  Training loss = 2.6027  Validation loss = 1.3328  \n",
      "\n",
      "Fold: 12  Epoch: 84  Training loss = 2.6013  Validation loss = 1.3303  \n",
      "\n",
      "Fold: 12  Epoch: 85  Training loss = 2.6007  Validation loss = 1.3300  \n",
      "\n",
      "Fold: 12  Epoch: 86  Training loss = 2.6004  Validation loss = 1.3290  \n",
      "\n",
      "Fold: 12  Epoch: 87  Training loss = 2.5999  Validation loss = 1.3273  \n",
      "\n",
      "Fold: 12  Epoch: 88  Training loss = 2.5988  Validation loss = 1.3202  \n",
      "\n",
      "Fold: 12  Epoch: 89  Training loss = 2.5983  Validation loss = 1.3183  \n",
      "\n",
      "Fold: 12  Epoch: 90  Training loss = 2.5976  Validation loss = 1.3133  \n",
      "\n",
      "Fold: 12  Epoch: 91  Training loss = 2.5971  Validation loss = 1.3123  \n",
      "\n",
      "Fold: 12  Epoch: 92  Training loss = 2.5959  Validation loss = 1.3062  \n",
      "\n",
      "Fold: 12  Epoch: 93  Training loss = 2.5947  Validation loss = 1.3045  \n",
      "\n",
      "Fold: 12  Epoch: 94  Training loss = 2.5938  Validation loss = 1.3000  \n",
      "\n",
      "Fold: 12  Epoch: 95  Training loss = 2.5932  Validation loss = 1.2966  \n",
      "\n",
      "Fold: 12  Epoch: 96  Training loss = 2.5922  Validation loss = 1.2913  \n",
      "\n",
      "Fold: 12  Epoch: 97  Training loss = 2.5913  Validation loss = 1.2870  \n",
      "\n",
      "Fold: 12  Epoch: 98  Training loss = 2.5905  Validation loss = 1.2818  \n",
      "\n",
      "Fold: 12  Epoch: 99  Training loss = 2.5900  Validation loss = 1.2806  \n",
      "\n",
      "Fold: 12  Epoch: 100  Training loss = 2.5892  Validation loss = 1.2773  \n",
      "\n",
      "Fold: 12  Epoch: 101  Training loss = 2.5889  Validation loss = 1.2749  \n",
      "\n",
      "Fold: 12  Epoch: 102  Training loss = 2.5879  Validation loss = 1.2705  \n",
      "\n",
      "Fold: 12  Epoch: 103  Training loss = 2.5877  Validation loss = 1.2707  \n",
      "\n",
      "Fold: 12  Epoch: 104  Training loss = 2.5873  Validation loss = 1.2709  \n",
      "\n",
      "Fold: 12  Epoch: 105  Training loss = 2.5872  Validation loss = 1.2722  \n",
      "\n",
      "Fold: 12  Epoch: 106  Training loss = 2.5861  Validation loss = 1.2652  \n",
      "\n",
      "Fold: 12  Epoch: 107  Training loss = 2.5853  Validation loss = 1.2607  \n",
      "\n",
      "Fold: 12  Epoch: 108  Training loss = 2.5851  Validation loss = 1.2608  \n",
      "\n",
      "Fold: 12  Epoch: 109  Training loss = 2.5848  Validation loss = 1.2600  \n",
      "\n",
      "Fold: 12  Epoch: 110  Training loss = 2.5845  Validation loss = 1.2592  \n",
      "\n",
      "Fold: 12  Epoch: 111  Training loss = 2.5839  Validation loss = 1.2557  \n",
      "\n",
      "Fold: 12  Epoch: 112  Training loss = 2.5835  Validation loss = 1.2551  \n",
      "\n",
      "Fold: 12  Epoch: 113  Training loss = 2.5830  Validation loss = 1.2516  \n",
      "\n",
      "Fold: 12  Epoch: 114  Training loss = 2.5822  Validation loss = 1.2470  \n",
      "\n",
      "Fold: 12  Epoch: 115  Training loss = 2.5814  Validation loss = 1.2413  \n",
      "\n",
      "Fold: 12  Epoch: 116  Training loss = 2.5809  Validation loss = 1.2374  \n",
      "\n",
      "Fold: 12  Epoch: 117  Training loss = 2.5805  Validation loss = 1.2372  \n",
      "\n",
      "Fold: 12  Epoch: 118  Training loss = 2.5798  Validation loss = 1.2327  \n",
      "\n",
      "Fold: 12  Epoch: 119  Training loss = 2.5795  Validation loss = 1.2324  \n",
      "\n",
      "Fold: 12  Epoch: 120  Training loss = 2.5795  Validation loss = 1.2351  \n",
      "\n",
      "Fold: 12  Epoch: 121  Training loss = 2.5791  Validation loss = 1.2323  \n",
      "\n",
      "Fold: 12  Epoch: 122  Training loss = 2.5787  Validation loss = 1.2316  \n",
      "\n",
      "Fold: 12  Epoch: 123  Training loss = 2.5784  Validation loss = 1.2307  \n",
      "\n",
      "Fold: 12  Epoch: 124  Training loss = 2.5775  Validation loss = 1.2252  \n",
      "\n",
      "Fold: 12  Epoch: 125  Training loss = 2.5771  Validation loss = 1.2232  \n",
      "\n",
      "Fold: 12  Epoch: 126  Training loss = 2.5765  Validation loss = 1.2195  \n",
      "\n",
      "Fold: 12  Epoch: 127  Training loss = 2.5758  Validation loss = 1.2172  \n",
      "\n",
      "Fold: 12  Epoch: 128  Training loss = 2.5753  Validation loss = 1.2158  \n",
      "\n",
      "Fold: 12  Epoch: 129  Training loss = 2.5749  Validation loss = 1.2139  \n",
      "\n",
      "Fold: 12  Epoch: 130  Training loss = 2.5744  Validation loss = 1.2117  \n",
      "\n",
      "Fold: 12  Epoch: 131  Training loss = 2.5740  Validation loss = 1.2107  \n",
      "\n",
      "Fold: 12  Epoch: 132  Training loss = 2.5737  Validation loss = 1.2090  \n",
      "\n",
      "Fold: 12  Epoch: 133  Training loss = 2.5727  Validation loss = 1.2056  \n",
      "\n",
      "Fold: 12  Epoch: 134  Training loss = 2.5721  Validation loss = 1.2043  \n",
      "\n",
      "Fold: 12  Epoch: 135  Training loss = 2.5717  Validation loss = 1.2037  \n",
      "\n",
      "Fold: 12  Epoch: 136  Training loss = 2.5711  Validation loss = 1.2003  \n",
      "\n",
      "Fold: 12  Epoch: 137  Training loss = 2.5709  Validation loss = 1.2026  \n",
      "\n",
      "Fold: 12  Epoch: 138  Training loss = 2.5703  Validation loss = 1.2012  \n",
      "\n",
      "Fold: 12  Epoch: 139  Training loss = 2.5698  Validation loss = 1.1987  \n",
      "\n",
      "Fold: 12  Epoch: 140  Training loss = 2.5693  Validation loss = 1.1956  \n",
      "\n",
      "Fold: 12  Epoch: 141  Training loss = 2.5685  Validation loss = 1.1922  \n",
      "\n",
      "Fold: 12  Epoch: 142  Training loss = 2.5684  Validation loss = 1.1929  \n",
      "\n",
      "Fold: 12  Epoch: 143  Training loss = 2.5679  Validation loss = 1.1900  \n",
      "\n",
      "Fold: 12  Epoch: 144  Training loss = 2.5675  Validation loss = 1.1881  \n",
      "\n",
      "Fold: 12  Epoch: 145  Training loss = 2.5668  Validation loss = 1.1814  \n",
      "\n",
      "Fold: 12  Epoch: 146  Training loss = 2.5663  Validation loss = 1.1791  \n",
      "\n",
      "Fold: 12  Epoch: 147  Training loss = 2.5656  Validation loss = 1.1754  \n",
      "\n",
      "Fold: 12  Epoch: 148  Training loss = 2.5655  Validation loss = 1.1791  \n",
      "\n",
      "Fold: 12  Epoch: 149  Training loss = 2.5651  Validation loss = 1.1778  \n",
      "\n",
      "Fold: 12  Epoch: 150  Training loss = 2.5647  Validation loss = 1.1772  \n",
      "\n",
      "Fold: 12  Epoch: 151  Training loss = 2.5643  Validation loss = 1.1756  \n",
      "\n",
      "Fold: 12  Epoch: 152  Training loss = 2.5636  Validation loss = 1.1709  \n",
      "\n",
      "Fold: 12  Epoch: 153  Training loss = 2.5628  Validation loss = 1.1666  \n",
      "\n",
      "Fold: 12  Epoch: 154  Training loss = 2.5625  Validation loss = 1.1639  \n",
      "\n",
      "Fold: 12  Epoch: 155  Training loss = 2.5619  Validation loss = 1.1610  \n",
      "\n",
      "Fold: 12  Epoch: 156  Training loss = 2.5616  Validation loss = 1.1603  \n",
      "\n",
      "Fold: 12  Epoch: 157  Training loss = 2.5613  Validation loss = 1.1614  \n",
      "\n",
      "Fold: 12  Epoch: 158  Training loss = 2.5609  Validation loss = 1.1604  \n",
      "\n",
      "Fold: 12  Epoch: 159  Training loss = 2.5603  Validation loss = 1.1566  \n",
      "\n",
      "Fold: 12  Epoch: 160  Training loss = 2.5600  Validation loss = 1.1580  \n",
      "\n",
      "Fold: 12  Epoch: 161  Training loss = 2.5593  Validation loss = 1.1516  \n",
      "\n",
      "Fold: 12  Epoch: 162  Training loss = 2.5591  Validation loss = 1.1527  \n",
      "\n",
      "Fold: 12  Epoch: 163  Training loss = 2.5585  Validation loss = 1.1507  \n",
      "\n",
      "Fold: 12  Epoch: 164  Training loss = 2.5584  Validation loss = 1.1518  \n",
      "\n",
      "Fold: 12  Epoch: 165  Training loss = 2.5581  Validation loss = 1.1507  \n",
      "\n",
      "Fold: 12  Epoch: 166  Training loss = 2.5578  Validation loss = 1.1500  \n",
      "\n",
      "Fold: 12  Epoch: 167  Training loss = 2.5571  Validation loss = 1.1449  \n",
      "\n",
      "Fold: 12  Epoch: 168  Training loss = 2.5566  Validation loss = 1.1438  \n",
      "\n",
      "Fold: 12  Epoch: 169  Training loss = 2.5562  Validation loss = 1.1438  \n",
      "\n",
      "Fold: 12  Epoch: 170  Training loss = 2.5558  Validation loss = 1.1438  \n",
      "\n",
      "Fold: 12  Epoch: 171  Training loss = 2.5553  Validation loss = 1.1377  \n",
      "\n",
      "Fold: 12  Epoch: 172  Training loss = 2.5551  Validation loss = 1.1357  \n",
      "\n",
      "Fold: 12  Epoch: 173  Training loss = 2.5549  Validation loss = 1.1356  \n",
      "\n",
      "Fold: 12  Epoch: 174  Training loss = 2.5544  Validation loss = 1.1323  \n",
      "\n",
      "Fold: 12  Epoch: 175  Training loss = 2.5542  Validation loss = 1.1352  \n",
      "\n",
      "Fold: 12  Epoch: 176  Training loss = 2.5538  Validation loss = 1.1346  \n",
      "\n",
      "Fold: 12  Epoch: 177  Training loss = 2.5455  Validation loss = 1.1382  \n",
      "\n",
      "Fold: 12  Epoch: 178  Training loss = 2.5447  Validation loss = 1.1400  \n",
      "\n",
      "Fold: 12  Epoch: 179  Training loss = 2.5440  Validation loss = 1.1363  \n",
      "\n",
      "Fold: 12  Epoch: 180  Training loss = 2.5437  Validation loss = 1.1369  \n",
      "\n",
      "Fold: 12  Epoch: 181  Training loss = 2.5433  Validation loss = 1.1341  \n",
      "\n",
      "Fold: 12  Epoch: 182  Training loss = 2.5427  Validation loss = 1.1299  \n",
      "\n",
      "Fold: 12  Epoch: 183  Training loss = 2.5426  Validation loss = 1.1334  \n",
      "\n",
      "Fold: 12  Epoch: 184  Training loss = 2.5421  Validation loss = 1.1326  \n",
      "\n",
      "Fold: 12  Epoch: 185  Training loss = 2.5416  Validation loss = 1.1293  \n",
      "\n",
      "Fold: 12  Epoch: 186  Training loss = 2.5415  Validation loss = 1.1295  \n",
      "\n",
      "Fold: 12  Epoch: 187  Training loss = 2.5408  Validation loss = 1.1250  \n",
      "\n",
      "Fold: 12  Epoch: 188  Training loss = 2.5407  Validation loss = 1.1240  \n",
      "\n",
      "Fold: 12  Epoch: 189  Training loss = 2.5402  Validation loss = 1.1204  \n",
      "\n",
      "Fold: 12  Epoch: 190  Training loss = 2.5398  Validation loss = 1.1206  \n",
      "\n",
      "Fold: 12  Epoch: 191  Training loss = 2.5393  Validation loss = 1.1191  \n",
      "\n",
      "Fold: 12  Epoch: 192  Training loss = 2.5382  Validation loss = 1.1172  \n",
      "\n",
      "Fold: 12  Epoch: 193  Training loss = 2.5352  Validation loss = 1.1142  \n",
      "\n",
      "Fold: 12  Epoch: 194  Training loss = 2.5350  Validation loss = 1.1157  \n",
      "\n",
      "Fold: 12  Epoch: 195  Training loss = 2.5347  Validation loss = 1.1153  \n",
      "\n",
      "Fold: 12  Epoch: 196  Training loss = 2.5342  Validation loss = 1.1120  \n",
      "\n",
      "Fold: 12  Epoch: 197  Training loss = 2.5338  Validation loss = 1.1107  \n",
      "\n",
      "Fold: 12  Epoch: 198  Training loss = 2.5335  Validation loss = 1.1112  \n",
      "\n",
      "Fold: 12  Epoch: 199  Training loss = 2.5331  Validation loss = 1.1077  \n",
      "\n",
      "Fold: 12  Epoch: 200  Training loss = 2.5330  Validation loss = 1.1084  \n",
      "\n",
      "Fold: 12  Epoch: 201  Training loss = 2.5328  Validation loss = 1.1089  \n",
      "\n",
      "Fold: 12  Epoch: 202  Training loss = 2.5323  Validation loss = 1.1071  \n",
      "\n",
      "Fold: 12  Epoch: 203  Training loss = 2.5319  Validation loss = 1.1059  \n",
      "\n",
      "Fold: 12  Epoch: 204  Training loss = 2.5314  Validation loss = 1.1004  \n",
      "\n",
      "Fold: 12  Epoch: 205  Training loss = 2.5309  Validation loss = 1.0977  \n",
      "\n",
      "Fold: 12  Epoch: 206  Training loss = 2.5303  Validation loss = 1.0954  \n",
      "\n",
      "Fold: 12  Epoch: 207  Training loss = 2.5300  Validation loss = 1.0950  \n",
      "\n",
      "Fold: 12  Epoch: 208  Training loss = 2.5296  Validation loss = 1.0924  \n",
      "\n",
      "Fold: 12  Epoch: 209  Training loss = 2.5292  Validation loss = 1.0922  \n",
      "\n",
      "Fold: 12  Epoch: 210  Training loss = 2.5289  Validation loss = 1.0928  \n",
      "\n",
      "Fold: 12  Epoch: 211  Training loss = 2.5285  Validation loss = 1.0947  \n",
      "\n",
      "Fold: 12  Epoch: 212  Training loss = 2.5281  Validation loss = 1.0943  \n",
      "\n",
      "Fold: 12  Epoch: 213  Training loss = 2.5278  Validation loss = 1.0930  \n",
      "\n",
      "Fold: 12  Epoch: 214  Training loss = 2.5275  Validation loss = 1.0915  \n",
      "\n",
      "Fold: 12  Epoch: 215  Training loss = 2.5271  Validation loss = 1.0896  \n",
      "\n",
      "Fold: 12  Epoch: 216  Training loss = 2.5268  Validation loss = 1.0895  \n",
      "\n",
      "Fold: 12  Epoch: 217  Training loss = 2.5265  Validation loss = 1.0878  \n",
      "\n",
      "Fold: 12  Epoch: 218  Training loss = 2.5261  Validation loss = 1.0862  \n",
      "\n",
      "Fold: 12  Epoch: 219  Training loss = 2.5257  Validation loss = 1.0835  \n",
      "\n",
      "Fold: 12  Epoch: 220  Training loss = 2.5252  Validation loss = 1.0809  \n",
      "\n",
      "Fold: 12  Epoch: 221  Training loss = 2.5249  Validation loss = 1.0783  \n",
      "\n",
      "Fold: 12  Epoch: 222  Training loss = 2.5245  Validation loss = 1.0772  \n",
      "\n",
      "Fold: 12  Epoch: 223  Training loss = 2.5242  Validation loss = 1.0765  \n",
      "\n",
      "Fold: 12  Epoch: 224  Training loss = 2.5238  Validation loss = 1.0729  \n",
      "\n",
      "Fold: 12  Epoch: 225  Training loss = 2.5234  Validation loss = 1.0699  \n",
      "\n",
      "Fold: 12  Epoch: 226  Training loss = 2.5233  Validation loss = 1.0709  \n",
      "\n",
      "Fold: 12  Epoch: 227  Training loss = 2.5231  Validation loss = 1.0727  \n",
      "\n",
      "Fold: 12  Epoch: 228  Training loss = 2.5226  Validation loss = 1.0706  \n",
      "\n",
      "Fold: 12  Epoch: 229  Training loss = 2.5224  Validation loss = 1.0701  \n",
      "\n",
      "Fold: 12  Epoch: 230  Training loss = 2.5221  Validation loss = 1.0680  \n",
      "\n",
      "Fold: 12  Epoch: 231  Training loss = 2.5217  Validation loss = 1.0650  \n",
      "\n",
      "Fold: 12  Epoch: 232  Training loss = 2.5215  Validation loss = 1.0652  \n",
      "\n",
      "Fold: 12  Epoch: 233  Training loss = 2.5213  Validation loss = 1.0641  \n",
      "\n",
      "Fold: 12  Epoch: 234  Training loss = 2.5211  Validation loss = 1.0651  \n",
      "\n",
      "Fold: 12  Epoch: 235  Training loss = 2.5210  Validation loss = 1.0643  \n",
      "\n",
      "Fold: 12  Epoch: 236  Training loss = 2.5208  Validation loss = 1.0639  \n",
      "\n",
      "Fold: 12  Epoch: 237  Training loss = 2.5204  Validation loss = 1.0620  \n",
      "\n",
      "Fold: 12  Epoch: 238  Training loss = 2.5201  Validation loss = 1.0599  \n",
      "\n",
      "Fold: 12  Epoch: 239  Training loss = 2.5197  Validation loss = 1.0587  \n",
      "\n",
      "Fold: 12  Epoch: 240  Training loss = 2.5194  Validation loss = 1.0561  \n",
      "\n",
      "Fold: 12  Epoch: 241  Training loss = 2.5190  Validation loss = 1.0525  \n",
      "\n",
      "Fold: 12  Epoch: 242  Training loss = 2.5186  Validation loss = 1.0517  \n",
      "\n",
      "Fold: 12  Epoch: 243  Training loss = 2.5183  Validation loss = 1.0532  \n",
      "\n",
      "Fold: 12  Epoch: 244  Training loss = 2.5179  Validation loss = 1.0498  \n",
      "\n",
      "Fold: 12  Epoch: 245  Training loss = 2.5177  Validation loss = 1.0504  \n",
      "\n",
      "Fold: 12  Epoch: 246  Training loss = 2.5177  Validation loss = 1.0537  \n",
      "\n",
      "Fold: 12  Epoch: 247  Training loss = 2.5174  Validation loss = 1.0540  \n",
      "\n",
      "Fold: 12  Epoch: 248  Training loss = 2.5170  Validation loss = 1.0495  \n",
      "\n",
      "Fold: 12  Epoch: 249  Training loss = 2.5167  Validation loss = 1.0486  \n",
      "\n",
      "Fold: 12  Epoch: 250  Training loss = 2.5163  Validation loss = 1.0481  \n",
      "\n",
      "Fold: 12  Epoch: 251  Training loss = 2.5160  Validation loss = 1.0467  \n",
      "\n",
      "Fold: 12  Epoch: 252  Training loss = 2.5156  Validation loss = 1.0452  \n",
      "\n",
      "Fold: 12  Epoch: 253  Training loss = 2.5153  Validation loss = 1.0452  \n",
      "\n",
      "Fold: 12  Epoch: 254  Training loss = 2.5149  Validation loss = 1.0434  \n",
      "\n",
      "Fold: 12  Epoch: 255  Training loss = 2.5149  Validation loss = 1.0463  \n",
      "\n",
      "Fold: 12  Epoch: 256  Training loss = 2.5144  Validation loss = 1.0425  \n",
      "\n",
      "Fold: 12  Epoch: 257  Training loss = 2.5141  Validation loss = 1.0427  \n",
      "\n",
      "Fold: 12  Epoch: 258  Training loss = 2.5139  Validation loss = 1.0428  \n",
      "\n",
      "Fold: 12  Epoch: 259  Training loss = 2.5135  Validation loss = 1.0395  \n",
      "\n",
      "Fold: 12  Epoch: 260  Training loss = 2.5132  Validation loss = 1.0392  \n",
      "\n",
      "Fold: 12  Epoch: 261  Training loss = 2.5130  Validation loss = 1.0398  \n",
      "\n",
      "Fold: 12  Epoch: 262  Training loss = 2.5125  Validation loss = 1.0361  \n",
      "\n",
      "Fold: 12  Epoch: 263  Training loss = 2.5122  Validation loss = 1.0324  \n",
      "\n",
      "Fold: 12  Epoch: 264  Training loss = 2.5120  Validation loss = 1.0348  \n",
      "\n",
      "Fold: 12  Epoch: 265  Training loss = 2.5116  Validation loss = 1.0333  \n",
      "\n",
      "Fold: 12  Epoch: 266  Training loss = 2.5116  Validation loss = 1.0363  \n",
      "\n",
      "Fold: 12  Epoch: 267  Training loss = 2.5111  Validation loss = 1.0329  \n",
      "\n",
      "Fold: 12  Epoch: 268  Training loss = 2.5108  Validation loss = 1.0327  \n",
      "\n",
      "Fold: 12  Epoch: 269  Training loss = 2.5104  Validation loss = 1.0309  \n",
      "\n",
      "Fold: 12  Epoch: 270  Training loss = 2.5101  Validation loss = 1.0289  \n",
      "\n",
      "Fold: 12  Epoch: 271  Training loss = 2.5097  Validation loss = 1.0269  \n",
      "\n",
      "Fold: 12  Epoch: 272  Training loss = 2.5095  Validation loss = 1.0268  \n",
      "\n",
      "Fold: 12  Epoch: 273  Training loss = 2.5090  Validation loss = 1.0245  \n",
      "\n",
      "Fold: 12  Epoch: 274  Training loss = 2.5087  Validation loss = 1.0249  \n",
      "\n",
      "Fold: 12  Epoch: 275  Training loss = 2.5082  Validation loss = 1.0237  \n",
      "\n",
      "Fold: 12  Epoch: 276  Training loss = 2.5077  Validation loss = 1.0202  \n",
      "\n",
      "Fold: 12  Epoch: 277  Training loss = 2.5072  Validation loss = 1.0197  \n",
      "\n",
      "Fold: 12  Epoch: 278  Training loss = 2.5067  Validation loss = 1.0171  \n",
      "\n",
      "Fold: 12  Epoch: 279  Training loss = 2.5066  Validation loss = 1.0179  \n",
      "\n",
      "Fold: 12  Epoch: 280  Training loss = 2.5063  Validation loss = 1.0187  \n",
      "\n",
      "Fold: 12  Epoch: 281  Training loss = 2.5059  Validation loss = 1.0165  \n",
      "\n",
      "Fold: 12  Epoch: 282  Training loss = 2.5058  Validation loss = 1.0187  \n",
      "\n",
      "Fold: 12  Epoch: 283  Training loss = 2.5055  Validation loss = 1.0164  \n",
      "\n",
      "Fold: 12  Epoch: 284  Training loss = 2.5053  Validation loss = 1.0174  \n",
      "\n",
      "Fold: 12  Epoch: 285  Training loss = 2.5051  Validation loss = 1.0162  \n",
      "\n",
      "Fold: 12  Epoch: 286  Training loss = 2.5050  Validation loss = 1.0198  \n",
      "\n",
      "Fold: 12  Epoch: 287  Training loss = 2.5047  Validation loss = 1.0197  \n",
      "\n",
      "Fold: 12  Epoch: 288  Training loss = 2.5042  Validation loss = 1.0138  \n",
      "\n",
      "Fold: 12  Epoch: 289  Training loss = 2.5038  Validation loss = 1.0124  \n",
      "\n",
      "Fold: 12  Epoch: 290  Training loss = 2.5036  Validation loss = 1.0131  \n",
      "\n",
      "Fold: 12  Epoch: 291  Training loss = 2.5033  Validation loss = 1.0124  \n",
      "\n",
      "Fold: 12  Epoch: 292  Training loss = 2.5031  Validation loss = 1.0119  \n",
      "\n",
      "Fold: 12  Epoch: 293  Training loss = 2.5027  Validation loss = 1.0106  \n",
      "\n",
      "Fold: 12  Epoch: 294  Training loss = 2.5025  Validation loss = 1.0118  \n",
      "\n",
      "Fold: 12  Epoch: 295  Training loss = 2.5024  Validation loss = 1.0147  \n",
      "\n",
      "Fold: 12  Epoch: 296  Training loss = 2.5021  Validation loss = 1.0133  \n",
      "\n",
      "Fold: 12  Epoch: 297  Training loss = 2.5018  Validation loss = 1.0130  \n",
      "\n",
      "Fold: 12  Epoch: 298  Training loss = 2.5018  Validation loss = 1.0150  \n",
      "\n",
      "Fold: 12  Epoch: 299  Training loss = 2.5014  Validation loss = 1.0123  \n",
      "\n",
      "Fold: 12  Epoch: 300  Training loss = 2.5013  Validation loss = 1.0118  \n",
      "\n",
      "Fold: 12  Epoch: 301  Training loss = 2.5010  Validation loss = 1.0104  \n",
      "\n",
      "Fold: 12  Epoch: 302  Training loss = 2.5007  Validation loss = 1.0097  \n",
      "\n",
      "Fold: 12  Epoch: 303  Training loss = 2.5006  Validation loss = 1.0101  \n",
      "\n",
      "Fold: 12  Epoch: 304  Training loss = 2.5004  Validation loss = 1.0109  \n",
      "\n",
      "Fold: 12  Epoch: 305  Training loss = 2.5003  Validation loss = 1.0107  \n",
      "\n",
      "Fold: 12  Epoch: 306  Training loss = 2.5000  Validation loss = 1.0110  \n",
      "\n",
      "Fold: 12  Epoch: 307  Training loss = 2.4999  Validation loss = 1.0111  \n",
      "\n",
      "Fold: 12  Epoch: 308  Training loss = 2.4995  Validation loss = 1.0098  \n",
      "\n",
      "Fold: 12  Epoch: 309  Training loss = 2.4994  Validation loss = 1.0111  \n",
      "\n",
      "Fold: 12  Epoch: 310  Training loss = 2.4991  Validation loss = 1.0112  \n",
      "\n",
      "Fold: 12  Epoch: 311  Training loss = 2.4985  Validation loss = 1.0086  \n",
      "\n",
      "Fold: 12  Epoch: 312  Training loss = 2.4984  Validation loss = 1.0095  \n",
      "\n",
      "Fold: 12  Epoch: 313  Training loss = 2.4981  Validation loss = 1.0058  \n",
      "\n",
      "Fold: 12  Epoch: 314  Training loss = 2.4977  Validation loss = 1.0034  \n",
      "\n",
      "Fold: 12  Epoch: 315  Training loss = 2.4948  Validation loss = 0.9983  \n",
      "\n",
      "Fold: 12  Epoch: 316  Training loss = 2.4945  Validation loss = 0.9983  \n",
      "\n",
      "Fold: 12  Epoch: 317  Training loss = 2.4941  Validation loss = 0.9948  \n",
      "\n",
      "Fold: 12  Epoch: 318  Training loss = 2.4937  Validation loss = 0.9910  \n",
      "\n",
      "Fold: 12  Epoch: 319  Training loss = 2.4935  Validation loss = 0.9923  \n",
      "\n",
      "Fold: 12  Epoch: 320  Training loss = 2.4933  Validation loss = 0.9898  \n",
      "\n",
      "Fold: 12  Epoch: 321  Training loss = 2.4930  Validation loss = 0.9892  \n",
      "\n",
      "Fold: 12  Epoch: 322  Training loss = 2.4926  Validation loss = 0.9867  \n",
      "\n",
      "Fold: 12  Epoch: 323  Training loss = 2.4924  Validation loss = 0.9864  \n",
      "\n",
      "Fold: 12  Epoch: 324  Training loss = 2.4922  Validation loss = 0.9856  \n",
      "\n",
      "Fold: 12  Epoch: 325  Training loss = 2.4920  Validation loss = 0.9858  \n",
      "\n",
      "Fold: 12  Epoch: 326  Training loss = 2.4917  Validation loss = 0.9858  \n",
      "\n",
      "Fold: 12  Epoch: 327  Training loss = 2.4916  Validation loss = 0.9870  \n",
      "\n",
      "Fold: 12  Epoch: 328  Training loss = 2.4912  Validation loss = 0.9850  \n",
      "\n",
      "Fold: 12  Epoch: 329  Training loss = 2.4910  Validation loss = 0.9871  \n",
      "\n",
      "Fold: 12  Epoch: 330  Training loss = 2.4907  Validation loss = 0.9877  \n",
      "\n",
      "Fold: 12  Epoch: 331  Training loss = 2.4904  Validation loss = 0.9873  \n",
      "\n",
      "Fold: 12  Epoch: 332  Training loss = 2.4901  Validation loss = 0.9865  \n",
      "\n",
      "Fold: 12  Epoch: 333  Training loss = 2.4899  Validation loss = 0.9885  \n",
      "\n",
      "Fold: 12  Epoch: 334  Training loss = 2.4897  Validation loss = 0.9905  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 328  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 2.4404  Validation loss = 3.2885  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 2.4399  Validation loss = 3.2851  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 2.4397  Validation loss = 3.2839  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 2.4395  Validation loss = 3.2836  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 2.4395  Validation loss = 3.2853  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 2.4392  Validation loss = 3.2836  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 2.4391  Validation loss = 3.2828  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 2.4386  Validation loss = 3.2795  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 2.4383  Validation loss = 3.2779  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 2.4379  Validation loss = 3.2738  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 2.4377  Validation loss = 3.2738  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 2.4369  Validation loss = 3.2670  \n",
      "\n",
      "Fold: 13  Epoch: 13  Training loss = 2.4366  Validation loss = 3.2647  \n",
      "\n",
      "Fold: 13  Epoch: 14  Training loss = 2.4364  Validation loss = 3.2657  \n",
      "\n",
      "Fold: 13  Epoch: 15  Training loss = 2.4360  Validation loss = 3.2617  \n",
      "\n",
      "Fold: 13  Epoch: 16  Training loss = 2.4357  Validation loss = 3.2590  \n",
      "\n",
      "Fold: 13  Epoch: 17  Training loss = 2.4350  Validation loss = 3.2523  \n",
      "\n",
      "Fold: 13  Epoch: 18  Training loss = 2.4346  Validation loss = 3.2505  \n",
      "\n",
      "Fold: 13  Epoch: 19  Training loss = 2.4343  Validation loss = 3.2475  \n",
      "\n",
      "Fold: 13  Epoch: 20  Training loss = 2.4336  Validation loss = 3.2421  \n",
      "\n",
      "Fold: 13  Epoch: 21  Training loss = 2.4333  Validation loss = 3.2399  \n",
      "\n",
      "Fold: 13  Epoch: 22  Training loss = 2.4326  Validation loss = 3.2341  \n",
      "\n",
      "Fold: 13  Epoch: 23  Training loss = 2.4322  Validation loss = 3.2316  \n",
      "\n",
      "Fold: 13  Epoch: 24  Training loss = 2.4320  Validation loss = 3.2305  \n",
      "\n",
      "Fold: 13  Epoch: 25  Training loss = 2.4322  Validation loss = 3.2350  \n",
      "\n",
      "Fold: 13  Epoch: 26  Training loss = 2.4318  Validation loss = 3.2324  \n",
      "\n",
      "Fold: 13  Epoch: 27  Training loss = 2.4314  Validation loss = 3.2288  \n",
      "\n",
      "Fold: 13  Epoch: 28  Training loss = 2.4312  Validation loss = 3.2279  \n",
      "\n",
      "Fold: 13  Epoch: 29  Training loss = 2.4310  Validation loss = 3.2263  \n",
      "\n",
      "Fold: 13  Epoch: 30  Training loss = 2.4306  Validation loss = 3.2240  \n",
      "\n",
      "Fold: 13  Epoch: 31  Training loss = 2.4303  Validation loss = 3.2230  \n",
      "\n",
      "Fold: 13  Epoch: 32  Training loss = 2.4298  Validation loss = 3.2199  \n",
      "\n",
      "Fold: 13  Epoch: 33  Training loss = 2.4298  Validation loss = 3.2206  \n",
      "\n",
      "Fold: 13  Epoch: 34  Training loss = 2.4294  Validation loss = 3.2174  \n",
      "\n",
      "Fold: 13  Epoch: 35  Training loss = 2.4292  Validation loss = 3.2156  \n",
      "\n",
      "Fold: 13  Epoch: 36  Training loss = 2.4289  Validation loss = 3.2139  \n",
      "\n",
      "Fold: 13  Epoch: 37  Training loss = 2.4286  Validation loss = 3.2110  \n",
      "\n",
      "Fold: 13  Epoch: 38  Training loss = 2.4282  Validation loss = 3.2088  \n",
      "\n",
      "Fold: 13  Epoch: 39  Training loss = 2.4280  Validation loss = 3.2086  \n",
      "\n",
      "Fold: 13  Epoch: 40  Training loss = 2.4279  Validation loss = 3.2091  \n",
      "\n",
      "Fold: 13  Epoch: 41  Training loss = 2.4275  Validation loss = 3.2067  \n",
      "\n",
      "Fold: 13  Epoch: 42  Training loss = 2.4272  Validation loss = 3.2050  \n",
      "\n",
      "Fold: 13  Epoch: 43  Training loss = 2.4269  Validation loss = 3.2034  \n",
      "\n",
      "Fold: 13  Epoch: 44  Training loss = 2.4266  Validation loss = 3.2009  \n",
      "\n",
      "Fold: 13  Epoch: 45  Training loss = 2.4264  Validation loss = 3.1996  \n",
      "\n",
      "Fold: 13  Epoch: 46  Training loss = 2.4262  Validation loss = 3.1982  \n",
      "\n",
      "Fold: 13  Epoch: 47  Training loss = 2.4259  Validation loss = 3.1962  \n",
      "\n",
      "Fold: 13  Epoch: 48  Training loss = 2.4257  Validation loss = 3.1968  \n",
      "\n",
      "Fold: 13  Epoch: 49  Training loss = 2.4253  Validation loss = 3.1970  \n",
      "\n",
      "Fold: 13  Epoch: 50  Training loss = 2.4252  Validation loss = 3.2003  \n",
      "\n",
      "Fold: 13  Epoch: 51  Training loss = 2.4251  Validation loss = 3.2019  \n",
      "\n",
      "Fold: 13  Epoch: 52  Training loss = 2.4250  Validation loss = 3.2019  \n",
      "\n",
      "Fold: 13  Epoch: 53  Training loss = 2.4248  Validation loss = 3.2023  \n",
      "\n",
      "Fold: 13  Epoch: 54  Training loss = 2.4247  Validation loss = 3.2007  \n",
      "\n",
      "Fold: 13  Epoch: 55  Training loss = 2.4244  Validation loss = 3.2000  \n",
      "\n",
      "Fold: 13  Epoch: 56  Training loss = 2.4238  Validation loss = 3.1939  \n",
      "\n",
      "Fold: 13  Epoch: 57  Training loss = 2.4236  Validation loss = 3.1926  \n",
      "\n",
      "Fold: 13  Epoch: 58  Training loss = 2.4232  Validation loss = 3.1885  \n",
      "\n",
      "Fold: 13  Epoch: 59  Training loss = 2.4227  Validation loss = 3.1845  \n",
      "\n",
      "Fold: 13  Epoch: 60  Training loss = 2.4226  Validation loss = 3.1859  \n",
      "\n",
      "Fold: 13  Epoch: 61  Training loss = 2.4224  Validation loss = 3.1844  \n",
      "\n",
      "Fold: 13  Epoch: 62  Training loss = 2.4220  Validation loss = 3.1808  \n",
      "\n",
      "Fold: 13  Epoch: 63  Training loss = 2.4219  Validation loss = 3.1818  \n",
      "\n",
      "Fold: 13  Epoch: 64  Training loss = 2.4215  Validation loss = 3.1775  \n",
      "\n",
      "Fold: 13  Epoch: 65  Training loss = 2.4206  Validation loss = 3.1783  \n",
      "\n",
      "Fold: 13  Epoch: 66  Training loss = 2.4181  Validation loss = 3.1739  \n",
      "\n",
      "Fold: 13  Epoch: 67  Training loss = 2.4180  Validation loss = 3.1743  \n",
      "\n",
      "Fold: 13  Epoch: 68  Training loss = 2.4173  Validation loss = 3.1673  \n",
      "\n",
      "Fold: 13  Epoch: 69  Training loss = 2.4172  Validation loss = 3.1676  \n",
      "\n",
      "Fold: 13  Epoch: 70  Training loss = 2.4169  Validation loss = 3.1671  \n",
      "\n",
      "Fold: 13  Epoch: 71  Training loss = 2.4166  Validation loss = 3.1654  \n",
      "\n",
      "Fold: 13  Epoch: 72  Training loss = 2.4163  Validation loss = 3.1612  \n",
      "\n",
      "Fold: 13  Epoch: 73  Training loss = 2.4163  Validation loss = 3.1619  \n",
      "\n",
      "Fold: 13  Epoch: 74  Training loss = 2.4157  Validation loss = 3.1565  \n",
      "\n",
      "Fold: 13  Epoch: 75  Training loss = 2.4159  Validation loss = 3.1608  \n",
      "\n",
      "Fold: 13  Epoch: 76  Training loss = 2.4158  Validation loss = 3.1607  \n",
      "\n",
      "Fold: 13  Epoch: 77  Training loss = 2.4159  Validation loss = 3.1628  \n",
      "\n",
      "Fold: 13  Epoch: 78  Training loss = 2.4155  Validation loss = 3.1596  \n",
      "\n",
      "Fold: 13  Epoch: 79  Training loss = 2.4152  Validation loss = 3.1582  \n",
      "\n",
      "Fold: 13  Epoch: 80  Training loss = 2.4153  Validation loss = 3.1622  \n",
      "\n",
      "Fold: 13  Epoch: 81  Training loss = 2.4151  Validation loss = 3.1612  \n",
      "\n",
      "Fold: 13  Epoch: 82  Training loss = 2.4146  Validation loss = 3.1569  \n",
      "\n",
      "Fold: 13  Epoch: 83  Training loss = 2.4145  Validation loss = 3.1573  \n",
      "\n",
      "Fold: 13  Epoch: 84  Training loss = 2.4143  Validation loss = 3.1549  \n",
      "\n",
      "Fold: 13  Epoch: 85  Training loss = 2.4143  Validation loss = 3.1572  \n",
      "\n",
      "Fold: 13  Epoch: 86  Training loss = 2.4140  Validation loss = 3.1557  \n",
      "\n",
      "Fold: 13  Epoch: 87  Training loss = 2.4136  Validation loss = 3.1529  \n",
      "\n",
      "Fold: 13  Epoch: 88  Training loss = 2.4131  Validation loss = 3.1483  \n",
      "\n",
      "Fold: 13  Epoch: 89  Training loss = 2.4126  Validation loss = 3.1433  \n",
      "\n",
      "Fold: 13  Epoch: 90  Training loss = 2.4123  Validation loss = 3.1393  \n",
      "\n",
      "Fold: 13  Epoch: 91  Training loss = 2.4117  Validation loss = 3.1324  \n",
      "\n",
      "Fold: 13  Epoch: 92  Training loss = 2.4114  Validation loss = 3.1321  \n",
      "\n",
      "Fold: 13  Epoch: 93  Training loss = 2.4109  Validation loss = 3.1255  \n",
      "\n",
      "Fold: 13  Epoch: 94  Training loss = 2.4108  Validation loss = 3.1269  \n",
      "\n",
      "Fold: 13  Epoch: 95  Training loss = 2.4106  Validation loss = 3.1247  \n",
      "\n",
      "Fold: 13  Epoch: 96  Training loss = 2.4105  Validation loss = 3.1261  \n",
      "\n",
      "Fold: 13  Epoch: 97  Training loss = 2.4103  Validation loss = 3.1259  \n",
      "\n",
      "Fold: 13  Epoch: 98  Training loss = 2.4100  Validation loss = 3.1236  \n",
      "\n",
      "Fold: 13  Epoch: 99  Training loss = 2.4100  Validation loss = 3.1247  \n",
      "\n",
      "Fold: 13  Epoch: 100  Training loss = 2.4098  Validation loss = 3.1239  \n",
      "\n",
      "Fold: 13  Epoch: 101  Training loss = 2.4095  Validation loss = 3.1212  \n",
      "\n",
      "Fold: 13  Epoch: 102  Training loss = 2.4092  Validation loss = 3.1190  \n",
      "\n",
      "Fold: 13  Epoch: 103  Training loss = 2.4093  Validation loss = 3.1220  \n",
      "\n",
      "Fold: 13  Epoch: 104  Training loss = 2.4089  Validation loss = 3.1175  \n",
      "\n",
      "Fold: 13  Epoch: 105  Training loss = 2.4084  Validation loss = 3.1109  \n",
      "\n",
      "Fold: 13  Epoch: 106  Training loss = 2.4082  Validation loss = 3.1106  \n",
      "\n",
      "Fold: 13  Epoch: 107  Training loss = 2.4080  Validation loss = 3.1108  \n",
      "\n",
      "Fold: 13  Epoch: 108  Training loss = 2.4076  Validation loss = 3.1066  \n",
      "\n",
      "Fold: 13  Epoch: 109  Training loss = 2.4074  Validation loss = 3.1072  \n",
      "\n",
      "Fold: 13  Epoch: 110  Training loss = 2.4071  Validation loss = 3.1026  \n",
      "\n",
      "Fold: 13  Epoch: 111  Training loss = 2.4067  Validation loss = 3.0986  \n",
      "\n",
      "Fold: 13  Epoch: 112  Training loss = 2.4063  Validation loss = 3.0952  \n",
      "\n",
      "Fold: 13  Epoch: 113  Training loss = 2.4061  Validation loss = 3.0934  \n",
      "\n",
      "Fold: 13  Epoch: 114  Training loss = 2.4061  Validation loss = 3.0962  \n",
      "\n",
      "Fold: 13  Epoch: 115  Training loss = 2.4058  Validation loss = 3.0933  \n",
      "\n",
      "Fold: 13  Epoch: 116  Training loss = 2.4058  Validation loss = 3.0951  \n",
      "\n",
      "Fold: 13  Epoch: 117  Training loss = 2.4055  Validation loss = 3.0915  \n",
      "\n",
      "Fold: 13  Epoch: 118  Training loss = 2.4052  Validation loss = 3.0902  \n",
      "\n",
      "Fold: 13  Epoch: 119  Training loss = 2.4048  Validation loss = 3.0854  \n",
      "\n",
      "Fold: 13  Epoch: 120  Training loss = 2.4046  Validation loss = 3.0834  \n",
      "\n",
      "Fold: 13  Epoch: 121  Training loss = 2.4042  Validation loss = 3.0805  \n",
      "\n",
      "Fold: 13  Epoch: 122  Training loss = 2.4040  Validation loss = 3.0781  \n",
      "\n",
      "Fold: 13  Epoch: 123  Training loss = 2.4037  Validation loss = 3.0750  \n",
      "\n",
      "Fold: 13  Epoch: 124  Training loss = 2.4035  Validation loss = 3.0745  \n",
      "\n",
      "Fold: 13  Epoch: 125  Training loss = 2.4033  Validation loss = 3.0714  \n",
      "\n",
      "Fold: 13  Epoch: 126  Training loss = 2.4031  Validation loss = 3.0700  \n",
      "\n",
      "Fold: 13  Epoch: 127  Training loss = 2.4030  Validation loss = 3.0700  \n",
      "\n",
      "Fold: 13  Epoch: 128  Training loss = 2.4027  Validation loss = 3.0668  \n",
      "\n",
      "Fold: 13  Epoch: 129  Training loss = 2.4023  Validation loss = 3.0624  \n",
      "\n",
      "Fold: 13  Epoch: 130  Training loss = 2.4020  Validation loss = 3.0609  \n",
      "\n",
      "Fold: 13  Epoch: 131  Training loss = 2.4018  Validation loss = 3.0577  \n",
      "\n",
      "Fold: 13  Epoch: 132  Training loss = 2.4016  Validation loss = 3.0594  \n",
      "\n",
      "Fold: 13  Epoch: 133  Training loss = 2.4015  Validation loss = 3.0592  \n",
      "\n",
      "Fold: 13  Epoch: 134  Training loss = 2.4014  Validation loss = 3.0606  \n",
      "\n",
      "Fold: 13  Epoch: 135  Training loss = 2.4011  Validation loss = 3.0589  \n",
      "\n",
      "Fold: 13  Epoch: 136  Training loss = 2.4010  Validation loss = 3.0597  \n",
      "\n",
      "Fold: 13  Epoch: 137  Training loss = 2.4005  Validation loss = 3.0547  \n",
      "\n",
      "Fold: 13  Epoch: 138  Training loss = 2.4005  Validation loss = 3.0567  \n",
      "\n",
      "Fold: 13  Epoch: 139  Training loss = 2.4004  Validation loss = 3.0574  \n",
      "\n",
      "Fold: 13  Epoch: 140  Training loss = 2.4002  Validation loss = 3.0557  \n",
      "\n",
      "Fold: 13  Epoch: 141  Training loss = 2.4000  Validation loss = 3.0544  \n",
      "\n",
      "Fold: 13  Epoch: 142  Training loss = 2.3996  Validation loss = 3.0493  \n",
      "\n",
      "Fold: 13  Epoch: 143  Training loss = 2.3994  Validation loss = 3.0483  \n",
      "\n",
      "Fold: 13  Epoch: 144  Training loss = 2.3991  Validation loss = 3.0455  \n",
      "\n",
      "Fold: 13  Epoch: 145  Training loss = 2.3989  Validation loss = 3.0446  \n",
      "\n",
      "Fold: 13  Epoch: 146  Training loss = 2.3986  Validation loss = 3.0390  \n",
      "\n",
      "Fold: 13  Epoch: 147  Training loss = 2.3984  Validation loss = 3.0392  \n",
      "\n",
      "Fold: 13  Epoch: 148  Training loss = 2.3982  Validation loss = 3.0382  \n",
      "\n",
      "Fold: 13  Epoch: 149  Training loss = 2.3980  Validation loss = 3.0364  \n",
      "\n",
      "Fold: 13  Epoch: 150  Training loss = 2.3977  Validation loss = 3.0342  \n",
      "\n",
      "Fold: 13  Epoch: 151  Training loss = 2.3974  Validation loss = 3.0320  \n",
      "\n",
      "Fold: 13  Epoch: 152  Training loss = 2.3973  Validation loss = 3.0297  \n",
      "\n",
      "Fold: 13  Epoch: 153  Training loss = 2.3970  Validation loss = 3.0283  \n",
      "\n",
      "Fold: 13  Epoch: 154  Training loss = 2.3967  Validation loss = 3.0232  \n",
      "\n",
      "Fold: 13  Epoch: 155  Training loss = 2.3964  Validation loss = 3.0212  \n",
      "\n",
      "Fold: 13  Epoch: 156  Training loss = 2.3963  Validation loss = 3.0222  \n",
      "\n",
      "Fold: 13  Epoch: 157  Training loss = 2.3962  Validation loss = 3.0233  \n",
      "\n",
      "Fold: 13  Epoch: 158  Training loss = 2.3962  Validation loss = 3.0293  \n",
      "\n",
      "Fold: 13  Epoch: 159  Training loss = 2.3958  Validation loss = 3.0231  \n",
      "\n",
      "Fold: 13  Epoch: 160  Training loss = 2.3957  Validation loss = 3.0261  \n",
      "\n",
      "Fold: 13  Epoch: 161  Training loss = 2.3955  Validation loss = 3.0257  \n",
      "\n",
      "Fold: 13  Epoch: 162  Training loss = 2.3954  Validation loss = 3.0262  \n",
      "\n",
      "Fold: 13  Epoch: 163  Training loss = 2.3952  Validation loss = 3.0275  \n",
      "\n",
      "Fold: 13  Epoch: 164  Training loss = 2.3951  Validation loss = 3.0284  \n",
      "\n",
      "Fold: 13  Epoch: 165  Training loss = 2.3949  Validation loss = 3.0267  \n",
      "\n",
      "Fold: 13  Epoch: 166  Training loss = 2.3946  Validation loss = 3.0235  \n",
      "\n",
      "Fold: 13  Epoch: 167  Training loss = 2.3943  Validation loss = 3.0205  \n",
      "\n",
      "Fold: 13  Epoch: 168  Training loss = 2.3942  Validation loss = 3.0223  \n",
      "\n",
      "Fold: 13  Epoch: 169  Training loss = 2.3940  Validation loss = 3.0245  \n",
      "\n",
      "Fold: 13  Epoch: 170  Training loss = 2.3938  Validation loss = 3.0219  \n",
      "\n",
      "Fold: 13  Epoch: 171  Training loss = 2.3937  Validation loss = 3.0235  \n",
      "\n",
      "Fold: 13  Epoch: 172  Training loss = 2.3935  Validation loss = 3.0197  \n",
      "\n",
      "Fold: 13  Epoch: 173  Training loss = 2.3934  Validation loss = 3.0189  \n",
      "\n",
      "Fold: 13  Epoch: 174  Training loss = 2.3933  Validation loss = 3.0182  \n",
      "\n",
      "Fold: 13  Epoch: 175  Training loss = 2.3931  Validation loss = 3.0166  \n",
      "\n",
      "Fold: 13  Epoch: 176  Training loss = 2.3929  Validation loss = 3.0168  \n",
      "\n",
      "Fold: 13  Epoch: 177  Training loss = 2.3928  Validation loss = 3.0169  \n",
      "\n",
      "Fold: 13  Epoch: 178  Training loss = 2.3925  Validation loss = 3.0131  \n",
      "\n",
      "Fold: 13  Epoch: 179  Training loss = 2.3924  Validation loss = 3.0161  \n",
      "\n",
      "Fold: 13  Epoch: 180  Training loss = 2.3922  Validation loss = 3.0161  \n",
      "\n",
      "Fold: 13  Epoch: 181  Training loss = 2.3921  Validation loss = 3.0174  \n",
      "\n",
      "Fold: 13  Epoch: 182  Training loss = 2.3919  Validation loss = 3.0169  \n",
      "\n",
      "Fold: 13  Epoch: 183  Training loss = 2.3917  Validation loss = 3.0159  \n",
      "\n",
      "Fold: 13  Epoch: 184  Training loss = 2.3915  Validation loss = 3.0134  \n",
      "\n",
      "Fold: 13  Epoch: 185  Training loss = 2.3913  Validation loss = 3.0149  \n",
      "\n",
      "Fold: 13  Epoch: 186  Training loss = 2.3911  Validation loss = 3.0124  \n",
      "\n",
      "Fold: 13  Epoch: 187  Training loss = 2.3910  Validation loss = 3.0111  \n",
      "\n",
      "Fold: 13  Epoch: 188  Training loss = 2.3908  Validation loss = 3.0113  \n",
      "\n",
      "Fold: 13  Epoch: 189  Training loss = 2.3906  Validation loss = 3.0096  \n",
      "\n",
      "Fold: 13  Epoch: 190  Training loss = 2.3906  Validation loss = 3.0130  \n",
      "\n",
      "Fold: 13  Epoch: 191  Training loss = 2.3903  Validation loss = 3.0090  \n",
      "\n",
      "Fold: 13  Epoch: 192  Training loss = 2.3902  Validation loss = 3.0089  \n",
      "\n",
      "Fold: 13  Epoch: 193  Training loss = 2.3900  Validation loss = 3.0095  \n",
      "\n",
      "Fold: 13  Epoch: 194  Training loss = 2.3899  Validation loss = 3.0110  \n",
      "\n",
      "Fold: 13  Epoch: 195  Training loss = 2.3898  Validation loss = 3.0110  \n",
      "\n",
      "Fold: 13  Epoch: 196  Training loss = 2.3897  Validation loss = 3.0114  \n",
      "\n",
      "Fold: 13  Epoch: 197  Training loss = 2.3896  Validation loss = 3.0124  \n",
      "\n",
      "Fold: 13  Epoch: 198  Training loss = 2.3895  Validation loss = 3.0143  \n",
      "\n",
      "Fold: 13  Epoch: 199  Training loss = 2.3892  Validation loss = 3.0117  \n",
      "\n",
      "Fold: 13  Epoch: 200  Training loss = 2.3891  Validation loss = 3.0135  \n",
      "\n",
      "Fold: 13  Epoch: 201  Training loss = 2.3888  Validation loss = 3.0119  \n",
      "\n",
      "Fold: 13  Epoch: 202  Training loss = 2.3887  Validation loss = 3.0118  \n",
      "\n",
      "Fold: 13  Epoch: 203  Training loss = 2.3885  Validation loss = 3.0131  \n",
      "\n",
      "Fold: 13  Epoch: 204  Training loss = 2.3884  Validation loss = 3.0102  \n",
      "\n",
      "Fold: 13  Epoch: 205  Training loss = 2.3882  Validation loss = 3.0088  \n",
      "\n",
      "Fold: 13  Epoch: 206  Training loss = 2.3879  Validation loss = 3.0069  \n",
      "\n",
      "Fold: 13  Epoch: 207  Training loss = 2.3878  Validation loss = 3.0067  \n",
      "\n",
      "Fold: 13  Epoch: 208  Training loss = 2.3877  Validation loss = 3.0099  \n",
      "\n",
      "Fold: 13  Epoch: 209  Training loss = 2.3876  Validation loss = 3.0099  \n",
      "\n",
      "Fold: 13  Epoch: 210  Training loss = 2.3876  Validation loss = 3.0106  \n",
      "\n",
      "Fold: 13  Epoch: 211  Training loss = 2.3874  Validation loss = 3.0104  \n",
      "\n",
      "Fold: 13  Epoch: 212  Training loss = 2.3870  Validation loss = 3.0071  \n",
      "\n",
      "Fold: 13  Epoch: 213  Training loss = 2.3869  Validation loss = 3.0075  \n",
      "\n",
      "Fold: 13  Epoch: 214  Training loss = 2.3867  Validation loss = 3.0070  \n",
      "\n",
      "Fold: 13  Epoch: 215  Training loss = 2.3865  Validation loss = 3.0059  \n",
      "\n",
      "Fold: 13  Epoch: 216  Training loss = 2.3863  Validation loss = 3.0039  \n",
      "\n",
      "Fold: 13  Epoch: 217  Training loss = 2.3861  Validation loss = 3.0040  \n",
      "\n",
      "Fold: 13  Epoch: 218  Training loss = 2.3860  Validation loss = 3.0053  \n",
      "\n",
      "Fold: 13  Epoch: 219  Training loss = 2.3859  Validation loss = 3.0075  \n",
      "\n",
      "Fold: 13  Epoch: 220  Training loss = 2.3859  Validation loss = 3.0095  \n",
      "\n",
      "Fold: 13  Epoch: 221  Training loss = 2.3859  Validation loss = 3.0133  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 216  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 2.4689  Validation loss = 6.2108  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 2.4680  Validation loss = 6.2057  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 2.4672  Validation loss = 6.2005  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 2.4666  Validation loss = 6.1966  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 2.4659  Validation loss = 6.1918  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 2.4661  Validation loss = 6.1934  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 2.4650  Validation loss = 6.1864  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 2.4639  Validation loss = 6.1793  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 2.4631  Validation loss = 6.1739  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 2.4626  Validation loss = 6.1703  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 2.4625  Validation loss = 6.1703  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 2.4624  Validation loss = 6.1700  \n",
      "\n",
      "Fold: 14  Epoch: 13  Training loss = 2.4621  Validation loss = 6.1685  \n",
      "\n",
      "Fold: 14  Epoch: 14  Training loss = 2.4624  Validation loss = 6.1709  \n",
      "\n",
      "Fold: 14  Epoch: 15  Training loss = 2.4618  Validation loss = 6.1665  \n",
      "\n",
      "Fold: 14  Epoch: 16  Training loss = 2.4610  Validation loss = 6.1617  \n",
      "\n",
      "Fold: 14  Epoch: 17  Training loss = 2.4604  Validation loss = 6.1576  \n",
      "\n",
      "Fold: 14  Epoch: 18  Training loss = 2.4598  Validation loss = 6.1530  \n",
      "\n",
      "Fold: 14  Epoch: 19  Training loss = 2.4593  Validation loss = 6.1495  \n",
      "\n",
      "Fold: 14  Epoch: 20  Training loss = 2.4585  Validation loss = 6.1442  \n",
      "\n",
      "Fold: 14  Epoch: 21  Training loss = 2.4578  Validation loss = 6.1391  \n",
      "\n",
      "Fold: 14  Epoch: 22  Training loss = 2.4579  Validation loss = 6.1402  \n",
      "\n",
      "Fold: 14  Epoch: 23  Training loss = 2.4579  Validation loss = 6.1403  \n",
      "\n",
      "Fold: 14  Epoch: 24  Training loss = 2.4575  Validation loss = 6.1378  \n",
      "\n",
      "Fold: 14  Epoch: 25  Training loss = 2.4570  Validation loss = 6.1340  \n",
      "\n",
      "Fold: 14  Epoch: 26  Training loss = 2.4562  Validation loss = 6.1287  \n",
      "\n",
      "Fold: 14  Epoch: 27  Training loss = 2.4559  Validation loss = 6.1261  \n",
      "\n",
      "Fold: 14  Epoch: 28  Training loss = 2.4551  Validation loss = 6.1205  \n",
      "\n",
      "Fold: 14  Epoch: 29  Training loss = 2.4543  Validation loss = 6.1166  \n",
      "\n",
      "Fold: 14  Epoch: 30  Training loss = 2.4539  Validation loss = 6.1129  \n",
      "\n",
      "Fold: 14  Epoch: 31  Training loss = 2.4527  Validation loss = 6.1077  \n",
      "\n",
      "Fold: 14  Epoch: 32  Training loss = 2.4516  Validation loss = 6.1048  \n",
      "\n",
      "Fold: 14  Epoch: 33  Training loss = 2.4511  Validation loss = 6.1022  \n",
      "\n",
      "Fold: 14  Epoch: 34  Training loss = 2.4504  Validation loss = 6.0963  \n",
      "\n",
      "Fold: 14  Epoch: 35  Training loss = 2.4496  Validation loss = 6.0898  \n",
      "\n",
      "Fold: 14  Epoch: 36  Training loss = 2.4491  Validation loss = 6.0863  \n",
      "\n",
      "Fold: 14  Epoch: 37  Training loss = 2.4485  Validation loss = 6.0810  \n",
      "\n",
      "Fold: 14  Epoch: 38  Training loss = 2.4482  Validation loss = 6.0792  \n",
      "\n",
      "Fold: 14  Epoch: 39  Training loss = 2.4478  Validation loss = 6.0759  \n",
      "\n",
      "Fold: 14  Epoch: 40  Training loss = 2.4472  Validation loss = 6.0706  \n",
      "\n",
      "Fold: 14  Epoch: 41  Training loss = 2.4467  Validation loss = 6.0664  \n",
      "\n",
      "Fold: 14  Epoch: 42  Training loss = 2.4462  Validation loss = 6.0629  \n",
      "\n",
      "Fold: 14  Epoch: 43  Training loss = 2.4461  Validation loss = 6.0627  \n",
      "\n",
      "Fold: 14  Epoch: 44  Training loss = 2.4461  Validation loss = 6.0633  \n",
      "\n",
      "Fold: 14  Epoch: 45  Training loss = 2.4459  Validation loss = 6.0613  \n",
      "\n",
      "Fold: 14  Epoch: 46  Training loss = 2.4454  Validation loss = 6.0574  \n",
      "\n",
      "Fold: 14  Epoch: 47  Training loss = 2.4453  Validation loss = 6.0562  \n",
      "\n",
      "Fold: 14  Epoch: 48  Training loss = 2.4451  Validation loss = 6.0546  \n",
      "\n",
      "Fold: 14  Epoch: 49  Training loss = 2.4449  Validation loss = 6.0540  \n",
      "\n",
      "Fold: 14  Epoch: 50  Training loss = 2.4440  Validation loss = 6.0449  \n",
      "\n",
      "Fold: 14  Epoch: 51  Training loss = 2.4436  Validation loss = 6.0419  \n",
      "\n",
      "Fold: 14  Epoch: 52  Training loss = 2.4434  Validation loss = 6.0407  \n",
      "\n",
      "Fold: 14  Epoch: 53  Training loss = 2.4430  Validation loss = 6.0369  \n",
      "\n",
      "Fold: 14  Epoch: 54  Training loss = 2.4427  Validation loss = 6.0348  \n",
      "\n",
      "Fold: 14  Epoch: 55  Training loss = 2.4425  Validation loss = 6.0328  \n",
      "\n",
      "Fold: 14  Epoch: 56  Training loss = 2.4423  Validation loss = 6.0309  \n",
      "\n",
      "Fold: 14  Epoch: 57  Training loss = 2.4417  Validation loss = 6.0254  \n",
      "\n",
      "Fold: 14  Epoch: 58  Training loss = 2.4414  Validation loss = 6.0222  \n",
      "\n",
      "Fold: 14  Epoch: 59  Training loss = 2.4412  Validation loss = 6.0200  \n",
      "\n",
      "Fold: 14  Epoch: 60  Training loss = 2.4409  Validation loss = 6.0175  \n",
      "\n",
      "Fold: 14  Epoch: 61  Training loss = 2.4403  Validation loss = 6.0106  \n",
      "\n",
      "Fold: 14  Epoch: 62  Training loss = 2.4397  Validation loss = 6.0059  \n",
      "\n",
      "Fold: 14  Epoch: 63  Training loss = 2.4394  Validation loss = 6.0026  \n",
      "\n",
      "Fold: 14  Epoch: 64  Training loss = 2.4391  Validation loss = 6.0000  \n",
      "\n",
      "Fold: 14  Epoch: 65  Training loss = 2.4391  Validation loss = 6.0006  \n",
      "\n",
      "Fold: 14  Epoch: 66  Training loss = 2.4389  Validation loss = 5.9987  \n",
      "\n",
      "Fold: 14  Epoch: 67  Training loss = 2.4385  Validation loss = 5.9946  \n",
      "\n",
      "Fold: 14  Epoch: 68  Training loss = 2.4383  Validation loss = 5.9933  \n",
      "\n",
      "Fold: 14  Epoch: 69  Training loss = 2.4377  Validation loss = 5.9864  \n",
      "\n",
      "Fold: 14  Epoch: 70  Training loss = 2.4374  Validation loss = 5.9839  \n",
      "\n",
      "Fold: 14  Epoch: 71  Training loss = 2.4372  Validation loss = 5.9828  \n",
      "\n",
      "Fold: 14  Epoch: 72  Training loss = 2.4369  Validation loss = 5.9797  \n",
      "\n",
      "Fold: 14  Epoch: 73  Training loss = 2.4365  Validation loss = 5.9752  \n",
      "\n",
      "Fold: 14  Epoch: 74  Training loss = 2.4361  Validation loss = 5.9709  \n",
      "\n",
      "Fold: 14  Epoch: 75  Training loss = 2.4360  Validation loss = 5.9715  \n",
      "\n",
      "Fold: 14  Epoch: 76  Training loss = 2.4358  Validation loss = 5.9703  \n",
      "\n",
      "Fold: 14  Epoch: 77  Training loss = 2.4354  Validation loss = 5.9665  \n",
      "\n",
      "Fold: 14  Epoch: 78  Training loss = 2.4351  Validation loss = 5.9627  \n",
      "\n",
      "Fold: 14  Epoch: 79  Training loss = 2.4350  Validation loss = 5.9622  \n",
      "\n",
      "Fold: 14  Epoch: 80  Training loss = 2.4349  Validation loss = 5.9623  \n",
      "\n",
      "Fold: 14  Epoch: 81  Training loss = 2.4347  Validation loss = 5.9612  \n",
      "\n",
      "Fold: 14  Epoch: 82  Training loss = 2.4346  Validation loss = 5.9601  \n",
      "\n",
      "Fold: 14  Epoch: 83  Training loss = 2.4345  Validation loss = 5.9600  \n",
      "\n",
      "Fold: 14  Epoch: 84  Training loss = 2.4342  Validation loss = 5.9573  \n",
      "\n",
      "Fold: 14  Epoch: 85  Training loss = 2.4339  Validation loss = 5.9537  \n",
      "\n",
      "Fold: 14  Epoch: 86  Training loss = 2.4338  Validation loss = 5.9534  \n",
      "\n",
      "Fold: 14  Epoch: 87  Training loss = 2.4338  Validation loss = 5.9541  \n",
      "\n",
      "Fold: 14  Epoch: 88  Training loss = 2.4336  Validation loss = 5.9532  \n",
      "\n",
      "Fold: 14  Epoch: 89  Training loss = 2.4335  Validation loss = 5.9535  \n",
      "\n",
      "Fold: 14  Epoch: 90  Training loss = 2.4337  Validation loss = 5.9587  \n",
      "\n",
      "Fold: 14  Epoch: 91  Training loss = 2.4335  Validation loss = 5.9573  \n",
      "\n",
      "Fold: 14  Epoch: 92  Training loss = 2.4330  Validation loss = 5.9546  \n",
      "\n",
      "Fold: 14  Epoch: 93  Training loss = 2.4328  Validation loss = 5.9533  \n",
      "\n",
      "Fold: 14  Epoch: 94  Training loss = 2.4329  Validation loss = 5.9552  \n",
      "\n",
      "Fold: 14  Epoch: 95  Training loss = 2.4323  Validation loss = 5.9487  \n",
      "\n",
      "Fold: 14  Epoch: 96  Training loss = 2.4318  Validation loss = 5.9423  \n",
      "\n",
      "Fold: 14  Epoch: 97  Training loss = 2.4314  Validation loss = 5.9376  \n",
      "\n",
      "Fold: 14  Epoch: 98  Training loss = 2.4313  Validation loss = 5.9369  \n",
      "\n",
      "Fold: 14  Epoch: 99  Training loss = 2.4310  Validation loss = 5.9335  \n",
      "\n",
      "Fold: 14  Epoch: 100  Training loss = 2.4308  Validation loss = 5.9317  \n",
      "\n",
      "Fold: 14  Epoch: 101  Training loss = 2.4302  Validation loss = 5.9246  \n",
      "\n",
      "Fold: 14  Epoch: 102  Training loss = 2.4301  Validation loss = 5.9242  \n",
      "\n",
      "Fold: 14  Epoch: 103  Training loss = 2.4298  Validation loss = 5.9192  \n",
      "\n",
      "Fold: 14  Epoch: 104  Training loss = 2.4298  Validation loss = 5.9209  \n",
      "\n",
      "Fold: 14  Epoch: 105  Training loss = 2.4293  Validation loss = 5.9141  \n",
      "\n",
      "Fold: 14  Epoch: 106  Training loss = 2.4293  Validation loss = 5.9148  \n",
      "\n",
      "Fold: 14  Epoch: 107  Training loss = 2.4291  Validation loss = 5.9141  \n",
      "\n",
      "Fold: 14  Epoch: 108  Training loss = 2.4288  Validation loss = 5.9104  \n",
      "\n",
      "Fold: 14  Epoch: 109  Training loss = 2.4287  Validation loss = 5.9106  \n",
      "\n",
      "Fold: 14  Epoch: 110  Training loss = 2.4285  Validation loss = 5.9070  \n",
      "\n",
      "Fold: 14  Epoch: 111  Training loss = 2.4284  Validation loss = 5.9074  \n",
      "\n",
      "Fold: 14  Epoch: 112  Training loss = 2.4282  Validation loss = 5.9065  \n",
      "\n",
      "Fold: 14  Epoch: 113  Training loss = 2.4281  Validation loss = 5.9059  \n",
      "\n",
      "Fold: 14  Epoch: 114  Training loss = 2.4278  Validation loss = 5.9027  \n",
      "\n",
      "Fold: 14  Epoch: 115  Training loss = 2.4276  Validation loss = 5.8996  \n",
      "\n",
      "Fold: 14  Epoch: 116  Training loss = 2.4276  Validation loss = 5.9001  \n",
      "\n",
      "Fold: 14  Epoch: 117  Training loss = 2.4276  Validation loss = 5.9031  \n",
      "\n",
      "Fold: 14  Epoch: 118  Training loss = 2.4274  Validation loss = 5.9011  \n",
      "\n",
      "Fold: 14  Epoch: 119  Training loss = 2.4272  Validation loss = 5.8990  \n",
      "\n",
      "Fold: 14  Epoch: 120  Training loss = 2.4271  Validation loss = 5.8997  \n",
      "\n",
      "Fold: 14  Epoch: 121  Training loss = 2.4271  Validation loss = 5.9013  \n",
      "\n",
      "Fold: 14  Epoch: 122  Training loss = 2.4269  Validation loss = 5.8996  \n",
      "\n",
      "Fold: 14  Epoch: 123  Training loss = 2.4271  Validation loss = 5.9035  \n",
      "\n",
      "Fold: 14  Epoch: 124  Training loss = 2.4270  Validation loss = 5.9037  \n",
      "\n",
      "Fold: 14  Epoch: 125  Training loss = 2.4270  Validation loss = 5.9052  \n",
      "\n",
      "Fold: 14  Epoch: 126  Training loss = 2.4267  Validation loss = 5.9018  \n",
      "\n",
      "Fold: 14  Epoch: 127  Training loss = 2.4266  Validation loss = 5.9021  \n",
      "\n",
      "Fold: 14  Epoch: 128  Training loss = 2.4264  Validation loss = 5.8995  \n",
      "\n",
      "Fold: 14  Epoch: 129  Training loss = 2.4263  Validation loss = 5.9001  \n",
      "\n",
      "Fold: 14  Epoch: 130  Training loss = 2.4264  Validation loss = 5.9034  \n",
      "\n",
      "Fold: 14  Epoch: 131  Training loss = 2.4262  Validation loss = 5.9006  \n",
      "\n",
      "Fold: 14  Epoch: 132  Training loss = 2.4261  Validation loss = 5.9011  \n",
      "\n",
      "Fold: 14  Epoch: 133  Training loss = 2.4261  Validation loss = 5.9017  \n",
      "\n",
      "Fold: 14  Epoch: 134  Training loss = 2.4258  Validation loss = 5.8979  \n",
      "\n",
      "Fold: 14  Epoch: 135  Training loss = 2.4253  Validation loss = 5.8916  \n",
      "\n",
      "Fold: 14  Epoch: 136  Training loss = 2.4252  Validation loss = 5.8911  \n",
      "\n",
      "Fold: 14  Epoch: 137  Training loss = 2.4251  Validation loss = 5.8904  \n",
      "\n",
      "Fold: 14  Epoch: 138  Training loss = 2.4249  Validation loss = 5.8879  \n",
      "\n",
      "Fold: 14  Epoch: 139  Training loss = 2.4247  Validation loss = 5.8861  \n",
      "\n",
      "Fold: 14  Epoch: 140  Training loss = 2.4245  Validation loss = 5.8838  \n",
      "\n",
      "Fold: 14  Epoch: 141  Training loss = 2.4242  Validation loss = 5.8789  \n",
      "\n",
      "Fold: 14  Epoch: 142  Training loss = 2.4240  Validation loss = 5.8758  \n",
      "\n",
      "Fold: 14  Epoch: 143  Training loss = 2.4241  Validation loss = 5.8802  \n",
      "\n",
      "Fold: 14  Epoch: 144  Training loss = 2.4242  Validation loss = 5.8854  \n",
      "\n",
      "Fold: 14  Epoch: 145  Training loss = 2.4239  Validation loss = 5.8805  \n",
      "\n",
      "Fold: 14  Epoch: 146  Training loss = 2.4238  Validation loss = 5.8802  \n",
      "\n",
      "Fold: 14  Epoch: 147  Training loss = 2.4237  Validation loss = 5.8809  \n",
      "\n",
      "Fold: 14  Epoch: 148  Training loss = 2.4236  Validation loss = 5.8804  \n",
      "\n",
      "Fold: 14  Epoch: 149  Training loss = 2.4233  Validation loss = 5.8772  \n",
      "\n",
      "Fold: 14  Epoch: 150  Training loss = 2.4233  Validation loss = 5.8784  \n",
      "\n",
      "Fold: 14  Epoch: 151  Training loss = 2.4231  Validation loss = 5.8754  \n",
      "\n",
      "Fold: 14  Epoch: 152  Training loss = 2.4228  Validation loss = 5.8722  \n",
      "\n",
      "Fold: 14  Epoch: 153  Training loss = 2.4228  Validation loss = 5.8724  \n",
      "\n",
      "Fold: 14  Epoch: 154  Training loss = 2.4224  Validation loss = 5.8659  \n",
      "\n",
      "Fold: 14  Epoch: 155  Training loss = 2.4223  Validation loss = 5.8670  \n",
      "\n",
      "Fold: 14  Epoch: 156  Training loss = 2.4222  Validation loss = 5.8673  \n",
      "\n",
      "Fold: 14  Epoch: 157  Training loss = 2.4219  Validation loss = 5.8641  \n",
      "\n",
      "Fold: 14  Epoch: 158  Training loss = 2.4218  Validation loss = 5.8628  \n",
      "\n",
      "Fold: 14  Epoch: 159  Training loss = 2.4215  Validation loss = 5.8588  \n",
      "\n",
      "Fold: 14  Epoch: 160  Training loss = 2.4215  Validation loss = 5.8611  \n",
      "\n",
      "Fold: 14  Epoch: 161  Training loss = 2.4215  Validation loss = 5.8631  \n",
      "\n",
      "Fold: 14  Epoch: 162  Training loss = 2.4217  Validation loss = 5.8657  \n",
      "\n",
      "Fold: 14  Epoch: 163  Training loss = 2.4216  Validation loss = 5.8657  \n",
      "\n",
      "Fold: 14  Epoch: 164  Training loss = 2.4211  Validation loss = 5.8579  \n",
      "\n",
      "Fold: 14  Epoch: 165  Training loss = 2.4209  Validation loss = 5.8532  \n",
      "\n",
      "Fold: 14  Epoch: 166  Training loss = 2.4208  Validation loss = 5.8543  \n",
      "\n",
      "Fold: 14  Epoch: 167  Training loss = 2.4206  Validation loss = 5.8516  \n",
      "\n",
      "Fold: 14  Epoch: 168  Training loss = 2.4205  Validation loss = 5.8520  \n",
      "\n",
      "Fold: 14  Epoch: 169  Training loss = 2.4206  Validation loss = 5.8555  \n",
      "\n",
      "Fold: 14  Epoch: 170  Training loss = 2.4203  Validation loss = 5.8515  \n",
      "\n",
      "Fold: 14  Epoch: 171  Training loss = 2.4201  Validation loss = 5.8490  \n",
      "\n",
      "Fold: 14  Epoch: 172  Training loss = 2.4198  Validation loss = 5.8464  \n",
      "\n",
      "Fold: 14  Epoch: 173  Training loss = 2.4196  Validation loss = 5.8441  \n",
      "\n",
      "Fold: 14  Epoch: 174  Training loss = 2.4195  Validation loss = 5.8435  \n",
      "\n",
      "Fold: 14  Epoch: 175  Training loss = 2.4194  Validation loss = 5.8442  \n",
      "\n",
      "Fold: 14  Epoch: 176  Training loss = 2.4193  Validation loss = 5.8438  \n",
      "\n",
      "Fold: 14  Epoch: 177  Training loss = 2.4190  Validation loss = 5.8382  \n",
      "\n",
      "Fold: 14  Epoch: 178  Training loss = 2.4187  Validation loss = 5.8336  \n",
      "\n",
      "Fold: 14  Epoch: 179  Training loss = 2.4184  Validation loss = 5.8296  \n",
      "\n",
      "Fold: 14  Epoch: 180  Training loss = 2.4182  Validation loss = 5.8251  \n",
      "\n",
      "Fold: 14  Epoch: 181  Training loss = 2.4181  Validation loss = 5.8251  \n",
      "\n",
      "Fold: 14  Epoch: 182  Training loss = 2.4178  Validation loss = 5.8215  \n",
      "\n",
      "Fold: 14  Epoch: 183  Training loss = 2.4176  Validation loss = 5.8181  \n",
      "\n",
      "Fold: 14  Epoch: 184  Training loss = 2.4174  Validation loss = 5.8137  \n",
      "\n",
      "Fold: 14  Epoch: 185  Training loss = 2.4172  Validation loss = 5.8120  \n",
      "\n",
      "Fold: 14  Epoch: 186  Training loss = 2.4170  Validation loss = 5.8068  \n",
      "\n",
      "Fold: 14  Epoch: 187  Training loss = 2.4168  Validation loss = 5.8027  \n",
      "\n",
      "Fold: 14  Epoch: 188  Training loss = 2.4167  Validation loss = 5.8046  \n",
      "\n",
      "Fold: 14  Epoch: 189  Training loss = 2.4166  Validation loss = 5.8060  \n",
      "\n",
      "Fold: 14  Epoch: 190  Training loss = 2.4164  Validation loss = 5.8037  \n",
      "\n",
      "Fold: 14  Epoch: 191  Training loss = 2.4164  Validation loss = 5.8044  \n",
      "\n",
      "Fold: 14  Epoch: 192  Training loss = 2.4163  Validation loss = 5.8035  \n",
      "\n",
      "Fold: 14  Epoch: 193  Training loss = 2.4161  Validation loss = 5.7995  \n",
      "\n",
      "Fold: 14  Epoch: 194  Training loss = 2.4158  Validation loss = 5.7935  \n",
      "\n",
      "Fold: 14  Epoch: 195  Training loss = 2.4157  Validation loss = 5.7942  \n",
      "\n",
      "Fold: 14  Epoch: 196  Training loss = 2.4156  Validation loss = 5.7947  \n",
      "\n",
      "Fold: 14  Epoch: 197  Training loss = 2.4155  Validation loss = 5.7944  \n",
      "\n",
      "Fold: 14  Epoch: 198  Training loss = 2.4153  Validation loss = 5.7920  \n",
      "\n",
      "Fold: 14  Epoch: 199  Training loss = 2.4153  Validation loss = 5.7942  \n",
      "\n",
      "Fold: 14  Epoch: 200  Training loss = 2.4152  Validation loss = 5.7946  \n",
      "\n",
      "Fold: 14  Epoch: 201  Training loss = 2.4152  Validation loss = 5.7945  \n",
      "\n",
      "Fold: 14  Epoch: 202  Training loss = 2.4151  Validation loss = 5.7932  \n",
      "\n",
      "Fold: 14  Epoch: 203  Training loss = 2.4151  Validation loss = 5.7963  \n",
      "\n",
      "Fold: 14  Epoch: 204  Training loss = 2.4149  Validation loss = 5.7953  \n",
      "\n",
      "Fold: 14  Epoch: 205  Training loss = 2.4149  Validation loss = 5.7967  \n",
      "\n",
      "Fold: 14  Epoch: 206  Training loss = 2.4149  Validation loss = 5.7964  \n",
      "\n",
      "Fold: 14  Epoch: 207  Training loss = 2.4149  Validation loss = 5.7986  \n",
      "\n",
      "Fold: 14  Epoch: 208  Training loss = 2.4146  Validation loss = 5.7920  \n",
      "\n",
      "Fold: 14  Epoch: 209  Training loss = 2.4144  Validation loss = 5.7878  \n",
      "\n",
      "Fold: 14  Epoch: 210  Training loss = 2.4142  Validation loss = 5.7837  \n",
      "\n",
      "Fold: 14  Epoch: 211  Training loss = 2.4141  Validation loss = 5.7835  \n",
      "\n",
      "Fold: 14  Epoch: 212  Training loss = 2.4140  Validation loss = 5.7822  \n",
      "\n",
      "Fold: 14  Epoch: 213  Training loss = 2.4138  Validation loss = 5.7806  \n",
      "\n",
      "Fold: 14  Epoch: 214  Training loss = 2.4136  Validation loss = 5.7756  \n",
      "\n",
      "Fold: 14  Epoch: 215  Training loss = 2.4136  Validation loss = 5.7774  \n",
      "\n",
      "Fold: 14  Epoch: 216  Training loss = 2.4134  Validation loss = 5.7751  \n",
      "\n",
      "Fold: 14  Epoch: 217  Training loss = 2.4133  Validation loss = 5.7726  \n",
      "\n",
      "Fold: 14  Epoch: 218  Training loss = 2.4131  Validation loss = 5.7699  \n",
      "\n",
      "Fold: 14  Epoch: 219  Training loss = 2.4130  Validation loss = 5.7715  \n",
      "\n",
      "Fold: 14  Epoch: 220  Training loss = 2.4130  Validation loss = 5.7738  \n",
      "\n",
      "Fold: 14  Epoch: 221  Training loss = 2.4130  Validation loss = 5.7762  \n",
      "\n",
      "Fold: 14  Epoch: 222  Training loss = 2.4129  Validation loss = 5.7757  \n",
      "\n",
      "Fold: 14  Epoch: 223  Training loss = 2.4128  Validation loss = 5.7757  \n",
      "\n",
      "Fold: 14  Epoch: 224  Training loss = 2.4127  Validation loss = 5.7728  \n",
      "\n",
      "Fold: 14  Epoch: 225  Training loss = 2.4125  Validation loss = 5.7699  \n",
      "\n",
      "Fold: 14  Epoch: 226  Training loss = 2.4122  Validation loss = 5.7629  \n",
      "\n",
      "Fold: 14  Epoch: 227  Training loss = 2.4121  Validation loss = 5.7614  \n",
      "\n",
      "Fold: 14  Epoch: 228  Training loss = 2.4121  Validation loss = 5.7652  \n",
      "\n",
      "Fold: 14  Epoch: 229  Training loss = 2.4119  Validation loss = 5.7632  \n",
      "\n",
      "Fold: 14  Epoch: 230  Training loss = 2.4117  Validation loss = 5.7597  \n",
      "\n",
      "Fold: 14  Epoch: 231  Training loss = 2.4117  Validation loss = 5.7631  \n",
      "\n",
      "Fold: 14  Epoch: 232  Training loss = 2.4116  Validation loss = 5.7600  \n",
      "\n",
      "Fold: 14  Epoch: 233  Training loss = 2.4115  Validation loss = 5.7586  \n",
      "\n",
      "Fold: 14  Epoch: 234  Training loss = 2.4115  Validation loss = 5.7615  \n",
      "\n",
      "Fold: 14  Epoch: 235  Training loss = 2.4113  Validation loss = 5.7594  \n",
      "\n",
      "Fold: 14  Epoch: 236  Training loss = 2.4112  Validation loss = 5.7614  \n",
      "\n",
      "Fold: 14  Epoch: 237  Training loss = 2.4112  Validation loss = 5.7615  \n",
      "\n",
      "Fold: 14  Epoch: 238  Training loss = 2.4111  Validation loss = 5.7634  \n",
      "\n",
      "Fold: 14  Epoch: 239  Training loss = 2.4110  Validation loss = 5.7646  \n",
      "\n",
      "Fold: 14  Epoch: 240  Training loss = 2.4109  Validation loss = 5.7634  \n",
      "\n",
      "Fold: 14  Epoch: 241  Training loss = 2.4107  Validation loss = 5.7606  \n",
      "\n",
      "Fold: 14  Epoch: 242  Training loss = 2.4105  Validation loss = 5.7569  \n",
      "\n",
      "Fold: 14  Epoch: 243  Training loss = 2.4104  Validation loss = 5.7559  \n",
      "\n",
      "Fold: 14  Epoch: 244  Training loss = 2.4103  Validation loss = 5.7567  \n",
      "\n",
      "Fold: 14  Epoch: 245  Training loss = 2.4103  Validation loss = 5.7586  \n",
      "\n",
      "Fold: 14  Epoch: 246  Training loss = 2.4103  Validation loss = 5.7619  \n",
      "\n",
      "Fold: 14  Epoch: 247  Training loss = 2.4102  Validation loss = 5.7612  \n",
      "\n",
      "Fold: 14  Epoch: 248  Training loss = 2.4101  Validation loss = 5.7639  \n",
      "\n",
      "Fold: 14  Epoch: 249  Training loss = 2.4100  Validation loss = 5.7631  \n",
      "\n",
      "Fold: 14  Epoch: 250  Training loss = 2.4099  Validation loss = 5.7614  \n",
      "\n",
      "Fold: 14  Epoch: 251  Training loss = 2.4097  Validation loss = 5.7590  \n",
      "\n",
      "Fold: 14  Epoch: 252  Training loss = 2.4096  Validation loss = 5.7578  \n",
      "\n",
      "Fold: 14  Epoch: 253  Training loss = 2.4096  Validation loss = 5.7618  \n",
      "\n",
      "Fold: 14  Epoch: 254  Training loss = 2.4095  Validation loss = 5.7600  \n",
      "\n",
      "Fold: 14  Epoch: 255  Training loss = 2.4094  Validation loss = 5.7592  \n",
      "\n",
      "Fold: 14  Epoch: 256  Training loss = 2.4094  Validation loss = 5.7622  \n",
      "\n",
      "Fold: 14  Epoch: 257  Training loss = 2.4093  Validation loss = 5.7601  \n",
      "\n",
      "Fold: 14  Epoch: 258  Training loss = 2.4090  Validation loss = 5.7563  \n",
      "\n",
      "Fold: 14  Epoch: 259  Training loss = 2.4089  Validation loss = 5.7542  \n",
      "\n",
      "Fold: 14  Epoch: 260  Training loss = 2.4088  Validation loss = 5.7557  \n",
      "\n",
      "Fold: 14  Epoch: 261  Training loss = 2.4087  Validation loss = 5.7533  \n",
      "\n",
      "Fold: 14  Epoch: 262  Training loss = 2.4087  Validation loss = 5.7549  \n",
      "\n",
      "Fold: 14  Epoch: 263  Training loss = 2.4087  Validation loss = 5.7562  \n",
      "\n",
      "Fold: 14  Epoch: 264  Training loss = 2.4085  Validation loss = 5.7520  \n",
      "\n",
      "Fold: 14  Epoch: 265  Training loss = 2.4084  Validation loss = 5.7516  \n",
      "\n",
      "Fold: 14  Epoch: 266  Training loss = 2.4083  Validation loss = 5.7538  \n",
      "\n",
      "Fold: 14  Epoch: 267  Training loss = 2.4082  Validation loss = 5.7534  \n",
      "\n",
      "Fold: 14  Epoch: 268  Training loss = 2.4080  Validation loss = 5.7503  \n",
      "\n",
      "Fold: 14  Epoch: 269  Training loss = 2.4079  Validation loss = 5.7496  \n",
      "\n",
      "Fold: 14  Epoch: 270  Training loss = 2.4078  Validation loss = 5.7470  \n",
      "\n",
      "Fold: 14  Epoch: 271  Training loss = 2.4077  Validation loss = 5.7459  \n",
      "\n",
      "Fold: 14  Epoch: 272  Training loss = 2.4074  Validation loss = 5.7419  \n",
      "\n",
      "Fold: 14  Epoch: 273  Training loss = 2.4074  Validation loss = 5.7439  \n",
      "\n",
      "Fold: 14  Epoch: 274  Training loss = 2.4073  Validation loss = 5.7449  \n",
      "\n",
      "Fold: 14  Epoch: 275  Training loss = 2.4070  Validation loss = 5.7397  \n",
      "\n",
      "Fold: 14  Epoch: 276  Training loss = 2.4069  Validation loss = 5.7384  \n",
      "\n",
      "Fold: 14  Epoch: 277  Training loss = 2.4068  Validation loss = 5.7362  \n",
      "\n",
      "Fold: 14  Epoch: 278  Training loss = 2.4068  Validation loss = 5.7382  \n",
      "\n",
      "Fold: 14  Epoch: 279  Training loss = 2.4066  Validation loss = 5.7348  \n",
      "\n",
      "Fold: 14  Epoch: 280  Training loss = 2.4066  Validation loss = 5.7363  \n",
      "\n",
      "Fold: 14  Epoch: 281  Training loss = 2.4064  Validation loss = 5.7345  \n",
      "\n",
      "Fold: 14  Epoch: 282  Training loss = 2.4064  Validation loss = 5.7385  \n",
      "\n",
      "Fold: 14  Epoch: 283  Training loss = 2.4063  Validation loss = 5.7396  \n",
      "\n",
      "Fold: 14  Epoch: 284  Training loss = 2.4063  Validation loss = 5.7416  \n",
      "\n",
      "Fold: 14  Epoch: 285  Training loss = 2.4062  Validation loss = 5.7414  \n",
      "\n",
      "Fold: 14  Epoch: 286  Training loss = 2.4061  Validation loss = 5.7395  \n",
      "\n",
      "Fold: 14  Epoch: 287  Training loss = 2.4059  Validation loss = 5.7358  \n",
      "\n",
      "Fold: 14  Epoch: 288  Training loss = 2.4058  Validation loss = 5.7371  \n",
      "\n",
      "Fold: 14  Epoch: 289  Training loss = 2.4057  Validation loss = 5.7358  \n",
      "\n",
      "Fold: 14  Epoch: 290  Training loss = 2.4056  Validation loss = 5.7338  \n",
      "\n",
      "Fold: 14  Epoch: 291  Training loss = 2.4054  Validation loss = 5.7292  \n",
      "\n",
      "Fold: 14  Epoch: 292  Training loss = 2.4053  Validation loss = 5.7254  \n",
      "\n",
      "Fold: 14  Epoch: 293  Training loss = 2.4053  Validation loss = 5.7283  \n",
      "\n",
      "Fold: 14  Epoch: 294  Training loss = 2.4051  Validation loss = 5.7242  \n",
      "\n",
      "Fold: 14  Epoch: 295  Training loss = 2.4050  Validation loss = 5.7233  \n",
      "\n",
      "Fold: 14  Epoch: 296  Training loss = 2.4050  Validation loss = 5.7271  \n",
      "\n",
      "Fold: 14  Epoch: 297  Training loss = 2.4048  Validation loss = 5.7216  \n",
      "\n",
      "Fold: 14  Epoch: 298  Training loss = 2.4047  Validation loss = 5.7203  \n",
      "\n",
      "Fold: 14  Epoch: 299  Training loss = 2.4046  Validation loss = 5.7233  \n",
      "\n",
      "Fold: 14  Epoch: 300  Training loss = 2.4045  Validation loss = 5.7206  \n",
      "\n",
      "Fold: 14  Epoch: 301  Training loss = 2.4043  Validation loss = 5.7181  \n",
      "\n",
      "Fold: 14  Epoch: 302  Training loss = 2.4043  Validation loss = 5.7218  \n",
      "\n",
      "Fold: 14  Epoch: 303  Training loss = 2.4042  Validation loss = 5.7213  \n",
      "\n",
      "Fold: 14  Epoch: 304  Training loss = 2.4042  Validation loss = 5.7220  \n",
      "\n",
      "Fold: 14  Epoch: 305  Training loss = 2.4041  Validation loss = 5.7219  \n",
      "\n",
      "Fold: 14  Epoch: 306  Training loss = 2.4039  Validation loss = 5.7190  \n",
      "\n",
      "Fold: 14  Epoch: 307  Training loss = 2.4039  Validation loss = 5.7199  \n",
      "\n",
      "Fold: 14  Epoch: 308  Training loss = 2.4038  Validation loss = 5.7158  \n",
      "\n",
      "Fold: 14  Epoch: 309  Training loss = 2.4036  Validation loss = 5.7144  \n",
      "\n",
      "Fold: 14  Epoch: 310  Training loss = 2.4036  Validation loss = 5.7155  \n",
      "\n",
      "Fold: 14  Epoch: 311  Training loss = 2.4035  Validation loss = 5.7153  \n",
      "\n",
      "Fold: 14  Epoch: 312  Training loss = 2.4035  Validation loss = 5.7188  \n",
      "\n",
      "Fold: 14  Epoch: 313  Training loss = 2.4034  Validation loss = 5.7187  \n",
      "\n",
      "Fold: 14  Epoch: 314  Training loss = 2.4034  Validation loss = 5.7231  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 309  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 2.7677  Validation loss = 6.2853  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 2.7662  Validation loss = 6.2779  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 2.7649  Validation loss = 6.2716  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 2.7638  Validation loss = 6.2656  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 2.7631  Validation loss = 6.2621  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 2.7618  Validation loss = 6.2563  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 2.7607  Validation loss = 6.2515  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 2.7600  Validation loss = 6.2483  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 2.7585  Validation loss = 6.2404  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 2.7580  Validation loss = 6.2386  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 2.7570  Validation loss = 6.2335  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 2.7564  Validation loss = 6.2304  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 2.7559  Validation loss = 6.2276  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 2.7546  Validation loss = 6.2212  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 2.7539  Validation loss = 6.2176  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 2.7534  Validation loss = 6.2152  \n",
      "\n",
      "Fold: 15  Epoch: 17  Training loss = 2.7530  Validation loss = 6.2127  \n",
      "\n",
      "Fold: 15  Epoch: 18  Training loss = 2.7531  Validation loss = 6.2143  \n",
      "\n",
      "Fold: 15  Epoch: 19  Training loss = 2.7522  Validation loss = 6.2098  \n",
      "\n",
      "Fold: 15  Epoch: 20  Training loss = 2.7514  Validation loss = 6.2056  \n",
      "\n",
      "Fold: 15  Epoch: 21  Training loss = 2.7509  Validation loss = 6.2037  \n",
      "\n",
      "Fold: 15  Epoch: 22  Training loss = 2.7505  Validation loss = 6.2011  \n",
      "\n",
      "Fold: 15  Epoch: 23  Training loss = 2.7505  Validation loss = 6.2012  \n",
      "\n",
      "Fold: 15  Epoch: 24  Training loss = 2.7494  Validation loss = 6.1957  \n",
      "\n",
      "Fold: 15  Epoch: 25  Training loss = 2.7485  Validation loss = 6.1902  \n",
      "\n",
      "Fold: 15  Epoch: 26  Training loss = 2.7477  Validation loss = 6.1859  \n",
      "\n",
      "Fold: 15  Epoch: 27  Training loss = 2.7482  Validation loss = 6.1889  \n",
      "\n",
      "Fold: 15  Epoch: 28  Training loss = 2.7479  Validation loss = 6.1876  \n",
      "\n",
      "Fold: 15  Epoch: 29  Training loss = 2.7477  Validation loss = 6.1864  \n",
      "\n",
      "Fold: 15  Epoch: 30  Training loss = 2.7477  Validation loss = 6.1867  \n",
      "\n",
      "Fold: 15  Epoch: 31  Training loss = 2.7472  Validation loss = 6.1847  \n",
      "\n",
      "Fold: 15  Epoch: 32  Training loss = 2.7467  Validation loss = 6.1824  \n",
      "\n",
      "Fold: 15  Epoch: 33  Training loss = 2.7457  Validation loss = 6.1764  \n",
      "\n",
      "Fold: 15  Epoch: 34  Training loss = 2.7451  Validation loss = 6.1729  \n",
      "\n",
      "Fold: 15  Epoch: 35  Training loss = 2.7446  Validation loss = 6.1703  \n",
      "\n",
      "Fold: 15  Epoch: 36  Training loss = 2.7439  Validation loss = 6.1664  \n",
      "\n",
      "Fold: 15  Epoch: 37  Training loss = 2.7428  Validation loss = 6.1592  \n",
      "\n",
      "Fold: 15  Epoch: 38  Training loss = 2.7425  Validation loss = 6.1579  \n",
      "\n",
      "Fold: 15  Epoch: 39  Training loss = 2.7424  Validation loss = 6.1574  \n",
      "\n",
      "Fold: 15  Epoch: 40  Training loss = 2.7422  Validation loss = 6.1558  \n",
      "\n",
      "Fold: 15  Epoch: 41  Training loss = 2.7409  Validation loss = 6.1482  \n",
      "\n",
      "Fold: 15  Epoch: 42  Training loss = 2.7401  Validation loss = 6.1433  \n",
      "\n",
      "Fold: 15  Epoch: 43  Training loss = 2.7396  Validation loss = 6.1403  \n",
      "\n",
      "Fold: 15  Epoch: 44  Training loss = 2.7390  Validation loss = 6.1371  \n",
      "\n",
      "Fold: 15  Epoch: 45  Training loss = 2.7386  Validation loss = 6.1349  \n",
      "\n",
      "Fold: 15  Epoch: 46  Training loss = 2.7384  Validation loss = 6.1339  \n",
      "\n",
      "Fold: 15  Epoch: 47  Training loss = 2.7386  Validation loss = 6.1353  \n",
      "\n",
      "Fold: 15  Epoch: 48  Training loss = 2.7379  Validation loss = 6.1308  \n",
      "\n",
      "Fold: 15  Epoch: 49  Training loss = 2.7375  Validation loss = 6.1287  \n",
      "\n",
      "Fold: 15  Epoch: 50  Training loss = 2.7368  Validation loss = 6.1254  \n",
      "\n",
      "Fold: 15  Epoch: 51  Training loss = 2.7362  Validation loss = 6.1213  \n",
      "\n",
      "Fold: 15  Epoch: 52  Training loss = 2.7358  Validation loss = 6.1191  \n",
      "\n",
      "Fold: 15  Epoch: 53  Training loss = 2.7353  Validation loss = 6.1156  \n",
      "\n",
      "Fold: 15  Epoch: 54  Training loss = 2.7355  Validation loss = 6.1175  \n",
      "\n",
      "Fold: 15  Epoch: 55  Training loss = 2.7349  Validation loss = 6.1141  \n",
      "\n",
      "Fold: 15  Epoch: 56  Training loss = 2.7338  Validation loss = 6.1069  \n",
      "\n",
      "Fold: 15  Epoch: 57  Training loss = 2.7331  Validation loss = 6.1017  \n",
      "\n",
      "Fold: 15  Epoch: 58  Training loss = 2.7329  Validation loss = 6.1013  \n",
      "\n",
      "Fold: 15  Epoch: 59  Training loss = 2.7300  Validation loss = 6.0941  \n",
      "\n",
      "Fold: 15  Epoch: 60  Training loss = 2.7294  Validation loss = 6.0904  \n",
      "\n",
      "Fold: 15  Epoch: 61  Training loss = 2.7289  Validation loss = 6.0867  \n",
      "\n",
      "Fold: 15  Epoch: 62  Training loss = 2.7284  Validation loss = 6.0840  \n",
      "\n",
      "Fold: 15  Epoch: 63  Training loss = 2.7282  Validation loss = 6.0828  \n",
      "\n",
      "Fold: 15  Epoch: 64  Training loss = 2.7277  Validation loss = 6.0818  \n",
      "\n",
      "Fold: 15  Epoch: 65  Training loss = 2.7269  Validation loss = 6.0816  \n",
      "\n",
      "Fold: 15  Epoch: 66  Training loss = 2.7257  Validation loss = 6.0775  \n",
      "\n",
      "Fold: 15  Epoch: 67  Training loss = 2.7249  Validation loss = 6.0723  \n",
      "\n",
      "Fold: 15  Epoch: 68  Training loss = 2.7244  Validation loss = 6.0689  \n",
      "\n",
      "Fold: 15  Epoch: 69  Training loss = 2.7240  Validation loss = 6.0668  \n",
      "\n",
      "Fold: 15  Epoch: 70  Training loss = 2.7241  Validation loss = 6.0676  \n",
      "\n",
      "Fold: 15  Epoch: 71  Training loss = 2.7240  Validation loss = 6.0678  \n",
      "\n",
      "Fold: 15  Epoch: 72  Training loss = 2.7237  Validation loss = 6.0656  \n",
      "\n",
      "Fold: 15  Epoch: 73  Training loss = 2.7229  Validation loss = 6.0599  \n",
      "\n",
      "Fold: 15  Epoch: 74  Training loss = 2.7226  Validation loss = 6.0577  \n",
      "\n",
      "Fold: 15  Epoch: 75  Training loss = 2.7222  Validation loss = 6.0560  \n",
      "\n",
      "Fold: 15  Epoch: 76  Training loss = 2.7220  Validation loss = 6.0547  \n",
      "\n",
      "Fold: 15  Epoch: 77  Training loss = 2.7211  Validation loss = 6.0477  \n",
      "\n",
      "Fold: 15  Epoch: 78  Training loss = 2.7210  Validation loss = 6.0476  \n",
      "\n",
      "Fold: 15  Epoch: 79  Training loss = 2.7209  Validation loss = 6.0472  \n",
      "\n",
      "Fold: 15  Epoch: 80  Training loss = 2.7200  Validation loss = 6.0401  \n",
      "\n",
      "Fold: 15  Epoch: 81  Training loss = 2.7195  Validation loss = 6.0358  \n",
      "\n",
      "Fold: 15  Epoch: 82  Training loss = 2.7192  Validation loss = 6.0345  \n",
      "\n",
      "Fold: 15  Epoch: 83  Training loss = 2.7192  Validation loss = 6.0352  \n",
      "\n",
      "Fold: 15  Epoch: 84  Training loss = 2.7186  Validation loss = 6.0307  \n",
      "\n",
      "Fold: 15  Epoch: 85  Training loss = 2.7182  Validation loss = 6.0279  \n",
      "\n",
      "Fold: 15  Epoch: 86  Training loss = 2.7179  Validation loss = 6.0270  \n",
      "\n",
      "Fold: 15  Epoch: 87  Training loss = 2.7172  Validation loss = 6.0201  \n",
      "\n",
      "Fold: 15  Epoch: 88  Training loss = 2.7170  Validation loss = 6.0198  \n",
      "\n",
      "Fold: 15  Epoch: 89  Training loss = 2.7166  Validation loss = 6.0164  \n",
      "\n",
      "Fold: 15  Epoch: 90  Training loss = 2.7163  Validation loss = 6.0149  \n",
      "\n",
      "Fold: 15  Epoch: 91  Training loss = 2.7159  Validation loss = 6.0112  \n",
      "\n",
      "Fold: 15  Epoch: 92  Training loss = 2.7157  Validation loss = 6.0104  \n",
      "\n",
      "Fold: 15  Epoch: 93  Training loss = 2.7155  Validation loss = 6.0100  \n",
      "\n",
      "Fold: 15  Epoch: 94  Training loss = 2.7152  Validation loss = 6.0077  \n",
      "\n",
      "Fold: 15  Epoch: 95  Training loss = 2.7148  Validation loss = 6.0049  \n",
      "\n",
      "Fold: 15  Epoch: 96  Training loss = 2.7148  Validation loss = 6.0056  \n",
      "\n",
      "Fold: 15  Epoch: 97  Training loss = 2.7146  Validation loss = 6.0046  \n",
      "\n",
      "Fold: 15  Epoch: 98  Training loss = 2.7142  Validation loss = 6.0021  \n",
      "\n",
      "Fold: 15  Epoch: 99  Training loss = 2.7136  Validation loss = 5.9959  \n",
      "\n",
      "Fold: 15  Epoch: 100  Training loss = 2.7134  Validation loss = 5.9944  \n",
      "\n",
      "Fold: 15  Epoch: 101  Training loss = 2.7129  Validation loss = 5.9901  \n",
      "\n",
      "Fold: 15  Epoch: 102  Training loss = 2.7128  Validation loss = 5.9909  \n",
      "\n",
      "Fold: 15  Epoch: 103  Training loss = 2.7123  Validation loss = 5.9860  \n",
      "\n",
      "Fold: 15  Epoch: 104  Training loss = 2.7117  Validation loss = 5.9813  \n",
      "\n",
      "Fold: 15  Epoch: 105  Training loss = 2.7118  Validation loss = 5.9837  \n",
      "\n",
      "Fold: 15  Epoch: 106  Training loss = 2.7117  Validation loss = 5.9839  \n",
      "\n",
      "Fold: 15  Epoch: 107  Training loss = 2.7110  Validation loss = 5.9781  \n",
      "\n",
      "Fold: 15  Epoch: 108  Training loss = 2.7111  Validation loss = 5.9801  \n",
      "\n",
      "Fold: 15  Epoch: 109  Training loss = 2.7110  Validation loss = 5.9805  \n",
      "\n",
      "Fold: 15  Epoch: 110  Training loss = 2.7109  Validation loss = 5.9807  \n",
      "\n",
      "Fold: 15  Epoch: 111  Training loss = 2.7105  Validation loss = 5.9775  \n",
      "\n",
      "Fold: 15  Epoch: 112  Training loss = 2.7101  Validation loss = 5.9754  \n",
      "\n",
      "Fold: 15  Epoch: 113  Training loss = 2.7101  Validation loss = 5.9753  \n",
      "\n",
      "Fold: 15  Epoch: 114  Training loss = 2.7102  Validation loss = 5.9778  \n",
      "\n",
      "Fold: 15  Epoch: 115  Training loss = 2.7099  Validation loss = 5.9751  \n",
      "\n",
      "Fold: 15  Epoch: 116  Training loss = 2.7097  Validation loss = 5.9750  \n",
      "\n",
      "Fold: 15  Epoch: 117  Training loss = 2.7095  Validation loss = 5.9735  \n",
      "\n",
      "Fold: 15  Epoch: 118  Training loss = 2.7087  Validation loss = 5.9656  \n",
      "\n",
      "Fold: 15  Epoch: 119  Training loss = 2.7086  Validation loss = 5.9665  \n",
      "\n",
      "Fold: 15  Epoch: 120  Training loss = 2.7086  Validation loss = 5.9668  \n",
      "\n",
      "Fold: 15  Epoch: 121  Training loss = 2.7083  Validation loss = 5.9649  \n",
      "\n",
      "Fold: 15  Epoch: 122  Training loss = 2.7086  Validation loss = 5.9691  \n",
      "\n",
      "Fold: 15  Epoch: 123  Training loss = 2.7083  Validation loss = 5.9671  \n",
      "\n",
      "Fold: 15  Epoch: 124  Training loss = 2.7081  Validation loss = 5.9661  \n",
      "\n",
      "Fold: 15  Epoch: 125  Training loss = 2.7076  Validation loss = 5.9617  \n",
      "\n",
      "Fold: 15  Epoch: 126  Training loss = 2.7074  Validation loss = 5.9606  \n",
      "\n",
      "Fold: 15  Epoch: 127  Training loss = 2.7071  Validation loss = 5.9586  \n",
      "\n",
      "Fold: 15  Epoch: 128  Training loss = 2.7067  Validation loss = 5.9556  \n",
      "\n",
      "Fold: 15  Epoch: 129  Training loss = 2.7064  Validation loss = 5.9536  \n",
      "\n",
      "Fold: 15  Epoch: 130  Training loss = 2.7061  Validation loss = 5.9509  \n",
      "\n",
      "Fold: 15  Epoch: 131  Training loss = 2.7055  Validation loss = 5.9439  \n",
      "\n",
      "Fold: 15  Epoch: 132  Training loss = 2.7051  Validation loss = 5.9391  \n",
      "\n",
      "Fold: 15  Epoch: 133  Training loss = 2.7052  Validation loss = 5.9424  \n",
      "\n",
      "Fold: 15  Epoch: 134  Training loss = 2.7047  Validation loss = 5.9392  \n",
      "\n",
      "Fold: 15  Epoch: 135  Training loss = 2.7045  Validation loss = 5.9368  \n",
      "\n",
      "Fold: 15  Epoch: 136  Training loss = 2.7047  Validation loss = 5.9405  \n",
      "\n",
      "Fold: 15  Epoch: 137  Training loss = 2.7042  Validation loss = 5.9364  \n",
      "\n",
      "Fold: 15  Epoch: 138  Training loss = 2.7037  Validation loss = 5.9303  \n",
      "\n",
      "Fold: 15  Epoch: 139  Training loss = 2.7035  Validation loss = 5.9296  \n",
      "\n",
      "Fold: 15  Epoch: 140  Training loss = 2.7036  Validation loss = 5.9335  \n",
      "\n",
      "Fold: 15  Epoch: 141  Training loss = 2.7033  Validation loss = 5.9301  \n",
      "\n",
      "Fold: 15  Epoch: 142  Training loss = 2.7028  Validation loss = 5.9260  \n",
      "\n",
      "Fold: 15  Epoch: 143  Training loss = 2.7025  Validation loss = 5.9246  \n",
      "\n",
      "Fold: 15  Epoch: 144  Training loss = 2.7024  Validation loss = 5.9254  \n",
      "\n",
      "Fold: 15  Epoch: 145  Training loss = 2.7021  Validation loss = 5.9227  \n",
      "\n",
      "Fold: 15  Epoch: 146  Training loss = 2.7015  Validation loss = 5.9209  \n",
      "\n",
      "Fold: 15  Epoch: 147  Training loss = 2.7003  Validation loss = 5.9148  \n",
      "\n",
      "Fold: 15  Epoch: 148  Training loss = 2.7000  Validation loss = 5.9126  \n",
      "\n",
      "Fold: 15  Epoch: 149  Training loss = 2.6997  Validation loss = 5.9104  \n",
      "\n",
      "Fold: 15  Epoch: 150  Training loss = 2.6997  Validation loss = 5.9125  \n",
      "\n",
      "Fold: 15  Epoch: 151  Training loss = 2.6993  Validation loss = 5.9099  \n",
      "\n",
      "Fold: 15  Epoch: 152  Training loss = 2.6993  Validation loss = 5.9119  \n",
      "\n",
      "Fold: 15  Epoch: 153  Training loss = 2.6993  Validation loss = 5.9127  \n",
      "\n",
      "Fold: 15  Epoch: 154  Training loss = 2.6989  Validation loss = 5.9091  \n",
      "\n",
      "Fold: 15  Epoch: 155  Training loss = 2.6990  Validation loss = 5.9127  \n",
      "\n",
      "Fold: 15  Epoch: 156  Training loss = 2.6990  Validation loss = 5.9145  \n",
      "\n",
      "Fold: 15  Epoch: 157  Training loss = 2.6987  Validation loss = 5.9122  \n",
      "\n",
      "Fold: 15  Epoch: 158  Training loss = 2.6985  Validation loss = 5.9111  \n",
      "\n",
      "Fold: 15  Epoch: 159  Training loss = 2.6983  Validation loss = 5.9109  \n",
      "\n",
      "Fold: 15  Epoch: 160  Training loss = 2.6980  Validation loss = 5.9080  \n",
      "\n",
      "Fold: 15  Epoch: 161  Training loss = 2.6978  Validation loss = 5.9069  \n",
      "\n",
      "Fold: 15  Epoch: 162  Training loss = 2.6976  Validation loss = 5.9056  \n",
      "\n",
      "Fold: 15  Epoch: 163  Training loss = 2.6970  Validation loss = 5.8993  \n",
      "\n",
      "Fold: 15  Epoch: 164  Training loss = 2.6968  Validation loss = 5.8984  \n",
      "\n",
      "Fold: 15  Epoch: 165  Training loss = 2.6965  Validation loss = 5.8949  \n",
      "\n",
      "Fold: 15  Epoch: 166  Training loss = 2.6964  Validation loss = 5.8959  \n",
      "\n",
      "Fold: 15  Epoch: 167  Training loss = 2.6961  Validation loss = 5.8941  \n",
      "\n",
      "Fold: 15  Epoch: 168  Training loss = 2.6959  Validation loss = 5.8933  \n",
      "\n",
      "Fold: 15  Epoch: 169  Training loss = 2.6956  Validation loss = 5.8899  \n",
      "\n",
      "Fold: 15  Epoch: 170  Training loss = 2.6957  Validation loss = 5.8927  \n",
      "\n",
      "Fold: 15  Epoch: 171  Training loss = 2.6957  Validation loss = 5.8942  \n",
      "\n",
      "Fold: 15  Epoch: 172  Training loss = 2.6956  Validation loss = 5.8958  \n",
      "\n",
      "Fold: 15  Epoch: 173  Training loss = 2.6954  Validation loss = 5.8937  \n",
      "\n",
      "Fold: 15  Epoch: 174  Training loss = 2.6951  Validation loss = 5.8917  \n",
      "\n",
      "Fold: 15  Epoch: 175  Training loss = 2.6951  Validation loss = 5.8933  \n",
      "\n",
      "Fold: 15  Epoch: 176  Training loss = 2.6949  Validation loss = 5.8924  \n",
      "\n",
      "Fold: 15  Epoch: 177  Training loss = 2.6948  Validation loss = 5.8912  \n",
      "\n",
      "Fold: 15  Epoch: 178  Training loss = 2.6947  Validation loss = 5.8903  \n",
      "\n",
      "Fold: 15  Epoch: 179  Training loss = 2.6944  Validation loss = 5.8876  \n",
      "\n",
      "Fold: 15  Epoch: 180  Training loss = 2.6943  Validation loss = 5.8868  \n",
      "\n",
      "Fold: 15  Epoch: 181  Training loss = 2.6941  Validation loss = 5.8860  \n",
      "\n",
      "Fold: 15  Epoch: 182  Training loss = 2.6938  Validation loss = 5.8840  \n",
      "\n",
      "Fold: 15  Epoch: 183  Training loss = 2.6939  Validation loss = 5.8861  \n",
      "\n",
      "Fold: 15  Epoch: 184  Training loss = 2.6937  Validation loss = 5.8847  \n",
      "\n",
      "Fold: 15  Epoch: 185  Training loss = 2.6936  Validation loss = 5.8852  \n",
      "\n",
      "Fold: 15  Epoch: 186  Training loss = 2.6936  Validation loss = 5.8875  \n",
      "\n",
      "Fold: 15  Epoch: 187  Training loss = 2.6932  Validation loss = 5.8836  \n",
      "\n",
      "Fold: 15  Epoch: 188  Training loss = 2.6930  Validation loss = 5.8811  \n",
      "\n",
      "Fold: 15  Epoch: 189  Training loss = 2.6927  Validation loss = 5.8795  \n",
      "\n",
      "Fold: 15  Epoch: 190  Training loss = 2.6925  Validation loss = 5.8779  \n",
      "\n",
      "Fold: 15  Epoch: 191  Training loss = 2.6923  Validation loss = 5.8755  \n",
      "\n",
      "Fold: 15  Epoch: 192  Training loss = 2.6921  Validation loss = 5.8755  \n",
      "\n",
      "Fold: 15  Epoch: 193  Training loss = 2.6917  Validation loss = 5.8693  \n",
      "\n",
      "Fold: 15  Epoch: 194  Training loss = 2.6913  Validation loss = 5.8639  \n",
      "\n",
      "Fold: 15  Epoch: 195  Training loss = 2.6912  Validation loss = 5.8652  \n",
      "\n",
      "Fold: 15  Epoch: 196  Training loss = 2.6910  Validation loss = 5.8625  \n",
      "\n",
      "Fold: 15  Epoch: 197  Training loss = 2.6907  Validation loss = 5.8577  \n",
      "\n",
      "Fold: 15  Epoch: 198  Training loss = 2.6906  Validation loss = 5.8611  \n",
      "\n",
      "Fold: 15  Epoch: 199  Training loss = 2.6904  Validation loss = 5.8569  \n",
      "\n",
      "Fold: 15  Epoch: 200  Training loss = 2.6902  Validation loss = 5.8561  \n",
      "\n",
      "Fold: 15  Epoch: 201  Training loss = 2.6901  Validation loss = 5.8575  \n",
      "\n",
      "Fold: 15  Epoch: 202  Training loss = 2.6900  Validation loss = 5.8582  \n",
      "\n",
      "Fold: 15  Epoch: 203  Training loss = 2.6898  Validation loss = 5.8578  \n",
      "\n",
      "Fold: 15  Epoch: 204  Training loss = 2.6896  Validation loss = 5.8557  \n",
      "\n",
      "Fold: 15  Epoch: 205  Training loss = 2.6893  Validation loss = 5.8528  \n",
      "\n",
      "Fold: 15  Epoch: 206  Training loss = 2.6891  Validation loss = 5.8520  \n",
      "\n",
      "Fold: 15  Epoch: 207  Training loss = 2.6890  Validation loss = 5.8531  \n",
      "\n",
      "Fold: 15  Epoch: 208  Training loss = 2.6888  Validation loss = 5.8512  \n",
      "\n",
      "Fold: 15  Epoch: 209  Training loss = 2.6887  Validation loss = 5.8508  \n",
      "\n",
      "Fold: 15  Epoch: 210  Training loss = 2.6885  Validation loss = 5.8511  \n",
      "\n",
      "Fold: 15  Epoch: 211  Training loss = 2.6885  Validation loss = 5.8519  \n",
      "\n",
      "Fold: 15  Epoch: 212  Training loss = 2.6884  Validation loss = 5.8541  \n",
      "\n",
      "Fold: 15  Epoch: 213  Training loss = 2.6884  Validation loss = 5.8562  \n",
      "\n",
      "Fold: 15  Epoch: 214  Training loss = 2.6882  Validation loss = 5.8554  \n",
      "\n",
      "Fold: 15  Epoch: 215  Training loss = 2.6876  Validation loss = 5.8469  \n",
      "\n",
      "Fold: 15  Epoch: 216  Training loss = 2.6875  Validation loss = 5.8471  \n",
      "\n",
      "Fold: 15  Epoch: 217  Training loss = 2.6875  Validation loss = 5.8491  \n",
      "\n",
      "Fold: 15  Epoch: 218  Training loss = 2.6872  Validation loss = 5.8455  \n",
      "\n",
      "Fold: 15  Epoch: 219  Training loss = 2.6870  Validation loss = 5.8438  \n",
      "\n",
      "Fold: 15  Epoch: 220  Training loss = 2.6869  Validation loss = 5.8457  \n",
      "\n",
      "Fold: 15  Epoch: 221  Training loss = 2.6867  Validation loss = 5.8435  \n",
      "\n",
      "Fold: 15  Epoch: 222  Training loss = 2.6866  Validation loss = 5.8433  \n",
      "\n",
      "Fold: 15  Epoch: 223  Training loss = 2.6866  Validation loss = 5.8481  \n",
      "\n",
      "Fold: 15  Epoch: 224  Training loss = 2.6863  Validation loss = 5.8431  \n",
      "\n",
      "Fold: 15  Epoch: 225  Training loss = 2.6861  Validation loss = 5.8458  \n",
      "\n",
      "Fold: 15  Epoch: 226  Training loss = 2.6860  Validation loss = 5.8444  \n",
      "\n",
      "Fold: 15  Epoch: 227  Training loss = 2.6858  Validation loss = 5.8452  \n",
      "\n",
      "Fold: 15  Epoch: 228  Training loss = 2.6860  Validation loss = 5.8503  \n",
      "\n",
      "Fold: 15  Epoch: 229  Training loss = 2.6857  Validation loss = 5.8502  \n",
      "\n",
      "Fold: 15  Epoch: 230  Training loss = 2.6855  Validation loss = 5.8485  \n",
      "\n",
      "Fold: 15  Epoch: 231  Training loss = 2.6853  Validation loss = 5.8483  \n",
      "\n",
      "Fold: 15  Epoch: 232  Training loss = 2.6852  Validation loss = 5.8495  \n",
      "\n",
      "Fold: 15  Epoch: 233  Training loss = 2.6849  Validation loss = 5.8459  \n",
      "\n",
      "Fold: 15  Epoch: 234  Training loss = 2.6848  Validation loss = 5.8449  \n",
      "\n",
      "Fold: 15  Epoch: 235  Training loss = 2.6845  Validation loss = 5.8420  \n",
      "\n",
      "Fold: 15  Epoch: 236  Training loss = 2.6845  Validation loss = 5.8459  \n",
      "\n",
      "Fold: 15  Epoch: 237  Training loss = 2.6844  Validation loss = 5.8486  \n",
      "\n",
      "Fold: 15  Epoch: 238  Training loss = 2.6841  Validation loss = 5.8442  \n",
      "\n",
      "Fold: 15  Epoch: 239  Training loss = 2.6839  Validation loss = 5.8427  \n",
      "\n",
      "Fold: 15  Epoch: 240  Training loss = 2.6839  Validation loss = 5.8466  \n",
      "\n",
      "Fold: 15  Epoch: 241  Training loss = 2.6835  Validation loss = 5.8424  \n",
      "\n",
      "Fold: 15  Epoch: 242  Training loss = 2.6836  Validation loss = 5.8459  \n",
      "\n",
      "Fold: 15  Epoch: 243  Training loss = 2.6835  Validation loss = 5.8484  \n",
      "\n",
      "Fold: 15  Epoch: 244  Training loss = 2.6834  Validation loss = 5.8496  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 235  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 3.0128  Validation loss = 4.1586  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 3.0114  Validation loss = 4.1470  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 3.0092  Validation loss = 4.1381  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 3.0086  Validation loss = 4.1342  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 3.0069  Validation loss = 4.1175  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 3.0057  Validation loss = 4.1088  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 3.0052  Validation loss = 4.1064  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 3.0046  Validation loss = 4.1020  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 3.0034  Validation loss = 4.0962  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 3.0028  Validation loss = 4.0948  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 3.0012  Validation loss = 4.0862  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 3.0005  Validation loss = 4.0789  \n",
      "\n",
      "Fold: 16  Epoch: 13  Training loss = 3.0010  Validation loss = 4.0855  \n",
      "\n",
      "Fold: 16  Epoch: 14  Training loss = 2.9995  Validation loss = 4.0790  \n",
      "\n",
      "Fold: 16  Epoch: 15  Training loss = 2.9979  Validation loss = 4.0755  \n",
      "\n",
      "Fold: 16  Epoch: 16  Training loss = 2.9973  Validation loss = 4.0772  \n",
      "\n",
      "Fold: 16  Epoch: 17  Training loss = 2.9966  Validation loss = 4.0727  \n",
      "\n",
      "Fold: 16  Epoch: 18  Training loss = 2.9953  Validation loss = 4.0665  \n",
      "\n",
      "Fold: 16  Epoch: 19  Training loss = 2.9940  Validation loss = 4.0589  \n",
      "\n",
      "Fold: 16  Epoch: 20  Training loss = 2.9938  Validation loss = 4.0602  \n",
      "\n",
      "Fold: 16  Epoch: 21  Training loss = 2.9932  Validation loss = 4.0535  \n",
      "\n",
      "Fold: 16  Epoch: 22  Training loss = 2.9919  Validation loss = 4.0405  \n",
      "\n",
      "Fold: 16  Epoch: 23  Training loss = 2.9913  Validation loss = 4.0356  \n",
      "\n",
      "Fold: 16  Epoch: 24  Training loss = 2.9903  Validation loss = 4.0280  \n",
      "\n",
      "Fold: 16  Epoch: 25  Training loss = 2.9904  Validation loss = 4.0281  \n",
      "\n",
      "Fold: 16  Epoch: 26  Training loss = 2.9894  Validation loss = 4.0269  \n",
      "\n",
      "Fold: 16  Epoch: 27  Training loss = 2.9888  Validation loss = 4.0242  \n",
      "\n",
      "Fold: 16  Epoch: 28  Training loss = 2.9885  Validation loss = 4.0214  \n",
      "\n",
      "Fold: 16  Epoch: 29  Training loss = 2.9884  Validation loss = 4.0207  \n",
      "\n",
      "Fold: 16  Epoch: 30  Training loss = 2.9874  Validation loss = 4.0121  \n",
      "\n",
      "Fold: 16  Epoch: 31  Training loss = 2.9870  Validation loss = 4.0072  \n",
      "\n",
      "Fold: 16  Epoch: 32  Training loss = 2.9862  Validation loss = 4.0056  \n",
      "\n",
      "Fold: 16  Epoch: 33  Training loss = 2.9849  Validation loss = 3.9978  \n",
      "\n",
      "Fold: 16  Epoch: 34  Training loss = 2.9842  Validation loss = 3.9918  \n",
      "\n",
      "Fold: 16  Epoch: 35  Training loss = 2.9840  Validation loss = 3.9894  \n",
      "\n",
      "Fold: 16  Epoch: 36  Training loss = 2.9839  Validation loss = 3.9918  \n",
      "\n",
      "Fold: 16  Epoch: 37  Training loss = 2.9831  Validation loss = 3.9857  \n",
      "\n",
      "Fold: 16  Epoch: 38  Training loss = 2.9826  Validation loss = 3.9846  \n",
      "\n",
      "Fold: 16  Epoch: 39  Training loss = 2.9821  Validation loss = 3.9824  \n",
      "\n",
      "Fold: 16  Epoch: 40  Training loss = 2.9816  Validation loss = 3.9792  \n",
      "\n",
      "Fold: 16  Epoch: 41  Training loss = 2.9811  Validation loss = 3.9796  \n",
      "\n",
      "Fold: 16  Epoch: 42  Training loss = 2.9808  Validation loss = 3.9801  \n",
      "\n",
      "Fold: 16  Epoch: 43  Training loss = 2.9807  Validation loss = 3.9787  \n",
      "\n",
      "Fold: 16  Epoch: 44  Training loss = 2.9808  Validation loss = 3.9792  \n",
      "\n",
      "Fold: 16  Epoch: 45  Training loss = 2.9804  Validation loss = 3.9743  \n",
      "\n",
      "Fold: 16  Epoch: 46  Training loss = 2.9791  Validation loss = 3.9652  \n",
      "\n",
      "Fold: 16  Epoch: 47  Training loss = 2.9790  Validation loss = 3.9686  \n",
      "\n",
      "Fold: 16  Epoch: 48  Training loss = 2.9786  Validation loss = 3.9605  \n",
      "\n",
      "Fold: 16  Epoch: 49  Training loss = 2.9777  Validation loss = 3.9552  \n",
      "\n",
      "Fold: 16  Epoch: 50  Training loss = 2.9770  Validation loss = 3.9489  \n",
      "\n",
      "Fold: 16  Epoch: 51  Training loss = 2.9767  Validation loss = 3.9495  \n",
      "\n",
      "Fold: 16  Epoch: 52  Training loss = 2.9761  Validation loss = 3.9481  \n",
      "\n",
      "Fold: 16  Epoch: 53  Training loss = 2.9755  Validation loss = 3.9458  \n",
      "\n",
      "Fold: 16  Epoch: 54  Training loss = 2.9753  Validation loss = 3.9439  \n",
      "\n",
      "Fold: 16  Epoch: 55  Training loss = 2.9745  Validation loss = 3.9408  \n",
      "\n",
      "Fold: 16  Epoch: 56  Training loss = 2.9743  Validation loss = 3.9425  \n",
      "\n",
      "Fold: 16  Epoch: 57  Training loss = 2.9731  Validation loss = 3.9317  \n",
      "\n",
      "Fold: 16  Epoch: 58  Training loss = 2.9729  Validation loss = 3.9323  \n",
      "\n",
      "Fold: 16  Epoch: 59  Training loss = 2.9727  Validation loss = 3.9284  \n",
      "\n",
      "Fold: 16  Epoch: 60  Training loss = 2.9719  Validation loss = 3.9188  \n",
      "\n",
      "Fold: 16  Epoch: 61  Training loss = 2.9709  Validation loss = 3.9107  \n",
      "\n",
      "Fold: 16  Epoch: 62  Training loss = 2.9705  Validation loss = 3.9072  \n",
      "\n",
      "Fold: 16  Epoch: 63  Training loss = 2.9705  Validation loss = 3.9087  \n",
      "\n",
      "Fold: 16  Epoch: 64  Training loss = 2.9699  Validation loss = 3.9066  \n",
      "\n",
      "Fold: 16  Epoch: 65  Training loss = 2.9697  Validation loss = 3.9092  \n",
      "\n",
      "Fold: 16  Epoch: 66  Training loss = 2.9690  Validation loss = 3.9039  \n",
      "\n",
      "Fold: 16  Epoch: 67  Training loss = 2.9683  Validation loss = 3.9015  \n",
      "\n",
      "Fold: 16  Epoch: 68  Training loss = 2.9732  Validation loss = 3.9082  \n",
      "\n",
      "Fold: 16  Epoch: 69  Training loss = 2.9707  Validation loss = 3.9050  \n",
      "\n",
      "Fold: 16  Epoch: 70  Training loss = 2.9686  Validation loss = 3.8975  \n",
      "\n",
      "Fold: 16  Epoch: 71  Training loss = 2.9676  Validation loss = 3.8957  \n",
      "\n",
      "Fold: 16  Epoch: 72  Training loss = 2.9669  Validation loss = 3.8948  \n",
      "\n",
      "Fold: 16  Epoch: 73  Training loss = 2.9665  Validation loss = 3.8964  \n",
      "\n",
      "Fold: 16  Epoch: 74  Training loss = 2.9659  Validation loss = 3.8902  \n",
      "\n",
      "Fold: 16  Epoch: 75  Training loss = 2.9652  Validation loss = 3.8870  \n",
      "\n",
      "Fold: 16  Epoch: 76  Training loss = 2.9650  Validation loss = 3.8836  \n",
      "\n",
      "Fold: 16  Epoch: 77  Training loss = 2.9645  Validation loss = 3.8804  \n",
      "\n",
      "Fold: 16  Epoch: 78  Training loss = 2.9638  Validation loss = 3.8755  \n",
      "\n",
      "Fold: 16  Epoch: 79  Training loss = 2.9632  Validation loss = 3.8656  \n",
      "\n",
      "Fold: 16  Epoch: 80  Training loss = 2.9624  Validation loss = 3.8603  \n",
      "\n",
      "Fold: 16  Epoch: 81  Training loss = 2.9622  Validation loss = 3.8646  \n",
      "\n",
      "Fold: 16  Epoch: 82  Training loss = 2.9618  Validation loss = 3.8657  \n",
      "\n",
      "Fold: 16  Epoch: 83  Training loss = 2.9612  Validation loss = 3.8664  \n",
      "\n",
      "Fold: 16  Epoch: 84  Training loss = 2.9610  Validation loss = 3.8655  \n",
      "\n",
      "Fold: 16  Epoch: 85  Training loss = 2.9605  Validation loss = 3.8636  \n",
      "\n",
      "Fold: 16  Epoch: 86  Training loss = 2.9603  Validation loss = 3.8676  \n",
      "\n",
      "Fold: 16  Epoch: 87  Training loss = 2.9600  Validation loss = 3.8646  \n",
      "\n",
      "Fold: 16  Epoch: 88  Training loss = 2.9594  Validation loss = 3.8599  \n",
      "\n",
      "Fold: 16  Epoch: 89  Training loss = 2.9594  Validation loss = 3.8655  \n",
      "\n",
      "Fold: 16  Epoch: 90  Training loss = 2.9594  Validation loss = 3.8694  \n",
      "\n",
      "Fold: 16  Epoch: 91  Training loss = 2.9588  Validation loss = 3.8605  \n",
      "\n",
      "Fold: 16  Epoch: 92  Training loss = 2.9589  Validation loss = 3.8625  \n",
      "\n",
      "Fold: 16  Epoch: 93  Training loss = 2.9583  Validation loss = 3.8601  \n",
      "\n",
      "Fold: 16  Epoch: 94  Training loss = 2.9581  Validation loss = 3.8611  \n",
      "\n",
      "Fold: 16  Epoch: 95  Training loss = 2.9579  Validation loss = 3.8615  \n",
      "\n",
      "Fold: 16  Epoch: 96  Training loss = 2.9575  Validation loss = 3.8607  \n",
      "\n",
      "Fold: 16  Epoch: 97  Training loss = 2.9573  Validation loss = 3.8544  \n",
      "\n",
      "Fold: 16  Epoch: 98  Training loss = 2.9570  Validation loss = 3.8538  \n",
      "\n",
      "Fold: 16  Epoch: 99  Training loss = 2.9566  Validation loss = 3.8532  \n",
      "\n",
      "Fold: 16  Epoch: 100  Training loss = 2.9565  Validation loss = 3.8541  \n",
      "\n",
      "Fold: 16  Epoch: 101  Training loss = 2.9563  Validation loss = 3.8532  \n",
      "\n",
      "Fold: 16  Epoch: 102  Training loss = 2.9559  Validation loss = 3.8470  \n",
      "\n",
      "Fold: 16  Epoch: 103  Training loss = 2.9554  Validation loss = 3.8436  \n",
      "\n",
      "Fold: 16  Epoch: 104  Training loss = 2.9550  Validation loss = 3.8380  \n",
      "\n",
      "Fold: 16  Epoch: 105  Training loss = 2.9547  Validation loss = 3.8362  \n",
      "\n",
      "Fold: 16  Epoch: 106  Training loss = 2.9542  Validation loss = 3.8259  \n",
      "\n",
      "Fold: 16  Epoch: 107  Training loss = 2.9537  Validation loss = 3.8283  \n",
      "\n",
      "Fold: 16  Epoch: 108  Training loss = 2.9534  Validation loss = 3.8285  \n",
      "\n",
      "Fold: 16  Epoch: 109  Training loss = 2.9532  Validation loss = 3.8260  \n",
      "\n",
      "Fold: 16  Epoch: 110  Training loss = 2.9531  Validation loss = 3.8268  \n",
      "\n",
      "Fold: 16  Epoch: 111  Training loss = 2.9531  Validation loss = 3.8308  \n",
      "\n",
      "Fold: 16  Epoch: 112  Training loss = 2.9528  Validation loss = 3.8311  \n",
      "\n",
      "Fold: 16  Epoch: 113  Training loss = 2.9523  Validation loss = 3.8283  \n",
      "\n",
      "Fold: 16  Epoch: 114  Training loss = 2.9522  Validation loss = 3.8292  \n",
      "\n",
      "Fold: 16  Epoch: 115  Training loss = 2.9523  Validation loss = 3.8340  \n",
      "\n",
      "Fold: 16  Epoch: 116  Training loss = 2.9520  Validation loss = 3.8378  \n",
      "\n",
      "Fold: 16  Epoch: 117  Training loss = 2.9518  Validation loss = 3.8370  \n",
      "\n",
      "Fold: 16  Epoch: 118  Training loss = 2.9520  Validation loss = 3.8427  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 106  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 3.0794  Validation loss = 3.8145  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 3.0784  Validation loss = 3.8182  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 3.0770  Validation loss = 3.8208  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 3.0769  Validation loss = 3.8205  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 3.0752  Validation loss = 3.8278  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 3.0746  Validation loss = 3.8298  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 3.0741  Validation loss = 3.8311  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 3.0733  Validation loss = 3.8344  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 3.0726  Validation loss = 3.8362  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 3.0713  Validation loss = 3.8413  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 3.0713  Validation loss = 3.8404  \n",
      "\n",
      "Fold: 17  Epoch: 12  Training loss = 3.0702  Validation loss = 3.8457  \n",
      "\n",
      "Fold: 17  Epoch: 13  Training loss = 3.0681  Validation loss = 3.8553  \n",
      "\n",
      "Fold: 17  Epoch: 14  Training loss = 3.0672  Validation loss = 3.8569  \n",
      "\n",
      "Fold: 17  Epoch: 15  Training loss = 3.0678  Validation loss = 3.8535  \n",
      "\n",
      "Fold: 17  Epoch: 16  Training loss = 3.0657  Validation loss = 3.8616  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 1  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 3.2060  Validation loss = 2.3234  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 3.2048  Validation loss = 2.3239  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 3.2039  Validation loss = 2.3239  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 3.2034  Validation loss = 2.3239  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 3.2030  Validation loss = 2.3232  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 3.2024  Validation loss = 2.3215  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 3.2013  Validation loss = 2.3222  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 3.2000  Validation loss = 2.3228  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 3.1990  Validation loss = 2.3235  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 3.1981  Validation loss = 2.3230  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 3.1972  Validation loss = 2.3216  \n",
      "\n",
      "Fold: 18  Epoch: 12  Training loss = 3.1970  Validation loss = 2.3216  \n",
      "\n",
      "Fold: 18  Epoch: 13  Training loss = 3.1958  Validation loss = 2.3178  \n",
      "\n",
      "Fold: 18  Epoch: 14  Training loss = 3.1947  Validation loss = 2.3200  \n",
      "\n",
      "Fold: 18  Epoch: 15  Training loss = 3.1942  Validation loss = 2.3185  \n",
      "\n",
      "Fold: 18  Epoch: 16  Training loss = 3.1933  Validation loss = 2.3189  \n",
      "\n",
      "Fold: 18  Epoch: 17  Training loss = 3.1927  Validation loss = 2.3201  \n",
      "\n",
      "Fold: 18  Epoch: 18  Training loss = 3.1925  Validation loss = 2.3176  \n",
      "\n",
      "Fold: 18  Epoch: 19  Training loss = 3.1916  Validation loss = 2.3160  \n",
      "\n",
      "Fold: 18  Epoch: 20  Training loss = 3.1912  Validation loss = 2.3156  \n",
      "\n",
      "Fold: 18  Epoch: 21  Training loss = 3.1908  Validation loss = 2.3144  \n",
      "\n",
      "Fold: 18  Epoch: 22  Training loss = 3.1903  Validation loss = 2.3174  \n",
      "\n",
      "Fold: 18  Epoch: 23  Training loss = 3.1895  Validation loss = 2.3169  \n",
      "\n",
      "Fold: 18  Epoch: 24  Training loss = 3.1896  Validation loss = 2.3136  \n",
      "\n",
      "Fold: 18  Epoch: 25  Training loss = 3.1890  Validation loss = 2.3124  \n",
      "\n",
      "Fold: 18  Epoch: 26  Training loss = 3.1887  Validation loss = 2.3122  \n",
      "\n",
      "Fold: 18  Epoch: 27  Training loss = 3.1880  Validation loss = 2.3100  \n",
      "\n",
      "Fold: 18  Epoch: 28  Training loss = 3.1872  Validation loss = 2.3107  \n",
      "\n",
      "Fold: 18  Epoch: 29  Training loss = 3.1867  Validation loss = 2.3128  \n",
      "\n",
      "Fold: 18  Epoch: 30  Training loss = 3.1861  Validation loss = 2.3154  \n",
      "\n",
      "Fold: 18  Epoch: 31  Training loss = 3.1854  Validation loss = 2.3173  \n",
      "\n",
      "Fold: 18  Epoch: 32  Training loss = 3.1843  Validation loss = 2.3187  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 27  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 3.1846  Validation loss = 0.8897  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 3.1828  Validation loss = 0.8891  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 3.1826  Validation loss = 0.8899  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 3.1819  Validation loss = 0.8938  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 3.1817  Validation loss = 0.8988  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 3.1801  Validation loss = 0.8967  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 3.1795  Validation loss = 0.8966  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 3.1785  Validation loss = 0.8961  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 3.1778  Validation loss = 0.8985  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 3.1769  Validation loss = 0.8937  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 3.1761  Validation loss = 0.8950  \n",
      "\n",
      "Fold: 19  Epoch: 12  Training loss = 3.1761  Validation loss = 0.8982  \n",
      "\n",
      "Fold: 19  Epoch: 13  Training loss = 3.1759  Validation loss = 0.9000  \n",
      "\n",
      "Fold: 19  Epoch: 14  Training loss = 3.1755  Validation loss = 0.8958  \n",
      "\n",
      "Fold: 19  Epoch: 15  Training loss = 3.1750  Validation loss = 0.8932  \n",
      "\n",
      "Fold: 19  Epoch: 16  Training loss = 3.1744  Validation loss = 0.8920  \n",
      "\n",
      "Fold: 19  Epoch: 17  Training loss = 3.1741  Validation loss = 0.8889  \n",
      "\n",
      "Fold: 19  Epoch: 18  Training loss = 3.1737  Validation loss = 0.8899  \n",
      "\n",
      "Fold: 19  Epoch: 19  Training loss = 3.1733  Validation loss = 0.8931  \n",
      "\n",
      "Fold: 19  Epoch: 20  Training loss = 3.1727  Validation loss = 0.8923  \n",
      "\n",
      "Fold: 19  Epoch: 21  Training loss = 3.1724  Validation loss = 0.8942  \n",
      "\n",
      "Fold: 19  Epoch: 22  Training loss = 3.1720  Validation loss = 0.8896  \n",
      "\n",
      "Fold: 19  Epoch: 23  Training loss = 3.1715  Validation loss = 0.8873  \n",
      "\n",
      "Fold: 19  Epoch: 24  Training loss = 3.1711  Validation loss = 0.8890  \n",
      "\n",
      "Fold: 19  Epoch: 25  Training loss = 3.1702  Validation loss = 0.8856  \n",
      "\n",
      "Fold: 19  Epoch: 26  Training loss = 3.1702  Validation loss = 0.8890  \n",
      "\n",
      "Fold: 19  Epoch: 27  Training loss = 3.1698  Validation loss = 0.8879  \n",
      "\n",
      "Fold: 19  Epoch: 28  Training loss = 3.1692  Validation loss = 0.8902  \n",
      "\n",
      "Fold: 19  Epoch: 29  Training loss = 3.1689  Validation loss = 0.8897  \n",
      "\n",
      "Fold: 19  Epoch: 30  Training loss = 3.1684  Validation loss = 0.8893  \n",
      "\n",
      "Fold: 19  Epoch: 31  Training loss = 3.1679  Validation loss = 0.8900  \n",
      "\n",
      "Fold: 19  Epoch: 32  Training loss = 3.1674  Validation loss = 0.8867  \n",
      "\n",
      "Fold: 19  Epoch: 33  Training loss = 3.1665  Validation loss = 0.8820  \n",
      "\n",
      "Fold: 19  Epoch: 34  Training loss = 3.1661  Validation loss = 0.8859  \n",
      "\n",
      "Fold: 19  Epoch: 35  Training loss = 3.1660  Validation loss = 0.8894  \n",
      "\n",
      "Fold: 19  Epoch: 36  Training loss = 3.1659  Validation loss = 0.8921  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 33  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 3.1564  Validation loss = 1.9578  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 3.1562  Validation loss = 1.9550  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 3.1555  Validation loss = 1.9608  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 3.1549  Validation loss = 1.9664  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 3.1550  Validation loss = 1.9586  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 3.1540  Validation loss = 1.9647  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 3.1530  Validation loss = 1.9758  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 3.1526  Validation loss = 1.9780  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 3.1519  Validation loss = 1.9844  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 3.1514  Validation loss = 1.9803  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 3.1509  Validation loss = 1.9820  \n",
      "\n",
      "Fold: 20  Epoch: 12  Training loss = 3.1505  Validation loss = 1.9814  \n",
      "\n",
      "Fold: 20  Epoch: 13  Training loss = 3.1500  Validation loss = 1.9816  \n",
      "\n",
      "Fold: 20  Epoch: 14  Training loss = 3.1498  Validation loss = 1.9818  \n",
      "\n",
      "Fold: 20  Epoch: 15  Training loss = 3.1491  Validation loss = 1.9868  \n",
      "\n",
      "Fold: 20  Epoch: 16  Training loss = 3.1488  Validation loss = 1.9828  \n",
      "\n",
      "Fold: 20  Epoch: 17  Training loss = 3.1478  Validation loss = 1.9895  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 2  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 3.1788  Validation loss = 2.5037  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 3.1782  Validation loss = 2.5068  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 3.1777  Validation loss = 2.5060  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 3.1772  Validation loss = 2.5057  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 3.1767  Validation loss = 2.5038  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 3.1760  Validation loss = 2.5049  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 3.1756  Validation loss = 2.5037  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 3.1751  Validation loss = 2.5019  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 3.1747  Validation loss = 2.5002  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 3.1741  Validation loss = 2.4963  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 3.1737  Validation loss = 2.4944  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 3.1733  Validation loss = 2.4977  \n",
      "\n",
      "Fold: 21  Epoch: 13  Training loss = 3.1728  Validation loss = 2.5002  \n",
      "\n",
      "Fold: 21  Epoch: 14  Training loss = 3.1719  Validation loss = 2.5022  \n",
      "\n",
      "Fold: 21  Epoch: 15  Training loss = 3.1714  Validation loss = 2.5033  \n",
      "\n",
      "Fold: 21  Epoch: 16  Training loss = 3.1706  Validation loss = 2.5000  \n",
      "\n",
      "Fold: 21  Epoch: 17  Training loss = 3.1697  Validation loss = 2.5027  \n",
      "\n",
      "Fold: 21  Epoch: 18  Training loss = 3.1691  Validation loss = 2.5007  \n",
      "\n",
      "Fold: 21  Epoch: 19  Training loss = 3.1685  Validation loss = 2.5037  \n",
      "\n",
      "Fold: 21  Epoch: 20  Training loss = 3.1679  Validation loss = 2.5076  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 11  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 3.2011  Validation loss = 1.4555  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 3.2000  Validation loss = 1.4576  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 3.1995  Validation loss = 1.4597  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 3.1991  Validation loss = 1.4585  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 3.1982  Validation loss = 1.4587  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 3.1973  Validation loss = 1.4601  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 3.1967  Validation loss = 1.4618  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 3.1961  Validation loss = 1.4631  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 3.1955  Validation loss = 1.4618  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 3.1952  Validation loss = 1.4604  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 3.1945  Validation loss = 1.4600  \n",
      "\n",
      "Fold: 22  Epoch: 12  Training loss = 3.1940  Validation loss = 1.4590  \n",
      "\n",
      "Fold: 22  Epoch: 13  Training loss = 3.1933  Validation loss = 1.4609  \n",
      "\n",
      "Fold: 22  Epoch: 14  Training loss = 3.1926  Validation loss = 1.4607  \n",
      "\n",
      "Fold: 22  Epoch: 15  Training loss = 3.1918  Validation loss = 1.4644  \n",
      "\n",
      "Fold: 22  Epoch: 16  Training loss = 3.1913  Validation loss = 1.4637  \n",
      "\n",
      "Fold: 22  Epoch: 17  Training loss = 3.1907  Validation loss = 1.4631  \n",
      "\n",
      "Fold: 22  Epoch: 18  Training loss = 3.1904  Validation loss = 1.4623  \n",
      "\n",
      "Fold: 22  Epoch: 19  Training loss = 3.1897  Validation loss = 1.4624  \n",
      "\n",
      "Fold: 22  Epoch: 20  Training loss = 3.1892  Validation loss = 1.4609  \n",
      "\n",
      "Fold: 22  Epoch: 21  Training loss = 3.1885  Validation loss = 1.4611  \n",
      "\n",
      "Fold: 22  Epoch: 22  Training loss = 3.1877  Validation loss = 1.4638  \n",
      "\n",
      "Fold: 22  Epoch: 23  Training loss = 3.1872  Validation loss = 1.4629  \n",
      "\n",
      "Fold: 22  Epoch: 24  Training loss = 3.1867  Validation loss = 1.4631  \n",
      "\n",
      "Fold: 22  Epoch: 25  Training loss = 3.1861  Validation loss = 1.4642  \n",
      "\n",
      "Fold: 22  Epoch: 26  Training loss = 3.1854  Validation loss = 1.4646  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 1  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 3.1111  Validation loss = 1.2379  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 3.1106  Validation loss = 1.2243  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 3.1098  Validation loss = 1.2201  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 3.1091  Validation loss = 1.2162  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 3.1084  Validation loss = 1.2218  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 3.1070  Validation loss = 1.2375  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 3.1065  Validation loss = 1.2287  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 3.1057  Validation loss = 1.2168  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 3.1048  Validation loss = 1.2229  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 3.1037  Validation loss = 1.2351  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 3.1023  Validation loss = 1.2631  \n",
      "\n",
      "Fold: 23  Epoch: 12  Training loss = 3.1013  Validation loss = 1.2532  \n",
      "\n",
      "Fold: 23  Epoch: 13  Training loss = 3.1001  Validation loss = 1.2599  \n",
      "\n",
      "Fold: 23  Epoch: 14  Training loss = 3.0994  Validation loss = 1.2627  \n",
      "\n",
      "Fold: 23  Epoch: 15  Training loss = 3.0988  Validation loss = 1.2510  \n",
      "\n",
      "Fold: 23  Epoch: 16  Training loss = 3.0980  Validation loss = 1.2509  \n",
      "\n",
      "Fold: 23  Epoch: 17  Training loss = 3.0967  Validation loss = 1.2626  \n",
      "\n",
      "Fold: 23  Epoch: 18  Training loss = 3.0958  Validation loss = 1.2678  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 4  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 2.9495  Validation loss = 1.9980  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 2.9492  Validation loss = 1.9995  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 2.9480  Validation loss = 2.0013  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 2.9471  Validation loss = 1.9995  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 2.9461  Validation loss = 2.0013  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 2.9438  Validation loss = 1.9983  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 2.9433  Validation loss = 1.9985  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 2.9410  Validation loss = 1.9973  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 2.9400  Validation loss = 1.9966  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 2.9392  Validation loss = 1.9965  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 2.9371  Validation loss = 1.9957  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 2.9351  Validation loss = 1.9944  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 2.9342  Validation loss = 1.9965  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 2.9354  Validation loss = 1.9952  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 2.9342  Validation loss = 1.9958  \n",
      "\n",
      "Fold: 24  Epoch: 16  Training loss = 2.9317  Validation loss = 1.9962  \n",
      "\n",
      "Fold: 24  Epoch: 17  Training loss = 2.9297  Validation loss = 1.9945  \n",
      "\n",
      "Fold: 24  Epoch: 18  Training loss = 2.9287  Validation loss = 1.9937  \n",
      "\n",
      "Fold: 24  Epoch: 19  Training loss = 2.9270  Validation loss = 1.9900  \n",
      "\n",
      "Fold: 24  Epoch: 20  Training loss = 2.9255  Validation loss = 1.9885  \n",
      "\n",
      "Fold: 24  Epoch: 21  Training loss = 2.9245  Validation loss = 1.9839  \n",
      "\n",
      "Fold: 24  Epoch: 22  Training loss = 2.9228  Validation loss = 1.9797  \n",
      "\n",
      "Fold: 24  Epoch: 23  Training loss = 2.9222  Validation loss = 1.9812  \n",
      "\n",
      "Fold: 24  Epoch: 24  Training loss = 2.9212  Validation loss = 1.9806  \n",
      "\n",
      "Fold: 24  Epoch: 25  Training loss = 2.9203  Validation loss = 1.9840  \n",
      "\n",
      "Fold: 24  Epoch: 26  Training loss = 2.9200  Validation loss = 1.9828  \n",
      "\n",
      "Fold: 24  Epoch: 27  Training loss = 2.9194  Validation loss = 1.9816  \n",
      "\n",
      "Fold: 24  Epoch: 28  Training loss = 2.9180  Validation loss = 1.9825  \n",
      "\n",
      "Fold: 24  Epoch: 29  Training loss = 2.9171  Validation loss = 1.9832  \n",
      "\n",
      "Fold: 24  Epoch: 30  Training loss = 2.9151  Validation loss = 1.9894  \n",
      "\n",
      "Fold: 24  Epoch: 31  Training loss = 2.9138  Validation loss = 1.9862  \n",
      "\n",
      "Fold: 24  Epoch: 32  Training loss = 2.9123  Validation loss = 1.9804  \n",
      "\n",
      "Fold: 24  Epoch: 33  Training loss = 2.9110  Validation loss = 1.9799  \n",
      "\n",
      "Fold: 24  Epoch: 34  Training loss = 2.9100  Validation loss = 1.9844  \n",
      "\n",
      "Fold: 24  Epoch: 35  Training loss = 2.9088  Validation loss = 1.9833  \n",
      "\n",
      "Fold: 24  Epoch: 36  Training loss = 2.9076  Validation loss = 1.9847  \n",
      "\n",
      "Fold: 24  Epoch: 37  Training loss = 2.9063  Validation loss = 1.9826  \n",
      "\n",
      "Fold: 24  Epoch: 38  Training loss = 2.9055  Validation loss = 1.9828  \n",
      "\n",
      "Fold: 24  Epoch: 39  Training loss = 2.9047  Validation loss = 1.9826  \n",
      "\n",
      "Fold: 24  Epoch: 40  Training loss = 2.9030  Validation loss = 1.9808  \n",
      "\n",
      "Fold: 24  Epoch: 41  Training loss = 2.9019  Validation loss = 1.9779  \n",
      "\n",
      "Fold: 24  Epoch: 42  Training loss = 2.9012  Validation loss = 1.9788  \n",
      "\n",
      "Fold: 24  Epoch: 43  Training loss = 2.8998  Validation loss = 1.9777  \n",
      "\n",
      "Fold: 24  Epoch: 44  Training loss = 2.8992  Validation loss = 1.9798  \n",
      "\n",
      "Fold: 24  Epoch: 45  Training loss = 2.8986  Validation loss = 1.9801  \n",
      "\n",
      "Fold: 24  Epoch: 46  Training loss = 2.8971  Validation loss = 1.9821  \n",
      "\n",
      "Fold: 24  Epoch: 47  Training loss = 2.8965  Validation loss = 1.9832  \n",
      "\n",
      "Fold: 24  Epoch: 48  Training loss = 2.8957  Validation loss = 1.9830  \n",
      "\n",
      "Fold: 24  Epoch: 49  Training loss = 2.8947  Validation loss = 1.9840  \n",
      "\n",
      "Fold: 24  Epoch: 50  Training loss = 2.8942  Validation loss = 1.9824  \n",
      "\n",
      "Fold: 24  Epoch: 51  Training loss = 2.8928  Validation loss = 1.9856  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 43  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 2.8760  Validation loss = 1.7695  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 2.8752  Validation loss = 1.7581  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 2.8746  Validation loss = 1.7631  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 2.8740  Validation loss = 1.7707  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 2.8726  Validation loss = 1.7621  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 2.8712  Validation loss = 1.7579  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 2.8703  Validation loss = 1.7902  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 2.8694  Validation loss = 1.7881  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 2.8683  Validation loss = 1.7924  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 2.8670  Validation loss = 1.7621  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 2.8656  Validation loss = 1.7796  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 2.8647  Validation loss = 1.7756  \n",
      "\n",
      "Fold: 25  Epoch: 13  Training loss = 2.8635  Validation loss = 1.7737  \n",
      "\n",
      "Fold: 25  Epoch: 14  Training loss = 2.8630  Validation loss = 1.7535  \n",
      "\n",
      "Fold: 25  Epoch: 15  Training loss = 2.8621  Validation loss = 1.7852  \n",
      "\n",
      "Fold: 25  Epoch: 16  Training loss = 2.8627  Validation loss = 1.8056  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 14  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 2.6446  Validation loss = 2.8541  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 2.6431  Validation loss = 2.8542  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 2.6415  Validation loss = 2.8708  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 2.6400  Validation loss = 2.8783  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 2.6384  Validation loss = 2.8874  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 2.6378  Validation loss = 2.8982  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 2.6369  Validation loss = 2.8926  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 2.6357  Validation loss = 2.9008  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 2.6344  Validation loss = 2.9243  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 2.6335  Validation loss = 2.9333  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 2.6323  Validation loss = 2.9229  \n",
      "\n",
      "Fold: 26  Epoch: 12  Training loss = 2.6310  Validation loss = 2.9061  \n",
      "\n",
      "Fold: 26  Epoch: 13  Training loss = 2.6300  Validation loss = 2.8806  \n",
      "\n",
      "Fold: 26  Epoch: 14  Training loss = 2.6291  Validation loss = 2.8734  \n",
      "\n",
      "Fold: 26  Epoch: 15  Training loss = 2.6279  Validation loss = 2.8714  \n",
      "\n",
      "Fold: 26  Epoch: 16  Training loss = 2.6269  Validation loss = 2.8786  \n",
      "\n",
      "Fold: 26  Epoch: 17  Training loss = 2.6258  Validation loss = 2.8840  \n",
      "\n",
      "Fold: 26  Epoch: 18  Training loss = 2.6248  Validation loss = 2.8941  \n",
      "\n",
      "Fold: 26  Epoch: 19  Training loss = 2.6238  Validation loss = 2.8938  \n",
      "\n",
      "Fold: 26  Epoch: 20  Training loss = 2.6226  Validation loss = 2.8933  \n",
      "\n",
      "Fold: 26  Epoch: 21  Training loss = 2.6216  Validation loss = 2.8928  \n",
      "\n",
      "Fold: 26  Epoch: 22  Training loss = 2.6206  Validation loss = 2.8877  \n",
      "\n",
      "Fold: 26  Epoch: 23  Training loss = 2.6192  Validation loss = 2.8832  \n",
      "\n",
      "Fold: 26  Epoch: 24  Training loss = 2.6181  Validation loss = 2.8958  \n",
      "\n",
      "Fold: 26  Epoch: 25  Training loss = 2.6170  Validation loss = 2.8978  \n",
      "\n",
      "Fold: 26  Epoch: 26  Training loss = 2.6162  Validation loss = 2.9096  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 1  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 2.6320  Validation loss = 1.9238  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 2.6309  Validation loss = 1.9223  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 2.6298  Validation loss = 1.9064  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 2.6282  Validation loss = 1.8530  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 2.6271  Validation loss = 1.8271  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 2.6264  Validation loss = 1.8559  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 2.6256  Validation loss = 1.8645  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 2.6241  Validation loss = 1.8159  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 2.6224  Validation loss = 1.7262  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 2.6215  Validation loss = 1.7826  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 2.6211  Validation loss = 1.7966  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 2.6200  Validation loss = 1.7874  \n",
      "\n",
      "Fold: 27  Epoch: 13  Training loss = 2.6192  Validation loss = 1.7642  \n",
      "\n",
      "Fold: 27  Epoch: 14  Training loss = 2.6175  Validation loss = 1.7286  \n",
      "\n",
      "Fold: 27  Epoch: 15  Training loss = 2.6160  Validation loss = 1.6443  \n",
      "\n",
      "Fold: 27  Epoch: 16  Training loss = 2.6150  Validation loss = 1.6690  \n",
      "\n",
      "Fold: 27  Epoch: 17  Training loss = 2.6143  Validation loss = 1.6951  \n",
      "\n",
      "Fold: 27  Epoch: 18  Training loss = 2.6131  Validation loss = 1.6726  \n",
      "\n",
      "Fold: 27  Epoch: 19  Training loss = 2.6125  Validation loss = 1.6256  \n",
      "\n",
      "Fold: 27  Epoch: 20  Training loss = 2.6112  Validation loss = 1.6102  \n",
      "\n",
      "Fold: 27  Epoch: 21  Training loss = 2.6103  Validation loss = 1.6025  \n",
      "\n",
      "Fold: 27  Epoch: 22  Training loss = 2.6097  Validation loss = 1.6303  \n",
      "\n",
      "Fold: 27  Epoch: 23  Training loss = 2.6093  Validation loss = 1.5758  \n",
      "\n",
      "Fold: 27  Epoch: 24  Training loss = 2.6080  Validation loss = 1.6359  \n",
      "\n",
      "Fold: 27  Epoch: 25  Training loss = 2.6068  Validation loss = 1.6170  \n",
      "\n",
      "Fold: 27  Epoch: 26  Training loss = 2.6058  Validation loss = 1.6121  \n",
      "\n",
      "Fold: 27  Epoch: 27  Training loss = 2.6054  Validation loss = 1.6297  \n",
      "\n",
      "Fold: 27  Epoch: 28  Training loss = 2.6047  Validation loss = 1.6478  \n",
      "\n",
      "Fold: 27  Epoch: 29  Training loss = 2.6038  Validation loss = 1.6578  \n",
      "\n",
      "Fold: 27  Epoch: 30  Training loss = 2.6030  Validation loss = 1.6260  \n",
      "\n",
      "Fold: 27  Epoch: 31  Training loss = 2.6028  Validation loss = 1.5527  \n",
      "\n",
      "Fold: 27  Epoch: 32  Training loss = 2.6017  Validation loss = 1.5585  \n",
      "\n",
      "Fold: 27  Epoch: 33  Training loss = 2.6009  Validation loss = 1.4979  \n",
      "\n",
      "Fold: 27  Epoch: 34  Training loss = 2.6005  Validation loss = 1.5810  \n",
      "\n",
      "Fold: 27  Epoch: 35  Training loss = 2.5999  Validation loss = 1.5536  \n",
      "\n",
      "Fold: 27  Epoch: 36  Training loss = 2.5992  Validation loss = 1.5135  \n",
      "\n",
      "Fold: 27  Epoch: 37  Training loss = 2.5988  Validation loss = 1.4765  \n",
      "\n",
      "Fold: 27  Epoch: 38  Training loss = 2.5981  Validation loss = 1.4572  \n",
      "\n",
      "Fold: 27  Epoch: 39  Training loss = 2.5974  Validation loss = 1.4592  \n",
      "\n",
      "Fold: 27  Epoch: 40  Training loss = 2.5963  Validation loss = 1.5286  \n",
      "\n",
      "Fold: 27  Epoch: 41  Training loss = 2.5959  Validation loss = 1.4716  \n",
      "\n",
      "Fold: 27  Epoch: 42  Training loss = 2.5954  Validation loss = 1.4972  \n",
      "\n",
      "Fold: 27  Epoch: 43  Training loss = 2.5951  Validation loss = 1.4471  \n",
      "\n",
      "Fold: 27  Epoch: 44  Training loss = 2.5946  Validation loss = 1.4564  \n",
      "\n",
      "Fold: 27  Epoch: 45  Training loss = 2.5944  Validation loss = 1.4098  \n",
      "\n",
      "Fold: 27  Epoch: 46  Training loss = 2.5927  Validation loss = 1.4799  \n",
      "\n",
      "Fold: 27  Epoch: 47  Training loss = 2.5921  Validation loss = 1.4624  \n",
      "\n",
      "Fold: 27  Epoch: 48  Training loss = 2.5910  Validation loss = 1.5042  \n",
      "\n",
      "Fold: 27  Epoch: 49  Training loss = 2.5901  Validation loss = 1.5466  \n",
      "\n",
      "Fold: 27  Epoch: 50  Training loss = 2.5893  Validation loss = 1.4792  \n",
      "\n",
      "Fold: 27  Epoch: 51  Training loss = 2.5890  Validation loss = 1.4402  \n",
      "\n",
      "Fold: 27  Epoch: 52  Training loss = 2.5882  Validation loss = 1.4668  \n",
      "\n",
      "Fold: 27  Epoch: 53  Training loss = 2.5877  Validation loss = 1.4082  \n",
      "\n",
      "Fold: 27  Epoch: 54  Training loss = 2.5870  Validation loss = 1.4209  \n",
      "\n",
      "Fold: 27  Epoch: 55  Training loss = 2.5862  Validation loss = 1.4264  \n",
      "\n",
      "Fold: 27  Epoch: 56  Training loss = 2.5857  Validation loss = 1.4562  \n",
      "\n",
      "Fold: 27  Epoch: 57  Training loss = 2.5848  Validation loss = 1.4812  \n",
      "\n",
      "Fold: 27  Epoch: 58  Training loss = 2.5837  Validation loss = 1.5049  \n",
      "\n",
      "Fold: 27  Epoch: 59  Training loss = 2.5828  Validation loss = 1.4555  \n",
      "\n",
      "Fold: 27  Epoch: 60  Training loss = 2.5824  Validation loss = 1.4769  \n",
      "\n",
      "Fold: 27  Epoch: 61  Training loss = 2.5812  Validation loss = 1.5169  \n",
      "\n",
      "Fold: 27  Epoch: 62  Training loss = 2.5802  Validation loss = 1.4646  \n",
      "\n",
      "Fold: 27  Epoch: 63  Training loss = 2.5795  Validation loss = 1.4791  \n",
      "\n",
      "Fold: 27  Epoch: 64  Training loss = 2.5790  Validation loss = 1.4472  \n",
      "\n",
      "Fold: 27  Epoch: 65  Training loss = 2.5790  Validation loss = 1.4120  \n",
      "\n",
      "Fold: 27  Epoch: 66  Training loss = 2.5777  Validation loss = 1.5164  \n",
      "\n",
      "Fold: 27  Epoch: 67  Training loss = 2.5774  Validation loss = 1.6685  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 53  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 2.4637  Validation loss = 1.6966  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 2.4580  Validation loss = 1.6971  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 2.4544  Validation loss = 1.6907  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 2.4484  Validation loss = 1.6800  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 2.4441  Validation loss = 1.6752  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 2.4424  Validation loss = 1.6738  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 2.4422  Validation loss = 1.6769  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 2.4396  Validation loss = 1.6685  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 2.4383  Validation loss = 1.6621  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 2.4372  Validation loss = 1.6735  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 2.4351  Validation loss = 1.6747  \n",
      "\n",
      "Fold: 28  Epoch: 12  Training loss = 2.4339  Validation loss = 1.6799  \n",
      "\n",
      "Fold: 28  Epoch: 13  Training loss = 2.4328  Validation loss = 1.6797  \n",
      "\n",
      "Fold: 28  Epoch: 14  Training loss = 2.4347  Validation loss = 1.6885  \n",
      "\n",
      "Fold: 28  Epoch: 15  Training loss = 2.4312  Validation loss = 1.6881  \n",
      "\n",
      "Fold: 28  Epoch: 16  Training loss = 2.4290  Validation loss = 1.6738  \n",
      "\n",
      "Fold: 28  Epoch: 17  Training loss = 2.4275  Validation loss = 1.6808  \n",
      "\n",
      "Fold: 28  Epoch: 18  Training loss = 2.4264  Validation loss = 1.6734  \n",
      "\n",
      "Fold: 28  Epoch: 19  Training loss = 2.4246  Validation loss = 1.6784  \n",
      "\n",
      "Fold: 28  Epoch: 20  Training loss = 2.4236  Validation loss = 1.6755  \n",
      "\n",
      "Fold: 28  Epoch: 21  Training loss = 2.4230  Validation loss = 1.6665  \n",
      "\n",
      "Fold: 28  Epoch: 22  Training loss = 2.4217  Validation loss = 1.6702  \n",
      "\n",
      "Fold: 28  Epoch: 23  Training loss = 2.4205  Validation loss = 1.6700  \n",
      "\n",
      "Fold: 28  Epoch: 24  Training loss = 2.4191  Validation loss = 1.6638  \n",
      "\n",
      "Fold: 28  Epoch: 25  Training loss = 2.4184  Validation loss = 1.6597  \n",
      "\n",
      "Fold: 28  Epoch: 26  Training loss = 2.4172  Validation loss = 1.6560  \n",
      "\n",
      "Fold: 28  Epoch: 27  Training loss = 2.4162  Validation loss = 1.6510  \n",
      "\n",
      "Fold: 28  Epoch: 28  Training loss = 2.4140  Validation loss = 1.6642  \n",
      "\n",
      "Fold: 28  Epoch: 29  Training loss = 2.4126  Validation loss = 1.6599  \n",
      "\n",
      "Fold: 28  Epoch: 30  Training loss = 2.4117  Validation loss = 1.6570  \n",
      "\n",
      "Fold: 28  Epoch: 31  Training loss = 2.4109  Validation loss = 1.6617  \n",
      "\n",
      "Fold: 28  Epoch: 32  Training loss = 2.4099  Validation loss = 1.6566  \n",
      "\n",
      "Fold: 28  Epoch: 33  Training loss = 2.4097  Validation loss = 1.6584  \n",
      "\n",
      "Fold: 28  Epoch: 34  Training loss = 2.4081  Validation loss = 1.6544  \n",
      "\n",
      "Fold: 28  Epoch: 35  Training loss = 2.4066  Validation loss = 1.6558  \n",
      "\n",
      "Fold: 28  Epoch: 36  Training loss = 2.4059  Validation loss = 1.6605  \n",
      "\n",
      "Fold: 28  Epoch: 37  Training loss = 2.4045  Validation loss = 1.6619  \n",
      "\n",
      "Fold: 28  Epoch: 38  Training loss = 2.4031  Validation loss = 1.6578  \n",
      "\n",
      "Fold: 28  Epoch: 39  Training loss = 2.4023  Validation loss = 1.6552  \n",
      "\n",
      "Fold: 28  Epoch: 40  Training loss = 2.4010  Validation loss = 1.6574  \n",
      "\n",
      "Fold: 28  Epoch: 41  Training loss = 2.3999  Validation loss = 1.6551  \n",
      "\n",
      "Fold: 28  Epoch: 42  Training loss = 2.3991  Validation loss = 1.6572  \n",
      "\n",
      "Fold: 28  Epoch: 43  Training loss = 2.3986  Validation loss = 1.6648  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 27  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 2.3943  Validation loss = 2.2395  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 2.3933  Validation loss = 2.2386  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 2.3914  Validation loss = 2.2363  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 2.3903  Validation loss = 2.2270  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 2.3889  Validation loss = 2.2280  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 2.3876  Validation loss = 2.2259  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 2.3870  Validation loss = 2.2253  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 2.3858  Validation loss = 2.2259  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 2.3847  Validation loss = 2.2306  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 2.3834  Validation loss = 2.2279  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 2.3830  Validation loss = 2.2216  \n",
      "\n",
      "Fold: 29  Epoch: 12  Training loss = 2.3813  Validation loss = 2.2246  \n",
      "\n",
      "Fold: 29  Epoch: 13  Training loss = 2.3806  Validation loss = 2.2314  \n",
      "\n",
      "Fold: 29  Epoch: 14  Training loss = 2.3793  Validation loss = 2.2278  \n",
      "\n",
      "Fold: 29  Epoch: 15  Training loss = 2.3791  Validation loss = 2.2162  \n",
      "\n",
      "Fold: 29  Epoch: 16  Training loss = 2.3781  Validation loss = 2.2199  \n",
      "\n",
      "Fold: 29  Epoch: 17  Training loss = 2.3775  Validation loss = 2.2157  \n",
      "\n",
      "Fold: 29  Epoch: 18  Training loss = 2.3809  Validation loss = 2.2021  \n",
      "\n",
      "Fold: 29  Epoch: 19  Training loss = 2.3790  Validation loss = 2.2021  \n",
      "\n",
      "Fold: 29  Epoch: 20  Training loss = 2.3756  Validation loss = 2.2070  \n",
      "\n",
      "Fold: 29  Epoch: 21  Training loss = 2.3733  Validation loss = 2.2104  \n",
      "\n",
      "Fold: 29  Epoch: 22  Training loss = 2.3723  Validation loss = 2.2096  \n",
      "\n",
      "Fold: 29  Epoch: 23  Training loss = 2.3707  Validation loss = 2.2115  \n",
      "\n",
      "Fold: 29  Epoch: 24  Training loss = 2.3693  Validation loss = 2.2130  \n",
      "\n",
      "Fold: 29  Epoch: 25  Training loss = 2.3682  Validation loss = 2.2102  \n",
      "\n",
      "Fold: 29  Epoch: 26  Training loss = 2.3681  Validation loss = 2.2054  \n",
      "\n",
      "Fold: 29  Epoch: 27  Training loss = 2.3657  Validation loss = 2.2148  \n",
      "\n",
      "Fold: 29  Epoch: 28  Training loss = 2.3644  Validation loss = 2.2136  \n",
      "\n",
      "Fold: 29  Epoch: 29  Training loss = 2.3635  Validation loss = 2.2166  \n",
      "\n",
      "Fold: 29  Epoch: 30  Training loss = 2.3627  Validation loss = 2.2210  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 19  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 2.4051  Validation loss = 1.4935  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 2.4035  Validation loss = 1.4917  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 2.4022  Validation loss = 1.4906  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 2.4020  Validation loss = 1.4864  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 2.3998  Validation loss = 1.4875  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 2.3986  Validation loss = 1.4865  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 2.3979  Validation loss = 1.4852  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 2.3974  Validation loss = 1.4810  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 2.3961  Validation loss = 1.4803  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 2.3952  Validation loss = 1.4870  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 2.3942  Validation loss = 1.4888  \n",
      "\n",
      "Fold: 30  Epoch: 12  Training loss = 2.3931  Validation loss = 1.4840  \n",
      "\n",
      "Fold: 30  Epoch: 13  Training loss = 2.3923  Validation loss = 1.4838  \n",
      "\n",
      "Fold: 30  Epoch: 14  Training loss = 2.3912  Validation loss = 1.4839  \n",
      "\n",
      "Fold: 30  Epoch: 15  Training loss = 2.3900  Validation loss = 1.4757  \n",
      "\n",
      "Fold: 30  Epoch: 16  Training loss = 2.3890  Validation loss = 1.4810  \n",
      "\n",
      "Fold: 30  Epoch: 17  Training loss = 2.3882  Validation loss = 1.4813  \n",
      "\n",
      "Fold: 30  Epoch: 18  Training loss = 2.3872  Validation loss = 1.4784  \n",
      "\n",
      "Fold: 30  Epoch: 19  Training loss = 2.3862  Validation loss = 1.4747  \n",
      "\n",
      "Fold: 30  Epoch: 20  Training loss = 2.3846  Validation loss = 1.4743  \n",
      "\n",
      "Fold: 30  Epoch: 21  Training loss = 2.3836  Validation loss = 1.4710  \n",
      "\n",
      "Fold: 30  Epoch: 22  Training loss = 2.3840  Validation loss = 1.4792  \n",
      "\n",
      "Fold: 30  Epoch: 23  Training loss = 2.3821  Validation loss = 1.4762  \n",
      "\n",
      "Fold: 30  Epoch: 24  Training loss = 2.3812  Validation loss = 1.4768  \n",
      "\n",
      "Fold: 30  Epoch: 25  Training loss = 2.3828  Validation loss = 1.4786  \n",
      "\n",
      "Fold: 30  Epoch: 26  Training loss = 2.3792  Validation loss = 1.4742  \n",
      "\n",
      "Fold: 30  Epoch: 27  Training loss = 2.3788  Validation loss = 1.4709  \n",
      "\n",
      "Fold: 30  Epoch: 28  Training loss = 2.3769  Validation loss = 1.4715  \n",
      "\n",
      "Fold: 30  Epoch: 29  Training loss = 2.3763  Validation loss = 1.4681  \n",
      "\n",
      "Fold: 30  Epoch: 30  Training loss = 2.3750  Validation loss = 1.4661  \n",
      "\n",
      "Fold: 30  Epoch: 31  Training loss = 2.3738  Validation loss = 1.4651  \n",
      "\n",
      "Fold: 30  Epoch: 32  Training loss = 2.3728  Validation loss = 1.4596  \n",
      "\n",
      "Fold: 30  Epoch: 33  Training loss = 2.3721  Validation loss = 1.4589  \n",
      "\n",
      "Fold: 30  Epoch: 34  Training loss = 2.3709  Validation loss = 1.4596  \n",
      "\n",
      "Fold: 30  Epoch: 35  Training loss = 2.3704  Validation loss = 1.4594  \n",
      "\n",
      "Fold: 30  Epoch: 36  Training loss = 2.3700  Validation loss = 1.4615  \n",
      "\n",
      "Fold: 30  Epoch: 37  Training loss = 2.3706  Validation loss = 1.4614  \n",
      "\n",
      "Fold: 30  Epoch: 38  Training loss = 2.3675  Validation loss = 1.4543  \n",
      "\n",
      "Fold: 30  Epoch: 39  Training loss = 2.3663  Validation loss = 1.4536  \n",
      "\n",
      "Fold: 30  Epoch: 40  Training loss = 2.3647  Validation loss = 1.4529  \n",
      "\n",
      "Fold: 30  Epoch: 41  Training loss = 2.3634  Validation loss = 1.4549  \n",
      "\n",
      "Fold: 30  Epoch: 42  Training loss = 2.3622  Validation loss = 1.4550  \n",
      "\n",
      "Fold: 30  Epoch: 43  Training loss = 2.3612  Validation loss = 1.4535  \n",
      "\n",
      "Fold: 30  Epoch: 44  Training loss = 2.3601  Validation loss = 1.4568  \n",
      "\n",
      "Fold: 30  Epoch: 45  Training loss = 2.3610  Validation loss = 1.4604  \n",
      "\n",
      "Fold: 30  Epoch: 46  Training loss = 2.3590  Validation loss = 1.4566  \n",
      "\n",
      "Fold: 30  Epoch: 47  Training loss = 2.3605  Validation loss = 1.4619  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 40  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 2.2470  Validation loss = 1.4826  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 2.2444  Validation loss = 1.4628  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 2.2427  Validation loss = 1.4607  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 2.2432  Validation loss = 1.4761  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 2.2400  Validation loss = 1.4521  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 2.2391  Validation loss = 1.4586  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 2.2389  Validation loss = 1.4643  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 2.2381  Validation loss = 1.4578  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 2.2352  Validation loss = 1.4010  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 2.2340  Validation loss = 1.4025  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 2.2331  Validation loss = 1.3985  \n",
      "\n",
      "Fold: 31  Epoch: 12  Training loss = 2.2325  Validation loss = 1.3779  \n",
      "\n",
      "Fold: 31  Epoch: 13  Training loss = 2.2310  Validation loss = 1.4061  \n",
      "\n",
      "Fold: 31  Epoch: 14  Training loss = 2.2301  Validation loss = 1.4059  \n",
      "\n",
      "Fold: 31  Epoch: 15  Training loss = 2.2299  Validation loss = 1.4088  \n",
      "\n",
      "Fold: 31  Epoch: 16  Training loss = 2.2285  Validation loss = 1.4024  \n",
      "\n",
      "Fold: 31  Epoch: 17  Training loss = 2.2296  Validation loss = 1.4266  \n",
      "\n",
      "Fold: 31  Epoch: 18  Training loss = 2.2269  Validation loss = 1.4061  \n",
      "\n",
      "Fold: 31  Epoch: 19  Training loss = 2.2260  Validation loss = 1.4014  \n",
      "\n",
      "Fold: 31  Epoch: 20  Training loss = 2.2266  Validation loss = 1.3678  \n",
      "\n",
      "Fold: 31  Epoch: 21  Training loss = 2.2244  Validation loss = 1.3836  \n",
      "\n",
      "Fold: 31  Epoch: 22  Training loss = 2.2235  Validation loss = 1.3826  \n",
      "\n",
      "Fold: 31  Epoch: 23  Training loss = 2.2232  Validation loss = 1.3666  \n",
      "\n",
      "Fold: 31  Epoch: 24  Training loss = 2.2222  Validation loss = 1.3744  \n",
      "\n",
      "Fold: 31  Epoch: 25  Training loss = 2.2222  Validation loss = 1.3543  \n",
      "\n",
      "Fold: 31  Epoch: 26  Training loss = 2.2206  Validation loss = 1.3995  \n",
      "\n",
      "Fold: 31  Epoch: 27  Training loss = 2.2194  Validation loss = 1.3723  \n",
      "\n",
      "Fold: 31  Epoch: 28  Training loss = 2.2188  Validation loss = 1.3999  \n",
      "\n",
      "Fold: 31  Epoch: 29  Training loss = 2.2169  Validation loss = 1.3625  \n",
      "\n",
      "Fold: 31  Epoch: 30  Training loss = 2.2158  Validation loss = 1.3725  \n",
      "\n",
      "Fold: 31  Epoch: 31  Training loss = 2.2154  Validation loss = 1.3776  \n",
      "\n",
      "Fold: 31  Epoch: 32  Training loss = 2.2148  Validation loss = 1.3567  \n",
      "\n",
      "Fold: 31  Epoch: 33  Training loss = 2.2135  Validation loss = 1.3734  \n",
      "\n",
      "Fold: 31  Epoch: 34  Training loss = 2.2139  Validation loss = 1.3889  \n",
      "\n",
      "Fold: 31  Epoch: 35  Training loss = 2.2119  Validation loss = 1.3712  \n",
      "\n",
      "Fold: 31  Epoch: 36  Training loss = 2.2113  Validation loss = 1.3773  \n",
      "\n",
      "Fold: 31  Epoch: 37  Training loss = 2.2102  Validation loss = 1.3706  \n",
      "\n",
      "Fold: 31  Epoch: 38  Training loss = 2.2107  Validation loss = 1.3917  \n",
      "\n",
      "Fold: 31  Epoch: 39  Training loss = 2.2082  Validation loss = 1.3675  \n",
      "\n",
      "Fold: 31  Epoch: 40  Training loss = 2.2076  Validation loss = 1.3624  \n",
      "\n",
      "Fold: 31  Epoch: 41  Training loss = 2.2070  Validation loss = 1.3570  \n",
      "\n",
      "Fold: 31  Epoch: 42  Training loss = 2.2082  Validation loss = 1.3356  \n",
      "\n",
      "Fold: 31  Epoch: 43  Training loss = 2.2059  Validation loss = 1.3488  \n",
      "\n",
      "Fold: 31  Epoch: 44  Training loss = 2.2040  Validation loss = 1.3547  \n",
      "\n",
      "Fold: 31  Epoch: 45  Training loss = 2.2035  Validation loss = 1.3681  \n",
      "\n",
      "Fold: 31  Epoch: 46  Training loss = 2.2030  Validation loss = 1.3705  \n",
      "\n",
      "Fold: 31  Epoch: 47  Training loss = 2.2021  Validation loss = 1.3665  \n",
      "\n",
      "Fold: 31  Epoch: 48  Training loss = 2.2021  Validation loss = 1.3445  \n",
      "\n",
      "Fold: 31  Epoch: 49  Training loss = 2.2011  Validation loss = 1.3398  \n",
      "\n",
      "Fold: 31  Epoch: 50  Training loss = 2.2007  Validation loss = 1.3347  \n",
      "\n",
      "Fold: 31  Epoch: 51  Training loss = 2.1993  Validation loss = 1.3724  \n",
      "\n",
      "Fold: 31  Epoch: 52  Training loss = 2.1986  Validation loss = 1.3574  \n",
      "\n",
      "Fold: 31  Epoch: 53  Training loss = 2.1978  Validation loss = 1.3371  \n",
      "\n",
      "Fold: 31  Epoch: 54  Training loss = 2.1962  Validation loss = 1.3544  \n",
      "\n",
      "Fold: 31  Epoch: 55  Training loss = 2.1960  Validation loss = 1.3655  \n",
      "\n",
      "Fold: 31  Epoch: 56  Training loss = 2.1947  Validation loss = 1.3446  \n",
      "\n",
      "Fold: 31  Epoch: 57  Training loss = 2.1950  Validation loss = 1.3137  \n",
      "\n",
      "Fold: 31  Epoch: 58  Training loss = 2.1936  Validation loss = 1.3272  \n",
      "\n",
      "Fold: 31  Epoch: 59  Training loss = 2.1924  Validation loss = 1.3372  \n",
      "\n",
      "Fold: 31  Epoch: 60  Training loss = 2.1921  Validation loss = 1.3548  \n",
      "\n",
      "Fold: 31  Epoch: 61  Training loss = 2.1922  Validation loss = 1.3177  \n",
      "\n",
      "Fold: 31  Epoch: 62  Training loss = 2.1924  Validation loss = 1.3073  \n",
      "\n",
      "Fold: 31  Epoch: 63  Training loss = 2.1894  Validation loss = 1.3231  \n",
      "\n",
      "Fold: 31  Epoch: 64  Training loss = 2.1889  Validation loss = 1.3146  \n",
      "\n",
      "Fold: 31  Epoch: 65  Training loss = 2.1880  Validation loss = 1.3329  \n",
      "\n",
      "Fold: 31  Epoch: 66  Training loss = 2.1874  Validation loss = 1.3422  \n",
      "\n",
      "Fold: 31  Epoch: 67  Training loss = 2.1867  Validation loss = 1.3145  \n",
      "\n",
      "Fold: 31  Epoch: 68  Training loss = 2.1880  Validation loss = 1.2937  \n",
      "\n",
      "Fold: 31  Epoch: 69  Training loss = 2.1857  Validation loss = 1.3001  \n",
      "\n",
      "Fold: 31  Epoch: 70  Training loss = 2.1846  Validation loss = 1.3113  \n",
      "\n",
      "Fold: 31  Epoch: 71  Training loss = 2.1836  Validation loss = 1.3331  \n",
      "\n",
      "Fold: 31  Epoch: 72  Training loss = 2.1846  Validation loss = 1.3544  \n",
      "\n",
      "Fold: 31  Epoch: 73  Training loss = 2.1821  Validation loss = 1.3234  \n",
      "\n",
      "Fold: 31  Epoch: 74  Training loss = 2.1814  Validation loss = 1.3259  \n",
      "\n",
      "Fold: 31  Epoch: 75  Training loss = 2.1808  Validation loss = 1.3227  \n",
      "\n",
      "Fold: 31  Epoch: 76  Training loss = 2.1800  Validation loss = 1.3198  \n",
      "\n",
      "Fold: 31  Epoch: 77  Training loss = 2.1797  Validation loss = 1.2984  \n",
      "\n",
      "Fold: 31  Epoch: 78  Training loss = 2.1789  Validation loss = 1.3004  \n",
      "\n",
      "Fold: 31  Epoch: 79  Training loss = 2.1782  Validation loss = 1.3185  \n",
      "\n",
      "Fold: 31  Epoch: 80  Training loss = 2.1784  Validation loss = 1.3343  \n",
      "\n",
      "Fold: 31  Epoch: 81  Training loss = 2.1780  Validation loss = 1.3344  \n",
      "\n",
      "Fold: 31  Epoch: 82  Training loss = 2.1782  Validation loss = 1.3453  \n",
      "\n",
      "Fold: 31  Epoch: 83  Training loss = 2.1756  Validation loss = 1.3215  \n",
      "\n",
      "Fold: 31  Epoch: 84  Training loss = 2.1787  Validation loss = 1.3580  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 68  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 1.8756  Validation loss = 3.0260  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 1.8726  Validation loss = 2.9980  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 1.8723  Validation loss = 2.9871  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 1.8735  Validation loss = 3.0230  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 1.8708  Validation loss = 3.0089  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 1.8680  Validation loss = 2.9725  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 1.8669  Validation loss = 2.9936  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 1.8656  Validation loss = 2.9915  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 1.8635  Validation loss = 2.9769  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 1.8624  Validation loss = 2.9556  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 1.8611  Validation loss = 2.9598  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 1.8598  Validation loss = 2.9672  \n",
      "\n",
      "Fold: 32  Epoch: 13  Training loss = 1.8587  Validation loss = 2.9421  \n",
      "\n",
      "Fold: 32  Epoch: 14  Training loss = 1.8570  Validation loss = 2.9427  \n",
      "\n",
      "Fold: 32  Epoch: 15  Training loss = 1.8567  Validation loss = 2.9604  \n",
      "\n",
      "Fold: 32  Epoch: 16  Training loss = 1.8553  Validation loss = 2.9546  \n",
      "\n",
      "Fold: 32  Epoch: 17  Training loss = 1.8545  Validation loss = 2.9234  \n",
      "\n",
      "Fold: 32  Epoch: 18  Training loss = 1.8552  Validation loss = 2.9006  \n",
      "\n",
      "Fold: 32  Epoch: 19  Training loss = 1.8528  Validation loss = 2.9308  \n",
      "\n",
      "Fold: 32  Epoch: 20  Training loss = 1.8525  Validation loss = 2.9558  \n",
      "\n",
      "Fold: 32  Epoch: 21  Training loss = 1.8534  Validation loss = 2.9750  \n",
      "\n",
      "Fold: 32  Epoch: 22  Training loss = 1.8516  Validation loss = 2.9594  \n",
      "\n",
      "Fold: 32  Epoch: 23  Training loss = 1.8495  Validation loss = 2.9418  \n",
      "\n",
      "Fold: 32  Epoch: 24  Training loss = 1.8491  Validation loss = 2.9337  \n",
      "\n",
      "Fold: 32  Epoch: 25  Training loss = 1.8476  Validation loss = 2.9153  \n",
      "\n",
      "Fold: 32  Epoch: 26  Training loss = 1.8463  Validation loss = 2.9281  \n",
      "\n",
      "Fold: 32  Epoch: 27  Training loss = 1.8455  Validation loss = 2.9358  \n",
      "\n",
      "Fold: 32  Epoch: 28  Training loss = 1.8436  Validation loss = 2.9216  \n",
      "\n",
      "Fold: 32  Epoch: 29  Training loss = 1.8431  Validation loss = 2.8981  \n",
      "\n",
      "Fold: 32  Epoch: 30  Training loss = 1.8435  Validation loss = 2.8818  \n",
      "\n",
      "Fold: 32  Epoch: 31  Training loss = 1.8459  Validation loss = 2.8575  \n",
      "\n",
      "Fold: 32  Epoch: 32  Training loss = 1.8398  Validation loss = 2.9011  \n",
      "\n",
      "Fold: 32  Epoch: 33  Training loss = 1.8402  Validation loss = 2.8761  \n",
      "\n",
      "Fold: 32  Epoch: 34  Training loss = 1.8388  Validation loss = 2.8854  \n",
      "\n",
      "Fold: 32  Epoch: 35  Training loss = 1.8370  Validation loss = 2.8819  \n",
      "\n",
      "Fold: 32  Epoch: 36  Training loss = 1.8378  Validation loss = 2.8488  \n",
      "\n",
      "Fold: 32  Epoch: 37  Training loss = 1.8363  Validation loss = 2.8444  \n",
      "\n",
      "Fold: 32  Epoch: 38  Training loss = 1.8347  Validation loss = 2.8480  \n",
      "\n",
      "Fold: 32  Epoch: 39  Training loss = 1.8338  Validation loss = 2.8588  \n",
      "\n",
      "Fold: 32  Epoch: 40  Training loss = 1.8336  Validation loss = 2.8457  \n",
      "\n",
      "Fold: 32  Epoch: 41  Training loss = 1.8312  Validation loss = 2.8685  \n",
      "\n",
      "Fold: 32  Epoch: 42  Training loss = 1.8307  Validation loss = 2.8864  \n",
      "\n",
      "Fold: 32  Epoch: 43  Training loss = 1.8290  Validation loss = 2.8470  \n",
      "\n",
      "Fold: 32  Epoch: 44  Training loss = 1.8278  Validation loss = 2.8656  \n",
      "\n",
      "Fold: 32  Epoch: 45  Training loss = 1.8270  Validation loss = 2.8743  \n",
      "\n",
      "Fold: 32  Epoch: 46  Training loss = 1.8267  Validation loss = 2.8735  \n",
      "\n",
      "Fold: 32  Epoch: 47  Training loss = 1.8258  Validation loss = 2.8587  \n",
      "\n",
      "Fold: 32  Epoch: 48  Training loss = 1.8255  Validation loss = 2.8591  \n",
      "\n",
      "Fold: 32  Epoch: 49  Training loss = 1.8247  Validation loss = 2.8563  \n",
      "\n",
      "Fold: 32  Epoch: 50  Training loss = 1.8242  Validation loss = 2.8425  \n",
      "\n",
      "Fold: 32  Epoch: 51  Training loss = 1.8242  Validation loss = 2.8174  \n",
      "\n",
      "Fold: 32  Epoch: 52  Training loss = 1.8221  Validation loss = 2.8394  \n",
      "\n",
      "Fold: 32  Epoch: 53  Training loss = 1.8208  Validation loss = 2.8587  \n",
      "\n",
      "Fold: 32  Epoch: 54  Training loss = 1.8202  Validation loss = 2.8682  \n",
      "\n",
      "Fold: 32  Epoch: 55  Training loss = 1.8192  Validation loss = 2.8659  \n",
      "\n",
      "Fold: 32  Epoch: 56  Training loss = 1.8176  Validation loss = 2.8377  \n",
      "\n",
      "Fold: 32  Epoch: 57  Training loss = 1.8173  Validation loss = 2.8188  \n",
      "\n",
      "Fold: 32  Epoch: 58  Training loss = 1.8168  Validation loss = 2.8106  \n",
      "\n",
      "Fold: 32  Epoch: 59  Training loss = 1.8158  Validation loss = 2.8112  \n",
      "\n",
      "Fold: 32  Epoch: 60  Training loss = 1.8146  Validation loss = 2.8162  \n",
      "\n",
      "Fold: 32  Epoch: 61  Training loss = 1.8142  Validation loss = 2.8011  \n",
      "\n",
      "Fold: 32  Epoch: 62  Training loss = 1.8130  Validation loss = 2.8014  \n",
      "\n",
      "Fold: 32  Epoch: 63  Training loss = 1.8121  Validation loss = 2.8464  \n",
      "\n",
      "Fold: 32  Epoch: 64  Training loss = 1.8118  Validation loss = 2.8579  \n",
      "\n",
      "Fold: 32  Epoch: 65  Training loss = 1.8094  Validation loss = 2.8320  \n",
      "\n",
      "Fold: 32  Epoch: 66  Training loss = 1.8086  Validation loss = 2.8197  \n",
      "\n",
      "Fold: 32  Epoch: 67  Training loss = 1.8078  Validation loss = 2.8049  \n",
      "\n",
      "Fold: 32  Epoch: 68  Training loss = 1.8069  Validation loss = 2.7958  \n",
      "\n",
      "Fold: 32  Epoch: 69  Training loss = 1.8065  Validation loss = 2.7855  \n",
      "\n",
      "Fold: 32  Epoch: 70  Training loss = 1.8064  Validation loss = 2.7728  \n",
      "\n",
      "Fold: 32  Epoch: 71  Training loss = 1.8044  Validation loss = 2.7922  \n",
      "\n",
      "Fold: 32  Epoch: 72  Training loss = 1.8029  Validation loss = 2.8128  \n",
      "\n",
      "Fold: 32  Epoch: 73  Training loss = 1.8023  Validation loss = 2.8040  \n",
      "\n",
      "Fold: 32  Epoch: 74  Training loss = 1.8014  Validation loss = 2.8013  \n",
      "\n",
      "Fold: 32  Epoch: 75  Training loss = 1.8009  Validation loss = 2.8119  \n",
      "\n",
      "Fold: 32  Epoch: 76  Training loss = 1.8019  Validation loss = 2.7636  \n",
      "\n",
      "Fold: 32  Epoch: 77  Training loss = 1.7993  Validation loss = 2.7995  \n",
      "\n",
      "Fold: 32  Epoch: 78  Training loss = 1.7986  Validation loss = 2.8070  \n",
      "\n",
      "Fold: 32  Epoch: 79  Training loss = 1.7992  Validation loss = 2.8284  \n",
      "\n",
      "Fold: 32  Epoch: 80  Training loss = 1.7987  Validation loss = 2.8296  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 76  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    # Load model with lowest validation error for each fold\n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 159\n",
      "Average validation error: 2.76228\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 1.7742  Test loss = 2.8827  \n",
      "\n",
      "Epoch: 2  Training loss = 1.7733  Test loss = 2.8795  \n",
      "\n",
      "Epoch: 3  Training loss = 1.7725  Test loss = 2.8764  \n",
      "\n",
      "Epoch: 4  Training loss = 1.7716  Test loss = 2.8733  \n",
      "\n",
      "Epoch: 5  Training loss = 1.7708  Test loss = 2.8704  \n",
      "\n",
      "Epoch: 6  Training loss = 1.7700  Test loss = 2.8675  \n",
      "\n",
      "Epoch: 7  Training loss = 1.7692  Test loss = 2.8647  \n",
      "\n",
      "Epoch: 8  Training loss = 1.7685  Test loss = 2.8619  \n",
      "\n",
      "Epoch: 9  Training loss = 1.7678  Test loss = 2.8593  \n",
      "\n",
      "Epoch: 10  Training loss = 1.7670  Test loss = 2.8567  \n",
      "\n",
      "Epoch: 11  Training loss = 1.7663  Test loss = 2.8542  \n",
      "\n",
      "Epoch: 12  Training loss = 1.7657  Test loss = 2.8517  \n",
      "\n",
      "Epoch: 13  Training loss = 1.7650  Test loss = 2.8493  \n",
      "\n",
      "Epoch: 14  Training loss = 1.7644  Test loss = 2.8470  \n",
      "\n",
      "Epoch: 15  Training loss = 1.7638  Test loss = 2.8447  \n",
      "\n",
      "Epoch: 16  Training loss = 1.7632  Test loss = 2.8425  \n",
      "\n",
      "Epoch: 17  Training loss = 1.7626  Test loss = 2.8404  \n",
      "\n",
      "Epoch: 18  Training loss = 1.7620  Test loss = 2.8383  \n",
      "\n",
      "Epoch: 19  Training loss = 1.7615  Test loss = 2.8362  \n",
      "\n",
      "Epoch: 20  Training loss = 1.7609  Test loss = 2.8343  \n",
      "\n",
      "Epoch: 21  Training loss = 1.7604  Test loss = 2.8323  \n",
      "\n",
      "Epoch: 22  Training loss = 1.7599  Test loss = 2.8304  \n",
      "\n",
      "Epoch: 23  Training loss = 1.7594  Test loss = 2.8286  \n",
      "\n",
      "Epoch: 24  Training loss = 1.7589  Test loss = 2.8268  \n",
      "\n",
      "Epoch: 25  Training loss = 1.7584  Test loss = 2.8250  \n",
      "\n",
      "Epoch: 26  Training loss = 1.7579  Test loss = 2.8233  \n",
      "\n",
      "Epoch: 27  Training loss = 1.7575  Test loss = 2.8216  \n",
      "\n",
      "Epoch: 28  Training loss = 1.7570  Test loss = 2.8199  \n",
      "\n",
      "Epoch: 29  Training loss = 1.7566  Test loss = 2.8183  \n",
      "\n",
      "Epoch: 30  Training loss = 1.7561  Test loss = 2.8167  \n",
      "\n",
      "Epoch: 31  Training loss = 1.7557  Test loss = 2.8151  \n",
      "\n",
      "Epoch: 32  Training loss = 1.7553  Test loss = 2.8136  \n",
      "\n",
      "Epoch: 33  Training loss = 1.7549  Test loss = 2.8121  \n",
      "\n",
      "Epoch: 34  Training loss = 1.7545  Test loss = 2.8106  \n",
      "\n",
      "Epoch: 35  Training loss = 1.7541  Test loss = 2.8092  \n",
      "\n",
      "Epoch: 36  Training loss = 1.7537  Test loss = 2.8078  \n",
      "\n",
      "Epoch: 37  Training loss = 1.7533  Test loss = 2.8064  \n",
      "\n",
      "Epoch: 38  Training loss = 1.7529  Test loss = 2.8050  \n",
      "\n",
      "Epoch: 39  Training loss = 1.7525  Test loss = 2.8037  \n",
      "\n",
      "Epoch: 40  Training loss = 1.7522  Test loss = 2.8024  \n",
      "\n",
      "Epoch: 41  Training loss = 1.7518  Test loss = 2.8011  \n",
      "\n",
      "Epoch: 42  Training loss = 1.7514  Test loss = 2.7998  \n",
      "\n",
      "Epoch: 43  Training loss = 1.7511  Test loss = 2.7985  \n",
      "\n",
      "Epoch: 44  Training loss = 1.7507  Test loss = 2.7973  \n",
      "\n",
      "Epoch: 45  Training loss = 1.7503  Test loss = 2.7961  \n",
      "\n",
      "Epoch: 46  Training loss = 1.7500  Test loss = 2.7949  \n",
      "\n",
      "Epoch: 47  Training loss = 1.7497  Test loss = 2.7937  \n",
      "\n",
      "Epoch: 48  Training loss = 1.7493  Test loss = 2.7925  \n",
      "\n",
      "Epoch: 49  Training loss = 1.7490  Test loss = 2.7914  \n",
      "\n",
      "Epoch: 50  Training loss = 1.7486  Test loss = 2.7902  \n",
      "\n",
      "Epoch: 51  Training loss = 1.7483  Test loss = 2.7891  \n",
      "\n",
      "Epoch: 52  Training loss = 1.7480  Test loss = 2.7880  \n",
      "\n",
      "Epoch: 53  Training loss = 1.7476  Test loss = 2.7869  \n",
      "\n",
      "Epoch: 54  Training loss = 1.7473  Test loss = 2.7858  \n",
      "\n",
      "Epoch: 55  Training loss = 1.7470  Test loss = 2.7848  \n",
      "\n",
      "Epoch: 56  Training loss = 1.7467  Test loss = 2.7837  \n",
      "\n",
      "Epoch: 57  Training loss = 1.7463  Test loss = 2.7827  \n",
      "\n",
      "Epoch: 58  Training loss = 1.7460  Test loss = 2.7817  \n",
      "\n",
      "Epoch: 59  Training loss = 1.7457  Test loss = 2.7807  \n",
      "\n",
      "Epoch: 60  Training loss = 1.7454  Test loss = 2.7797  \n",
      "\n",
      "Epoch: 61  Training loss = 1.7451  Test loss = 2.7787  \n",
      "\n",
      "Epoch: 62  Training loss = 1.7448  Test loss = 2.7777  \n",
      "\n",
      "Epoch: 63  Training loss = 1.7445  Test loss = 2.7767  \n",
      "\n",
      "Epoch: 64  Training loss = 1.7441  Test loss = 2.7758  \n",
      "\n",
      "Epoch: 65  Training loss = 1.7438  Test loss = 2.7749  \n",
      "\n",
      "Epoch: 66  Training loss = 1.7435  Test loss = 2.7739  \n",
      "\n",
      "Epoch: 67  Training loss = 1.7432  Test loss = 2.7730  \n",
      "\n",
      "Epoch: 68  Training loss = 1.7429  Test loss = 2.7721  \n",
      "\n",
      "Epoch: 69  Training loss = 1.7426  Test loss = 2.7712  \n",
      "\n",
      "Epoch: 70  Training loss = 1.7423  Test loss = 2.7703  \n",
      "\n",
      "Epoch: 71  Training loss = 1.7420  Test loss = 2.7694  \n",
      "\n",
      "Epoch: 72  Training loss = 1.7417  Test loss = 2.7686  \n",
      "\n",
      "Epoch: 73  Training loss = 1.7414  Test loss = 2.7677  \n",
      "\n",
      "Epoch: 74  Training loss = 1.7412  Test loss = 2.7668  \n",
      "\n",
      "Epoch: 75  Training loss = 1.7409  Test loss = 2.7660  \n",
      "\n",
      "Epoch: 76  Training loss = 1.7406  Test loss = 2.7652  \n",
      "\n",
      "Epoch: 77  Training loss = 1.7403  Test loss = 2.7643  \n",
      "\n",
      "Epoch: 78  Training loss = 1.7400  Test loss = 2.7635  \n",
      "\n",
      "Epoch: 79  Training loss = 1.7397  Test loss = 2.7627  \n",
      "\n",
      "Epoch: 80  Training loss = 1.7394  Test loss = 2.7619  \n",
      "\n",
      "Epoch: 81  Training loss = 1.7391  Test loss = 2.7611  \n",
      "\n",
      "Epoch: 82  Training loss = 1.7388  Test loss = 2.7603  \n",
      "\n",
      "Epoch: 83  Training loss = 1.7386  Test loss = 2.7595  \n",
      "\n",
      "Epoch: 84  Training loss = 1.7383  Test loss = 2.7587  \n",
      "\n",
      "Epoch: 85  Training loss = 1.7380  Test loss = 2.7580  \n",
      "\n",
      "Epoch: 86  Training loss = 1.7377  Test loss = 2.7572  \n",
      "\n",
      "Epoch: 87  Training loss = 1.7374  Test loss = 2.7565  \n",
      "\n",
      "Epoch: 88  Training loss = 1.7372  Test loss = 2.7557  \n",
      "\n",
      "Epoch: 89  Training loss = 1.7369  Test loss = 2.7550  \n",
      "\n",
      "Epoch: 90  Training loss = 1.7366  Test loss = 2.7542  \n",
      "\n",
      "Epoch: 91  Training loss = 1.7363  Test loss = 2.7535  \n",
      "\n",
      "Epoch: 92  Training loss = 1.7360  Test loss = 2.7528  \n",
      "\n",
      "Epoch: 93  Training loss = 1.7358  Test loss = 2.7521  \n",
      "\n",
      "Epoch: 94  Training loss = 1.7355  Test loss = 2.7513  \n",
      "\n",
      "Epoch: 95  Training loss = 1.7352  Test loss = 2.7506  \n",
      "\n",
      "Epoch: 96  Training loss = 1.7349  Test loss = 2.7499  \n",
      "\n",
      "Epoch: 97  Training loss = 1.7347  Test loss = 2.7492  \n",
      "\n",
      "Epoch: 98  Training loss = 1.7344  Test loss = 2.7486  \n",
      "\n",
      "Epoch: 99  Training loss = 1.7341  Test loss = 2.7479  \n",
      "\n",
      "Epoch: 100  Training loss = 1.7339  Test loss = 2.7472  \n",
      "\n",
      "Epoch: 101  Training loss = 1.7336  Test loss = 2.7465  \n",
      "\n",
      "Epoch: 102  Training loss = 1.7333  Test loss = 2.7459  \n",
      "\n",
      "Epoch: 103  Training loss = 1.7330  Test loss = 2.7452  \n",
      "\n",
      "Epoch: 104  Training loss = 1.7328  Test loss = 2.7445  \n",
      "\n",
      "Epoch: 105  Training loss = 1.7325  Test loss = 2.7439  \n",
      "\n",
      "Epoch: 106  Training loss = 1.7322  Test loss = 2.7432  \n",
      "\n",
      "Epoch: 107  Training loss = 1.7320  Test loss = 2.7426  \n",
      "\n",
      "Epoch: 108  Training loss = 1.7317  Test loss = 2.7420  \n",
      "\n",
      "Epoch: 109  Training loss = 1.7314  Test loss = 2.7413  \n",
      "\n",
      "Epoch: 110  Training loss = 1.7312  Test loss = 2.7407  \n",
      "\n",
      "Epoch: 111  Training loss = 1.7309  Test loss = 2.7401  \n",
      "\n",
      "Epoch: 112  Training loss = 1.7307  Test loss = 2.7395  \n",
      "\n",
      "Epoch: 113  Training loss = 1.7304  Test loss = 2.7388  \n",
      "\n",
      "Epoch: 114  Training loss = 1.7301  Test loss = 2.7382  \n",
      "\n",
      "Epoch: 115  Training loss = 1.7299  Test loss = 2.7376  \n",
      "\n",
      "Epoch: 116  Training loss = 1.7296  Test loss = 2.7370  \n",
      "\n",
      "Epoch: 117  Training loss = 1.7293  Test loss = 2.7364  \n",
      "\n",
      "Epoch: 118  Training loss = 1.7291  Test loss = 2.7358  \n",
      "\n",
      "Epoch: 119  Training loss = 1.7288  Test loss = 2.7352  \n",
      "\n",
      "Epoch: 120  Training loss = 1.7286  Test loss = 2.7347  \n",
      "\n",
      "Epoch: 121  Training loss = 1.7283  Test loss = 2.7341  \n",
      "\n",
      "Epoch: 122  Training loss = 1.7280  Test loss = 2.7335  \n",
      "\n",
      "Epoch: 123  Training loss = 1.7278  Test loss = 2.7329  \n",
      "\n",
      "Epoch: 124  Training loss = 1.7275  Test loss = 2.7324  \n",
      "\n",
      "Epoch: 125  Training loss = 1.7273  Test loss = 2.7318  \n",
      "\n",
      "Epoch: 126  Training loss = 1.7270  Test loss = 2.7312  \n",
      "\n",
      "Epoch: 127  Training loss = 1.7268  Test loss = 2.7307  \n",
      "\n",
      "Epoch: 128  Training loss = 1.7265  Test loss = 2.7301  \n",
      "\n",
      "Epoch: 129  Training loss = 1.7263  Test loss = 2.7296  \n",
      "\n",
      "Epoch: 130  Training loss = 1.7260  Test loss = 2.7290  \n",
      "\n",
      "Epoch: 131  Training loss = 1.7257  Test loss = 2.7285  \n",
      "\n",
      "Epoch: 132  Training loss = 1.7255  Test loss = 2.7279  \n",
      "\n",
      "Epoch: 133  Training loss = 1.7252  Test loss = 2.7274  \n",
      "\n",
      "Epoch: 134  Training loss = 1.7250  Test loss = 2.7269  \n",
      "\n",
      "Epoch: 135  Training loss = 1.7247  Test loss = 2.7263  \n",
      "\n",
      "Epoch: 136  Training loss = 1.7245  Test loss = 2.7258  \n",
      "\n",
      "Epoch: 137  Training loss = 1.7242  Test loss = 2.7253  \n",
      "\n",
      "Epoch: 138  Training loss = 1.7240  Test loss = 2.7248  \n",
      "\n",
      "Epoch: 139  Training loss = 1.7237  Test loss = 2.7243  \n",
      "\n",
      "Epoch: 140  Training loss = 1.7235  Test loss = 2.7238  \n",
      "\n",
      "Epoch: 141  Training loss = 1.7232  Test loss = 2.7232  \n",
      "\n",
      "Epoch: 142  Training loss = 1.7230  Test loss = 2.7227  \n",
      "\n",
      "Epoch: 143  Training loss = 1.7227  Test loss = 2.7222  \n",
      "\n",
      "Epoch: 144  Training loss = 1.7225  Test loss = 2.7217  \n",
      "\n",
      "Epoch: 145  Training loss = 1.7222  Test loss = 2.7212  \n",
      "\n",
      "Epoch: 146  Training loss = 1.7220  Test loss = 2.7207  \n",
      "\n",
      "Epoch: 147  Training loss = 1.7218  Test loss = 2.7203  \n",
      "\n",
      "Epoch: 148  Training loss = 1.7215  Test loss = 2.7198  \n",
      "\n",
      "Epoch: 149  Training loss = 1.7213  Test loss = 2.7193  \n",
      "\n",
      "Epoch: 150  Training loss = 1.7210  Test loss = 2.7188  \n",
      "\n",
      "Epoch: 151  Training loss = 1.7208  Test loss = 2.7183  \n",
      "\n",
      "Epoch: 152  Training loss = 1.7205  Test loss = 2.7178  \n",
      "\n",
      "Epoch: 153  Training loss = 1.7203  Test loss = 2.7174  \n",
      "\n",
      "Epoch: 154  Training loss = 1.7200  Test loss = 2.7169  \n",
      "\n",
      "Epoch: 155  Training loss = 1.7198  Test loss = 2.7164  \n",
      "\n",
      "Epoch: 156  Training loss = 1.7196  Test loss = 2.7160  \n",
      "\n",
      "Epoch: 157  Training loss = 1.7193  Test loss = 2.7155  \n",
      "\n",
      "Epoch: 158  Training loss = 1.7191  Test loss = 2.7151  \n",
      "\n",
      "Epoch: 159  Training loss = 1.7188  Test loss = 2.7146  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXl4U1X+xt+bJi3d0hUopS0tWxdaKEtRUFFkEVlEi4BF\nHNFxGWF0ZtTfzKjjLqOjMuOOgxsDKiAoooILIDoOBS0IFArdKBS60dKF7ltyfn+cnOQmuTdLk6ZN\ncj7Pw9OS3NzcpMl7v/c930UghIDD4XA4noOirw+Aw+FwOM6FCzuHw+F4GFzYORwOx8Pgws7hcDge\nBhd2DofD8TC4sHM4HI6HwYWdw+FwPAwu7BwOh+NhcGHncDgcD0PZF08aGRlJ4uPj++KpORwOx205\nfPjwRULIQGvb9Ymwx8fH49ChQ33x1BwOh+O2CIJQast23IrhcDgcD4MLO4fD4XgYXNg5HA7Hw+DC\nzuFwOB4GF3YOh8PxMLiwczgcjofBhZ3D4XA8DPcT9h6M8mtvb8f7778PPgaQw+F4A+4l7M89B1x5\npd0P27JlC377298iNze3Fw6K48kQQqwGBFqtFk8//TT27dvnoqPicCzjXsIeEgJkZwMnT9r1sGPH\njgEAmpube+OoOB7MX//6V1x77bUWtzl16hSeeuopXHvttbjppptQXFzsoqPjcKRxL2FfvBhQKIDN\nm+162PHjxwEAbW1tvXFUHA/m+PHjOHHihMVt6urqAAA333wz9uzZg5SUFDz44IOor693xSFyOGa4\nl7BHRQHTpwObNtnltTMLprW1tbeOjOOh1NbWoq6uDlqtVnYbJux//etfUVRUhN/85jd45ZVXkJaW\nhpaWFlcdKoejx72EHQCysoDiYuDwYZs2v3DhAqqrqwHwiJ1jP7W1tdBqtWhsbJTdhgl7eHg4oqKi\n8O677+Ltt99GeXk5zp0756pD5XD0uJ+wZ2YCKhWN2m2A2TAAj9g59lNbWwvAIN5SsPsiIiL0t8XG\nxgKAxRMCh9NbuJ+wh4UB118PbNkCWLg8ZogzYXjEzrGH7u5uNDQ0ALAu7D4+PggODtbfplarAXBh\n5/QN7ifsALVjysuBn36yuunx48f1XzgesXPsQbz4ySJ3Kerq6hAeHg5BEPS3cWHn9CXuKewLFgAB\nATbZMbm5uZg0aRIAHrFz7EMs5pYi9traWoSHhxvdxoT90qVLvXNwHI4F3ErYjx07hi+++AIIDAQW\nLgS2bQO6umS37+7uxsmTJzFhwgSoVCoesXPswlZhZxG7mJCQEAA8Yuf0DW4l7OvWrcMdd9xB/5OV\nBdTWArt3y25fXFyM9vZ2pKWlISAggEfsXsy+fftw5swZux4jFnZbrBgxzP7jws7pC9xK2CMiIlBf\nX09zimfPBkJDLdoxLCNm7Nix8Pf35xG7F5OZmYkXX3zRrsfYE7GLM2IAwMfHB4GBgVzYOX2CWwl7\neHg4CCHUt/TzAxYtAj7/HJAR7NzcXPj4+CA5Odk8Yj90CFi71kVHzulLGhsb0dDQYDHqloJtHx4e\nbrcVA1CfnQs7py9wO2EHRNFTVhbQ3Azs3Cm5/fHjxzF69GgMGDDAPGJ/6CHgwQd7+5A5/YDy8nIA\n0Kcu2kptbS1UKhWGDRsme1Lo6upCU1MTF3ZOv8IthV3/JbvmGmDwYBq1S5Cbm4uxY8cCgHHEfvo0\n8N//Au3t9B/HoykrKwPQM2GPiIhARESEbMTOUiLlhJ1nxXD6ArcSduZj6r9kPj5AejpQUGC2bVNT\nE86cOYO0tDQAMI7YN2wwbGjnl53jfjBht7cpFxN2S1aM2K4xJSQkhEfsnD7BrYTdzIoBgOHDAYls\nB9aRzyxi12qB//wH8PWlG/IOfB6PoxG7JWEX94kxhVsxnL7C/YU9IQGoqwNMLnlZKwFxxN7W1gb8\n+CNQWkr9eYALuxcg9tjtmaJlasVIdXiU6hPD4MLO6SvcStjDwsIAmOQUDx9Of5pE7ayVwLBhwwCI\nrJj16wG1Glixgm7Ihd3jYRF7d3e3XSmv4ohdrsMjj9g5/RG3EnalUomQkBDziB0ASkqMtmULp6x/\nR0BAABQtLbRa9ZZbgOhouiH32D0eJuyA7XYMIcRI2AHpXHZbhJ3P2uW4GrcSdkAip1giYieE4Pjx\n43obBqAR+6xLl2jO+4oVtEskwCN2L6CsrExvldgq7M3Nzejs7NRbMYB09WldXR0UCoW+N4wYtVoN\nrVbLh21wXI5ThF0QhFBBELYJgpAvCMIpQRCmOGO/UpgJe2go/SeK2MvKytDQ0KBfOAVoxH5Lezsw\nejRw+eX0MQAXdg+nra0NtbW1SE1NBWC7sDMRtxax19bWIiwsDAqF+VeJd3jk9BXOithfBfANISQJ\nwDgAp5y0XzPCw8PNIyeTzBjWSkAcsUe3teEqQqD9zW8AQaDDOgIDuRXj4bCFUybstqY82irsclWn\nAG8Exuk7HBZ2QRBCAEwD8B4AEEI6CSG9ppaSqWcJCUYRu2lGDABMOnkSWgDtixcbHhcWxiN2D4f5\n645E7NasGDlh5xE7p69wRsSeAKAGwAeCIBwRBOFdQRACnbBfSSSrAFnErktHO378OIYNG6aPmKDV\nIvXXX7EbQAvz1gFqx3Bh92iYsLOTfE+EnWVjyUXsUqmOABd2Tt/hDGFXApgAYC0hZDyAFgB/Nd1I\nEIR7BEE4JAjCoZqamh4/WXh4uKHDIyMhAejsBCorAdCIXRytY/9+qOvqsB4mwzbCwrgV4+EwK2bM\nmDEAeibsKpUKwcHBdlsxXNg5fYUzhL0MQBkh5Gfd/7eBCr0RhJB1hJBJhJBJAwcO7PGTsZxiox4c\nLDOmpASdnZ3Iz883WjjFgQMAgG9hMh6PWzEeT1lZGUJDQxEaGorAwEC7hZ2Jtlz1KRd2Tn/EYWEn\nhFQBOC8IQqLuphkATjq6Xzlk2woAwJkzqKioQHd3N0aMGGG4/8gRtA4ciHqYROzcivF4ysrKEBMT\nAwAIDQ21S9hDQkKgVCoB0Mjd1GPv7u7GpUuXrAo7bwTGcTXOyoq5H8BHgiDkAkgH8Hcn7dcMs0Zg\nABAXRzNdSkpQVVUFABgyZIjh/qNH0TxyJADXReylpaV6G4DTdzgi7GLvXCpit9TZEeBTlDh9h1OE\nnRByVGezjCWE3EgI6bUwWDJi9/MDYmKAM2dQqfPZ9cLe0gIUFKAtORmAhMfe1AR0dzv9OJctW4Z7\n7rnHOTu7/35g1Srn7MvLMBV2e9IdrQm7papTAFCpVAgICODCznE5bll5CkiknulSHlnEHhUVRW8/\nfhwgBF0pKQAkInbArIGYM8jPz8fp06eds7MffgA2brQ4uJtjTmdnJy5cuIChQ4cCcCxil7JiLDUA\nY/B+MZy+wG2FXS7lsaqqCgqFAvoF2iNHAAAkPR2AhMcOON2OaWxsRF1dHcrKypzTJ6S6ml5Z5OQ4\nvi8vorKyEoQQp1kxptlY1iJ2gAs7p2/wHGFPSADKy3GxrAwDBw6Ej48Pvf3oUSAsDCrdAqtkxO7k\nlMfS0lIAQEtLi909wM3QaICLF+nve/c6eGTeBcthZ8IeFhbmkLCbdnjkws7pr7idsCuVSqjVaumI\nHYC2pMRgwwA0Yk9Ph39AAAAJjx1wesR+9uxZ/e/izoI9orZWX3iFPXsc25eXYSrsLGK3dhXV1dWF\nxsZGMysGMLYAbRV2nhXDcTVuJ+yAhbYCAHzLyw0Lp93d1GMfPx4BUsLeS1aMU4W9upr+jI+n+fi8\nU6DNsKwksbBrtVo0NzdbfJyUdy51pVhXVwdBEAwVzhLwiJ3TF7itsEs2AgMQXFNjiNgLC+mw6vR0\n+Pv7A3CNFSMW9vPnzzu2MybsWVl08fR//3Nsf15EWVkZAgMD9cIbqjuRW7NjxFWnDClhr62tRWho\nqMH2k4DPPeX0BW4p7JL9YqKiQAYMQHhjo0HYdQunGD8eSqUSKpXKZVbM6NGjoVAonBexZ2bSOa3c\njrEZlurIhq0wYbeW8igl7FL1E5b6xDB4xM7pC9xS2CWtGEGAJi4O8VqtQdiPHqU57om0KFY/Ho/h\n70/b9zpZ2M+cOYORI0diyJAhzhP2hARgyhS+gGoH4hx2wDkRu6nHbslfB/gUJU7f4DnCDqAtKgoJ\nEBUnHTkCpKVR8QYdtmEUsQtCrzQCO3v2LOLj4xETE+McK8bHhx7nzJn0NbEsGY5FnCnsUh0ebRV2\njUZj/LnjcHoZtxZ206nxDWFhGA4gavBggBAasevy1wGJiB1weluBS5cuob6+Xi/sTonYBw4EFApg\nxgx62759jh+oh6PRaFBRUaEvTgIM4twTYVepVGbZWLYKO8D7xXBci9sKu9TU+OqgIIQAiPb3B8rK\naKrg+PH6+80idsDpws5y2OPj4xEbG4vz5887dhleXQ0MGkR/z8gAgoO5HWMDFy5cgEaj6XHE7uvr\ni8BA47ECpov29gg799k5rsQthV2yERiA87pOfFGtrTRaB6xH7E7u8MgyYhISEhATE4OWlhbHorUL\nFwzCrlQC11zDF1BtwDSHHTCIrC3CHhERoV90ZYgtQI1Gg4aGBi7snH6JWwq7XPXpaV1kHHjhAvWi\nBQEwGWgtGbE70WNnws6sGMDBXHZxxA5QO+b0aUCUUskxxzSHHaDFbcHBwTYLuynibCxW6GQtK8Zo\n7mlXF69D4LgEtxZ201z2U+3tAADh7FkasY8aBQQF6e93hcd+9uxZBAYGIiIiArGxsQAczGU3FfaZ\nM+lPbsdYRCpiB2zr8Cgn7GIrxpaqU8AkYr/7bmDqVNteAMf9WbEC2L69T57arYXdNGI/W1uLeqWS\nDrY+csTIXwdkIvbQUBqxOykd7cyZM4iPj4cgCI5H7K2tQHMzMHiw4baUFCAqigu7FcrKyuDr64vI\nyEij221pBGZJ2Nlnzl5h1xYX0w6dubm0oRvHs6mrA/7zH+CuuwAHRoH2FLcUdjmPvbKyEheDg6mo\nnz1r5K8DFiJ2jYYKqBNgqY4ATbt0qEiJfSDEEbsgUDtm716nnYxs4osvgMhIeiyxscDIkfQk88wz\nrjsGOzAtTmI4Kuysw6O9wj7qyy8NPX/y8mx9GRx3paCA/qyrA/78Z5c/vVsKu9zU+KqqKlyKiDC0\nt7UlYndy9alY2FUqFaKionpuxbDiJLGwA1TYq6uBEyd6fqD28tNPQGMjsGgRMGsWcNlltF3DBx+4\n7BAIIcjNzbUpy8g0h51hTdgJIRY9djZv11ZhDw4ORiSA5IMHgWuvpTceP271+DluDhP2m28G1q8H\nfvzRpU/vlsLOpsaLPfaOjg7U1dWhQzwSz5aI3YmNwBoaGtDQ0KAXdgCO5bJbEnbAtfnslZXA0KHA\n2rXA++8DH32EjiVLQM6dAzo6XHIIa9euxbhx4/Dvf//b6rZywm6tdW9TUxO6u7tlI3aABhS2Cruv\nry/+6OMDn64u4I03gMBALuzeQEEBzWJ77z3awO+++4DOTpc9vVsKO2BefVqtE0EtE9UhQ4y9aViJ\n2J2QGcNy2BN0nSYB6HPZe4ScsMfF0RNSYWHP9tsTqqroeypi3fffQ9Bq0Zib2+tPf/z4cTz44IMA\ngDfeeMMQtR8+DPzznwabAzTqLisrMypOYliL2KWKkxhSws6uHmVpacF9Wi1y4+OB5GQgNdX6lda+\nffRqiOO+FBYCI0YAajXw5pvAqVPAmjUue3q3FXbTRmBs1qlS1xfGNFoHaMTe0dFhXLHqRCtGnOrI\nYG0FelSkxISdTYMSExMDONquwB4qK82E/RedCK59+OHe6YXS0AC0taG1tRVLly5FWFgYnn/+eeTl\n5eGnffuA554DLr8ceOghYPdu/cMuXryIzs5OWSvm0qVLZlXLDEvCLl7bsaWzIwDg3XcRTgi2jxpF\n/5+aqh/XKElBAbVs/vUvy/vl9G8KCvQ9qjB3LrUwn3mGJna4ALcVdtOInc06DRwzht5g4q8D0Lfu\n7a0Oj3LC3tLS0rMClepqeuluUgEJgC5gOtquwB4qK2k2jogc3XtW/t//4qOPPnL+c6anA9HRODB5\nMhSnTmHjxo144IEHMF6txuAlS4DHH6ce5sCBwNtv6x8ml+oIUGEnhMj+PWyJ2Gtra22qOkVXF/DP\nf+LXoCAc9vWlt6Wl0V4/Fy5IP2b/fvpzyxbL++b0XzQaoLjYIOwA8Mor1Jr5/e9dkvTg1sIu9tiZ\nsEdMmgQ88QRw551mj7E4bMMJVsyZM2cQFBRk9IV3KJddXHVqSmys6yL29nZ64hNF7O3t7Sior0eH\nnx+mDRmClStX4syZM857zsZGoLQUTf7+uCovDycAzHzqKQT8/e840NaGQbW1qH/zTWDTJuC3v6VZ\nOzpBlypOYlhrK2CPFWNV2LdsAc6dw7aEBMOJJC2N/pSzYw4epD+PHTMswHHci9JSuu4kFvaYGODZ\nZ4GvvwZ27Oj1Q3BrYZeK2AcNHgw8/TT1t0yQHLahVtMUQomInRCC5557DoU2etksI0acYudQLrtp\ncZKY2FiaDukKL5ZFlyJhr6ioAAA0Dx6MuUlJEAQBy5cvR3d3t3OeU3fSeqChAQsnToTmhRfo+7F6\nNTQTJ2IsgDfYif2ee2gU9M47AKxH7IALhJ0Q4MUXgZQUnEpIMLSVSE2lP+UWUA8cAMaNo79v3Sq/\nf07/hZ2QxcIO0Gj95ZdpVlkv47bCzjx25pVWVVUhMjISKl2LXikkI3aFQrZfTH19PR5//HG8/PLL\nNh2TONWRwSL2XhF2umP792svuvULsbCzqLhr2DAElJdj7dq1yM7Oxt///nfnPOe5cwCA84KAt7Zu\nhc9f/kK/MCUlCNi/HymzZ+Pf//43PZEkJABz5lBh7+pCWVkZfHx8MNhk8RywXdilFkXZvF2brJg9\ne6h4//nPCBZPURo0iP6TEvZLl2iOe2YmcOWVwCefWHqHOP0VFgiOHm18u1JJ14OkrFUn47bCzjo8\nNumq+CorK42HWEsgGbEDssLO9r1r1y6bFgelhH3IkCEQBKFnVowlYWfRqCuFXfT+sojdZ/Ro4MwZ\nLFu6FLfeeiueeeYZHDhwwOGn7CwqAgBcd/fdhiwjQaAirlBg1apVKC8vxxdffEHvu+8+oLISms8/\nR05ODqKjoyUXNq217mWLokpdQzlT2JWiVWH/4gsgIAC45RbzKUppadJWTE4OjfQvvxxYsoSK/6lT\n8s/BcYw1a2jGirMpKKCaIpX04CLcWtgBQ4RVVVVlVdglI3ZAthEYE/by8nLkWknpa2howKVLl4xS\nHQFDkZLdEbtWS60WiagTgCFid4XPbiFiD0hLo4uE58/jzTffRGxsLJYtW2a1utMal44fRxeA2IwM\nyfvnzZuHuLg4vPXWW/SGuXOhHToUx+67D9999x1++9vfSj7OlojdUmOviIgIXLx4EfX19ZYbgO3Z\nA0ybBvj56eee6oOD1FQamZtm5hw4QE9el11GsygEgdsxvcXp08Bf/kJrC5wNy4gxqXp2JW4v7Mxn\nr6qqMkxOkkE2YpdpBCaeZr9r1y6L+5bKiGHExsbaL+wNDUB3t/WI3RXCXlVFLSvRsZSXl8Pf3x8B\nrHtmcTFCQkKwadMmlJWV4a677nIoBbKtsBBlAEaa+pQ6fHx8cO+992Lv3r3Iz89HUUkJXuvowITa\nWmx65hk8+eSTko9zVNjDw8NRUlICrVYrH7GXlQH5+fqGbWq1Gt3d3Whn6yFpabQPkGnq28GDtE1D\nSAgQHQ1cdRW3Y3qL554zZK90dTl33+JUxz7CbYVdnFNMCHEsYrdixfj6+mLnzp0W921J2Hs0Ik+u\nOIkREABERLguYh80iI7o08GmEwksP7u4GABw+eWXY/Xq1fj000+xbt26Hj+lcP48zgEYxfYvwV13\n3QWVSoX7778fkydPxjqNBlofH9xi4WpBrVZDEATZDo+2CDvL/pEVdtagTSTsgKgnO1tAFdsxhFBh\nv/xyw21LltDInveWcS5FRcCGDcCwYTR4On3a8vb2vP/NzUB5ubm/7mLcVtjFEfulS5fQ3t7ec49d\nxophEfucOXNw4MAByTmrDPZllxN2uyN2a8JOd+w6YTd5b8vLy2llZ3Q0HRgu+nI8/PDDuO666/DH\nP/4Rx3tYPu9fU4NqnY0hx6BBg7B48WLs2bMHQ4cOxc7Dh6HIzKS9OWRmjCoUCqjVaoesmC5dhCcr\n7Hv2UH9Vl9poNh6P1VqI35uiItowasoUw23cjukdnn2WfmaZDZOfL7/t/v30RGxrXYFubYhH7D1E\n7LGzVEeHPHYLEfvSpUuh1Wrx7bffyu777NmzCA4OlsymiI2NRVNTk32TlGwRdlcVKUlUnZaXlyM6\nOppaNCNG6CN2gIrnhg0bEBoaiqVLl6LF3uESGg3CmpvRatJyV4rVq1fjySefRHZ2Nl3fuO8+KpAW\nxNBSWwFbInap3/UQQoV9xgz63kAiYg8KAoYPNxZ2tuAsjtijooCrr6Z2jCs7eQK0oKY3is76moIC\n+rpWraJrIIDlBWrWUPD55237G8ilOroYtxV2cYdHJuw99thDQ2k+uElOOBP26dOnIzIy0qLPLpXD\nzuhRLjvLHbcm7K7y2EXvLSHEeFD0yJFGwg7QaPrDDz9Efn4+/vCHP9j3fJWV8AFA2AKxBeLj4/HU\nU0/pxRPXXEO/VBZsIDlh7+zsRFNTk2PCnpdH3y9RrrLkeDzTnjEHDlBvPTnZeH9Ll1LhcbUd8+KL\nwKOPuv6E0ts88wzg709b6arV9IrTUsTOkiaOHQO++876/gsK6FXWyJHOOd4e4rbC7uvri6CgICNh\ndyhiB8zsGGbFhISEYM6cOfj666+h0Wgk9y2V6sjoUS57dTX9gFjKvIiNpdGp6YnKmWg09CQjEvb6\n+nq0t7cbhH3ECGrFmIjAjBkz8Oijj+K9997Dd7Z8KXS06aKeAT3xKQWBiuGBA7JtIuSEnVlt1qwY\nqd/1sHm0bNIVZIQ9LY3mO7POmAcP0mwYhclXMjOT3ubKFgOtrfQq7dw52mTNUzh5klYq//73hlTE\n5GTrwj5tGrU9X3jB+nMUFFDvXhdE9hVuK+yAoUiJNQBzyGMHzISgqakJCoUC/v7+mDdvHmpra/HL\nL7+Y7ZcQgrNnz5qlOjJYxG7XAmp1NRV1mXxq3Y6h27Ht+7WXixepuIveW5bqGB0dTW8YOZJ62iwt\nUsQTTzyBwYMH4/XXX7f5KasPHQIAhIrm1drFrFk0lfD77yXvlmvda6nqlCGO0iU7O+7ZQ0cyxsXp\nbzKae8pIS6Pv66lTdKLS8ePGNgxj0CBg+nTX2jHi1hCffuqa53QFzzxDi4MefthwW1IS/RtIvbca\nDb1SmjQJ+NOfgB9+AH7+2fJzFBb2+cIp4ObCzvrFVFVVwdfXV5/KJodSqYRKpbJ52EZzczOCgoIg\nCAJmz54NhUIhacc0NDSgsbFRNmKPjo6GIAj2R+yWbBjANbnsFnLYjawYwMyOAeiV1d13342dO3fq\nM4es0aizKIZcdlnPjvmyy4DgYNlLZ7mI3R5hV6vV5kVMXV30yy+K1tm2gIQVA1A75tAheiISL5yK\nuflmKhhsYa63YQvhgwZRYfcEO+bECXpyfOABOgmMkZxMT6wSQQmKi6k9m5ZG59WGhQH/+If8cxDS\nL1IdAQ8QdmbFREVFSfrbplgctmHyZW9qakJwcLD+uaZOnSqZ9mgp1RHoYZGSPcIus9/W1lYUONpI\nSmdzSfWJMbJiANm0sXvuuQeCINg0IAMAOouLUQdgOOuZYi8qFY1yRa18xcgNtLZH2CX99Z9/Blpa\nzISdfYaMhH30aHqcx48bFk7lTmTs9mPHZI/LqbC/4wMP0JOJJ6RbbthA3++HHjK+PSmJ/pRaQGX+\n+tixNFBYtQr4/HN566aykqY7epKwC4LgIwjCEUEQvnLWPq0hFnZrC6cMf39/uyJ29qUEgLlz5+LI\nkSN6YWNYSnVk2J3LXl0tX3XKYMIqs99XX30VEyZMQIcjE44sROz693zYMGoZSUTsAF1juOGGG/Du\nu+/adCw+5eWoUCqN3nu7mTWLWgoSJ5vQ0FD9pCQxbFiLLR67pLDv2UM9/unTjW728/ODn5+fcVaU\nSkWjxRMnqL+elGT4HJqSnExrCFw1een0abqw+Nvf0tfjCXZMXh59H03/bkzYpcQ6N5eub6Sk0P8/\n8AAwYADw0kvSz9FPMmIA50bsfwDg0sYWYo/dmr/OsGfuaVNTE4KCgvT/nzdvHgDg66+/NtqOtRuw\nJuxOj9gHDKCLQDLCnpeXh9bWVv0aRI+Q6BNTXl6OyMhI+Pn50RuUSjr+S0bYAWDlypW4ePEitm3b\nZvUpA+vqcIllufQUlpUiEbUzy860J/vhw4cRGhpq3BXy3/+mkZoO5qtLCvvu3dSPlRBos34xgGHo\nxoED8jYMQP/Oo0cbIsjepqSEXoVFRQFXXAF89plrnrc3ycszCLSY6GgajUtF7MePU5EeMID+f+BA\n2g5840ZahGQKE3ZP8dgFQYgBMA/Au87Yn62wiN0eYbdn7qnYigGAtLQ0xMTEYNeuXdBoNNi+fTuu\nuOIKPP3000hNTbXo8dvVVqCzk9pC1oSd7lhW2GsKC3EVDBF2j6ispGl4olV+o1RHBsuMkWHGjBkY\nNWqUobeLBSJbW9Fu7WrFGqNH0wVMC8Ju6rNnZ2djypQpUIgzU7ZsoXNedemnrMOjWVTf2EitGJmW\nrJLCnpZG/3YXL0ovnJpu68qIndlrixbRE4qFk3a/p7mZ9khnhWFiBIFG7XIRO+ufz3joIboeIjXh\nqrCQfk8k2kW7GmdF7K8A+DMA6XljvUR4eDg0Gg0uXrzoWMSuUtHVcol0R3HELggC5s6di2+++QaJ\niYnIzMxEZWUlXn31VRw4cMCixx8TE4PGxkbbJinV1NCftgq7zAljfl4e9gKocmQAhsSsU33VqRiW\nyy6z0KZQKHDfffchOzsbR48elX26pooKhBECxbBhPT9mgH5hZ82i5f0mlouUsDc0NCAvLw9Tp041\n3g97TV8ZHMYlS5Zg9uzZxtv9+CPNojDx1xmyETvDUsQOUIEpKaEi1ZtoNNTCYsJ+0030pzvbMSwa\nl4rYAemxkVzrAAAgAElEQVSUx6Ym+j6YZmYlJNB02rffBkyTAQoKaEBhmrLaBzh8BIIgzAdQTQix\nmPAqCMI9giAcEgThUA0TLgcRXw7b47GbReyAZPVpc2Mj7iwpMYqUbr75ZrS2tiIyMhKffPIJCgsL\n8cADDxidAKSwK5fdlqpTw44lI/bm5makt7ZCBeCSI4tfMu0E9KmOjJEjaT9x0VQrU1asWAF/f3+s\nXbtWdpuy7GwAQADzPh1h1ix6TLr0SYZU696DuslFRsLe1mZ4b1l7YADvvPMO7jSd0LVnD43WZAQ6\nRNyTncGiweBgedFhMIGxNgjbUcrKaHbP8OH0/8OGUXvJne0Y9vmXitgBGrGXl9OrLgZ7n6VSblev\npmsey5cbBw39JCMGcE7EfgWAGwRBOAtgM4BrBUH40HQjQsg6QsgkQsikgU7qUyy+HHYoYgckG4EN\nb2jAjXl5tBmTrip11qxZqKqqwoEDB7B48WLZvt2m2JXLbkvVqWHHVLx0VbKMM4WFmKT7vd2Ry2iT\ndgJdXV2orq6WtmIAi3ZMWFgYsrKy8OGHH8q2V6jRFcRESMystZsZM2jkbmLHSEXsBw4cgEKhwOTJ\nkw0bsu6LUVF0H5YKwfbsod0YmR9rgmTEHhdHRX3yZKMGa5Kwk0Bv2zHs7yeeQLZoEfDLL64dnu5M\nTp4EfH0NJytTWLWvOINMnBFjSnw8tef27wfYYJmODhrhe4qwE0IeIYTEEELiAdwC4HtCyHKHj8wG\nxBG7Qx47INkIbFpjI/WW8vPpHFUdgwcPtim1UoxdbQXsjdgBsy9d7b59YK440U0jshtCzIS9qqoK\nhBBpKwaw6sWuXLkSra2t2LBhg+T9zSdPAgCirXnOthAZCUyYICvs4pTH7OxsjBs3zvjKi72W+++n\n0Tvr2mjK2bNUPGbMkD0UtVptfjITBOCtt4CnnrL+WoYNoz1mensBlZ3MxMKemUl/umvUnpdHo3K5\nIEwq5TE3l2YGiQrNjFi2DLj1Vlr0dOAAPSFqtf1i4RTwgDx2hsMRu4kVo9FoMKe7G2WxsXSm5po1\nhkHDPWDo0KEICQnB+vXrrc8FdYKwa9i0ewAqlotuL01NVNAkUh3NrJiEBCpUVoR94sSJmDx5Mt56\n6y3Jfu2aM2fQDSBAYmZtj5g1i37xRFc0phG7RqPBwYMHpf11gKb9qdXyQ4jfeING3LfcInsYkhE7\nQC/nr7zS+utQKFyzgHr6NF1zEvfpGT2arge4q7CfPGnZ6hoxgoq+2GdnC6eWArg336Tv0623Guw+\nT4nYxRBCfiCEzHfmPi3h9IhdJOytRUWYCKA0LY3mrcbEACtWyLaDtYZKpcIbb7yB//3vf1i9erXl\njauraVtRXUZOW1sbzp49q5/vaoRMkVJwXh6qBAGNfn4ItNBu2CIWRuKZRewDBtD3yFpvawD33nsv\n8vPzJdszqKqqcNHPz7o1YSuzZlEf9Icf9DcFBQVBoVDohf3EiRNobm7GFFN/vLiY5j0PHgxcfz3w\n5ZfmU4+amuis1cWL5aM7GITdkeEjemHvzUrQ06ep1WD6/i9aBPz0k8EmdICuri5cccUVZmnDvUJz\nM72ikvPXAXoiGznSELETQt9nay0tQkJop8hz52iOO+CZwu5qmLCHhYUZcqqtYNFjF1kx3brFsqpJ\nk2i09t571IMTWTL2snz5ctx222145pln8L///U9+Q5bDLgg4d+4ckpOTkZCQgMDAQKSnp+OWW27B\nM888Q/PTo6NpVGESsQ8tK8NJtRpNoaEIb2mRPilYw5Z2AmIkujxKkZmZCV9fX2zatMnsPnVDAxrl\nCnV6whVX0EVNkR2jUCgQEhKiF/Zs3YKtZMTOLKYbbqB/F9OT0fvv00W3P/3J4mGo1Wp0dXU5Viw2\ndixt+mZSIOdUTp+W9qIzM6ngyV212PUUp5GdnW11KplTYFG4tcVpccrj+fN03cqWXkVTpwKPP063\nj4qiWtEPcGthZx0ebY3WASsRe2MjTfcCoPrmG5wB0M3OwDNnAvfeSy0ZB4Y1v/nmm0hISMCyZctk\np/iwqtOqqirMnDkTDQ0N+Ne//oWVK1di6NChyMnJwZNPPol//vOfdFFo8GBjYa+txdDWVlTExqJj\n4EBEE4KLFy/af7Aywq5SqaSrM036sssRGhqKuXPnYsuWLUbdMi9duoQhXV3osjHDySb8/GhPcwmf\nXSzsUVFR5gVmRUW0oRdAI3YfH6PsGGg0tG/5lVfSBVALSDYCsxdXLKCy4iSp546PN0r77CmFhYUA\n4Hi7C1uwlhHDSE42jMlj6ximOexyPPYYrTY2DQz6ELcWdoBG7fYIe0BAADo6OswjWHHr3rY2+Gdn\n4ysAQeKy9hdfpNbHihVmvdttJTg4GJs2bUJlZSXuvvtu6Uvz6mp0hoVh9uzZqKiowK5du/DHP/4R\na9aswc6dO3H69GlMnDjRkA9ukvKo0UWgzampIDExiEUPi5Qk+sSwVEeFVK7uyJE0B98G8crKykJV\nVRX++9//6m8rys9HDAClTJfMHjNrFo3GRHaVqbBPnTrVeEG8o4NeYrOIPSyMtm8VR6yff04v861E\n64BMIzB7YULTWwuodXX08y8l7IIAzJtHF5AtffZfeokepwW7iAl7vqV2uc5ClxGT39WF1atXQzbV\nOinJMCaPnTjFdQaWUCpp4NCPJl25vbAvWLAAc+fOtXl71rpXcu4pQH32ffvg09GBrwDjfiVqNU1z\nKiy0XrBx663AypWSd2VkZODvf/87Pv30U7z7rnmxrvbCBXx75AgKCwuxY8cOc4sAwLhx43Ds2DF6\nYjApUmreuxcaAKopU+A7fDjCAVTZ4H2bUVlJI15RRa1k1SmDiaANzzV//nwEBgYa2TFlhw5BBSDI\nWnRlLxLtBVjr3gsXLqCkpMT8PT5zhoqTeGDCwoVUKNhVyb/+RReNFy60eghm4/F6QlgYXcdwIGJv\nb2/H9u3bpQMKqVRHMXPn0pRP0XqFEYTQFgwnTli8cmPCfv78efuna9lLXh6QmIg3//1v/O1vf8OI\nESPw/PPPm3//xT1jcnPp1YmFsYxm+Pj0i8IkRv85kh7yxhtv4GFxf2Ur2DRs46uv0D1gAH4EzAuP\n5syhqWcbN8o/SWkpbej/7ru0XFyChx56CLNmzcL999+POXPm4Pe//z1eeeUVfPnFF+gqL0d+bS22\nbt2KGTIpdOPGjUNNTQ312VnErvuyavbvRy6AuORkBOo+sI09KVJixUmiSFayOInBBMEGOyYgIAAL\nFy7Etm3b0NnZCQCo012BRE6YYP+xWiI1lfb5EK1rsA6PB3S2mmxGjFjYFyygP7/8krYP2L8f+OMf\nbVrodUrEDjicGfPCCy8gMzNTevAJE3a5fO/p0+l6hZw3fuSIYR8W7Eom7Ka/9wq6jJj8/HyMGjUK\n06dPx6OPPorExERs2LDBcOUuTnnMzdX76zU1NVi0aBEuOGHR2JW4vbDbi9VhG3V1wFdfoTI1FR2A\neYdBhYKmqO3ebbAqTPnPf6jIdnVRgZdAoVBg48aNuOWWW3Dx4kVs3LgRf/rTn7B84UL4EYKZy5Zh\nARMSCdLT0wEAx44do1FcczNdwNFqEZiXh4MAhg8fDrXucrK1J18gmVmnshG7HcIOUDumvr5eLzKt\nuktzP+ZrOwtBoB6r6NKfWTHZ2dnw9fXFBNOTiZSwDx9OTxJffEGj9ZAQ4I47bDoEpwr7yZP0s2Un\nzc3NeO211wAAH35oVkNoyGGXE3Z/f+Daa4GdO6Wtlq1b6UkuMNCqsGdkZADoZTumpYVeeY0Zg/z8\nfFx22WXYsWMHfvjhBwwePBi333477rvvProtG5N37BhNktDZXt999x0+++wzyXbd/RmvFXZZK+bH\nH4Hz51GiW0WXbBWwfDlNe5MSba0WWL+efgHGjwc++ED2WAYPHoz169fj0KFDaGhoQE1NDX785BMA\nwPjrrrP4OsbqIopjx44Z57IXFMCvrQ05goC4uDgodT1XelSkZCLsTU1NaG5ulhd2Vhr/+ec27X72\n7NkICwvD5s2b6TGWltI7bJh1ajcmjZ7Ewj5p0iTzrKqiIircpovECxfStL9t2+jwBRtbCztN2MeO\npaLegxP1unXrUF9fj4yMDGzfvt3cBjl9ml6hBQbK72TePHoCMF34JIQK+4wZdBFRt85jSlNTEyor\nKzF37lwIgtC7C6i69MW24cNRVlaGJF1UfvXVV+Pnn3/GTTfdhK/Ei8HJyfRqRKPRR+zHdD3wc9hQ\nazfB64SdWTGyEbtuMvtJXYaEZE/wpCTaP0PKjvnpJxol3HEH/XfkiE0DEgRBQGRkJNKZzWGlOCk0\nNBTx8fF0AVUs7LoiqnPR0VCpVPqe7cqetO6tqpIciScr7ADwu9/RlEAbvgi+vr64+eab8fnnn6O1\ntRX+1dVo8fXtnZSxxER6NaazxkJDQ9HS0oKcnBzJNQwUF9OMGNMClRtu0GdO4f77bX56R4Rdq9Xi\nNLM47MmMaWqiIvv99+jo6MCaNWtw9dVX4+WXX0ZLSwt2mKYuirs6yqFrXY2dO0EIMSzKMxtm8WLa\nL+fECbM2F4DBehk7diwSEhJ6N2LXVTGX6No8JIn6DykUClx55ZWoqKjQ9+FHUpLhmHXCzlpyHzLp\nN9Tf8Tphl43YmbCXlgKTJ+OC7gsdKBe9LF9OP8ym3vUHH9AoLjOTlh2rVDSCtxU7qk7ZAqpRkdLB\ng2j08YHA0jT9/FDv54cAC825JOnooEJoS9WpmNtvp6Xvb75p09NkZWWhpaUFGzZswKCODrRYGt7t\nCCYDFVj1aWdnp7ywS02anzSJLpguW2axIMkUR9IdV61ahVGjRuHIkSOG0nhbMmO2baOWyJo1+PDD\nD1FRUYFHHnkEV155JeLi4sztGFuEPS6O2lE7d2Lv3r2IjY2l4sxsmJtuoicTrdY85x8GYR89ejSS\nkpLsFvbGxkbbF1zz8gCVCrm67ZNMGssZ2Zl0A/rTz0//t2f3HTt2zLEaBBfjdcIuu3jq709zwgFg\n/nw0NzcjICAAPnILY1lZ9IMs/nI0N9Mv09KlQEAAvYy/4Qa6jW6B0Cp2CnthYSFaQ0Ko96+L2A8p\nFEgQfUGb1GqE2Nvu1ZaReFKo1cBvfgNs3iy7cCxm2rRpGDJkCF544QXEAdD2Vi9rGWEHYF5x2tlJ\n0xilhF2hAH79lVab2oGfnx98fX3tFvb3338fb7/9NgghVIh9felrsSVi111Rkm+/xXurV2P8+PH6\n2b233norvvvuO8OiYHs77XAo56+LmTcP+OknnNi/H4QQHD50yGDDREQYRvlJ+OyFhYUQBAEjRoxA\nYmIiCgsLbS6ea2trw+TJk7F8uY2tqE6eBBITcaq4GAqFAiNN/p7jdKMX9WnDrBnYmDGAUonq6mpU\nVVVhypQp6OrqwnFX9cN3Al4n7LKLp4Jg8NnnzzcbsmHGoEHAdddR64Z9MLdupQs24gW1O+6gAmdr\nlR2zTGzogJmeng6tVosT+flUgE+dAjlxAv/t6sIIkbC3DxyIId3daLZH3C1UnVqM2AGa5tnRQat1\nreDj44MlS5agtLQUcQB8pcTUGcTF0bYHOmFnrXsTEhLM6yDOnqV/U7ljCQ2lUZ2dSDYCs8ChQ4ew\ncuVKzJgxA/Pnz8fmzZtpQVdamvWI/fx5mpaYlQVBo8GUM2fwyCOP6HP1ly9fDo1Ggy1bttDtz56l\nPrktPXrmzQO6u+H3008AgIZ9+ww2DEDfn5QUSZ+9sLAQcXFx8Pf3R1JSEtra2mweGfnss8+ioKBA\nshWFJLqpSQUFBUhISDBbR4mIiEBsbCy9EgIMJ38TG+auu+4C4F4+u9cJu2zEDlA7ZuhQID3dbMiG\nJMuX0y8QK7L54APaMEkcAV53HfWpLSyiGrFvH/3isqsHC7CIQ2/HfPMNBK1WnxHD0A4dan+REovY\nTTz2kJAQeXuKMWYMTY1bu9bgR1sgKysLQQDCAaidncPOUCjo30a3WMcidlkbBpAX9h4i2whMgpqa\nGmRmZmLw4MHYvHkzbrvtNlRUVOCnn36iwnPuHM2CkuOjjwBCQJ59FscDAnCPry8y2dAMACkpKUhP\nT8dHujUlqznsYqZMAcLCEK/rWR71008GG0a8zcGDZr11CgsLMVrXAZFZI7bYMUePHsWLL76I0NBQ\nVFRUoM5a/6OWFn2PmPz8fDMbhjF+/HhDxB4dTRu56Zq5MWGfP38+Bg4cyIW9PyMbsQM0uv7b3wBB\nsB6xAzRDIjiYWi3FxXThdMUK4wU3pRK47TaaImYtF/biRZprbUPBC0BnrAYHBxuEXecl/gJjYVcm\nJCAEQJU9mRQSEbvF4iRTVq2i6xU2pIlNnjwZU3UWjNIWK6CniDJjWEsEi8Lu5LRLW4W9u7sbWVlZ\nqK6uxmeffYbIyEjMnz8fQUFB+Pjjjw0LqHJDNwihNswVV+C74mKsbW1FYmcnfEyshOXLl+OXX36h\nvrc9wq5UAtddh0k1NRAATDp71mDDMKZOpcV+os8cIcRI2BN160DWMmO6u7tx1113ISIiAm+88QYA\nOs/XIvn5ACHQJiejsLBQ/1ympKeno6CggOqBINBMN11G2rFjxxAVFYVBgwZh0qRJbrWA6nXCbjFi\n/8tfaFYHzOedyuyMdr3bupWOylIoqL9syh130MiVRUdy7NxJIxwbhV2hUGDcuHE04tAJY93AgaiH\nsbAH6j7Udk1SqqykH3SR128xh92UhQvpMem+iJYQBAF/173v9ixI2k1iIk3V6+hAUlISNm7ciDuk\n8tCLi+kJ20kDYRi2Cvtjjz2GvXv34u2338bEiRMB0M/tjTfeSAu6mEiJ7BhCCG699VakpaXh/iuu\nAE6exFdhYXj00Ufx3yFDQHx9aX2FiKysLAiCQKP206fporeNr7l9xgwM1GqxytcXw7q60HXjjcYb\nsKtWkR1TXV2NxsZGvcgOGjQIoaGhViP2V199FYcPH8brr7+OadOmAbBB2HUZMZVhYWhvb5eN2PV2\npsRJMjc3V39VnJGRgby8vN6vlHUSXifsFiN2ETZZMQC1YxobacHK7Nn69EIjkpPpgtIHH1huubpj\nB3287stsC+PGjUNubq5+0bEwLAxhun+MUF2E12ZvxD5okNFwAotVp6YolfQkuXu3ec6zBBOZoPSm\nsCcl0RPn6dMQBAHLly/Xfx6MYBkxdg5TsYYtwl5YWIgXX3wR9957L1asWGF0H2sc9+3JkzTHXhSB\nb926FR9//DFCQ0Mx9fRpdAC47auv8Ouvv+J3jz4KYcEC4OOPjQqboqOjMWPGDHz44YcgJSV04dTG\n11w4fDi0AJ4nBN0Aikz7qiQmUq9dtIAqzogB6Ak9KSnJYsReUlKCxx9/HAsWLMDixYsRExMDtVot\nKcRG6DJiTuj62lgSdgBmc3i7urpw8uRJvbBPmjQJWq3W4rze/oTXCrtkxC7CpogdAK65hoqxVktt\nGDlWrKCXzr/+Kn1/Wxvw7bc0i8YOQUlPT0dTUxOqdZ58jo+PUbQOAP66L1K36fBdS5jksGs0GlRW\nVtoesQPAXXfRdM+33rK+7blz1Kd1ZmdHU0wyY2QpKnK6vw7QlMdaK2mnn+mGWfztb38zu2/mzJmI\njIzEx5s2GS2gtra24uGHH0Z6ejp+2LMHWQD8Fi3ChY4OlJeXY9WqVTQNtaYGMOmBvnz5cpSUlKDt\nxAnbbBgd+Rcv4iCAoK4u7AVwwrROQqGgUbsFYQeoHSMXsRNCcO+990KpVOKtt96CIAgQBAFjxoyx\nLuwnTwKjR+OUzlaTE/b4+Hio1WozwS4oKEBnZ6e+EJBVyrqLz+51wq5SqaBUKp0Xsfv40ArEoUMt\nWyi33EJTKuWsib17aYMlG20YBosofvXzAzIy8Glbm5mwIzoaWgBKe/p4m1Sd1tTUQKPR2CfsgwfT\nebHr10sWqxhRUECtGxtnyPYIJiiWhL2rSz7V0UEmT56Mc+fO4aTOJpBi+/btyMjI0I9SFKNSqbBk\nyRLs2LEDnRkZtFfN2rV48cUXcf78ebz22mvw2buXpszedht8fX0RHR1NM2HmzKE2i4kdc9NNN8Hf\nzw+q8+fRZcfVUmFhIdjqyaeCIP2apkyhAqvrollYWAhfX1/EiZ4nKSkJFRUVklcymzZtwp49e/CP\nf/zD6P1ITU3FiRMnLA8tycsDxoxBQUEBwsLCEBkZKbmZIAhIT083E3aWv86+X1FRUYiJieHC3p+R\nHbYhwuaIHaCN9k+flh1kDIBelt5zD11oZaXzYnbsoL7uNdfY9pw6UlNToVAo8HNZGTQHDiC7vNxc\n2FUq1Pv5wd+eIiUTYbep6lSKBx6gVtWtt9IUSCnWraP5/3Z06ewRQUH05GHJGjp3jrZvdXa/GgBL\nliyBQqGQHDAC0Pf4l19+wU3i7BITsrKy0NbWhk/T0oD584GVK9G+ejWWLl2Kq666ii6aRkTQ/vFi\nVCr6N/jyS0D0OVCr1bjjuuug0mhw/6uvIiQkBElJSZg+fTq2bdsmexxFRUX4JioKuPNO5CQkyAs7\nIbRhGqiwjxw50qg2hPntUs3A3nrrLSQnJ+Pee+81un3MmDGora01VIya0tpKq791zb+SkpIszihO\nT09Hbm6u0WyA3Nxc+Pr6Gi26utMCqlcKu+ywDR2EENsjdoBedtqS1/zww9RmefFF49u1WvqFu/56\nu/Oj/f39kZiYiKNHj6K8vBxdJjnsjEvBwQixtThGo6EZPD3JYTdl8mRqxXz5Jc1zNhX3LVuoFz93\nLvDqq/btuyeY9Iwxo5dSHQEa9U2fPh2bNm2SjDY/1/XYsSTsU6dOpVWj27YBn36KAzExeKG7G28P\nHUrTHz//nBbISaXL3n47vSLR9eZhrL7zTgDANXfcgdtvvx2pqanIy8vDSy+9JHscRUVFCElOBt57\nD8PS0qSFffJk+t3Q2THijBiGXMpjaWkp9u/fj+XLl5v1/k/V+fmydszhw/SEYiXVkZGeno6WlhZD\n2wbQiD0lJYW25dCRkZGBwsJCfR///oxXCru1iL2trQ1ardb2iN1W2NzU994zpBMCNKK5cMFuG4bB\nWguwD6ZZxA6gLTISgzo7rQ/SBmhEp9HY3ydGjvvukxb3r782DHPeupVGlb1NYqI+FU6SXhR2gEbc\np0+floz8PvvsMyQlJVkUIoVCgaysLHz33Xf49MsvcVVZGY6NHYvQf/6T2i3t7TS9Vor0dJoDv369\nUSV0qC4n/JbHHsNrr72Gbdu24fbbb8fRo0f1LZVNKSwsxCjdVU2yLqWwy7TjpFpN2w8cOACNRoPi\n4mIzYR8xYgR8fHzMFlBZY7isrCyz52bCLpkZc+4cbfcwaBAax49HVVWVbKojQ2oBNTc3V++vMyZN\nmgQA+FVunawf4ZXCbi1ib9L5wU4XdoCmVHZ10RF7jB07qLfcQyti3LhxKC0t1X/gpIRdEx2NWABV\ntjQDM8lh7+7uxjvvvIOYmBgMHjy4R8doJu5799JU0bQ0epsuDbXXSUqi1pBcTUFRET0WO6Zy2UNm\nZiZUKpWZHVNbW4sff/zRYrTOWLZsGbq7u7Fs2TIMjYvDqP37qc138CC1kFhJvxQrVgCHDhmGpcfH\nA48+SteKRN53RkYGOjs7Jcvo6+vrUVtbqxf2lJQUdHV1GUW8enSFSufOnkVnZ6eZsPv6+mL48OFm\nEfvHH3+MKVOmIEFimtagQYMQERFhHrHX1NDMtMZG4Ntvka+znKxF7CkpKVAqlXphZ3MOmL/OYMLu\nDj671wq7pYidld7bbMXYw4gRtM/M228bvM4dO+hcTlH/EntgEcf27duhVCoRK9H2VhkfjyAAVbY0\nXWIl27rWxW+99RaOHDmCf/3rX/K9c2xBLO4zZ+qrZe2aVOMo1jJjeinVkREWFobrr78eW7ZsMeqR\n8tVXX0Gj0SAzM9PqPtLS0pCSkoLOzk68/PLLCAgKop+nV1+lzdcsHfvvfkfXNJ59lmYtTZsGTJgA\nPPig0RWTJRErKioCYMhuSdF9TmR99sZGVOzZY/QYMabNwE6cOIHc3FzJaB2gC55sAVVPUxMNjEpL\n6VzW9HT9Pq0Ju5+fH1JSUvTCzipOTSP28PBwjBgxwi2EHYQQl/+bOHEi6Uuuuuoqcs0118jef+TI\nEQKAfPbZZ71zACdOEAIQ8vjjhBQU0N9fe63Hu6uoqCAAiCAIZMSIEZLbnPnHPwgByHdr1ljf4eLF\nhERHE6LVkoqKChIcHEyuu+46otVqe3yMRqxbR8gVVxBSWuqc/dnDuXP0/V67Vvr+xERCMjN79RA2\nbdpEAJAffvhBf9vChQtJbGysze/x1q1byf333++8v4kJWq2WREREkDvvvNPsvo0bNxIA5OTJk4QQ\nQpqbmwkA8uyzz5rvqLCQEIDsWbqUACBVVVVmm/zf//0f8fPzI93d3YQQQh599FGiUCgkt2WsXLmS\nqNVq+vrb2wm59lpCfHwI+fJL/TaPPvooUSqVpLOz0+rr/c1vfkOGDBlCCCFkzZo1BACprq42227p\n0qVk2LBhVvfXWwA4RGzQWB6xS9CrETtAe6ncdBPw+uuGnu433NDj3bGyZ0KIpA0DACFskpK1YiGN\nhtoks2cDgoAHH3wQnZ2deOONNyxmFtjF3XfT1gm9WYwkx9ChdJCEVMSu0dDK1F7IiBGzYMECBAQE\n6O2YlpYWfPvtt7jxxhttfo9vvvlmvPbaa877m5ggCIJsFkhRUREUCoX+sxYYGIj4+HjpiH3kSCAy\nEoHHjkGtVmOQRNfSxMREdHR0oLS0FIQQbNq0CTNnzrRo+6WmpqKxsRFl58/TNYXvv6cFgPPn67fJ\nz8/HiBEjjBZA5UhPT0dlZSUuXLiA3NxcREVFYaBEFW5GRgZKS0vlh2L3E7xS2AMCAvrOY2c89hjN\n733+ebqopZt01BMEQdD7gXLCzqpPNdaKlI4coX3YZ83Cnj17sHnzZjzyyCNmLU/dFoWCLqBKneDO\nnW7YhTEAABgOSURBVKPrH738WgMDA3HDDTdg27Zt6Orqwrfffov29nab/HVXwsroTb8rhYWFGDZs\nmFG3xJSUFGlhFwRg7FgEV1Zi9OjRkicicWbMzz//jDNnzmDZsmUWj40toJbu3EkX3p96ymzR2JaM\nGIa4N/uxY8fM/HUGs6j6e9qjVwq7tYjdJcI+cSJtNqTR9DgbRow1YReGDIEGgMJah0fd/NGOq67C\nqlWrMHLkSPzlL39x+Pj6FSwzxpRezogRk5WVhdraWuzevRvbt29HREQEzUPvR2RkZECj0Rja2uoo\nKirSL5wyUnQ54xqpbp7x8Yhsbpb01wHjZmAff/wx/Pz8rJ7kxui6gPp8+ik9WbPZpTq6u7tRXFxs\ns7Cz709OTo5RKwFTJkyYAEEQ+r3P7pXCbi3dsdetGMbTT9PsC12bUEdgEYdUDjsAQKnERV9fDLA2\n/GL3bmDcOLz0n/+gsLAQb775JgZYKrxyR5KS6CKb6Wdg3z7600p6nDO47rrrEBoaig0bNuDLL7/E\nggULoOzNqtseILWASghBUVGRmUinpKSgo6MDZyWuCLuiozFYo0GKzGczMjISERERyMvLw5YtWzB/\n/nz9KEE5wsPDMSQqCvE5ObSoz8TiOavLwrFV2MPDwxEXF4fNmzcbtRIwJTg4GMnJyVzY+yN9mu4o\n5rLLaGqhjR8+S8yZMwfLli3DNRYqVy8FB0NtqUippQXYvx/NU6di9erVWLJkCWbPnu3wsfU7kpJo\nHrsuuwMAnR70yiu0uKc3+9Xo8PPzw6JFi7BlyxZcunSp39kwAC1Gi46ONhIx1qFRKmIHpDNjLuj6\nM40TNaYzJSkpCVu2bEF1dbVVG4axID4eQxobadsKE1hGjLUcdjHp6en6TBu5iB2gJ7ycnBzLLQ36\nGK8U9n4TsTuRiIgIfPTRR/o+41K0RkRgYHu7/Afyxx+Bri4Uxcejvb3drJTbY2BfdrEd87e/UVvs\n+edddhgsnS8wMBCzZs1y2fPaQ0ZGhpGfzFIdTYWdRcZSwn5aZ88kWqiqTkxMRHNzM9RqNebaWM+x\nSKuFBoBWwspkBU/2CjsAs1YCUttduHDB+rCPPsQrhd3f3x/t7e2ysxabmprg6+sLXxumGLkTmiFD\nEEMI6uU+kLt3A35+KNcVhYS4Mr/clYwaRRf1mLAfPUqbYz3wAB1U7SKuueYaxMTEYMGCBdLtg/sB\npmX0Uh0aAfpZGTp0qKSw5+l6mMdamKbFTgyZmZm2WX+EYHJpKfYBOCPRIz0/Px+DBg1CeHi49X3p\nYMJu2krAFHZSKxJf8fUzvFLY2bCNdl2vZlOamprcKlq3FcWwYfAHUCXXXXD3buCqq3BJV0be61ZU\nXxEQQLOQCgqoJfPQQ3Qs4mOPufQwfHx88Msvv2DdunUufV57YO1qDx8+DICKmVKpxDCJLC65zJhf\nq6rQDWCAhQliTFRvk2uHYEpuLkIvXMBWSLcWyM/PtytaFx+DnL/OYBlixWyxvR/ilcJubdhGc3Oz\nR4oa68teLzUIubyctjqdPdt1awx9CcuM2bWL5kA/+WSPK38dYciQIf36fWYTnJjPXlRUhBEjRkgu\n9KakpODUqVNGV8IdHR3Y//PPqBkwgLZDlmHmzJk4ceIErr32WtsObOtWEIUCn0G6GVhBQYHNC6eM\n+Ph4LFq0CEuXLrW4XUJCAhQKBRf2/obF8Xiws2WvG8GKlFqkUv10Jd+YNcst1xjshnV5/L//o9YM\nG83HMSIiIgLDhw/XC7u4+ZcpKSkpaGlpwfnz5wHQDJqVK1ciPz8fyhEjLAo7G6BhE4QAn3wCYfp0\nBA4bZibstbW1qKmpsVvYBUHAtm3brHr8fn5+iIuL48Le37AlYvdEUYvQrfR3lZSY37l7Nx3EMHas\nPmIPDAx05eG5lsRE2rf71CnaRtnD1lOcCVtA1Wq1KC4utijsgGEB9fXXX8f777+Pxx9/HAMzMiwK\nu13k5tKMpiVL9C2GxWzUVXPba8XYw8iRI+322Ds6OvDSSy/JWsDOxGFhFwQhVhCEfYIgnBQEIU8Q\nhD8448B6E2+N2H1jY9EFQDCdpKTV0oh95kxAodCvMZj2wfYoWDR31VVOKRDzZDIyMnDu3DkcPXoU\nbW1tsoVGycnJAKiw7927Fw8++CBuvPFGPPXUU3RNo6LCqF1wj/nkE9qN8qabMEbXc72rqwutra24\n88478ac//QkzZ87EzJkzHX8uGUaOHGlXxF5YWIgpU6bgz3/+M3bu3Gn9AQ7ijG9uN4CHCCEpAC4H\nsEoQhBQn7LfXsBaxe+riKXx8UOPriwGmk2eOH6dtbHU56556xWJERgbt12OtGyJHv4D68ccfAzBP\ndWRERERg8ODB2LVrFxYvXoykpCRs2LCBBgjx8dRC0dk0PUZnw2D6dGDgQKSmpqKzsxNff/01pkyZ\ngvXr1+OJJ57AN998Y9TywNmMGjUKdXV1VlMeCSFYv349JkyYgNLSUuzYsQOLFi3qteNiOCzshJBK\nQsivut+bAJwC0INpDK7D2kBrT108BYDawEBMvnCBesusX8ru3fSnLpfaU69YjAgKAj77jPaD51hk\n/PjxEARB37RMLmIHqB3z/fffQxAEfPHFF4bPUXw8/emoHXPsGG39oCtKYr78woULUV5ejl27duHp\np592rL20DdiSGXPp0iXceuutuOOOO5CRkYHc3Fzc4ECzP3tw6rW2IAjxAMYD+NmZ+3U2zIqxFLF7\nqrB9NHEisv39aZVlUhLtA79+PZCcTDsfwrNfP8d+WBl9RUUFBgwYYHGK1tixY+Hj44OtW7ca9y1y\nlrCLbBiA2j+hoaG47LLL8Ouvv2LOnDmO7d9GrAk7IQTTpk3DJ598gueeew579uzp2fSxHuI0YRcE\nIQjApwD+SAgxq1sXBOEeQRAOCYJwqK9bXtoSsXuqFVEbH4871Gp6Sfz884Y0R9HwY09+/ZyeweyY\nkSNHWlx7eeKJJ5CTk2Oetjh0KG3WJTXI3R527aLDQSIjAdDvcklJCfbv3484F7aBHj58OARBkBX2\nM2fOIDc3Fy+//DIee+yxXr+CMMUpwi4IggpU1D8ihHwmtQ0hZB0hZBIhZJJUn2NXYili7+zsRGdn\np8dGrMHBwWhsbKTNx/76V6CwkI5Ke/pp/TY8YueYwoTdkg0D0GZa48ePN79DpaIzfx2J2FtbgRMn\ngCuuMLo5LCzM5cI5YMAAxMbGymbGsIKuK6+80pWHpccZWTECgPcAnCKE/NPxQ+p9LEXsLNXPUyNW\ntVqN5uZmQ3tVhYK2EBa9Xi7sHFOYsMstnNpEfLxjwn7kCO3nM3lyz/fhRCxlxhw+fBgqlQppfbSG\n44yI/QoAtwG4VhCEo7p/PZvK7CIsReysOMdThY21Q2WvUwqPzQri9Jj09HTMmzcPCxYs6PlOHBV2\n1mVSd5Lpa6wJe2pqaq9m5ljC4QbQhJD/AXCrfDFbInZPF/bGxkbZJl+enBXE6Rm+vr746quvHNvJ\nsGF0Taery2hwts3k5FA7JyrKseNwEqNGjcLFixfR0NCAUFE7CkIIDh8+7JK0Rjk8uAJFHpVKBaVS\nKSnsnl5OLxZ2KbRaLRd2Tu8QH0+L4crKevb4nJx+E60D8pkxZ8+eRX19vb7PTl/glcIOyA/b8KaI\nXYoWXQtUT339nD7EkZTH+nraRsANhJ0tnHJh7wPkhm14S8TOTmCmePrr5/Qhjgg7G/bRj4SdjaGU\nEnalUtlnC6eAFws7j9ilI3ZPf/2cPiQmpue57GzhVDeHtT/g7++PmJgYs5RHtnDal7OCvVbY5SJ2\nT093ZILNhZ3jcnx9gejonkXsOTm0vXIf9My3hGlmDFs47UsbBvBiYZeL2L0l3VFO2LkVw+lVepry\n2M8WThmmwl5aWoq6ujou7H2FpYhdoVD02xmUjsIjdk6f0hNhr6igaZL9pDBJzKhRo1BdXa3/PrGF\n00l9bBl5rbBbitiDgoIgeGgrV6VSiYCAAC7snL4hPp6mO3Z32/6YflaYJMY0M+bQoUN9vnAKeLGw\nW4rYPV3U1Go1t2I4fcOwYbQtQHm57Y/JyaEdHXXDpvsTpsLeHxZOAS8WdktZMZ4uamq1WjbdkUfs\nnF6lJymPOTlAaiqgawXSnxCnPPaXhVPAy4VdLo/d00XNUsTu6VlBnD7GXmEnhOaw90MbBqBzgaOj\no1FUVNRvFk4BLxb2gIAAyUZY3mDF6Fv3StDc3Ax/f3+Xt0HleAmxsXQUoa257CUlQF1dvxV2wJAZ\n0x8qThleK+yjR4/GpUuXcN5kBqM3DJmwFrF7+omN04f4+QFDhtgesffjhVOGWNiVSiXGjh3b14fk\nvcI+depUAMCBAweMbvcGYePCzulT7El5zMkBBgygHns/ZdSoUaiqqsKPP/6IMWPG9PnCKeDFwj52\n7FgEBAQgOzvb6HZvWTy1ZMV4+uvn9DH2CPsvv9BsmJ60+XURLDMmOzu7X9gwgBcLu0qlwuTJk82E\n3ZsWTwkhZvfxiJ3T68TH05m7bIqXHN3dwK+/9svCJDFM2IH+4a8DXizsALVjjhw5ok971Gg0aG1t\n9fiIVa1Wo7u7G+3t7Wb3cWHn9DrDhlHRrqiwvN2pU3TOaT/214H+KewOT1ByZ6ZOnYru7m4cOnQI\n06ZN85pe5OLWvaatE5qbm5GQkNAXh8XxFsQpjwMHAj/+CHz9NbVdgoKAyEggIgKorKTb9XNhDwoK\nQlRUFGpqavrFwing5cJ++eWXA6De2LRp07ymOEfcCGzQoEFG9/GIndPrMGH/3e+AM2eAtjaaLTN5\nMtDYSG+7eBFoaKDbOjJA20UkJSVhyJAh/abHlFcLe0REBJKSkvQ+u7cU51hqBMaFndPrDBtG2/d2\ndgJ33QVcfz1w9dXmlaWsn4yi/zvG77zzDjTW1gxciFcLO0DtmB07doAQ4vEtexlyrXvZe+DpJzZO\nH+PnRxuBWWu0p3QfeRL77P2B/n8q7GWmTp2K2tpaFBUVeU3ELifsbW1t0Gq1Hn9i4/QDPLR7an+B\nC7uuUCk7O9vrI3ZvWWPgcDwdrxf2xMREhIWFITs722uEjQs7h+PZeL2wKxQKTJkyxUjYvcWKMW3d\ny3uxcziegdcLO0DtmLy8PJSVlQHw/Ih1wIABUCqVPGLncDwULuww+Oy7d+8GQHssezKCIEi27uXC\nzuF4BlzYAWRkZMDHxwc5OTkICAjwil7kUo3AuBXD4XgGXNhBhWzcuHHQarVeI2pSws4jdg7HM+DC\nroPZMd4ialzYORzPhQu7Di7s3IrhcDwFLuw6mLB7i6ip1WqzdMempib4+flB1Y+HGnA4HOtwYdcR\nFxeH6OhofY63pyNnxXjLFQuH48m4T5edXkYQBHzwwQdeI+xS6Y68ARiH4xlwYRcxe/bsvj4El6FW\nq9HS0gKNRqNP7+QRO4fjGTjFihEEYY4gCAWCIBQLgvBXZ+yT07tItRXgws7heAYOC7sgCD4A3gRw\nPYAUAFmCIKQ4ul9O7yLVCIxbMRyOZ+CMiH0ygGJCSAkhpBPAZgALnbBfTi8iJew8YudwPANnCPtQ\nAOdF/y/T3cbpx3ArhsPxXFyW7igIwj2CIBwSBOFQTU2Nq56WIwO3Yjgcz8UZwl4OIFb0/xjdbUYQ\nQtYRQiYRQiYNHDjQCU/LcQRTYSeE8Iidw/EQnCHsOQBGCYKQIAiCL4BbAHzhhP1yehEm4EzYOzo6\n0N3dzYWdw/EAHM5jJ4R0C4LwewDfAvAB8D4hJM/hI+P0KqYRO+8Tw+F4Dk4pUCKE7AKwyxn74rgG\n04idd3bkcDwH3ivGS/Hx8UFgYCAXdg7HA+HC7sWIG4ExK4YLO4fj/nBh92LErXvZT+6xczjuDxd2\nL0YcsXMrhsPxHLiwezHi1r1c2Dkcz4ELuxcj5bFzK4bDcX+4sHsx3IrhcDwTLuxejKmwq1Qq+Pn5\n9fFRcTgcR+HC7sUwYSeE8AZgHI4HwYXdi1Gr1dBoNGhvb+cNwDgcD4ILuxcj7hfDhZ3D8Ry4sHsx\n4n4x3IrhcDwHLuxeDI/YORzPhAu7F8OFncPxTLiwezFiYedWDIfjOXBh92J4xM7heCZc2L0YJuxN\nTU1c2DkcD4ILuxfDhP3ixYvo7OzkVgyH4yFwYfdi/Pz8oFKpUFFRAYD3ieFwPAUu7F6MIAgIDg5G\neXk5AC7sHI6nwIXdy1Gr1fqInVsxHI5nwIXdy1Gr1Txi53A8DC7sXo5arUZ1dTUALuwcjqfAhd3L\nUavVIIQA4MLO4XgKXNi9HJbyCHCPncPxFLiwezliYecRO4fjGXBh93LEYs6FncPxDLiwezksYlco\nFBgwYEAfHw2Hw3EGXNi9HCbswcHBEAShj4+Gw+E4Ay7sXo5Y2DkcjmfAhd3LYcLOM2I4HM+BC7uX\nwyN2Dsfz4MLu5XBh53A8Dy7sXg63Yjgcz4MLu5fDInUesXM4noNDwi4IwkuCIOQLgpArCMJ2QRBC\nnXVgHNfArRgOx/NwNGLfDSCVEDIWQCGARxw/JI4rYRYMt2I4HM9B6ciDCSHfif57EMDNjh0Ox9X4\n+PhgzZo1mDlzZl8fCofDcRICa9nq8I4E4UsAWwghH8rcfw+AewAgLi5uYmlpqVOel8PhcLwFQRAO\nE0ImWdvOasQuCMIeAFESdz1GCNmh2+YxAN0APpLbDyFkHYB1ADBp0iTnnE04HA6HY4ZVYSeEWLxG\nFwRhBYD5AGYQZ4X/HA6Hw+kxDnnsgiDMAfBnAFcTQlqdc0gczv+3dz8hVpVhHMe/Pyz7Y5KaIkNK\nGkniIkcXpSRRRjFJtGpRtHDh0oVBG4cgaNmmEooi+reJiuyfuKjMXI+NOdboYBoNqGhjkAQtIutp\ncd6Jy9DMXGcGz3kOvw8c7nnfe2F+d+adZ+4895x7zGw2ZntUzCvAQuCApCFJr89BJjMzm4XZHhVz\nx1wFMTOzueEzT83MWsaF3cysZVzYzcxaZs5OULqiLypdBGZ6htJS4Nc5jHO1Zc6fOTvkzp85Ozj/\nXLktIpZN96BaCvtsSBrs5syrpsqcP3N2yJ0/c3Zw/qvNrRgzs5ZxYTcza5mMhf2NugPMUub8mbND\n7vyZs4PzX1XpeuxmZja1jK/YzcxsCqkKu6Q+SSclnZa0u+4805H0tqQxScMdc0skHZB0qtwurjPj\nZCStlHRI0glJxyXtKvONzy/pekmHJR0r2Z8v86slDZT186Gk+XVnnYqkeZKOStpfxinySxqV9EP5\n/KjBMtf4dTNO0iJJe8tlP0ckbc6UHxIVdknzgFeBR4B1wJOS1tWbalrvAn0T5nYDByNiDXCwjJvo\nMvBMRKwDNgE7y/c7Q/4/ga0RsR7oBfokbQJeAF4qn3H0G7Cjxozd2AWMdIwz5X8gIno7DhHMsG7G\n7QG+iIi1wHqqn0Gm/BARKTZgM/Blx7gf6K87Vxe5VwHDHeOTQE/Z7wFO1p2xy+fxOfBQtvzAjcB3\nwD1UJ5hc83/rqWkbsIKqgGwF9gPKkh8YBZZOmEuxboCbgZ8p7z9myz++pXnFDtwKnOkYny1z2SyP\niPNl/wKwvM4w3ZC0CtgADJAkf2ljDAFjVBdd/wm4FBGXy0Oavn5eprrWwT9lfAt58gfwlaQj5ZKY\nkGTdAKuBi8A7pQ32pqQF5MkPJGrFtFFUf/4bfViSpJuAj4GnI+L3zvuanD8i/o6IXqpXvncDa2uO\n1DVJjwJjEXGk7iwztCUiNlK1TXdKuq/zziavG6qPMt8IvBYRG4A/mNB2aXh+IFdhPwes7BivKHPZ\n/CKpB6DcjtWcZ1KSrqUq6u9FxCdlOk1+gIi4BByial0skjR+DYImr597gcckjQIfULVj9pAkf0Sc\nK7djwKdUf1izrJuzwNmIGCjjvVSFPkt+IFdh/xZYU44MmA88AeyrOdNM7AO2l/3tVL3rxpEk4C1g\nJCJe7Lir8fklLZO0qOzfQPXewAhVgX+8PKyR2QEioj8iVkTEKqp1/k1EPEWC/JIWSFo4vg88DAyT\nYN0ARMQF4IykO8vUg8AJkuT/T91N/it8Y2Mb8CNVv/TZuvN0kfd94DzwF9UrgR1UvdKDwCnga2BJ\n3Tknyb6F6t/N74Ghsm3LkB+4Czhasg8Dz5X524HDwGngI+C6urN28VzuB/ZnyV8yHivb8fHf0wzr\npuM59AKDZf18BizOlD8ifOapmVnbZGrFmJlZF1zYzcxaxoXdzKxlXNjNzFrGhd3MrGVc2M3MWsaF\n3cysZVzYzcxa5l8Csf3IZGhSbwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f36b586c1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3EFyRBG/beMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f36b0929b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 2.71459347876 \n",
      "Fixed scheme MAE:  1.91403500434\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 1.7188  Test loss = 3.6457  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 1.7765  Test loss = 2.5655  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 1.7694  Test loss = 0.1651  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 1.7642  Test loss = 0.5406  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 1.5735  Test loss = 0.4741  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 1.4663  Test loss = 1.0643  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 1.4269  Test loss = 1.4500  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 1.4352  Test loss = 2.2127  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 1.3964  Test loss = 2.4711  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 1.4250  Test loss = 0.6327  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 1.4003  Test loss = 0.8109  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 1.4015  Test loss = 0.2905  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 1.3472  Test loss = 1.2793  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 1.3564  Test loss = 0.9619  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 1.3560  Test loss = 2.3805  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 1.3860  Test loss = 4.4120  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 1.4184  Test loss = 2.6102  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 1.4548  Test loss = 0.0812  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 1.4535  Test loss = 0.2587  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 1.3483  Test loss = 0.9295  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 1.3003  Test loss = 1.7595  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 1.3123  Test loss = 3.4075  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 1.3759  Test loss = 0.4188  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 1.3732  Test loss = 0.9760  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 1.3464  Test loss = 0.1222  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 1.3449  Test loss = 0.5176  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 1.3451  Test loss = 0.5981  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 1.3340  Test loss = 1.2493  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 1.2765  Test loss = 0.3421  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 1.2716  Test loss = 0.1109  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 1.2654  Test loss = 3.8088  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 1.3039  Test loss = 1.3287  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 1.2885  Test loss = 0.1293  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 1.2862  Test loss = 1.1091  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 1.2472  Test loss = 1.4684  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 1.2599  Test loss = 4.2977  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 1.2823  Test loss = 1.4546  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 1.2370  Test loss = 1.2854  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 1.2469  Test loss = 0.3948  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 1.2430  Test loss = 2.4673  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 1.2524  Test loss = 1.9883  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 1.2763  Test loss = 2.0620  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 1.3017  Test loss = 3.9219  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 1.3887  Test loss = 11.8735  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 2.0133  Test loss = 5.2809  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.1171  Test loss = 0.5966  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.1183  Test loss = 0.1459  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.1182  Test loss = 0.0919  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 2.0011  Test loss = 2.4084  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 2.0201  Test loss = 3.3324  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 2.0599  Test loss = 1.2986  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 2.0651  Test loss = 1.1408  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 2.0180  Test loss = 0.6981  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 2.0194  Test loss = 1.7031  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 2.0304  Test loss = 0.2063  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 2.0301  Test loss = 0.0924  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 1.9971  Test loss = 0.9992  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 2.0006  Test loss = 0.4681  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 2.0013  Test loss = 1.1032  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 2.0050  Test loss = 0.4427  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.9864  Test loss = 1.5212  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.9940  Test loss = 2.8176  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 2.0239  Test loss = 0.5385  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 2.0219  Test loss = 0.9118  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 2.0048  Test loss = 0.0512  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 2.0044  Test loss = 0.3351  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.9929  Test loss = 0.0437  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 1.9879  Test loss = 2.9545  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 2.0080  Test loss = 3.7773  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 2.0583  Test loss = 0.4684  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 2.0523  Test loss = 0.8710  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 2.0542  Test loss = 2.4253  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 2.0530  Test loss = 1.8981  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 2.0663  Test loss = 0.3395  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 2.0611  Test loss = 0.2652  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 2.0576  Test loss = 1.2311  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 2.0240  Test loss = 1.5104  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f36a9b25c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.29426776192 \n",
      "Updating scheme MAE:  1.53632894288\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
