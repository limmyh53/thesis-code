{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"4Q/128_units/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 500\n",
    "learning_rate = 1e-4\n",
    "batch_size = 5\n",
    "early_stop_iters = 10\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 128 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag4',\n",
    "                                       'inflation.lag5',\n",
    "                                       'inflation.lag6',\n",
    "                                       'inflation.lag7']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag4',\n",
    "                                   'unemp.lag5',\n",
    "                                   'unemp.lag6',\n",
    "                                   'unemp.lag7']])\n",
    "train_4lag_oil = np.array(train[['oil.lag4',\n",
    "                                 'oil.lag5',\n",
    "                                 'oil.lag6',\n",
    "                                 'oil.lag7']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag4',\n",
    "                                     'inflation.lag5',\n",
    "                                     'inflation.lag6',\n",
    "                                     'inflation.lag7']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag4',\n",
    "                                 'unemp.lag5',\n",
    "                                 'unemp.lag6',\n",
    "                                 'unemp.lag7']])\n",
    "test_4lag_oil = np.array(test[['oil.lag4',\n",
    "                               'oil.lag5',\n",
    "                               'oil.lag6',\n",
    "                               'oil.lag7']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 128 \n",
      "Learning rate = 0.0001 \n",
      "Epochs = 500 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 10 \n",
      "Learning rate = 0.0001\n",
      "Fold: 1  Epoch: 1  Training loss = 3.3150  Validation loss = 3.4573  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 3.2847  Validation loss = 3.4081  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 3.2576  Validation loss = 3.3631  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 3.2343  Validation loss = 3.3230  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 3.2048  Validation loss = 3.2737  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 3.1813  Validation loss = 3.2325  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 3.1447  Validation loss = 3.1683  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 3.1201  Validation loss = 3.1244  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 3.1030  Validation loss = 3.0921  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 3.0843  Validation loss = 3.0570  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 3.0534  Validation loss = 3.0006  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 3.0302  Validation loss = 2.9569  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 3.0095  Validation loss = 2.9173  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 2.9754  Validation loss = 2.8504  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 2.9502  Validation loss = 2.8006  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 2.9321  Validation loss = 2.7618  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 2.9145  Validation loss = 2.7236  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 2.8948  Validation loss = 2.6806  \n",
      "\n",
      "Fold: 1  Epoch: 19  Training loss = 2.8805  Validation loss = 2.6495  \n",
      "\n",
      "Fold: 1  Epoch: 20  Training loss = 2.8677  Validation loss = 2.6208  \n",
      "\n",
      "Fold: 1  Epoch: 21  Training loss = 2.8540  Validation loss = 2.5895  \n",
      "\n",
      "Fold: 1  Epoch: 22  Training loss = 2.8393  Validation loss = 2.5542  \n",
      "\n",
      "Fold: 1  Epoch: 23  Training loss = 2.8285  Validation loss = 2.5266  \n",
      "\n",
      "Fold: 1  Epoch: 24  Training loss = 2.8200  Validation loss = 2.5059  \n",
      "\n",
      "Fold: 1  Epoch: 25  Training loss = 2.8089  Validation loss = 2.4767  \n",
      "\n",
      "Fold: 1  Epoch: 26  Training loss = 2.7917  Validation loss = 2.4332  \n",
      "\n",
      "Fold: 1  Epoch: 27  Training loss = 2.7754  Validation loss = 2.3918  \n",
      "\n",
      "Fold: 1  Epoch: 28  Training loss = 2.7647  Validation loss = 2.3629  \n",
      "\n",
      "Fold: 1  Epoch: 29  Training loss = 2.7605  Validation loss = 2.3521  \n",
      "\n",
      "Fold: 1  Epoch: 30  Training loss = 2.7491  Validation loss = 2.3212  \n",
      "\n",
      "Fold: 1  Epoch: 31  Training loss = 2.7378  Validation loss = 2.2875  \n",
      "\n",
      "Fold: 1  Epoch: 32  Training loss = 2.7352  Validation loss = 2.2795  \n",
      "\n",
      "Fold: 1  Epoch: 33  Training loss = 2.7289  Validation loss = 2.2601  \n",
      "\n",
      "Fold: 1  Epoch: 34  Training loss = 2.7242  Validation loss = 2.2463  \n",
      "\n",
      "Fold: 1  Epoch: 35  Training loss = 2.7100  Validation loss = 2.2029  \n",
      "\n",
      "Fold: 1  Epoch: 36  Training loss = 2.7046  Validation loss = 2.1855  \n",
      "\n",
      "Fold: 1  Epoch: 37  Training loss = 2.7027  Validation loss = 2.1787  \n",
      "\n",
      "Fold: 1  Epoch: 38  Training loss = 2.6897  Validation loss = 2.1360  \n",
      "\n",
      "Fold: 1  Epoch: 39  Training loss = 2.6858  Validation loss = 2.1207  \n",
      "\n",
      "Fold: 1  Epoch: 40  Training loss = 2.6824  Validation loss = 2.1104  \n",
      "\n",
      "Fold: 1  Epoch: 41  Training loss = 2.6786  Validation loss = 2.0994  \n",
      "\n",
      "Fold: 1  Epoch: 42  Training loss = 2.6707  Validation loss = 2.0725  \n",
      "\n",
      "Fold: 1  Epoch: 43  Training loss = 2.6662  Validation loss = 2.0570  \n",
      "\n",
      "Fold: 1  Epoch: 44  Training loss = 2.6654  Validation loss = 2.0560  \n",
      "\n",
      "Fold: 1  Epoch: 45  Training loss = 2.6602  Validation loss = 2.0369  \n",
      "\n",
      "Fold: 1  Epoch: 46  Training loss = 2.6600  Validation loss = 2.0355  \n",
      "\n",
      "Fold: 1  Epoch: 47  Training loss = 2.6562  Validation loss = 2.0265  \n",
      "\n",
      "Fold: 1  Epoch: 48  Training loss = 2.6543  Validation loss = 2.0208  \n",
      "\n",
      "Fold: 1  Epoch: 49  Training loss = 2.6537  Validation loss = 2.0193  \n",
      "\n",
      "Fold: 1  Epoch: 50  Training loss = 2.6474  Validation loss = 1.9960  \n",
      "\n",
      "Fold: 1  Epoch: 51  Training loss = 2.6436  Validation loss = 1.9801  \n",
      "\n",
      "Fold: 1  Epoch: 52  Training loss = 2.6416  Validation loss = 1.9703  \n",
      "\n",
      "Fold: 1  Epoch: 53  Training loss = 2.6400  Validation loss = 1.9637  \n",
      "\n",
      "Fold: 1  Epoch: 54  Training loss = 2.6359  Validation loss = 1.9477  \n",
      "\n",
      "Fold: 1  Epoch: 55  Training loss = 2.6333  Validation loss = 1.9364  \n",
      "\n",
      "Fold: 1  Epoch: 56  Training loss = 2.6282  Validation loss = 1.9137  \n",
      "\n",
      "Fold: 1  Epoch: 57  Training loss = 2.6281  Validation loss = 1.9155  \n",
      "\n",
      "Fold: 1  Epoch: 58  Training loss = 2.6254  Validation loss = 1.9029  \n",
      "\n",
      "Fold: 1  Epoch: 59  Training loss = 2.6230  Validation loss = 1.8938  \n",
      "\n",
      "Fold: 1  Epoch: 60  Training loss = 2.6216  Validation loss = 1.8855  \n",
      "\n",
      "Fold: 1  Epoch: 61  Training loss = 2.6192  Validation loss = 1.8736  \n",
      "\n",
      "Fold: 1  Epoch: 62  Training loss = 2.6162  Validation loss = 1.8584  \n",
      "\n",
      "Fold: 1  Epoch: 63  Training loss = 2.6133  Validation loss = 1.8432  \n",
      "\n",
      "Fold: 1  Epoch: 64  Training loss = 2.6126  Validation loss = 1.8412  \n",
      "\n",
      "Fold: 1  Epoch: 65  Training loss = 2.6079  Validation loss = 1.8179  \n",
      "\n",
      "Fold: 1  Epoch: 66  Training loss = 2.6055  Validation loss = 1.8083  \n",
      "\n",
      "Fold: 1  Epoch: 67  Training loss = 2.6037  Validation loss = 1.7980  \n",
      "\n",
      "Fold: 1  Epoch: 68  Training loss = 2.5996  Validation loss = 1.7791  \n",
      "\n",
      "Fold: 1  Epoch: 69  Training loss = 2.5956  Validation loss = 1.7591  \n",
      "\n",
      "Fold: 1  Epoch: 70  Training loss = 2.5914  Validation loss = 1.7360  \n",
      "\n",
      "Fold: 1  Epoch: 71  Training loss = 2.5903  Validation loss = 1.7306  \n",
      "\n",
      "Fold: 1  Epoch: 72  Training loss = 2.5890  Validation loss = 1.7261  \n",
      "\n",
      "Fold: 1  Epoch: 73  Training loss = 2.5868  Validation loss = 1.7160  \n",
      "\n",
      "Fold: 1  Epoch: 74  Training loss = 2.5827  Validation loss = 1.6900  \n",
      "\n",
      "Fold: 1  Epoch: 75  Training loss = 2.5813  Validation loss = 1.6799  \n",
      "\n",
      "Fold: 1  Epoch: 76  Training loss = 2.5774  Validation loss = 1.6515  \n",
      "\n",
      "Fold: 1  Epoch: 77  Training loss = 2.5760  Validation loss = 1.6396  \n",
      "\n",
      "Fold: 1  Epoch: 78  Training loss = 2.5756  Validation loss = 1.6422  \n",
      "\n",
      "Fold: 1  Epoch: 79  Training loss = 2.5738  Validation loss = 1.6320  \n",
      "\n",
      "Fold: 1  Epoch: 80  Training loss = 2.5744  Validation loss = 1.6475  \n",
      "\n",
      "Fold: 1  Epoch: 81  Training loss = 2.5731  Validation loss = 1.6389  \n",
      "\n",
      "Fold: 1  Epoch: 82  Training loss = 2.5726  Validation loss = 1.6414  \n",
      "\n",
      "Fold: 1  Epoch: 83  Training loss = 2.5729  Validation loss = 1.6518  \n",
      "\n",
      "Fold: 1  Epoch: 84  Training loss = 2.5688  Validation loss = 1.6220  \n",
      "\n",
      "Fold: 1  Epoch: 85  Training loss = 2.5685  Validation loss = 1.6288  \n",
      "\n",
      "Fold: 1  Epoch: 86  Training loss = 2.5681  Validation loss = 1.6314  \n",
      "\n",
      "Fold: 1  Epoch: 87  Training loss = 2.5662  Validation loss = 1.6213  \n",
      "\n",
      "Fold: 1  Epoch: 88  Training loss = 2.5662  Validation loss = 1.6279  \n",
      "\n",
      "Fold: 1  Epoch: 89  Training loss = 2.5650  Validation loss = 1.6225  \n",
      "\n",
      "Fold: 1  Epoch: 90  Training loss = 2.5640  Validation loss = 1.6112  \n",
      "\n",
      "Fold: 1  Epoch: 91  Training loss = 2.5633  Validation loss = 1.6143  \n",
      "\n",
      "Fold: 1  Epoch: 92  Training loss = 2.5629  Validation loss = 1.6146  \n",
      "\n",
      "Fold: 1  Epoch: 93  Training loss = 2.5620  Validation loss = 1.6101  \n",
      "\n",
      "Fold: 1  Epoch: 94  Training loss = 2.5622  Validation loss = 1.6238  \n",
      "\n",
      "Fold: 1  Epoch: 95  Training loss = 2.5598  Validation loss = 1.6109  \n",
      "\n",
      "Fold: 1  Epoch: 96  Training loss = 2.5587  Validation loss = 1.6077  \n",
      "\n",
      "Fold: 1  Epoch: 97  Training loss = 2.5570  Validation loss = 1.5949  \n",
      "\n",
      "Fold: 1  Epoch: 98  Training loss = 2.5565  Validation loss = 1.5927  \n",
      "\n",
      "Fold: 1  Epoch: 99  Training loss = 2.5560  Validation loss = 1.5942  \n",
      "\n",
      "Fold: 1  Epoch: 100  Training loss = 2.5558  Validation loss = 1.5980  \n",
      "\n",
      "Fold: 1  Epoch: 101  Training loss = 2.5548  Validation loss = 1.5997  \n",
      "\n",
      "Fold: 1  Epoch: 102  Training loss = 2.5525  Validation loss = 1.5846  \n",
      "\n",
      "Fold: 1  Epoch: 103  Training loss = 2.5520  Validation loss = 1.5876  \n",
      "\n",
      "Fold: 1  Epoch: 104  Training loss = 2.5516  Validation loss = 1.5928  \n",
      "\n",
      "Fold: 1  Epoch: 105  Training loss = 2.5505  Validation loss = 1.5840  \n",
      "\n",
      "Fold: 1  Epoch: 106  Training loss = 2.5504  Validation loss = 1.5906  \n",
      "\n",
      "Fold: 1  Epoch: 107  Training loss = 2.5498  Validation loss = 1.5913  \n",
      "\n",
      "Fold: 1  Epoch: 108  Training loss = 2.5480  Validation loss = 1.5804  \n",
      "\n",
      "Fold: 1  Epoch: 109  Training loss = 2.5467  Validation loss = 1.5670  \n",
      "\n",
      "Fold: 1  Epoch: 110  Training loss = 2.5464  Validation loss = 1.5699  \n",
      "\n",
      "Fold: 1  Epoch: 111  Training loss = 2.5455  Validation loss = 1.5650  \n",
      "\n",
      "Fold: 1  Epoch: 112  Training loss = 2.5444  Validation loss = 1.5630  \n",
      "\n",
      "Fold: 1  Epoch: 113  Training loss = 2.5437  Validation loss = 1.5649  \n",
      "\n",
      "Fold: 1  Epoch: 114  Training loss = 2.5427  Validation loss = 1.5541  \n",
      "\n",
      "Fold: 1  Epoch: 115  Training loss = 2.5419  Validation loss = 1.5543  \n",
      "\n",
      "Fold: 1  Epoch: 116  Training loss = 2.5419  Validation loss = 1.5649  \n",
      "\n",
      "Fold: 1  Epoch: 117  Training loss = 2.5415  Validation loss = 1.5679  \n",
      "\n",
      "Fold: 1  Epoch: 118  Training loss = 2.5404  Validation loss = 1.5616  \n",
      "\n",
      "Fold: 1  Epoch: 119  Training loss = 2.5395  Validation loss = 1.5553  \n",
      "\n",
      "Fold: 1  Epoch: 120  Training loss = 2.5396  Validation loss = 1.5565  \n",
      "\n",
      "Fold: 1  Epoch: 121  Training loss = 2.5400  Validation loss = 1.5716  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 114  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.4520  Validation loss = 2.0184  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.4511  Validation loss = 2.0130  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.4496  Validation loss = 2.0024  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.4493  Validation loss = 2.0012  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.4491  Validation loss = 2.0054  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.4493  Validation loss = 2.0070  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.4499  Validation loss = 2.0130  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.4499  Validation loss = 2.0160  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.4496  Validation loss = 2.0156  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.4505  Validation loss = 2.0239  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.4509  Validation loss = 2.0265  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 4  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.4909  Validation loss = 3.2850  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.4904  Validation loss = 3.2889  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.4898  Validation loss = 3.2949  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.4897  Validation loss = 3.2892  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.4887  Validation loss = 3.2971  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.4875  Validation loss = 3.3182  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.4872  Validation loss = 3.3193  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.4867  Validation loss = 3.3282  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.4862  Validation loss = 3.3255  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.4857  Validation loss = 3.3222  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.4852  Validation loss = 3.3223  \n",
      "\n",
      "Fold: 3  Epoch: 12  Training loss = 1.4849  Validation loss = 3.3208  \n",
      "\n",
      "Fold: 3  Epoch: 13  Training loss = 1.4844  Validation loss = 3.3244  \n",
      "\n",
      "Fold: 3  Epoch: 14  Training loss = 1.4844  Validation loss = 3.3203  \n",
      "\n",
      "Fold: 3  Epoch: 15  Training loss = 1.4840  Validation loss = 3.3210  \n",
      "\n",
      "Fold: 3  Epoch: 16  Training loss = 1.4834  Validation loss = 3.3265  \n",
      "\n",
      "Fold: 3  Epoch: 17  Training loss = 1.4830  Validation loss = 3.3217  \n",
      "\n",
      "Fold: 3  Epoch: 18  Training loss = 1.4825  Validation loss = 3.3322  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 1  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.5994  Validation loss = 4.5292  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.5983  Validation loss = 4.5179  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.5968  Validation loss = 4.4964  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.5967  Validation loss = 4.4972  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.5957  Validation loss = 4.4854  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.5949  Validation loss = 4.4765  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.5944  Validation loss = 4.4790  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.5933  Validation loss = 4.4722  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.5928  Validation loss = 4.4680  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 1.5922  Validation loss = 4.4668  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.5911  Validation loss = 4.4486  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 1.5907  Validation loss = 4.4469  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.5905  Validation loss = 4.4428  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 1.5898  Validation loss = 4.4281  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.5892  Validation loss = 4.4209  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 1.5886  Validation loss = 4.4266  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 1.5877  Validation loss = 4.4123  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 1.5874  Validation loss = 4.4157  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 1.5867  Validation loss = 4.4123  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 1.5861  Validation loss = 4.4138  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 1.5856  Validation loss = 4.4085  \n",
      "\n",
      "Fold: 4  Epoch: 22  Training loss = 1.5850  Validation loss = 4.4076  \n",
      "\n",
      "Fold: 4  Epoch: 23  Training loss = 1.5842  Validation loss = 4.4072  \n",
      "\n",
      "Fold: 4  Epoch: 24  Training loss = 1.5835  Validation loss = 4.3975  \n",
      "\n",
      "Fold: 4  Epoch: 25  Training loss = 1.5832  Validation loss = 4.3981  \n",
      "\n",
      "Fold: 4  Epoch: 26  Training loss = 1.5824  Validation loss = 4.3851  \n",
      "\n",
      "Fold: 4  Epoch: 27  Training loss = 1.5814  Validation loss = 4.3685  \n",
      "\n",
      "Fold: 4  Epoch: 28  Training loss = 1.5810  Validation loss = 4.3684  \n",
      "\n",
      "Fold: 4  Epoch: 29  Training loss = 1.5806  Validation loss = 4.3778  \n",
      "\n",
      "Fold: 4  Epoch: 30  Training loss = 1.5801  Validation loss = 4.3834  \n",
      "\n",
      "Fold: 4  Epoch: 31  Training loss = 1.5795  Validation loss = 4.3821  \n",
      "\n",
      "Fold: 4  Epoch: 32  Training loss = 1.5789  Validation loss = 4.3702  \n",
      "\n",
      "Fold: 4  Epoch: 33  Training loss = 1.5784  Validation loss = 4.3603  \n",
      "\n",
      "Fold: 4  Epoch: 34  Training loss = 1.5778  Validation loss = 4.3590  \n",
      "\n",
      "Fold: 4  Epoch: 35  Training loss = 1.5770  Validation loss = 4.3507  \n",
      "\n",
      "Fold: 4  Epoch: 36  Training loss = 1.5765  Validation loss = 4.3660  \n",
      "\n",
      "Fold: 4  Epoch: 37  Training loss = 1.5755  Validation loss = 4.3545  \n",
      "\n",
      "Fold: 4  Epoch: 38  Training loss = 1.5754  Validation loss = 4.3522  \n",
      "\n",
      "Fold: 4  Epoch: 39  Training loss = 1.5749  Validation loss = 4.3564  \n",
      "\n",
      "Fold: 4  Epoch: 40  Training loss = 1.5742  Validation loss = 4.3479  \n",
      "\n",
      "Fold: 4  Epoch: 41  Training loss = 1.5736  Validation loss = 4.3352  \n",
      "\n",
      "Fold: 4  Epoch: 42  Training loss = 1.5727  Validation loss = 4.3248  \n",
      "\n",
      "Fold: 4  Epoch: 43  Training loss = 1.5724  Validation loss = 4.3311  \n",
      "\n",
      "Fold: 4  Epoch: 44  Training loss = 1.5721  Validation loss = 4.3248  \n",
      "\n",
      "Fold: 4  Epoch: 45  Training loss = 1.5715  Validation loss = 4.3257  \n",
      "\n",
      "Fold: 4  Epoch: 46  Training loss = 1.5708  Validation loss = 4.3109  \n",
      "\n",
      "Fold: 4  Epoch: 47  Training loss = 1.5704  Validation loss = 4.3067  \n",
      "\n",
      "Fold: 4  Epoch: 48  Training loss = 1.5700  Validation loss = 4.3032  \n",
      "\n",
      "Fold: 4  Epoch: 49  Training loss = 1.5695  Validation loss = 4.3044  \n",
      "\n",
      "Fold: 4  Epoch: 50  Training loss = 1.5689  Validation loss = 4.3020  \n",
      "\n",
      "Fold: 4  Epoch: 51  Training loss = 1.5685  Validation loss = 4.2963  \n",
      "\n",
      "Fold: 4  Epoch: 52  Training loss = 1.5681  Validation loss = 4.2844  \n",
      "\n",
      "Fold: 4  Epoch: 53  Training loss = 1.5675  Validation loss = 4.2767  \n",
      "\n",
      "Fold: 4  Epoch: 54  Training loss = 1.5670  Validation loss = 4.2794  \n",
      "\n",
      "Fold: 4  Epoch: 55  Training loss = 1.5665  Validation loss = 4.2773  \n",
      "\n",
      "Fold: 4  Epoch: 56  Training loss = 1.5662  Validation loss = 4.2651  \n",
      "\n",
      "Fold: 4  Epoch: 57  Training loss = 1.5657  Validation loss = 4.2613  \n",
      "\n",
      "Fold: 4  Epoch: 58  Training loss = 1.5650  Validation loss = 4.2667  \n",
      "\n",
      "Fold: 4  Epoch: 59  Training loss = 1.5648  Validation loss = 4.2730  \n",
      "\n",
      "Fold: 4  Epoch: 60  Training loss = 1.5644  Validation loss = 4.2688  \n",
      "\n",
      "Fold: 4  Epoch: 61  Training loss = 1.5639  Validation loss = 4.2661  \n",
      "\n",
      "Fold: 4  Epoch: 62  Training loss = 1.5635  Validation loss = 4.2604  \n",
      "\n",
      "Fold: 4  Epoch: 63  Training loss = 1.5631  Validation loss = 4.2564  \n",
      "\n",
      "Fold: 4  Epoch: 64  Training loss = 1.5631  Validation loss = 4.2620  \n",
      "\n",
      "Fold: 4  Epoch: 65  Training loss = 1.5630  Validation loss = 4.2722  \n",
      "\n",
      "Fold: 4  Epoch: 66  Training loss = 1.5625  Validation loss = 4.2692  \n",
      "\n",
      "Fold: 4  Epoch: 67  Training loss = 1.5620  Validation loss = 4.2751  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 63  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.8447  Validation loss = 4.5076  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.8416  Validation loss = 4.4906  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.8395  Validation loss = 4.4780  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.8346  Validation loss = 4.4496  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.8319  Validation loss = 4.4341  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 1.8302  Validation loss = 4.4271  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 1.8274  Validation loss = 4.4089  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.8230  Validation loss = 4.3802  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 1.8206  Validation loss = 4.3627  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.8182  Validation loss = 4.3462  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 1.8166  Validation loss = 4.3395  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 1.8154  Validation loss = 4.3330  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 1.8142  Validation loss = 4.3297  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.8126  Validation loss = 4.3239  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 1.8101  Validation loss = 4.3021  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 1.8069  Validation loss = 4.2772  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 1.8061  Validation loss = 4.2824  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 1.8042  Validation loss = 4.2687  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 1.8034  Validation loss = 4.2716  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 1.8023  Validation loss = 4.2642  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 1.8016  Validation loss = 4.2660  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 1.8012  Validation loss = 4.2731  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 1.8002  Validation loss = 4.2696  \n",
      "\n",
      "Fold: 5  Epoch: 24  Training loss = 1.7992  Validation loss = 4.2669  \n",
      "\n",
      "Fold: 5  Epoch: 25  Training loss = 1.7974  Validation loss = 4.2546  \n",
      "\n",
      "Fold: 5  Epoch: 26  Training loss = 1.7961  Validation loss = 4.2485  \n",
      "\n",
      "Fold: 5  Epoch: 27  Training loss = 1.7939  Validation loss = 4.2383  \n",
      "\n",
      "Fold: 5  Epoch: 28  Training loss = 1.7927  Validation loss = 4.2348  \n",
      "\n",
      "Fold: 5  Epoch: 29  Training loss = 1.7921  Validation loss = 4.2346  \n",
      "\n",
      "Fold: 5  Epoch: 30  Training loss = 1.7908  Validation loss = 4.2289  \n",
      "\n",
      "Fold: 5  Epoch: 31  Training loss = 1.7897  Validation loss = 4.2243  \n",
      "\n",
      "Fold: 5  Epoch: 32  Training loss = 1.7881  Validation loss = 4.2124  \n",
      "\n",
      "Fold: 5  Epoch: 33  Training loss = 1.7876  Validation loss = 4.2212  \n",
      "\n",
      "Fold: 5  Epoch: 34  Training loss = 1.7858  Validation loss = 4.2065  \n",
      "\n",
      "Fold: 5  Epoch: 35  Training loss = 1.7849  Validation loss = 4.2131  \n",
      "\n",
      "Fold: 5  Epoch: 36  Training loss = 1.7839  Validation loss = 4.2097  \n",
      "\n",
      "Fold: 5  Epoch: 37  Training loss = 1.7820  Validation loss = 4.1976  \n",
      "\n",
      "Fold: 5  Epoch: 38  Training loss = 1.7799  Validation loss = 4.1808  \n",
      "\n",
      "Fold: 5  Epoch: 39  Training loss = 1.7780  Validation loss = 4.1654  \n",
      "\n",
      "Fold: 5  Epoch: 40  Training loss = 1.7763  Validation loss = 4.1570  \n",
      "\n",
      "Fold: 5  Epoch: 41  Training loss = 1.7741  Validation loss = 4.1388  \n",
      "\n",
      "Fold: 5  Epoch: 42  Training loss = 1.7732  Validation loss = 4.1450  \n",
      "\n",
      "Fold: 5  Epoch: 43  Training loss = 1.7705  Validation loss = 4.1224  \n",
      "\n",
      "Fold: 5  Epoch: 44  Training loss = 1.7695  Validation loss = 4.1210  \n",
      "\n",
      "Fold: 5  Epoch: 45  Training loss = 1.7688  Validation loss = 4.1191  \n",
      "\n",
      "Fold: 5  Epoch: 46  Training loss = 1.7674  Validation loss = 4.1157  \n",
      "\n",
      "Fold: 5  Epoch: 47  Training loss = 1.7656  Validation loss = 4.1024  \n",
      "\n",
      "Fold: 5  Epoch: 48  Training loss = 1.7644  Validation loss = 4.0947  \n",
      "\n",
      "Fold: 5  Epoch: 49  Training loss = 1.7629  Validation loss = 4.0855  \n",
      "\n",
      "Fold: 5  Epoch: 50  Training loss = 1.7611  Validation loss = 4.0686  \n",
      "\n",
      "Fold: 5  Epoch: 51  Training loss = 1.7601  Validation loss = 4.0649  \n",
      "\n",
      "Fold: 5  Epoch: 52  Training loss = 1.7579  Validation loss = 4.0458  \n",
      "\n",
      "Fold: 5  Epoch: 53  Training loss = 1.7563  Validation loss = 4.0289  \n",
      "\n",
      "Fold: 5  Epoch: 54  Training loss = 1.7544  Validation loss = 4.0116  \n",
      "\n",
      "Fold: 5  Epoch: 55  Training loss = 1.7532  Validation loss = 4.0053  \n",
      "\n",
      "Fold: 5  Epoch: 56  Training loss = 1.7521  Validation loss = 4.0152  \n",
      "\n",
      "Fold: 5  Epoch: 57  Training loss = 1.7508  Validation loss = 4.0030  \n",
      "\n",
      "Fold: 5  Epoch: 58  Training loss = 1.7500  Validation loss = 4.0089  \n",
      "\n",
      "Fold: 5  Epoch: 59  Training loss = 1.7489  Validation loss = 4.0059  \n",
      "\n",
      "Fold: 5  Epoch: 60  Training loss = 1.7471  Validation loss = 3.9977  \n",
      "\n",
      "Fold: 5  Epoch: 61  Training loss = 1.7457  Validation loss = 3.9818  \n",
      "\n",
      "Fold: 5  Epoch: 62  Training loss = 1.7450  Validation loss = 3.9974  \n",
      "\n",
      "Fold: 5  Epoch: 63  Training loss = 1.7438  Validation loss = 3.9923  \n",
      "\n",
      "Fold: 5  Epoch: 64  Training loss = 1.7425  Validation loss = 3.9921  \n",
      "\n",
      "Fold: 5  Epoch: 65  Training loss = 1.7406  Validation loss = 3.9748  \n",
      "\n",
      "Fold: 5  Epoch: 66  Training loss = 1.7394  Validation loss = 3.9730  \n",
      "\n",
      "Fold: 5  Epoch: 67  Training loss = 1.7384  Validation loss = 3.9695  \n",
      "\n",
      "Fold: 5  Epoch: 68  Training loss = 1.7377  Validation loss = 3.9700  \n",
      "\n",
      "Fold: 5  Epoch: 69  Training loss = 1.7365  Validation loss = 3.9692  \n",
      "\n",
      "Fold: 5  Epoch: 70  Training loss = 1.7355  Validation loss = 3.9577  \n",
      "\n",
      "Fold: 5  Epoch: 71  Training loss = 1.7345  Validation loss = 3.9516  \n",
      "\n",
      "Fold: 5  Epoch: 72  Training loss = 1.7337  Validation loss = 3.9613  \n",
      "\n",
      "Fold: 5  Epoch: 73  Training loss = 1.7324  Validation loss = 3.9545  \n",
      "\n",
      "Fold: 5  Epoch: 74  Training loss = 1.7308  Validation loss = 3.9361  \n",
      "\n",
      "Fold: 5  Epoch: 75  Training loss = 1.7300  Validation loss = 3.9357  \n",
      "\n",
      "Fold: 5  Epoch: 76  Training loss = 1.7292  Validation loss = 3.9418  \n",
      "\n",
      "Fold: 5  Epoch: 77  Training loss = 1.7273  Validation loss = 3.9282  \n",
      "\n",
      "Fold: 5  Epoch: 78  Training loss = 1.7263  Validation loss = 3.9341  \n",
      "\n",
      "Fold: 5  Epoch: 79  Training loss = 1.7254  Validation loss = 3.9347  \n",
      "\n",
      "Fold: 5  Epoch: 80  Training loss = 1.7238  Validation loss = 3.9195  \n",
      "\n",
      "Fold: 5  Epoch: 81  Training loss = 1.7232  Validation loss = 3.9176  \n",
      "\n",
      "Fold: 5  Epoch: 82  Training loss = 1.7221  Validation loss = 3.9147  \n",
      "\n",
      "Fold: 5  Epoch: 83  Training loss = 1.7212  Validation loss = 3.9107  \n",
      "\n",
      "Fold: 5  Epoch: 84  Training loss = 1.7199  Validation loss = 3.9046  \n",
      "\n",
      "Fold: 5  Epoch: 85  Training loss = 1.7193  Validation loss = 3.9141  \n",
      "\n",
      "Fold: 5  Epoch: 86  Training loss = 1.7175  Validation loss = 3.8990  \n",
      "\n",
      "Fold: 5  Epoch: 87  Training loss = 1.7166  Validation loss = 3.8927  \n",
      "\n",
      "Fold: 5  Epoch: 88  Training loss = 1.7160  Validation loss = 3.8966  \n",
      "\n",
      "Fold: 5  Epoch: 89  Training loss = 1.7152  Validation loss = 3.9020  \n",
      "\n",
      "Fold: 5  Epoch: 90  Training loss = 1.7137  Validation loss = 3.8824  \n",
      "\n",
      "Fold: 5  Epoch: 91  Training loss = 1.7126  Validation loss = 3.8638  \n",
      "\n",
      "Fold: 5  Epoch: 92  Training loss = 1.7121  Validation loss = 3.8584  \n",
      "\n",
      "Fold: 5  Epoch: 93  Training loss = 1.7114  Validation loss = 3.8403  \n",
      "\n",
      "Fold: 5  Epoch: 94  Training loss = 1.7102  Validation loss = 3.8541  \n",
      "\n",
      "Fold: 5  Epoch: 95  Training loss = 1.7090  Validation loss = 3.8447  \n",
      "\n",
      "Fold: 5  Epoch: 96  Training loss = 1.7078  Validation loss = 3.8338  \n",
      "\n",
      "Fold: 5  Epoch: 97  Training loss = 1.7069  Validation loss = 3.8320  \n",
      "\n",
      "Fold: 5  Epoch: 98  Training loss = 1.7060  Validation loss = 3.8306  \n",
      "\n",
      "Fold: 5  Epoch: 99  Training loss = 1.7053  Validation loss = 3.8401  \n",
      "\n",
      "Fold: 5  Epoch: 100  Training loss = 1.7040  Validation loss = 3.8362  \n",
      "\n",
      "Fold: 5  Epoch: 101  Training loss = 1.7032  Validation loss = 3.8438  \n",
      "\n",
      "Fold: 5  Epoch: 102  Training loss = 1.7020  Validation loss = 3.8261  \n",
      "\n",
      "Fold: 5  Epoch: 103  Training loss = 1.7006  Validation loss = 3.8212  \n",
      "\n",
      "Fold: 5  Epoch: 104  Training loss = 1.6994  Validation loss = 3.8159  \n",
      "\n",
      "Fold: 5  Epoch: 105  Training loss = 1.6987  Validation loss = 3.8052  \n",
      "\n",
      "Fold: 5  Epoch: 106  Training loss = 1.6978  Validation loss = 3.7977  \n",
      "\n",
      "Fold: 5  Epoch: 107  Training loss = 1.6962  Validation loss = 3.7826  \n",
      "\n",
      "Fold: 5  Epoch: 108  Training loss = 1.6950  Validation loss = 3.7717  \n",
      "\n",
      "Fold: 5  Epoch: 109  Training loss = 1.6941  Validation loss = 3.7687  \n",
      "\n",
      "Fold: 5  Epoch: 110  Training loss = 1.6934  Validation loss = 3.7652  \n",
      "\n",
      "Fold: 5  Epoch: 111  Training loss = 1.6930  Validation loss = 3.7615  \n",
      "\n",
      "Fold: 5  Epoch: 112  Training loss = 1.6922  Validation loss = 3.7537  \n",
      "\n",
      "Fold: 5  Epoch: 113  Training loss = 1.6911  Validation loss = 3.7572  \n",
      "\n",
      "Fold: 5  Epoch: 114  Training loss = 1.6905  Validation loss = 3.7602  \n",
      "\n",
      "Fold: 5  Epoch: 115  Training loss = 1.6892  Validation loss = 3.7511  \n",
      "\n",
      "Fold: 5  Epoch: 116  Training loss = 1.6886  Validation loss = 3.7429  \n",
      "\n",
      "Fold: 5  Epoch: 117  Training loss = 1.6873  Validation loss = 3.7361  \n",
      "\n",
      "Fold: 5  Epoch: 118  Training loss = 1.6859  Validation loss = 3.7205  \n",
      "\n",
      "Fold: 5  Epoch: 119  Training loss = 1.6850  Validation loss = 3.7219  \n",
      "\n",
      "Fold: 5  Epoch: 120  Training loss = 1.6842  Validation loss = 3.7332  \n",
      "\n",
      "Fold: 5  Epoch: 121  Training loss = 1.6832  Validation loss = 3.7302  \n",
      "\n",
      "Fold: 5  Epoch: 122  Training loss = 1.6820  Validation loss = 3.7368  \n",
      "\n",
      "Fold: 5  Epoch: 123  Training loss = 1.6814  Validation loss = 3.7235  \n",
      "\n",
      "Fold: 5  Epoch: 124  Training loss = 1.6806  Validation loss = 3.7207  \n",
      "\n",
      "Fold: 5  Epoch: 125  Training loss = 1.6796  Validation loss = 3.7082  \n",
      "\n",
      "Fold: 5  Epoch: 126  Training loss = 1.6789  Validation loss = 3.6978  \n",
      "\n",
      "Fold: 5  Epoch: 127  Training loss = 1.6779  Validation loss = 3.7016  \n",
      "\n",
      "Fold: 5  Epoch: 128  Training loss = 1.6772  Validation loss = 3.7004  \n",
      "\n",
      "Fold: 5  Epoch: 129  Training loss = 1.6763  Validation loss = 3.6992  \n",
      "\n",
      "Fold: 5  Epoch: 130  Training loss = 1.6757  Validation loss = 3.7038  \n",
      "\n",
      "Fold: 5  Epoch: 131  Training loss = 1.6747  Validation loss = 3.6904  \n",
      "\n",
      "Fold: 5  Epoch: 132  Training loss = 1.6740  Validation loss = 3.6903  \n",
      "\n",
      "Fold: 5  Epoch: 133  Training loss = 1.6730  Validation loss = 3.6999  \n",
      "\n",
      "Fold: 5  Epoch: 134  Training loss = 1.6721  Validation loss = 3.6946  \n",
      "\n",
      "Fold: 5  Epoch: 135  Training loss = 1.6712  Validation loss = 3.6778  \n",
      "\n",
      "Fold: 5  Epoch: 136  Training loss = 1.6699  Validation loss = 3.6762  \n",
      "\n",
      "Fold: 5  Epoch: 137  Training loss = 1.6688  Validation loss = 3.6736  \n",
      "\n",
      "Fold: 5  Epoch: 138  Training loss = 1.6673  Validation loss = 3.6745  \n",
      "\n",
      "Fold: 5  Epoch: 139  Training loss = 1.6660  Validation loss = 3.6624  \n",
      "\n",
      "Fold: 5  Epoch: 140  Training loss = 1.6650  Validation loss = 3.6614  \n",
      "\n",
      "Fold: 5  Epoch: 141  Training loss = 1.6640  Validation loss = 3.6624  \n",
      "\n",
      "Fold: 5  Epoch: 142  Training loss = 1.6628  Validation loss = 3.6642  \n",
      "\n",
      "Fold: 5  Epoch: 143  Training loss = 1.6622  Validation loss = 3.6678  \n",
      "\n",
      "Fold: 5  Epoch: 144  Training loss = 1.6615  Validation loss = 3.6848  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 140  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 1.8551  Validation loss = 1.3627  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 1.8479  Validation loss = 1.3305  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 1.8427  Validation loss = 1.3091  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 1.8419  Validation loss = 1.3099  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 1.8398  Validation loss = 1.3029  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 1.8345  Validation loss = 1.2773  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 1.8316  Validation loss = 1.2643  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 1.8283  Validation loss = 1.2475  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 1.8254  Validation loss = 1.2355  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 1.8241  Validation loss = 1.2317  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 1.8200  Validation loss = 1.2103  \n",
      "\n",
      "Fold: 6  Epoch: 12  Training loss = 1.8128  Validation loss = 1.1717  \n",
      "\n",
      "Fold: 6  Epoch: 13  Training loss = 1.8097  Validation loss = 1.1618  \n",
      "\n",
      "Fold: 6  Epoch: 14  Training loss = 1.8075  Validation loss = 1.1565  \n",
      "\n",
      "Fold: 6  Epoch: 15  Training loss = 1.8073  Validation loss = 1.1609  \n",
      "\n",
      "Fold: 6  Epoch: 16  Training loss = 1.8044  Validation loss = 1.1491  \n",
      "\n",
      "Fold: 6  Epoch: 17  Training loss = 1.8001  Validation loss = 1.1262  \n",
      "\n",
      "Fold: 6  Epoch: 18  Training loss = 1.7975  Validation loss = 1.1172  \n",
      "\n",
      "Fold: 6  Epoch: 19  Training loss = 1.7959  Validation loss = 1.1157  \n",
      "\n",
      "Fold: 6  Epoch: 20  Training loss = 1.7909  Validation loss = 1.0861  \n",
      "\n",
      "Fold: 6  Epoch: 21  Training loss = 1.7880  Validation loss = 1.0691  \n",
      "\n",
      "Fold: 6  Epoch: 22  Training loss = 1.7841  Validation loss = 1.0474  \n",
      "\n",
      "Fold: 6  Epoch: 23  Training loss = 1.7817  Validation loss = 1.0323  \n",
      "\n",
      "Fold: 6  Epoch: 24  Training loss = 1.7806  Validation loss = 1.0313  \n",
      "\n",
      "Fold: 6  Epoch: 25  Training loss = 1.7778  Validation loss = 1.0114  \n",
      "\n",
      "Fold: 6  Epoch: 26  Training loss = 1.7763  Validation loss = 1.0062  \n",
      "\n",
      "Fold: 6  Epoch: 27  Training loss = 1.7725  Validation loss = 0.9820  \n",
      "\n",
      "Fold: 6  Epoch: 28  Training loss = 1.7707  Validation loss = 0.9822  \n",
      "\n",
      "Fold: 6  Epoch: 29  Training loss = 1.7705  Validation loss = 0.9941  \n",
      "\n",
      "Fold: 6  Epoch: 30  Training loss = 1.7689  Validation loss = 0.9923  \n",
      "\n",
      "Fold: 6  Epoch: 31  Training loss = 1.7658  Validation loss = 0.9683  \n",
      "\n",
      "Fold: 6  Epoch: 32  Training loss = 1.7629  Validation loss = 0.9530  \n",
      "\n",
      "Fold: 6  Epoch: 33  Training loss = 1.7621  Validation loss = 0.9588  \n",
      "\n",
      "Fold: 6  Epoch: 34  Training loss = 1.7599  Validation loss = 0.9549  \n",
      "\n",
      "Fold: 6  Epoch: 35  Training loss = 1.7581  Validation loss = 0.9568  \n",
      "\n",
      "Fold: 6  Epoch: 36  Training loss = 1.7550  Validation loss = 0.9337  \n",
      "\n",
      "Fold: 6  Epoch: 37  Training loss = 1.7530  Validation loss = 0.9326  \n",
      "\n",
      "Fold: 6  Epoch: 38  Training loss = 1.7508  Validation loss = 0.9258  \n",
      "\n",
      "Fold: 6  Epoch: 39  Training loss = 1.7469  Validation loss = 0.8961  \n",
      "\n",
      "Fold: 6  Epoch: 40  Training loss = 1.7443  Validation loss = 0.8759  \n",
      "\n",
      "Fold: 6  Epoch: 41  Training loss = 1.7428  Validation loss = 0.8756  \n",
      "\n",
      "Fold: 6  Epoch: 42  Training loss = 1.7404  Validation loss = 0.8644  \n",
      "\n",
      "Fold: 6  Epoch: 43  Training loss = 1.7387  Validation loss = 0.8605  \n",
      "\n",
      "Fold: 6  Epoch: 44  Training loss = 1.7359  Validation loss = 0.8483  \n",
      "\n",
      "Fold: 6  Epoch: 45  Training loss = 1.7347  Validation loss = 0.8485  \n",
      "\n",
      "Fold: 6  Epoch: 46  Training loss = 1.7333  Validation loss = 0.8477  \n",
      "\n",
      "Fold: 6  Epoch: 47  Training loss = 1.7316  Validation loss = 0.8403  \n",
      "\n",
      "Fold: 6  Epoch: 48  Training loss = 1.7302  Validation loss = 0.8396  \n",
      "\n",
      "Fold: 6  Epoch: 49  Training loss = 1.7276  Validation loss = 0.8336  \n",
      "\n",
      "Fold: 6  Epoch: 50  Training loss = 1.7259  Validation loss = 0.8286  \n",
      "\n",
      "Fold: 6  Epoch: 51  Training loss = 1.7239  Validation loss = 0.8231  \n",
      "\n",
      "Fold: 6  Epoch: 52  Training loss = 1.7225  Validation loss = 0.8273  \n",
      "\n",
      "Fold: 6  Epoch: 53  Training loss = 1.7203  Validation loss = 0.8211  \n",
      "\n",
      "Fold: 6  Epoch: 54  Training loss = 1.7198  Validation loss = 0.8236  \n",
      "\n",
      "Fold: 6  Epoch: 55  Training loss = 1.7183  Validation loss = 0.8244  \n",
      "\n",
      "Fold: 6  Epoch: 56  Training loss = 1.7147  Validation loss = 0.7905  \n",
      "\n",
      "Fold: 6  Epoch: 57  Training loss = 1.7132  Validation loss = 0.7830  \n",
      "\n",
      "Fold: 6  Epoch: 58  Training loss = 1.7107  Validation loss = 0.7677  \n",
      "\n",
      "Fold: 6  Epoch: 59  Training loss = 1.7087  Validation loss = 0.7633  \n",
      "\n",
      "Fold: 6  Epoch: 60  Training loss = 1.7076  Validation loss = 0.7599  \n",
      "\n",
      "Fold: 6  Epoch: 61  Training loss = 1.7058  Validation loss = 0.7441  \n",
      "\n",
      "Fold: 6  Epoch: 62  Training loss = 1.7043  Validation loss = 0.7384  \n",
      "\n",
      "Fold: 6  Epoch: 63  Training loss = 1.7030  Validation loss = 0.7468  \n",
      "\n",
      "Fold: 6  Epoch: 64  Training loss = 1.7017  Validation loss = 0.7525  \n",
      "\n",
      "Fold: 6  Epoch: 65  Training loss = 1.7004  Validation loss = 0.7541  \n",
      "\n",
      "Fold: 6  Epoch: 66  Training loss = 1.6988  Validation loss = 0.7484  \n",
      "\n",
      "Fold: 6  Epoch: 67  Training loss = 1.6965  Validation loss = 0.7264  \n",
      "\n",
      "Fold: 6  Epoch: 68  Training loss = 1.6954  Validation loss = 0.7253  \n",
      "\n",
      "Fold: 6  Epoch: 69  Training loss = 1.6941  Validation loss = 0.7170  \n",
      "\n",
      "Fold: 6  Epoch: 70  Training loss = 1.6923  Validation loss = 0.6991  \n",
      "\n",
      "Fold: 6  Epoch: 71  Training loss = 1.6909  Validation loss = 0.6818  \n",
      "\n",
      "Fold: 6  Epoch: 72  Training loss = 1.6890  Validation loss = 0.6760  \n",
      "\n",
      "Fold: 6  Epoch: 73  Training loss = 1.6865  Validation loss = 0.6803  \n",
      "\n",
      "Fold: 6  Epoch: 74  Training loss = 1.6840  Validation loss = 0.6652  \n",
      "\n",
      "Fold: 6  Epoch: 75  Training loss = 1.6822  Validation loss = 0.6699  \n",
      "\n",
      "Fold: 6  Epoch: 76  Training loss = 1.6804  Validation loss = 0.6776  \n",
      "\n",
      "Fold: 6  Epoch: 77  Training loss = 1.6784  Validation loss = 0.6713  \n",
      "\n",
      "Fold: 6  Epoch: 78  Training loss = 1.6768  Validation loss = 0.6577  \n",
      "\n",
      "Fold: 6  Epoch: 79  Training loss = 1.6752  Validation loss = 0.6601  \n",
      "\n",
      "Fold: 6  Epoch: 80  Training loss = 1.6728  Validation loss = 0.6403  \n",
      "\n",
      "Fold: 6  Epoch: 81  Training loss = 1.6707  Validation loss = 0.6357  \n",
      "\n",
      "Fold: 6  Epoch: 82  Training loss = 1.6686  Validation loss = 0.6453  \n",
      "\n",
      "Fold: 6  Epoch: 83  Training loss = 1.6669  Validation loss = 0.6373  \n",
      "\n",
      "Fold: 6  Epoch: 84  Training loss = 1.6652  Validation loss = 0.6253  \n",
      "\n",
      "Fold: 6  Epoch: 85  Training loss = 1.6634  Validation loss = 0.6339  \n",
      "\n",
      "Fold: 6  Epoch: 86  Training loss = 1.6616  Validation loss = 0.6352  \n",
      "\n",
      "Fold: 6  Epoch: 87  Training loss = 1.6603  Validation loss = 0.6530  \n",
      "\n",
      "Fold: 6  Epoch: 88  Training loss = 1.6583  Validation loss = 0.6425  \n",
      "\n",
      "Fold: 6  Epoch: 89  Training loss = 1.6563  Validation loss = 0.6255  \n",
      "\n",
      "Fold: 6  Epoch: 90  Training loss = 1.6552  Validation loss = 0.6312  \n",
      "\n",
      "Fold: 6  Epoch: 91  Training loss = 1.6531  Validation loss = 0.6234  \n",
      "\n",
      "Fold: 6  Epoch: 92  Training loss = 1.6517  Validation loss = 0.6309  \n",
      "\n",
      "Fold: 6  Epoch: 93  Training loss = 1.6497  Validation loss = 0.6261  \n",
      "\n",
      "Fold: 6  Epoch: 94  Training loss = 1.6490  Validation loss = 0.6270  \n",
      "\n",
      "Fold: 6  Epoch: 95  Training loss = 1.6473  Validation loss = 0.6227  \n",
      "\n",
      "Fold: 6  Epoch: 96  Training loss = 1.6452  Validation loss = 0.6234  \n",
      "\n",
      "Fold: 6  Epoch: 97  Training loss = 1.6432  Validation loss = 0.6355  \n",
      "\n",
      "Fold: 6  Epoch: 98  Training loss = 1.6415  Validation loss = 0.6253  \n",
      "\n",
      "Fold: 6  Epoch: 99  Training loss = 1.6399  Validation loss = 0.6286  \n",
      "\n",
      "Fold: 6  Epoch: 100  Training loss = 1.6387  Validation loss = 0.6276  \n",
      "\n",
      "Fold: 6  Epoch: 101  Training loss = 1.6363  Validation loss = 0.6267  \n",
      "\n",
      "Fold: 6  Epoch: 102  Training loss = 1.6344  Validation loss = 0.6164  \n",
      "\n",
      "Fold: 6  Epoch: 103  Training loss = 1.6327  Validation loss = 0.6188  \n",
      "\n",
      "Fold: 6  Epoch: 104  Training loss = 1.6300  Validation loss = 0.5914  \n",
      "\n",
      "Fold: 6  Epoch: 105  Training loss = 1.6282  Validation loss = 0.5865  \n",
      "\n",
      "Fold: 6  Epoch: 106  Training loss = 1.6266  Validation loss = 0.5723  \n",
      "\n",
      "Fold: 6  Epoch: 107  Training loss = 1.6247  Validation loss = 0.5779  \n",
      "\n",
      "Fold: 6  Epoch: 108  Training loss = 1.6235  Validation loss = 0.5902  \n",
      "\n",
      "Fold: 6  Epoch: 109  Training loss = 1.6218  Validation loss = 0.5915  \n",
      "\n",
      "Fold: 6  Epoch: 110  Training loss = 1.6198  Validation loss = 0.5729  \n",
      "\n",
      "Fold: 6  Epoch: 111  Training loss = 1.6185  Validation loss = 0.5751  \n",
      "\n",
      "Fold: 6  Epoch: 112  Training loss = 1.6168  Validation loss = 0.5887  \n",
      "\n",
      "Fold: 6  Epoch: 113  Training loss = 1.6157  Validation loss = 0.5937  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 106  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 1.5740  Validation loss = 0.7025  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 1.5721  Validation loss = 0.6999  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 1.5692  Validation loss = 0.6905  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.5685  Validation loss = 0.6908  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 1.5662  Validation loss = 0.6862  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 1.5651  Validation loss = 0.6880  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 1.5635  Validation loss = 0.6883  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 1.5617  Validation loss = 0.6876  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 1.5591  Validation loss = 0.6839  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 1.5576  Validation loss = 0.6849  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 1.5563  Validation loss = 0.6854  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 1.5538  Validation loss = 0.6805  \n",
      "\n",
      "Fold: 7  Epoch: 13  Training loss = 1.5515  Validation loss = 0.6795  \n",
      "\n",
      "Fold: 7  Epoch: 14  Training loss = 1.5495  Validation loss = 0.6805  \n",
      "\n",
      "Fold: 7  Epoch: 15  Training loss = 1.5481  Validation loss = 0.6833  \n",
      "\n",
      "Fold: 7  Epoch: 16  Training loss = 1.5460  Validation loss = 0.6806  \n",
      "\n",
      "Fold: 7  Epoch: 17  Training loss = 1.5438  Validation loss = 0.6800  \n",
      "\n",
      "Fold: 7  Epoch: 18  Training loss = 1.5426  Validation loss = 0.6842  \n",
      "\n",
      "Fold: 7  Epoch: 19  Training loss = 1.5406  Validation loss = 0.6833  \n",
      "\n",
      "Fold: 7  Epoch: 20  Training loss = 1.5386  Validation loss = 0.6838  \n",
      "\n",
      "Fold: 7  Epoch: 21  Training loss = 1.5382  Validation loss = 0.6868  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 13  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 1.4830  Validation loss = 5.7003  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 1.4805  Validation loss = 5.6821  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 1.4787  Validation loss = 5.6753  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 1.4768  Validation loss = 5.6672  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 1.4759  Validation loss = 5.6839  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 1.4742  Validation loss = 5.6850  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 1.4732  Validation loss = 5.6873  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 1.4726  Validation loss = 5.6990  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 1.4707  Validation loss = 5.6921  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 1.4685  Validation loss = 5.6805  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 1.4661  Validation loss = 5.6685  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 1.4633  Validation loss = 5.6552  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 1.4625  Validation loss = 5.6636  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 1.4602  Validation loss = 5.6655  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 1.4586  Validation loss = 5.6532  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 1.4572  Validation loss = 5.6362  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 1.4559  Validation loss = 5.6372  \n",
      "\n",
      "Fold: 8  Epoch: 18  Training loss = 1.4544  Validation loss = 5.6327  \n",
      "\n",
      "Fold: 8  Epoch: 19  Training loss = 1.4535  Validation loss = 5.6375  \n",
      "\n",
      "Fold: 8  Epoch: 20  Training loss = 1.4519  Validation loss = 5.6209  \n",
      "\n",
      "Fold: 8  Epoch: 21  Training loss = 1.4500  Validation loss = 5.6006  \n",
      "\n",
      "Fold: 8  Epoch: 22  Training loss = 1.4483  Validation loss = 5.5935  \n",
      "\n",
      "Fold: 8  Epoch: 23  Training loss = 1.4469  Validation loss = 5.6047  \n",
      "\n",
      "Fold: 8  Epoch: 24  Training loss = 1.4460  Validation loss = 5.6262  \n",
      "\n",
      "Fold: 8  Epoch: 25  Training loss = 1.4446  Validation loss = 5.6100  \n",
      "\n",
      "Fold: 8  Epoch: 26  Training loss = 1.4436  Validation loss = 5.6238  \n",
      "\n",
      "Fold: 8  Epoch: 27  Training loss = 1.4418  Validation loss = 5.6054  \n",
      "\n",
      "Fold: 8  Epoch: 28  Training loss = 1.4406  Validation loss = 5.6166  \n",
      "\n",
      "Fold: 8  Epoch: 29  Training loss = 1.4389  Validation loss = 5.6188  \n",
      "\n",
      "Fold: 8  Epoch: 30  Training loss = 1.4373  Validation loss = 5.6090  \n",
      "\n",
      "Fold: 8  Epoch: 31  Training loss = 1.4364  Validation loss = 5.5955  \n",
      "\n",
      "Fold: 8  Epoch: 32  Training loss = 1.4356  Validation loss = 5.5909  \n",
      "\n",
      "Fold: 8  Epoch: 33  Training loss = 1.4348  Validation loss = 5.5928  \n",
      "\n",
      "Fold: 8  Epoch: 34  Training loss = 1.4333  Validation loss = 5.6017  \n",
      "\n",
      "Fold: 8  Epoch: 35  Training loss = 1.4320  Validation loss = 5.6224  \n",
      "\n",
      "Fold: 8  Epoch: 36  Training loss = 1.4308  Validation loss = 5.6219  \n",
      "\n",
      "Fold: 8  Epoch: 37  Training loss = 1.4293  Validation loss = 5.6267  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 32  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 1.9529  Validation loss = 8.5975  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 1.9439  Validation loss = 8.5348  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 1.9447  Validation loss = 8.5495  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 1.9381  Validation loss = 8.5036  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 1.9382  Validation loss = 8.5044  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 1.9352  Validation loss = 8.4841  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 1.9322  Validation loss = 8.4556  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 1.9314  Validation loss = 8.4553  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 1.9304  Validation loss = 8.4590  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 1.9267  Validation loss = 8.4260  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 1.9259  Validation loss = 8.4298  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 1.9244  Validation loss = 8.4193  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 1.9206  Validation loss = 8.3808  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 1.9175  Validation loss = 8.3472  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 1.9152  Validation loss = 8.3202  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 1.9139  Validation loss = 8.3200  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 1.9140  Validation loss = 8.3455  \n",
      "\n",
      "Fold: 9  Epoch: 18  Training loss = 1.9125  Validation loss = 8.3370  \n",
      "\n",
      "Fold: 9  Epoch: 19  Training loss = 1.9107  Validation loss = 8.3158  \n",
      "\n",
      "Fold: 9  Epoch: 20  Training loss = 1.9104  Validation loss = 8.3367  \n",
      "\n",
      "Fold: 9  Epoch: 21  Training loss = 1.9083  Validation loss = 8.3218  \n",
      "\n",
      "Fold: 9  Epoch: 22  Training loss = 1.9063  Validation loss = 8.2913  \n",
      "\n",
      "Fold: 9  Epoch: 23  Training loss = 1.9045  Validation loss = 8.2736  \n",
      "\n",
      "Fold: 9  Epoch: 24  Training loss = 1.9029  Validation loss = 8.2629  \n",
      "\n",
      "Fold: 9  Epoch: 25  Training loss = 1.9020  Validation loss = 8.3004  \n",
      "\n",
      "Fold: 9  Epoch: 26  Training loss = 1.8998  Validation loss = 8.2763  \n",
      "\n",
      "Fold: 9  Epoch: 27  Training loss = 1.8980  Validation loss = 8.2717  \n",
      "\n",
      "Fold: 9  Epoch: 28  Training loss = 1.8970  Validation loss = 8.2684  \n",
      "\n",
      "Fold: 9  Epoch: 29  Training loss = 1.8953  Validation loss = 8.2484  \n",
      "\n",
      "Fold: 9  Epoch: 30  Training loss = 1.8953  Validation loss = 8.2837  \n",
      "\n",
      "Fold: 9  Epoch: 31  Training loss = 1.8924  Validation loss = 8.2414  \n",
      "\n",
      "Fold: 9  Epoch: 32  Training loss = 1.8919  Validation loss = 8.2594  \n",
      "\n",
      "Fold: 9  Epoch: 33  Training loss = 1.8906  Validation loss = 8.2448  \n",
      "\n",
      "Fold: 9  Epoch: 34  Training loss = 1.8886  Validation loss = 8.2377  \n",
      "\n",
      "Fold: 9  Epoch: 35  Training loss = 1.8874  Validation loss = 8.2329  \n",
      "\n",
      "Fold: 9  Epoch: 36  Training loss = 1.8851  Validation loss = 8.2086  \n",
      "\n",
      "Fold: 9  Epoch: 37  Training loss = 1.8836  Validation loss = 8.1714  \n",
      "\n",
      "Fold: 9  Epoch: 38  Training loss = 1.8828  Validation loss = 8.1552  \n",
      "\n",
      "Fold: 9  Epoch: 39  Training loss = 1.8815  Validation loss = 8.1507  \n",
      "\n",
      "Fold: 9  Epoch: 40  Training loss = 1.8803  Validation loss = 8.1550  \n",
      "\n",
      "Fold: 9  Epoch: 41  Training loss = 1.8796  Validation loss = 8.1230  \n",
      "\n",
      "Fold: 9  Epoch: 42  Training loss = 1.8789  Validation loss = 8.1001  \n",
      "\n",
      "Fold: 9  Epoch: 43  Training loss = 1.8776  Validation loss = 8.1042  \n",
      "\n",
      "Fold: 9  Epoch: 44  Training loss = 1.8757  Validation loss = 8.1080  \n",
      "\n",
      "Fold: 9  Epoch: 45  Training loss = 1.8747  Validation loss = 8.0946  \n",
      "\n",
      "Fold: 9  Epoch: 46  Training loss = 1.8725  Validation loss = 8.1041  \n",
      "\n",
      "Fold: 9  Epoch: 47  Training loss = 1.8712  Validation loss = 8.0932  \n",
      "\n",
      "Fold: 9  Epoch: 48  Training loss = 1.8699  Validation loss = 8.1052  \n",
      "\n",
      "Fold: 9  Epoch: 49  Training loss = 1.8691  Validation loss = 8.0986  \n",
      "\n",
      "Fold: 9  Epoch: 50  Training loss = 1.8684  Validation loss = 8.0843  \n",
      "\n",
      "Fold: 9  Epoch: 51  Training loss = 1.8672  Validation loss = 8.0942  \n",
      "\n",
      "Fold: 9  Epoch: 52  Training loss = 1.8660  Validation loss = 8.1082  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 50  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 2.6973  Validation loss = 6.2964  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 2.6737  Validation loss = 6.1091  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 2.6623  Validation loss = 6.0548  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 2.6414  Validation loss = 5.8841  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 2.6394  Validation loss = 5.8906  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 2.6299  Validation loss = 5.8324  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 2.6257  Validation loss = 5.8007  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 2.6173  Validation loss = 5.7191  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 2.6094  Validation loss = 5.6691  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 2.6032  Validation loss = 5.6264  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 2.5998  Validation loss = 5.6125  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 2.5943  Validation loss = 5.5784  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 2.5903  Validation loss = 5.5547  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 2.5850  Validation loss = 5.4718  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 2.5825  Validation loss = 5.3661  \n",
      "\n",
      "Fold: 10  Epoch: 16  Training loss = 2.5797  Validation loss = 5.3441  \n",
      "\n",
      "Fold: 10  Epoch: 17  Training loss = 2.5764  Validation loss = 5.4940  \n",
      "\n",
      "Fold: 10  Epoch: 18  Training loss = 2.5667  Validation loss = 5.3331  \n",
      "\n",
      "Fold: 10  Epoch: 19  Training loss = 2.5623  Validation loss = 5.3114  \n",
      "\n",
      "Fold: 10  Epoch: 20  Training loss = 2.5574  Validation loss = 5.2141  \n",
      "\n",
      "Fold: 10  Epoch: 21  Training loss = 2.5558  Validation loss = 5.3615  \n",
      "\n",
      "Fold: 10  Epoch: 22  Training loss = 2.5518  Validation loss = 5.2599  \n",
      "\n",
      "Fold: 10  Epoch: 23  Training loss = 2.5503  Validation loss = 5.3693  \n",
      "\n",
      "Fold: 10  Epoch: 24  Training loss = 2.5472  Validation loss = 5.3692  \n",
      "\n",
      "Fold: 10  Epoch: 25  Training loss = 2.5436  Validation loss = 5.3701  \n",
      "\n",
      "Fold: 10  Epoch: 26  Training loss = 2.5396  Validation loss = 5.3514  \n",
      "\n",
      "Fold: 10  Epoch: 27  Training loss = 2.5360  Validation loss = 5.3449  \n",
      "\n",
      "Fold: 10  Epoch: 28  Training loss = 2.5326  Validation loss = 5.3570  \n",
      "\n",
      "Fold: 10  Epoch: 29  Training loss = 2.5282  Validation loss = 5.3488  \n",
      "\n",
      "Fold: 10  Epoch: 30  Training loss = 2.5232  Validation loss = 5.2944  \n",
      "\n",
      "Fold: 10  Epoch: 31  Training loss = 2.5179  Validation loss = 5.2534  \n",
      "\n",
      "Fold: 10  Epoch: 32  Training loss = 2.5154  Validation loss = 5.2719  \n",
      "\n",
      "Fold: 10  Epoch: 33  Training loss = 2.5135  Validation loss = 5.2894  \n",
      "\n",
      "Fold: 10  Epoch: 34  Training loss = 2.5100  Validation loss = 5.3155  \n",
      "\n",
      "Fold: 10  Epoch: 35  Training loss = 2.5094  Validation loss = 5.3274  \n",
      "\n",
      "Fold: 10  Epoch: 36  Training loss = 2.5046  Validation loss = 5.2902  \n",
      "\n",
      "Fold: 10  Epoch: 37  Training loss = 2.5002  Validation loss = 5.2631  \n",
      "\n",
      "Fold: 10  Epoch: 38  Training loss = 2.4994  Validation loss = 5.3206  \n",
      "\n",
      "Fold: 10  Epoch: 39  Training loss = 2.4954  Validation loss = 5.2559  \n",
      "\n",
      "Fold: 10  Epoch: 40  Training loss = 2.4930  Validation loss = 5.2361  \n",
      "\n",
      "Fold: 10  Epoch: 41  Training loss = 2.4899  Validation loss = 5.2071  \n",
      "\n",
      "Fold: 10  Epoch: 42  Training loss = 2.4884  Validation loss = 5.2113  \n",
      "\n",
      "Fold: 10  Epoch: 43  Training loss = 2.4862  Validation loss = 5.2453  \n",
      "\n",
      "Fold: 10  Epoch: 44  Training loss = 2.4839  Validation loss = 5.2495  \n",
      "\n",
      "Fold: 10  Epoch: 45  Training loss = 2.4783  Validation loss = 5.1714  \n",
      "\n",
      "Fold: 10  Epoch: 46  Training loss = 2.4753  Validation loss = 5.1647  \n",
      "\n",
      "Fold: 10  Epoch: 47  Training loss = 2.4739  Validation loss = 5.1451  \n",
      "\n",
      "Fold: 10  Epoch: 48  Training loss = 2.4725  Validation loss = 5.1501  \n",
      "\n",
      "Fold: 10  Epoch: 49  Training loss = 2.4695  Validation loss = 5.1691  \n",
      "\n",
      "Fold: 10  Epoch: 50  Training loss = 2.4690  Validation loss = 5.1962  \n",
      "\n",
      "Fold: 10  Epoch: 51  Training loss = 2.4619  Validation loss = 5.0750  \n",
      "\n",
      "Fold: 10  Epoch: 52  Training loss = 2.4587  Validation loss = 5.0396  \n",
      "\n",
      "Fold: 10  Epoch: 53  Training loss = 2.4574  Validation loss = 5.0285  \n",
      "\n",
      "Fold: 10  Epoch: 54  Training loss = 2.4535  Validation loss = 5.0280  \n",
      "\n",
      "Fold: 10  Epoch: 55  Training loss = 2.4504  Validation loss = 5.0087  \n",
      "\n",
      "Fold: 10  Epoch: 56  Training loss = 2.4467  Validation loss = 4.9461  \n",
      "\n",
      "Fold: 10  Epoch: 57  Training loss = 2.4443  Validation loss = 4.9307  \n",
      "\n",
      "Fold: 10  Epoch: 58  Training loss = 2.4414  Validation loss = 4.9412  \n",
      "\n",
      "Fold: 10  Epoch: 59  Training loss = 2.4382  Validation loss = 4.9577  \n",
      "\n",
      "Fold: 10  Epoch: 60  Training loss = 2.4356  Validation loss = 4.9542  \n",
      "\n",
      "Fold: 10  Epoch: 61  Training loss = 2.4338  Validation loss = 4.9478  \n",
      "\n",
      "Fold: 10  Epoch: 62  Training loss = 2.4304  Validation loss = 4.9437  \n",
      "\n",
      "Fold: 10  Epoch: 63  Training loss = 2.4277  Validation loss = 4.9460  \n",
      "\n",
      "Fold: 10  Epoch: 64  Training loss = 2.4254  Validation loss = 4.9493  \n",
      "\n",
      "Fold: 10  Epoch: 65  Training loss = 2.4216  Validation loss = 4.9378  \n",
      "\n",
      "Fold: 10  Epoch: 66  Training loss = 2.4178  Validation loss = 4.9035  \n",
      "\n",
      "Fold: 10  Epoch: 67  Training loss = 2.4146  Validation loss = 4.8892  \n",
      "\n",
      "Fold: 10  Epoch: 68  Training loss = 2.4128  Validation loss = 4.8744  \n",
      "\n",
      "Fold: 10  Epoch: 69  Training loss = 2.4088  Validation loss = 4.8980  \n",
      "\n",
      "Fold: 10  Epoch: 70  Training loss = 2.4061  Validation loss = 4.8727  \n",
      "\n",
      "Fold: 10  Epoch: 71  Training loss = 2.4043  Validation loss = 4.8923  \n",
      "\n",
      "Fold: 10  Epoch: 72  Training loss = 2.4023  Validation loss = 4.9105  \n",
      "\n",
      "Fold: 10  Epoch: 73  Training loss = 2.3989  Validation loss = 4.8972  \n",
      "\n",
      "Fold: 10  Epoch: 74  Training loss = 2.3963  Validation loss = 4.9251  \n",
      "\n",
      "Fold: 10  Epoch: 75  Training loss = 2.3940  Validation loss = 4.9163  \n",
      "\n",
      "Fold: 10  Epoch: 76  Training loss = 2.3922  Validation loss = 4.8692  \n",
      "\n",
      "Fold: 10  Epoch: 77  Training loss = 2.3902  Validation loss = 4.9001  \n",
      "\n",
      "Fold: 10  Epoch: 78  Training loss = 2.3881  Validation loss = 4.8913  \n",
      "\n",
      "Fold: 10  Epoch: 79  Training loss = 2.3859  Validation loss = 4.8519  \n",
      "\n",
      "Fold: 10  Epoch: 80  Training loss = 2.3837  Validation loss = 4.8542  \n",
      "\n",
      "Fold: 10  Epoch: 81  Training loss = 2.3824  Validation loss = 4.8606  \n",
      "\n",
      "Fold: 10  Epoch: 82  Training loss = 2.3781  Validation loss = 4.8876  \n",
      "\n",
      "Fold: 10  Epoch: 83  Training loss = 2.3762  Validation loss = 4.8876  \n",
      "\n",
      "Fold: 10  Epoch: 84  Training loss = 2.3739  Validation loss = 4.9244  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 79  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 2.5562  Validation loss = 2.1470  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 2.5519  Validation loss = 2.1575  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 2.5352  Validation loss = 2.1777  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 2.5307  Validation loss = 2.1808  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 2.5172  Validation loss = 2.1971  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 2.5102  Validation loss = 2.1995  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 2.4991  Validation loss = 2.1969  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 2.4957  Validation loss = 2.1995  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 2.4727  Validation loss = 2.2006  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 2.4252  Validation loss = 2.2271  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 2.4141  Validation loss = 2.2412  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 1  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 2.4507  Validation loss = 1.0932  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 2.4471  Validation loss = 1.1031  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 2.4446  Validation loss = 1.0834  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 2.4417  Validation loss = 1.0598  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 2.4378  Validation loss = 1.0325  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 2.4369  Validation loss = 1.0547  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 2.4310  Validation loss = 0.9957  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 2.4244  Validation loss = 0.9433  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 2.4223  Validation loss = 0.9657  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 2.4186  Validation loss = 0.9764  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 2.4231  Validation loss = 1.0144  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 2.4136  Validation loss = 0.9770  \n",
      "\n",
      "Fold: 12  Epoch: 13  Training loss = 2.4114  Validation loss = 0.9623  \n",
      "\n",
      "Fold: 12  Epoch: 14  Training loss = 2.4072  Validation loss = 0.9080  \n",
      "\n",
      "Fold: 12  Epoch: 15  Training loss = 2.4051  Validation loss = 0.9241  \n",
      "\n",
      "Fold: 12  Epoch: 16  Training loss = 2.4035  Validation loss = 0.9292  \n",
      "\n",
      "Fold: 12  Epoch: 17  Training loss = 2.3888  Validation loss = 0.9046  \n",
      "\n",
      "Fold: 12  Epoch: 18  Training loss = 2.3867  Validation loss = 0.9182  \n",
      "\n",
      "Fold: 12  Epoch: 19  Training loss = 2.3845  Validation loss = 0.9070  \n",
      "\n",
      "Fold: 12  Epoch: 20  Training loss = 2.3836  Validation loss = 0.9320  \n",
      "\n",
      "Fold: 12  Epoch: 21  Training loss = 2.3800  Validation loss = 0.9027  \n",
      "\n",
      "Fold: 12  Epoch: 22  Training loss = 2.3776  Validation loss = 0.8949  \n",
      "\n",
      "Fold: 12  Epoch: 23  Training loss = 2.3762  Validation loss = 0.9011  \n",
      "\n",
      "Fold: 12  Epoch: 24  Training loss = 2.3752  Validation loss = 0.9230  \n",
      "\n",
      "Fold: 12  Epoch: 25  Training loss = 2.3753  Validation loss = 0.9263  \n",
      "\n",
      "Fold: 12  Epoch: 26  Training loss = 2.3706  Validation loss = 0.8662  \n",
      "\n",
      "Fold: 12  Epoch: 27  Training loss = 2.3796  Validation loss = 0.9187  \n",
      "\n",
      "Fold: 12  Epoch: 28  Training loss = 2.3747  Validation loss = 0.8006  \n",
      "\n",
      "Fold: 12  Epoch: 29  Training loss = 2.3734  Validation loss = 0.8055  \n",
      "\n",
      "Fold: 12  Epoch: 30  Training loss = 2.3735  Validation loss = 0.8571  \n",
      "\n",
      "Fold: 12  Epoch: 31  Training loss = 2.3707  Validation loss = 0.8562  \n",
      "\n",
      "Fold: 12  Epoch: 32  Training loss = 2.3698  Validation loss = 0.8666  \n",
      "\n",
      "Fold: 12  Epoch: 33  Training loss = 2.3684  Validation loss = 0.8594  \n",
      "\n",
      "Fold: 12  Epoch: 34  Training loss = 2.3671  Validation loss = 0.8754  \n",
      "\n",
      "Fold: 12  Epoch: 35  Training loss = 2.3669  Validation loss = 0.8893  \n",
      "\n",
      "Fold: 12  Epoch: 36  Training loss = 2.3657  Validation loss = 0.8960  \n",
      "\n",
      "Fold: 12  Epoch: 37  Training loss = 2.3644  Validation loss = 0.9060  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 28  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 2.3228  Validation loss = 3.4339  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 2.3237  Validation loss = 3.4339  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 2.3207  Validation loss = 3.4266  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 2.3172  Validation loss = 3.3839  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 2.3171  Validation loss = 3.3719  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 2.3119  Validation loss = 3.3107  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 2.3092  Validation loss = 3.3027  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 2.3069  Validation loss = 3.2691  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 2.3059  Validation loss = 3.2521  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 2.3049  Validation loss = 3.2525  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 2.3061  Validation loss = 3.2824  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 2.3040  Validation loss = 3.2555  \n",
      "\n",
      "Fold: 13  Epoch: 13  Training loss = 2.2996  Validation loss = 3.2149  \n",
      "\n",
      "Fold: 13  Epoch: 14  Training loss = 2.2926  Validation loss = 3.1406  \n",
      "\n",
      "Fold: 13  Epoch: 15  Training loss = 2.2915  Validation loss = 3.1307  \n",
      "\n",
      "Fold: 13  Epoch: 16  Training loss = 2.2902  Validation loss = 3.1111  \n",
      "\n",
      "Fold: 13  Epoch: 17  Training loss = 2.2891  Validation loss = 3.0983  \n",
      "\n",
      "Fold: 13  Epoch: 18  Training loss = 2.2881  Validation loss = 3.1205  \n",
      "\n",
      "Fold: 13  Epoch: 19  Training loss = 2.2877  Validation loss = 3.0984  \n",
      "\n",
      "Fold: 13  Epoch: 20  Training loss = 2.2865  Validation loss = 3.0860  \n",
      "\n",
      "Fold: 13  Epoch: 21  Training loss = 2.2859  Validation loss = 3.0928  \n",
      "\n",
      "Fold: 13  Epoch: 22  Training loss = 2.2831  Validation loss = 3.0914  \n",
      "\n",
      "Fold: 13  Epoch: 23  Training loss = 2.2798  Validation loss = 3.0685  \n",
      "\n",
      "Fold: 13  Epoch: 24  Training loss = 2.2761  Validation loss = 3.1276  \n",
      "\n",
      "Fold: 13  Epoch: 25  Training loss = 2.2782  Validation loss = 3.1772  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 23  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 2.3930  Validation loss = 5.7103  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 2.3960  Validation loss = 5.7412  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 2.3855  Validation loss = 5.6830  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 2.3837  Validation loss = 5.6768  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 2.3748  Validation loss = 5.6090  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 2.3699  Validation loss = 5.5586  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 2.3651  Validation loss = 5.5008  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 2.3635  Validation loss = 5.4996  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 2.3606  Validation loss = 5.4584  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 2.3596  Validation loss = 5.4609  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 2.3593  Validation loss = 5.4743  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 2.3566  Validation loss = 5.4395  \n",
      "\n",
      "Fold: 14  Epoch: 13  Training loss = 2.3559  Validation loss = 5.4415  \n",
      "\n",
      "Fold: 14  Epoch: 14  Training loss = 2.3561  Validation loss = 5.4649  \n",
      "\n",
      "Fold: 14  Epoch: 15  Training loss = 2.3543  Validation loss = 5.4202  \n",
      "\n",
      "Fold: 14  Epoch: 16  Training loss = 2.3533  Validation loss = 5.4039  \n",
      "\n",
      "Fold: 14  Epoch: 17  Training loss = 2.3518  Validation loss = 5.4117  \n",
      "\n",
      "Fold: 14  Epoch: 18  Training loss = 2.3507  Validation loss = 5.4325  \n",
      "\n",
      "Fold: 14  Epoch: 19  Training loss = 2.3498  Validation loss = 5.4073  \n",
      "\n",
      "Fold: 14  Epoch: 20  Training loss = 2.3452  Validation loss = 5.3569  \n",
      "\n",
      "Fold: 14  Epoch: 21  Training loss = 2.3433  Validation loss = 5.3829  \n",
      "\n",
      "Fold: 14  Epoch: 22  Training loss = 2.3414  Validation loss = 5.4026  \n",
      "\n",
      "Fold: 14  Epoch: 23  Training loss = 2.3408  Validation loss = 5.4226  \n",
      "\n",
      "Fold: 14  Epoch: 24  Training loss = 2.3379  Validation loss = 5.4030  \n",
      "\n",
      "Fold: 14  Epoch: 25  Training loss = 2.3373  Validation loss = 5.3568  \n",
      "\n",
      "Fold: 14  Epoch: 26  Training loss = 2.3367  Validation loss = 5.3688  \n",
      "\n",
      "Fold: 14  Epoch: 27  Training loss = 2.3356  Validation loss = 5.3782  \n",
      "\n",
      "Fold: 14  Epoch: 28  Training loss = 2.3359  Validation loss = 5.3888  \n",
      "\n",
      "Fold: 14  Epoch: 29  Training loss = 2.3346  Validation loss = 5.4129  \n",
      "\n",
      "Fold: 14  Epoch: 30  Training loss = 2.3334  Validation loss = 5.4283  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 25  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 2.6776  Validation loss = 6.0683  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 2.6758  Validation loss = 6.0610  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 2.6711  Validation loss = 6.0335  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 2.6646  Validation loss = 6.0009  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 2.6615  Validation loss = 5.9882  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 2.6540  Validation loss = 5.9340  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 2.6499  Validation loss = 5.9028  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 2.6470  Validation loss = 5.8863  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 2.6448  Validation loss = 5.8764  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 2.6408  Validation loss = 5.8460  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 2.6357  Validation loss = 5.7783  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 2.6337  Validation loss = 5.7202  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 2.6339  Validation loss = 5.7295  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 2.6334  Validation loss = 5.7400  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 2.6305  Validation loss = 5.6984  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 2.6276  Validation loss = 5.6775  \n",
      "\n",
      "Fold: 15  Epoch: 17  Training loss = 2.6254  Validation loss = 5.6815  \n",
      "\n",
      "Fold: 15  Epoch: 18  Training loss = 2.6285  Validation loss = 5.7533  \n",
      "\n",
      "Fold: 15  Epoch: 19  Training loss = 2.6285  Validation loss = 5.7040  \n",
      "\n",
      "Fold: 15  Epoch: 20  Training loss = 2.6282  Validation loss = 5.7051  \n",
      "\n",
      "Fold: 15  Epoch: 21  Training loss = 2.6239  Validation loss = 5.6952  \n",
      "\n",
      "Fold: 15  Epoch: 22  Training loss = 2.6239  Validation loss = 5.7252  \n",
      "\n",
      "Fold: 15  Epoch: 23  Training loss = 2.6268  Validation loss = 5.7604  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 16  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 2.9517  Validation loss = 4.5613  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 2.9489  Validation loss = 4.5410  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 2.9393  Validation loss = 4.4923  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 2.9432  Validation loss = 4.5099  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 2.9383  Validation loss = 4.4571  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 2.9338  Validation loss = 4.4619  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 2.9236  Validation loss = 4.3200  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 2.9287  Validation loss = 4.3793  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 2.9257  Validation loss = 4.3113  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 2.9220  Validation loss = 4.3321  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 2.9157  Validation loss = 4.2882  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 2.9076  Validation loss = 4.2283  \n",
      "\n",
      "Fold: 16  Epoch: 13  Training loss = 2.9030  Validation loss = 4.1568  \n",
      "\n",
      "Fold: 16  Epoch: 14  Training loss = 2.9013  Validation loss = 4.1996  \n",
      "\n",
      "Fold: 16  Epoch: 15  Training loss = 2.8979  Validation loss = 4.1821  \n",
      "\n",
      "Fold: 16  Epoch: 16  Training loss = 2.9002  Validation loss = 4.2403  \n",
      "\n",
      "Fold: 16  Epoch: 17  Training loss = 2.8965  Validation loss = 4.2408  \n",
      "\n",
      "Fold: 16  Epoch: 18  Training loss = 2.8929  Validation loss = 4.1414  \n",
      "\n",
      "Fold: 16  Epoch: 19  Training loss = 2.8916  Validation loss = 4.0980  \n",
      "\n",
      "Fold: 16  Epoch: 20  Training loss = 2.8884  Validation loss = 4.1056  \n",
      "\n",
      "Fold: 16  Epoch: 21  Training loss = 2.8859  Validation loss = 4.0618  \n",
      "\n",
      "Fold: 16  Epoch: 22  Training loss = 2.8856  Validation loss = 4.0526  \n",
      "\n",
      "Fold: 16  Epoch: 23  Training loss = 2.8888  Validation loss = 4.0764  \n",
      "\n",
      "Fold: 16  Epoch: 24  Training loss = 2.8859  Validation loss = 4.0442  \n",
      "\n",
      "Fold: 16  Epoch: 25  Training loss = 2.8832  Validation loss = 4.0175  \n",
      "\n",
      "Fold: 16  Epoch: 26  Training loss = 2.8834  Validation loss = 4.1489  \n",
      "\n",
      "Fold: 16  Epoch: 27  Training loss = 2.8819  Validation loss = 4.1450  \n",
      "\n",
      "Fold: 16  Epoch: 28  Training loss = 2.8854  Validation loss = 4.2245  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 25  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 3.0350  Validation loss = 3.8088  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 3.0348  Validation loss = 3.7979  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 3.0119  Validation loss = 3.8526  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 2.9934  Validation loss = 3.9268  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 2.9826  Validation loss = 3.9486  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 2.9828  Validation loss = 3.9617  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 2.9738  Validation loss = 3.9444  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 2.9699  Validation loss = 3.9359  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 2.9659  Validation loss = 3.9394  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 2.9597  Validation loss = 3.9626  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 2.9452  Validation loss = 4.0538  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 2  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 3.0939  Validation loss = 2.5768  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 3.0839  Validation loss = 2.5569  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 3.0833  Validation loss = 2.5628  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 3.0787  Validation loss = 2.5654  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 3.0718  Validation loss = 2.5562  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 3.0698  Validation loss = 2.5520  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 3.0693  Validation loss = 2.5112  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 3.0640  Validation loss = 2.5247  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 3.0607  Validation loss = 2.5418  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 3.0598  Validation loss = 2.5409  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 3.0540  Validation loss = 2.5345  \n",
      "\n",
      "Fold: 18  Epoch: 12  Training loss = 3.0511  Validation loss = 2.5380  \n",
      "\n",
      "Fold: 18  Epoch: 13  Training loss = 3.0487  Validation loss = 2.5355  \n",
      "\n",
      "Fold: 18  Epoch: 14  Training loss = 3.0432  Validation loss = 2.5177  \n",
      "\n",
      "Fold: 18  Epoch: 15  Training loss = 3.0444  Validation loss = 2.5327  \n",
      "\n",
      "Fold: 18  Epoch: 16  Training loss = 3.0430  Validation loss = 2.5252  \n",
      "\n",
      "Fold: 18  Epoch: 17  Training loss = 3.0367  Validation loss = 2.5143  \n",
      "\n",
      "Fold: 18  Epoch: 18  Training loss = 3.0369  Validation loss = 2.5047  \n",
      "\n",
      "Fold: 18  Epoch: 19  Training loss = 3.0321  Validation loss = 2.4979  \n",
      "\n",
      "Fold: 18  Epoch: 20  Training loss = 3.0298  Validation loss = 2.5069  \n",
      "\n",
      "Fold: 18  Epoch: 21  Training loss = 3.0274  Validation loss = 2.4990  \n",
      "\n",
      "Fold: 18  Epoch: 22  Training loss = 3.0274  Validation loss = 2.5054  \n",
      "\n",
      "Fold: 18  Epoch: 23  Training loss = 3.0232  Validation loss = 2.4956  \n",
      "\n",
      "Fold: 18  Epoch: 24  Training loss = 3.0197  Validation loss = 2.4894  \n",
      "\n",
      "Fold: 18  Epoch: 25  Training loss = 3.0194  Validation loss = 2.4864  \n",
      "\n",
      "Fold: 18  Epoch: 26  Training loss = 3.0173  Validation loss = 2.4890  \n",
      "\n",
      "Fold: 18  Epoch: 27  Training loss = 3.0174  Validation loss = 2.4641  \n",
      "\n",
      "Fold: 18  Epoch: 28  Training loss = 3.0128  Validation loss = 2.4529  \n",
      "\n",
      "Fold: 18  Epoch: 29  Training loss = 3.0121  Validation loss = 2.4532  \n",
      "\n",
      "Fold: 18  Epoch: 30  Training loss = 3.0079  Validation loss = 2.4351  \n",
      "\n",
      "Fold: 18  Epoch: 31  Training loss = 3.0072  Validation loss = 2.4176  \n",
      "\n",
      "Fold: 18  Epoch: 32  Training loss = 3.0036  Validation loss = 2.4195  \n",
      "\n",
      "Fold: 18  Epoch: 33  Training loss = 3.0004  Validation loss = 2.4258  \n",
      "\n",
      "Fold: 18  Epoch: 34  Training loss = 2.9977  Validation loss = 2.3960  \n",
      "\n",
      "Fold: 18  Epoch: 35  Training loss = 2.9970  Validation loss = 2.4179  \n",
      "\n",
      "Fold: 18  Epoch: 36  Training loss = 2.9921  Validation loss = 2.4348  \n",
      "\n",
      "Fold: 18  Epoch: 37  Training loss = 2.9897  Validation loss = 2.4252  \n",
      "\n",
      "Fold: 18  Epoch: 38  Training loss = 2.9866  Validation loss = 2.3885  \n",
      "\n",
      "Fold: 18  Epoch: 39  Training loss = 2.9843  Validation loss = 2.3884  \n",
      "\n",
      "Fold: 18  Epoch: 40  Training loss = 2.9841  Validation loss = 2.3990  \n",
      "\n",
      "Fold: 18  Epoch: 41  Training loss = 2.9820  Validation loss = 2.3868  \n",
      "\n",
      "Fold: 18  Epoch: 42  Training loss = 2.9796  Validation loss = 2.3537  \n",
      "\n",
      "Fold: 18  Epoch: 43  Training loss = 2.9768  Validation loss = 2.3482  \n",
      "\n",
      "Fold: 18  Epoch: 44  Training loss = 2.9781  Validation loss = 2.3202  \n",
      "\n",
      "Fold: 18  Epoch: 45  Training loss = 2.9974  Validation loss = 2.2834  \n",
      "\n",
      "Fold: 18  Epoch: 46  Training loss = 3.0013  Validation loss = 2.3025  \n",
      "\n",
      "Fold: 18  Epoch: 47  Training loss = 2.9693  Validation loss = 2.2800  \n",
      "\n",
      "Fold: 18  Epoch: 48  Training loss = 2.9625  Validation loss = 2.2996  \n",
      "\n",
      "Fold: 18  Epoch: 49  Training loss = 2.9592  Validation loss = 2.2874  \n",
      "\n",
      "Fold: 18  Epoch: 50  Training loss = 2.9555  Validation loss = 2.2925  \n",
      "\n",
      "Fold: 18  Epoch: 51  Training loss = 2.9593  Validation loss = 2.2786  \n",
      "\n",
      "Fold: 18  Epoch: 52  Training loss = 2.9679  Validation loss = 2.2740  \n",
      "\n",
      "Fold: 18  Epoch: 53  Training loss = 2.9674  Validation loss = 2.2875  \n",
      "\n",
      "Fold: 18  Epoch: 54  Training loss = 2.9725  Validation loss = 2.2784  \n",
      "\n",
      "Fold: 18  Epoch: 55  Training loss = 2.9572  Validation loss = 2.2909  \n",
      "\n",
      "Fold: 18  Epoch: 56  Training loss = 2.9526  Validation loss = 2.3020  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 52  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 2.9480  Validation loss = 1.9552  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 2.9384  Validation loss = 2.0185  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 2.9348  Validation loss = 1.9462  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 2.9286  Validation loss = 1.9967  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 2.9270  Validation loss = 1.9638  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 2.9236  Validation loss = 1.9631  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 2.9238  Validation loss = 2.0388  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 2.9178  Validation loss = 1.9708  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 2.9159  Validation loss = 1.9350  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 2.9113  Validation loss = 1.8942  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 2.9083  Validation loss = 1.9482  \n",
      "\n",
      "Fold: 19  Epoch: 12  Training loss = 2.9079  Validation loss = 1.9106  \n",
      "\n",
      "Fold: 19  Epoch: 13  Training loss = 2.9066  Validation loss = 1.9300  \n",
      "\n",
      "Fold: 19  Epoch: 14  Training loss = 2.9027  Validation loss = 1.9787  \n",
      "\n",
      "Fold: 19  Epoch: 15  Training loss = 2.8966  Validation loss = 1.9739  \n",
      "\n",
      "Fold: 19  Epoch: 16  Training loss = 2.8962  Validation loss = 1.9695  \n",
      "\n",
      "Fold: 19  Epoch: 17  Training loss = 2.9022  Validation loss = 2.0084  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 10  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 2.9218  Validation loss = 1.2706  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 2.9066  Validation loss = 1.3867  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 2.9016  Validation loss = 1.4489  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 2.8991  Validation loss = 1.4644  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 2.9123  Validation loss = 1.4150  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 2.8974  Validation loss = 1.4446  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 2.8876  Validation loss = 1.5281  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 2.8823  Validation loss = 1.5072  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 2.8757  Validation loss = 1.5288  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 2.8718  Validation loss = 1.5204  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 2.8681  Validation loss = 1.5438  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 1  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 2.8786  Validation loss = 2.0959  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 2.8736  Validation loss = 2.1436  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 2.8683  Validation loss = 2.1374  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 2.8682  Validation loss = 2.1204  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 2.8625  Validation loss = 2.1535  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 2.8628  Validation loss = 2.1649  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 2.8632  Validation loss = 2.1300  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 2.8570  Validation loss = 2.2082  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 2.8603  Validation loss = 2.2481  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 2.8522  Validation loss = 2.1473  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 2.8458  Validation loss = 2.2395  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 2.8669  Validation loss = 2.1956  \n",
      "\n",
      "Fold: 21  Epoch: 13  Training loss = 2.8655  Validation loss = 2.1909  \n",
      "\n",
      "Fold: 21  Epoch: 14  Training loss = 2.8672  Validation loss = 2.1982  \n",
      "\n",
      "Fold: 21  Epoch: 15  Training loss = 2.8643  Validation loss = 2.1834  \n",
      "\n",
      "Fold: 21  Epoch: 16  Training loss = 2.8599  Validation loss = 2.1865  \n",
      "\n",
      "Fold: 21  Epoch: 17  Training loss = 2.8788  Validation loss = 2.1435  \n",
      "\n",
      "Fold: 21  Epoch: 18  Training loss = 2.8452  Validation loss = 2.2392  \n",
      "\n",
      "Fold: 21  Epoch: 19  Training loss = 2.8434  Validation loss = 2.2494  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 1  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 2.8553  Validation loss = 1.5142  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 2.8487  Validation loss = 1.4717  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 2.8447  Validation loss = 1.4444  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 2.8393  Validation loss = 1.4616  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 2.8308  Validation loss = 1.4478  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 2.8312  Validation loss = 1.4922  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 2.8242  Validation loss = 1.5070  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 2.8185  Validation loss = 1.4595  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 2.8179  Validation loss = 1.4434  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 2.8153  Validation loss = 1.4201  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 2.8107  Validation loss = 1.4133  \n",
      "\n",
      "Fold: 22  Epoch: 12  Training loss = 2.8071  Validation loss = 1.4080  \n",
      "\n",
      "Fold: 22  Epoch: 13  Training loss = 2.8098  Validation loss = 1.3520  \n",
      "\n",
      "Fold: 22  Epoch: 14  Training loss = 2.8090  Validation loss = 1.3241  \n",
      "\n",
      "Fold: 22  Epoch: 15  Training loss = 2.7982  Validation loss = 1.3820  \n",
      "\n",
      "Fold: 22  Epoch: 16  Training loss = 2.7945  Validation loss = 1.4138  \n",
      "\n",
      "Fold: 22  Epoch: 17  Training loss = 2.7926  Validation loss = 1.4459  \n",
      "\n",
      "Fold: 22  Epoch: 18  Training loss = 2.7937  Validation loss = 1.4620  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 14  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 2.6625  Validation loss = 2.0443  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 2.6581  Validation loss = 2.2542  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 2.6582  Validation loss = 2.3856  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 2.6484  Validation loss = 2.2549  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 2.6455  Validation loss = 2.1542  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 2.6490  Validation loss = 2.0840  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 2.6426  Validation loss = 2.1042  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 2.6395  Validation loss = 1.9523  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 2.6345  Validation loss = 2.0191  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 2.6372  Validation loss = 1.9623  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 2.6471  Validation loss = 1.6481  \n",
      "\n",
      "Fold: 23  Epoch: 12  Training loss = 2.6216  Validation loss = 1.9854  \n",
      "\n",
      "Fold: 23  Epoch: 13  Training loss = 2.6148  Validation loss = 2.0120  \n",
      "\n",
      "Fold: 23  Epoch: 14  Training loss = 2.6084  Validation loss = 2.0769  \n",
      "\n",
      "Fold: 23  Epoch: 15  Training loss = 2.6052  Validation loss = 2.2437  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 11  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 2.4796  Validation loss = 2.3528  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 2.4724  Validation loss = 2.4132  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 2.5005  Validation loss = 2.4858  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 2.4964  Validation loss = 2.4138  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 2.5060  Validation loss = 2.4334  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 2.4891  Validation loss = 2.4118  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 2.4733  Validation loss = 2.3724  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 2.4574  Validation loss = 2.3592  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 2.4522  Validation loss = 2.3606  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 2.4413  Validation loss = 2.3309  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 2.4359  Validation loss = 2.3631  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 2.4361  Validation loss = 2.3921  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 2.4221  Validation loss = 2.2941  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 2.4235  Validation loss = 2.2385  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 2.4144  Validation loss = 2.3539  \n",
      "\n",
      "Fold: 24  Epoch: 16  Training loss = 2.4116  Validation loss = 2.3969  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 14  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 2.4353  Validation loss = 2.2333  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 2.4275  Validation loss = 2.1702  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 2.4260  Validation loss = 2.1932  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 2.4239  Validation loss = 2.1177  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 2.4193  Validation loss = 2.1926  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 2.4129  Validation loss = 2.1044  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 2.4119  Validation loss = 2.1605  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 2.4054  Validation loss = 2.0600  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 2.3987  Validation loss = 2.0986  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 2.3947  Validation loss = 2.1152  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 2.4161  Validation loss = 2.2349  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 8  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 2.3068  Validation loss = 2.6950  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 2.2971  Validation loss = 2.6908  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 2.2878  Validation loss = 2.6267  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 2.2917  Validation loss = 2.5392  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 2.2775  Validation loss = 2.6211  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 2.2726  Validation loss = 2.6101  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 2.2655  Validation loss = 2.7521  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 2.2604  Validation loss = 2.7415  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 2.2568  Validation loss = 2.6506  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 2.2469  Validation loss = 2.8050  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 2.2445  Validation loss = 2.8777  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 4  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 2.2916  Validation loss = 2.3069  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 2.2684  Validation loss = 1.9034  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 2.2636  Validation loss = 1.9713  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 2.2592  Validation loss = 1.7747  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 2.2590  Validation loss = 1.8643  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 2.2545  Validation loss = 2.1998  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 2.2486  Validation loss = 1.9480  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 2.2447  Validation loss = 2.1315  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 2.2424  Validation loss = 2.2794  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 2.2360  Validation loss = 2.0663  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 2.2307  Validation loss = 1.9097  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 2.2268  Validation loss = 2.1691  \n",
      "\n",
      "Fold: 27  Epoch: 13  Training loss = 2.2398  Validation loss = 2.3477  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 4  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 2.0491  Validation loss = 1.8329  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 2.0435  Validation loss = 1.8039  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 2.0067  Validation loss = 1.9407  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 1.9929  Validation loss = 1.9021  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 1.9902  Validation loss = 1.8652  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 1.9741  Validation loss = 1.8613  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 1.9664  Validation loss = 1.8620  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 1.9639  Validation loss = 1.9279  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 1.9484  Validation loss = 1.8486  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 1.9632  Validation loss = 1.7855  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 1.9356  Validation loss = 1.8108  \n",
      "\n",
      "Fold: 28  Epoch: 12  Training loss = 1.9298  Validation loss = 1.8452  \n",
      "\n",
      "Fold: 28  Epoch: 13  Training loss = 1.9617  Validation loss = 1.9138  \n",
      "\n",
      "Fold: 28  Epoch: 14  Training loss = 1.9453  Validation loss = 1.9334  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 10  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 1.8993  Validation loss = 2.0666  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 1.8865  Validation loss = 2.1461  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 1.8776  Validation loss = 2.0892  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 1.8730  Validation loss = 2.0610  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 1.8636  Validation loss = 2.1208  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 1.8561  Validation loss = 2.0863  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 1.8529  Validation loss = 2.1213  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 1.8649  Validation loss = 2.1419  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 1.8507  Validation loss = 2.1444  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 1.8439  Validation loss = 2.0635  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 1.8299  Validation loss = 2.0869  \n",
      "\n",
      "Fold: 29  Epoch: 12  Training loss = 1.8252  Validation loss = 2.1012  \n",
      "\n",
      "Fold: 29  Epoch: 13  Training loss = 1.8277  Validation loss = 2.1433  \n",
      "\n",
      "Fold: 29  Epoch: 14  Training loss = 1.8290  Validation loss = 2.0842  \n",
      "\n",
      "Fold: 29  Epoch: 15  Training loss = 1.8152  Validation loss = 2.1577  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 4  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 1.8578  Validation loss = 1.5550  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 1.8428  Validation loss = 1.5503  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 1.8504  Validation loss = 1.5637  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 1.8377  Validation loss = 1.4475  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 1.8232  Validation loss = 1.5022  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 1.8127  Validation loss = 1.4829  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 1.8342  Validation loss = 1.4592  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 1.8133  Validation loss = 1.5568  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 1.8042  Validation loss = 1.5049  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 1.8049  Validation loss = 1.5356  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 1.8064  Validation loss = 1.5732  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 4  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 1.8019  Validation loss = 1.1984  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 1.7821  Validation loss = 1.0615  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 1.7696  Validation loss = 0.9230  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 1.7670  Validation loss = 1.0510  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 1.7546  Validation loss = 0.9047  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 1.7594  Validation loss = 0.7928  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 1.7553  Validation loss = 0.9495  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 1.7485  Validation loss = 0.7401  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 1.7473  Validation loss = 0.8270  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 1.7490  Validation loss = 0.9803  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 1.7614  Validation loss = 1.1886  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 8  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 1.6191  Validation loss = 2.5788  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 1.6009  Validation loss = 2.5430  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 1.6049  Validation loss = 2.4222  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 1.6053  Validation loss = 2.3848  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 1.5994  Validation loss = 2.5841  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 1.6002  Validation loss = 2.4274  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 1.5998  Validation loss = 2.5655  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 1.5777  Validation loss = 2.5439  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 1.5763  Validation loss = 2.5224  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 1.5861  Validation loss = 2.6297  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 1.5727  Validation loss = 2.4149  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 1.5670  Validation loss = 2.4337  \n",
      "\n",
      "Fold: 32  Epoch: 13  Training loss = 1.5844  Validation loss = 2.3609  \n",
      "\n",
      "Fold: 32  Epoch: 14  Training loss = 1.5585  Validation loss = 2.5815  \n",
      "\n",
      "Fold: 32  Epoch: 15  Training loss = 1.5628  Validation loss = 2.6739  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 13  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    # Load model with lowest validation error for each fold\n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 27\n",
      "Average validation error: 2.86921\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 1.5548  Test loss = 2.7914  \n",
      "\n",
      "Epoch: 2  Training loss = 1.5448  Test loss = 2.7409  \n",
      "\n",
      "Epoch: 3  Training loss = 1.5372  Test loss = 2.6996  \n",
      "\n",
      "Epoch: 4  Training loss = 1.5311  Test loss = 2.6657  \n",
      "\n",
      "Epoch: 5  Training loss = 1.5262  Test loss = 2.6382  \n",
      "\n",
      "Epoch: 6  Training loss = 1.5223  Test loss = 2.6163  \n",
      "\n",
      "Epoch: 7  Training loss = 1.5190  Test loss = 2.5993  \n",
      "\n",
      "Epoch: 8  Training loss = 1.5163  Test loss = 2.5860  \n",
      "\n",
      "Epoch: 9  Training loss = 1.5138  Test loss = 2.5754  \n",
      "\n",
      "Epoch: 10  Training loss = 1.5115  Test loss = 2.5669  \n",
      "\n",
      "Epoch: 11  Training loss = 1.5093  Test loss = 2.5598  \n",
      "\n",
      "Epoch: 12  Training loss = 1.5073  Test loss = 2.5539  \n",
      "\n",
      "Epoch: 13  Training loss = 1.5054  Test loss = 2.5490  \n",
      "\n",
      "Epoch: 14  Training loss = 1.5035  Test loss = 2.5448  \n",
      "\n",
      "Epoch: 15  Training loss = 1.5017  Test loss = 2.5412  \n",
      "\n",
      "Epoch: 16  Training loss = 1.5000  Test loss = 2.5382  \n",
      "\n",
      "Epoch: 17  Training loss = 1.4983  Test loss = 2.5355  \n",
      "\n",
      "Epoch: 18  Training loss = 1.4967  Test loss = 2.5332  \n",
      "\n",
      "Epoch: 19  Training loss = 1.4952  Test loss = 2.5313  \n",
      "\n",
      "Epoch: 20  Training loss = 1.4937  Test loss = 2.5296  \n",
      "\n",
      "Epoch: 21  Training loss = 1.4922  Test loss = 2.5281  \n",
      "\n",
      "Epoch: 22  Training loss = 1.4908  Test loss = 2.5269  \n",
      "\n",
      "Epoch: 23  Training loss = 1.4894  Test loss = 2.5258  \n",
      "\n",
      "Epoch: 24  Training loss = 1.4880  Test loss = 2.5248  \n",
      "\n",
      "Epoch: 25  Training loss = 1.4867  Test loss = 2.5240  \n",
      "\n",
      "Epoch: 26  Training loss = 1.4854  Test loss = 2.5234  \n",
      "\n",
      "Epoch: 27  Training loss = 1.4842  Test loss = 2.5228  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYGRhT5IkGTD+ARaA8+rQ31okAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc209b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naDIisQdBEDQZkdiDIAiajEjsQRAETcZPILkDnaYVxvYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcc21fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 2.52279479623 \n",
      "Fixed scheme MAE:  1.69516916969\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 1.4842  Test loss = 3.3630  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 1.5413  Test loss = 1.7658  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 1.5399  Test loss = 0.1305  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 1.5289  Test loss = 0.2682  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 1.3637  Test loss = 1.3407  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 1.3007  Test loss = 0.7680  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 1.2856  Test loss = 1.1963  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 1.2898  Test loss = 1.2230  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 1.2225  Test loss = 2.1051  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 1.2477  Test loss = 0.4894  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 1.2142  Test loss = 1.0110  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 1.2202  Test loss = 0.3717  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 1.1570  Test loss = 0.1889  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 1.1569  Test loss = 2.2139  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 1.1878  Test loss = 3.5966  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 1.2677  Test loss = 5.7912  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 1.3139  Test loss = 2.8352  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 1.3600  Test loss = 0.1340  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 1.3595  Test loss = 0.3414  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 1.2683  Test loss = 0.6357  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 1.1927  Test loss = 1.8385  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 1.2132  Test loss = 3.2906  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 1.2749  Test loss = 0.8357  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 1.2779  Test loss = 0.4804  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 1.2332  Test loss = 0.2728  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 1.2328  Test loss = 0.9108  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 1.2365  Test loss = 0.7745  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 1.2274  Test loss = 1.4331  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 1.1609  Test loss = 0.0779  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 1.1560  Test loss = 0.4346  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 1.1519  Test loss = 4.1500  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 1.2284  Test loss = 1.4100  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 1.1931  Test loss = 0.2418  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 1.1915  Test loss = 1.1316  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 1.1722  Test loss = 1.0843  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 1.1799  Test loss = 4.0095  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 1.2068  Test loss = 1.4160  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 1.1739  Test loss = 1.2781  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 1.1838  Test loss = 0.0123  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 1.1823  Test loss = 2.6745  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 1.1927  Test loss = 1.8005  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 1.2134  Test loss = 1.7778  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 1.2333  Test loss = 3.6574  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 1.3135  Test loss = 12.2369  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 1.9944  Test loss = 5.5481  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.1097  Test loss = 0.5639  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.1109  Test loss = 0.3181  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.1112  Test loss = 0.1941  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 1.9367  Test loss = 2.4645  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 1.9570  Test loss = 3.6184  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 2.0072  Test loss = 1.7847  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 2.0181  Test loss = 0.8702  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 1.9507  Test loss = 1.0483  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 1.9543  Test loss = 2.8363  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 1.9856  Test loss = 1.5687  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 1.9946  Test loss = 0.9385  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 1.9260  Test loss = 1.0670  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 1.9302  Test loss = 0.3760  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 1.9306  Test loss = 1.3173  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 1.9374  Test loss = 0.7898  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.8979  Test loss = 1.6913  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.9091  Test loss = 2.5933  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 1.9332  Test loss = 0.1340  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 1.9317  Test loss = 1.3722  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 1.9022  Test loss = 0.2557  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 1.9007  Test loss = 0.0912  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.8889  Test loss = 0.4858  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 1.8876  Test loss = 2.7535  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 1.8926  Test loss = 3.3560  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 1.9339  Test loss = 0.3127  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 1.9263  Test loss = 0.7605  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 1.9283  Test loss = 2.2239  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 1.9053  Test loss = 2.1701  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 1.9240  Test loss = 0.5364  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 1.9206  Test loss = 0.4868  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 1.9171  Test loss = 0.9990  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 1.8651  Test loss = 1.2996  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEPQY/wBNoICpO3OhLwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd038208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.38313467898 \n",
      "Updating scheme MAE:  1.60812008642\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_test_fixed.csv\", yhat_test_fixed)\n",
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_update.csv\", yhat_update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
