{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"4Q/32_units/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 500\n",
    "learning_rate = 1e-3\n",
    "batch_size = 5\n",
    "early_stop_iters = 10\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 32 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag4',\n",
    "                                       'inflation.lag5',\n",
    "                                       'inflation.lag6',\n",
    "                                       'inflation.lag7']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag4',\n",
    "                                   'unemp.lag5',\n",
    "                                   'unemp.lag6',\n",
    "                                   'unemp.lag7']])\n",
    "train_4lag_oil = np.array(train[['oil.lag4',\n",
    "                                 'oil.lag5',\n",
    "                                 'oil.lag6',\n",
    "                                 'oil.lag7']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag4',\n",
    "                                     'inflation.lag5',\n",
    "                                     'inflation.lag6',\n",
    "                                     'inflation.lag7']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag4',\n",
    "                                 'unemp.lag5',\n",
    "                                 'unemp.lag6',\n",
    "                                 'unemp.lag7']])\n",
    "test_4lag_oil = np.array(test[['oil.lag4',\n",
    "                               'oil.lag5',\n",
    "                               'oil.lag6',\n",
    "                               'oil.lag7']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 32 \n",
      "Learning rate = 0.001 \n",
      "Epochs = 500 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 10 \n",
      "Learning rate = 0.001\n",
      "Fold: 1  Epoch: 1  Training loss = 3.2560  Validation loss = 3.4240  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 3.0247  Validation loss = 2.9833  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 2.8956  Validation loss = 2.7145  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 2.8292  Validation loss = 2.5541  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 2.7718  Validation loss = 2.4063  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 2.7047  Validation loss = 2.2082  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 2.6754  Validation loss = 2.1152  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 2.6550  Validation loss = 2.0639  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 2.6305  Validation loss = 1.9631  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 2.6116  Validation loss = 1.8847  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 2.5923  Validation loss = 1.8001  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 2.5850  Validation loss = 1.7572  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 2.5769  Validation loss = 1.7100  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 2.5772  Validation loss = 1.7473  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 2.5736  Validation loss = 1.7703  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 2.5655  Validation loss = 1.7377  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 2.5579  Validation loss = 1.7044  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 2.5445  Validation loss = 1.6573  \n",
      "\n",
      "Fold: 1  Epoch: 19  Training loss = 2.5417  Validation loss = 1.6595  \n",
      "\n",
      "Fold: 1  Epoch: 20  Training loss = 2.5430  Validation loss = 1.6980  \n",
      "\n",
      "Fold: 1  Epoch: 21  Training loss = 2.5432  Validation loss = 1.7224  \n",
      "\n",
      "Fold: 1  Epoch: 22  Training loss = 2.5360  Validation loss = 1.6683  \n",
      "\n",
      "Fold: 1  Epoch: 23  Training loss = 2.5287  Validation loss = 1.6247  \n",
      "\n",
      "Fold: 1  Epoch: 24  Training loss = 2.5201  Validation loss = 1.5704  \n",
      "\n",
      "Fold: 1  Epoch: 25  Training loss = 2.5106  Validation loss = 1.5112  \n",
      "\n",
      "Fold: 1  Epoch: 26  Training loss = 2.5064  Validation loss = 1.4715  \n",
      "\n",
      "Fold: 1  Epoch: 27  Training loss = 2.5004  Validation loss = 1.4597  \n",
      "\n",
      "Fold: 1  Epoch: 28  Training loss = 2.4957  Validation loss = 1.4658  \n",
      "\n",
      "Fold: 1  Epoch: 29  Training loss = 2.4922  Validation loss = 1.5361  \n",
      "\n",
      "Fold: 1  Epoch: 30  Training loss = 2.4909  Validation loss = 1.6070  \n",
      "\n",
      "Fold: 1  Epoch: 31  Training loss = 2.4904  Validation loss = 1.6247  \n",
      "\n",
      "Fold: 1  Epoch: 32  Training loss = 2.4863  Validation loss = 1.6146  \n",
      "\n",
      "Fold: 1  Epoch: 33  Training loss = 2.4761  Validation loss = 1.4889  \n",
      "\n",
      "Fold: 1  Epoch: 34  Training loss = 2.4739  Validation loss = 1.5607  \n",
      "\n",
      "Fold: 1  Epoch: 35  Training loss = 2.4750  Validation loss = 1.5963  \n",
      "\n",
      "Fold: 1  Epoch: 36  Training loss = 2.4681  Validation loss = 1.5224  \n",
      "\n",
      "Fold: 1  Epoch: 37  Training loss = 2.4608  Validation loss = 1.5311  \n",
      "\n",
      "Fold: 1  Epoch: 38  Training loss = 2.4565  Validation loss = 1.5761  \n",
      "\n",
      "Fold: 1  Epoch: 39  Training loss = 2.4507  Validation loss = 1.4965  \n",
      "\n",
      "Fold: 1  Epoch: 40  Training loss = 2.4486  Validation loss = 1.3251  \n",
      "\n",
      "Fold: 1  Epoch: 41  Training loss = 2.4379  Validation loss = 1.5203  \n",
      "\n",
      "Fold: 1  Epoch: 42  Training loss = 2.4304  Validation loss = 1.4918  \n",
      "\n",
      "Fold: 1  Epoch: 43  Training loss = 2.4256  Validation loss = 1.5571  \n",
      "\n",
      "Fold: 1  Epoch: 44  Training loss = 2.4180  Validation loss = 1.5581  \n",
      "\n",
      "Fold: 1  Epoch: 45  Training loss = 2.4150  Validation loss = 1.6001  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 40  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.3603  Validation loss = 2.2720  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.3478  Validation loss = 2.2149  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.3431  Validation loss = 2.1610  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.3397  Validation loss = 2.1291  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.3322  Validation loss = 2.1552  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.3253  Validation loss = 2.1738  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.3234  Validation loss = 2.1482  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.3195  Validation loss = 2.2312  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.3148  Validation loss = 2.2030  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.3092  Validation loss = 2.2289  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.3105  Validation loss = 2.1532  \n",
      "\n",
      "Fold: 2  Epoch: 12  Training loss = 2.2978  Validation loss = 2.2129  \n",
      "\n",
      "Fold: 2  Epoch: 13  Training loss = 2.2910  Validation loss = 2.2365  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 4  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.5184  Validation loss = 3.5600  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.5036  Validation loss = 3.5641  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.4945  Validation loss = 3.5780  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.4889  Validation loss = 3.5509  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.4861  Validation loss = 3.5536  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.4799  Validation loss = 3.5263  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.4789  Validation loss = 3.5241  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.4749  Validation loss = 3.4930  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.4697  Validation loss = 3.4617  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.4665  Validation loss = 3.4815  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.4624  Validation loss = 3.4282  \n",
      "\n",
      "Fold: 3  Epoch: 12  Training loss = 1.4616  Validation loss = 3.4682  \n",
      "\n",
      "Fold: 3  Epoch: 13  Training loss = 1.4590  Validation loss = 3.4079  \n",
      "\n",
      "Fold: 3  Epoch: 14  Training loss = 1.4564  Validation loss = 3.3707  \n",
      "\n",
      "Fold: 3  Epoch: 15  Training loss = 1.4551  Validation loss = 3.3659  \n",
      "\n",
      "Fold: 3  Epoch: 16  Training loss = 1.4527  Validation loss = 3.3337  \n",
      "\n",
      "Fold: 3  Epoch: 17  Training loss = 1.4511  Validation loss = 3.3082  \n",
      "\n",
      "Fold: 3  Epoch: 18  Training loss = 1.4489  Validation loss = 3.3465  \n",
      "\n",
      "Fold: 3  Epoch: 19  Training loss = 1.4477  Validation loss = 3.3542  \n",
      "\n",
      "Fold: 3  Epoch: 20  Training loss = 1.4460  Validation loss = 3.3342  \n",
      "\n",
      "Fold: 3  Epoch: 21  Training loss = 1.4475  Validation loss = 3.2504  \n",
      "\n",
      "Fold: 3  Epoch: 22  Training loss = 1.4450  Validation loss = 3.2810  \n",
      "\n",
      "Fold: 3  Epoch: 23  Training loss = 1.4431  Validation loss = 3.2961  \n",
      "\n",
      "Fold: 3  Epoch: 24  Training loss = 1.4427  Validation loss = 3.3087  \n",
      "\n",
      "Fold: 3  Epoch: 25  Training loss = 1.4434  Validation loss = 3.2670  \n",
      "\n",
      "Fold: 3  Epoch: 26  Training loss = 1.4401  Validation loss = 3.3256  \n",
      "\n",
      "Fold: 3  Epoch: 27  Training loss = 1.4391  Validation loss = 3.3328  \n",
      "\n",
      "Fold: 3  Epoch: 28  Training loss = 1.4402  Validation loss = 3.2470  \n",
      "\n",
      "Fold: 3  Epoch: 29  Training loss = 1.4372  Validation loss = 3.2624  \n",
      "\n",
      "Fold: 3  Epoch: 30  Training loss = 1.4345  Validation loss = 3.2931  \n",
      "\n",
      "Fold: 3  Epoch: 31  Training loss = 1.4344  Validation loss = 3.2638  \n",
      "\n",
      "Fold: 3  Epoch: 32  Training loss = 1.4335  Validation loss = 3.2603  \n",
      "\n",
      "Fold: 3  Epoch: 33  Training loss = 1.4330  Validation loss = 3.2492  \n",
      "\n",
      "Fold: 3  Epoch: 34  Training loss = 1.4319  Validation loss = 3.2382  \n",
      "\n",
      "Fold: 3  Epoch: 35  Training loss = 1.4306  Validation loss = 3.2316  \n",
      "\n",
      "Fold: 3  Epoch: 36  Training loss = 1.4279  Validation loss = 3.2733  \n",
      "\n",
      "Fold: 3  Epoch: 37  Training loss = 1.4272  Validation loss = 3.2914  \n",
      "\n",
      "Fold: 3  Epoch: 38  Training loss = 1.4266  Validation loss = 3.2518  \n",
      "\n",
      "Fold: 3  Epoch: 39  Training loss = 1.4259  Validation loss = 3.2620  \n",
      "\n",
      "Fold: 3  Epoch: 40  Training loss = 1.4254  Validation loss = 3.2662  \n",
      "\n",
      "Fold: 3  Epoch: 41  Training loss = 1.4251  Validation loss = 3.2404  \n",
      "\n",
      "Fold: 3  Epoch: 42  Training loss = 1.4283  Validation loss = 3.2027  \n",
      "\n",
      "Fold: 3  Epoch: 43  Training loss = 1.4251  Validation loss = 3.2269  \n",
      "\n",
      "Fold: 3  Epoch: 44  Training loss = 1.4226  Validation loss = 3.2378  \n",
      "\n",
      "Fold: 3  Epoch: 45  Training loss = 1.4204  Validation loss = 3.3015  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 42  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.5399  Validation loss = 4.4862  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.5332  Validation loss = 4.4196  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.5323  Validation loss = 4.4366  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.5273  Validation loss = 4.3728  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.5236  Validation loss = 4.3027  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.5192  Validation loss = 4.3229  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.5176  Validation loss = 4.3013  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.5123  Validation loss = 4.2597  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.5089  Validation loss = 4.2806  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 1.5042  Validation loss = 4.2116  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.5029  Validation loss = 4.2474  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 1.4991  Validation loss = 4.2014  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.4962  Validation loss = 4.2246  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 1.4934  Validation loss = 4.2165  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.4910  Validation loss = 4.1956  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 1.4894  Validation loss = 4.1378  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 1.4871  Validation loss = 4.1103  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 1.4855  Validation loss = 4.1040  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 1.4833  Validation loss = 4.1228  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 1.4803  Validation loss = 4.0877  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 1.4796  Validation loss = 4.0806  \n",
      "\n",
      "Fold: 4  Epoch: 22  Training loss = 1.4780  Validation loss = 4.0971  \n",
      "\n",
      "Fold: 4  Epoch: 23  Training loss = 1.4761  Validation loss = 4.1317  \n",
      "\n",
      "Fold: 4  Epoch: 24  Training loss = 1.4747  Validation loss = 4.1318  \n",
      "\n",
      "Fold: 4  Epoch: 25  Training loss = 1.4749  Validation loss = 3.9982  \n",
      "\n",
      "Fold: 4  Epoch: 26  Training loss = 1.4694  Validation loss = 4.0353  \n",
      "\n",
      "Fold: 4  Epoch: 27  Training loss = 1.4667  Validation loss = 4.1003  \n",
      "\n",
      "Fold: 4  Epoch: 28  Training loss = 1.4661  Validation loss = 4.1368  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 25  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.7240  Validation loss = 4.3961  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.7098  Validation loss = 4.3368  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.6934  Validation loss = 4.2512  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.6828  Validation loss = 4.2249  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.6683  Validation loss = 4.1449  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 1.6596  Validation loss = 4.0919  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 1.6545  Validation loss = 4.1390  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.6436  Validation loss = 4.0881  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 1.6341  Validation loss = 4.0392  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.6271  Validation loss = 4.0517  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 1.6204  Validation loss = 4.0272  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 1.6072  Validation loss = 3.9389  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 1.5960  Validation loss = 3.9461  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.5867  Validation loss = 3.9163  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 1.5770  Validation loss = 3.8989  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 1.5708  Validation loss = 3.9560  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 1.5657  Validation loss = 3.9585  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 1.5541  Validation loss = 3.8330  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 1.5407  Validation loss = 3.9047  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 1.5309  Validation loss = 3.9055  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 1.5260  Validation loss = 3.9294  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 1.5069  Validation loss = 3.7650  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 1.5001  Validation loss = 3.7266  \n",
      "\n",
      "Fold: 5  Epoch: 24  Training loss = 1.4945  Validation loss = 3.7288  \n",
      "\n",
      "Fold: 5  Epoch: 25  Training loss = 1.4890  Validation loss = 3.6869  \n",
      "\n",
      "Fold: 5  Epoch: 26  Training loss = 1.4875  Validation loss = 3.6249  \n",
      "\n",
      "Fold: 5  Epoch: 27  Training loss = 1.4729  Validation loss = 3.7412  \n",
      "\n",
      "Fold: 5  Epoch: 28  Training loss = 1.4614  Validation loss = 3.7076  \n",
      "\n",
      "Fold: 5  Epoch: 29  Training loss = 1.4541  Validation loss = 3.6986  \n",
      "\n",
      "Fold: 5  Epoch: 30  Training loss = 1.4532  Validation loss = 3.7496  \n",
      "\n",
      "Fold: 5  Epoch: 31  Training loss = 1.4369  Validation loss = 3.5831  \n",
      "\n",
      "Fold: 5  Epoch: 32  Training loss = 1.4291  Validation loss = 3.5478  \n",
      "\n",
      "Fold: 5  Epoch: 33  Training loss = 1.4227  Validation loss = 3.5562  \n",
      "\n",
      "Fold: 5  Epoch: 34  Training loss = 1.4154  Validation loss = 3.5394  \n",
      "\n",
      "Fold: 5  Epoch: 35  Training loss = 1.4089  Validation loss = 3.6002  \n",
      "\n",
      "Fold: 5  Epoch: 36  Training loss = 1.3979  Validation loss = 3.5626  \n",
      "\n",
      "Fold: 5  Epoch: 37  Training loss = 1.3850  Validation loss = 3.4155  \n",
      "\n",
      "Fold: 5  Epoch: 38  Training loss = 1.3781  Validation loss = 3.3627  \n",
      "\n",
      "Fold: 5  Epoch: 39  Training loss = 1.3702  Validation loss = 3.3876  \n",
      "\n",
      "Fold: 5  Epoch: 40  Training loss = 1.3671  Validation loss = 3.3579  \n",
      "\n",
      "Fold: 5  Epoch: 41  Training loss = 1.3597  Validation loss = 3.4253  \n",
      "\n",
      "Fold: 5  Epoch: 42  Training loss = 1.3483  Validation loss = 3.3677  \n",
      "\n",
      "Fold: 5  Epoch: 43  Training loss = 1.3425  Validation loss = 3.3586  \n",
      "\n",
      "Fold: 5  Epoch: 44  Training loss = 1.3344  Validation loss = 3.3205  \n",
      "\n",
      "Fold: 5  Epoch: 45  Training loss = 1.3307  Validation loss = 3.3022  \n",
      "\n",
      "Fold: 5  Epoch: 46  Training loss = 1.3239  Validation loss = 3.2929  \n",
      "\n",
      "Fold: 5  Epoch: 47  Training loss = 1.3230  Validation loss = 3.3152  \n",
      "\n",
      "Fold: 5  Epoch: 48  Training loss = 1.3175  Validation loss = 3.2821  \n",
      "\n",
      "Fold: 5  Epoch: 49  Training loss = 1.3133  Validation loss = 3.2324  \n",
      "\n",
      "Fold: 5  Epoch: 50  Training loss = 1.3091  Validation loss = 3.2099  \n",
      "\n",
      "Fold: 5  Epoch: 51  Training loss = 1.2954  Validation loss = 3.2689  \n",
      "\n",
      "Fold: 5  Epoch: 52  Training loss = 1.2895  Validation loss = 3.2822  \n",
      "\n",
      "Fold: 5  Epoch: 53  Training loss = 1.2839  Validation loss = 3.1821  \n",
      "\n",
      "Fold: 5  Epoch: 54  Training loss = 1.2853  Validation loss = 3.1037  \n",
      "\n",
      "Fold: 5  Epoch: 55  Training loss = 1.2726  Validation loss = 3.1676  \n",
      "\n",
      "Fold: 5  Epoch: 56  Training loss = 1.2675  Validation loss = 3.1839  \n",
      "\n",
      "Fold: 5  Epoch: 57  Training loss = 1.2677  Validation loss = 3.1295  \n",
      "\n",
      "Fold: 5  Epoch: 58  Training loss = 1.2723  Validation loss = 3.1312  \n",
      "\n",
      "Fold: 5  Epoch: 59  Training loss = 1.2650  Validation loss = 3.1021  \n",
      "\n",
      "Fold: 5  Epoch: 60  Training loss = 1.2624  Validation loss = 3.1123  \n",
      "\n",
      "Fold: 5  Epoch: 61  Training loss = 1.2526  Validation loss = 3.1081  \n",
      "\n",
      "Fold: 5  Epoch: 62  Training loss = 1.2451  Validation loss = 3.1100  \n",
      "\n",
      "Fold: 5  Epoch: 63  Training loss = 1.2428  Validation loss = 3.1108  \n",
      "\n",
      "Fold: 5  Epoch: 64  Training loss = 1.2396  Validation loss = 3.1093  \n",
      "\n",
      "Fold: 5  Epoch: 65  Training loss = 1.2397  Validation loss = 3.1349  \n",
      "\n",
      "Fold: 5  Epoch: 66  Training loss = 1.2311  Validation loss = 3.0776  \n",
      "\n",
      "Fold: 5  Epoch: 67  Training loss = 1.2289  Validation loss = 3.0933  \n",
      "\n",
      "Fold: 5  Epoch: 68  Training loss = 1.2264  Validation loss = 3.1154  \n",
      "\n",
      "Fold: 5  Epoch: 69  Training loss = 1.2225  Validation loss = 3.0913  \n",
      "\n",
      "Fold: 5  Epoch: 70  Training loss = 1.2195  Validation loss = 3.1331  \n",
      "\n",
      "Fold: 5  Epoch: 71  Training loss = 1.2217  Validation loss = 3.1731  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 66  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 1.3640  Validation loss = 1.1101  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 1.3270  Validation loss = 1.0652  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 1.3033  Validation loss = 1.0358  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 1.2905  Validation loss = 1.0749  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 1.2784  Validation loss = 1.0819  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 1.2685  Validation loss = 1.1139  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 1.2573  Validation loss = 1.1102  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 1.2411  Validation loss = 1.0394  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 1.2466  Validation loss = 0.9519  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 1.2436  Validation loss = 0.9421  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 1.2324  Validation loss = 1.1388  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 10  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 1.2410  Validation loss = 1.6293  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 1.2337  Validation loss = 1.6337  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 1.2279  Validation loss = 1.6599  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.2309  Validation loss = 1.6172  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 1.2198  Validation loss = 1.7025  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 1.2177  Validation loss = 1.6795  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 1.2087  Validation loss = 1.7638  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 1.2106  Validation loss = 1.7100  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 1.2149  Validation loss = 1.6918  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 1.2021  Validation loss = 1.7701  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 1.2016  Validation loss = 1.7554  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 1.2052  Validation loss = 1.8177  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 4  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 1.2058  Validation loss = 6.4069  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 1.2004  Validation loss = 6.3459  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 1.1950  Validation loss = 6.2779  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 1.1941  Validation loss = 6.2283  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 1.1903  Validation loss = 6.3433  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 1.1854  Validation loss = 6.3332  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 1.1928  Validation loss = 6.4755  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 1.1823  Validation loss = 6.2846  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 1.1948  Validation loss = 6.0291  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 1.1744  Validation loss = 6.2400  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 1.1742  Validation loss = 6.1825  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 1.1675  Validation loss = 6.3306  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 1.1734  Validation loss = 6.4133  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 1.1672  Validation loss = 6.2258  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 1.1802  Validation loss = 6.4248  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 1.1797  Validation loss = 6.4037  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 1.1661  Validation loss = 6.1712  \n",
      "\n",
      "Fold: 8  Epoch: 18  Training loss = 1.1681  Validation loss = 6.0952  \n",
      "\n",
      "Fold: 8  Epoch: 19  Training loss = 1.1618  Validation loss = 6.1250  \n",
      "\n",
      "Fold: 8  Epoch: 20  Training loss = 1.1528  Validation loss = 6.3446  \n",
      "\n",
      "Fold: 8  Epoch: 21  Training loss = 1.1521  Validation loss = 6.3682  \n",
      "\n",
      "Fold: 8  Epoch: 22  Training loss = 1.1484  Validation loss = 6.3854  \n",
      "\n",
      "Fold: 8  Epoch: 23  Training loss = 1.1572  Validation loss = 6.4813  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 9  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 1.8136  Validation loss = 7.7295  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 1.7946  Validation loss = 7.7355  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 1.7605  Validation loss = 7.4103  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 1.7159  Validation loss = 7.4026  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 1.7140  Validation loss = 7.3473  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 1.6718  Validation loss = 7.5957  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 1.6810  Validation loss = 7.5124  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 1.6761  Validation loss = 7.4943  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 1.6503  Validation loss = 7.4764  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 1.6282  Validation loss = 7.4326  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 1.6979  Validation loss = 7.6444  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 1.6441  Validation loss = 7.5328  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 1.6193  Validation loss = 7.4104  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 1.6442  Validation loss = 7.5627  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 1.6290  Validation loss = 7.5185  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 1.6145  Validation loss = 7.5549  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 1.6577  Validation loss = 7.6895  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 5  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 2.3677  Validation loss = 4.8983  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 2.3874  Validation loss = 4.6344  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 2.3027  Validation loss = 4.6577  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 2.2996  Validation loss = 4.6288  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 2.2912  Validation loss = 4.5945  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 2.2742  Validation loss = 4.6738  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 2.2940  Validation loss = 4.9792  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 2.2360  Validation loss = 4.5483  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 2.2564  Validation loss = 4.8042  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 2.2368  Validation loss = 4.7368  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 2.2026  Validation loss = 4.3498  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 2.2287  Validation loss = 4.1554  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 2.2185  Validation loss = 4.3872  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 2.1576  Validation loss = 4.5670  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 2.1616  Validation loss = 4.3481  \n",
      "\n",
      "Fold: 10  Epoch: 16  Training loss = 2.1481  Validation loss = 4.7418  \n",
      "\n",
      "Fold: 10  Epoch: 17  Training loss = 2.1429  Validation loss = 4.7004  \n",
      "\n",
      "Fold: 10  Epoch: 18  Training loss = 2.1301  Validation loss = 4.7643  \n",
      "\n",
      "Fold: 10  Epoch: 19  Training loss = 2.1456  Validation loss = 4.5961  \n",
      "\n",
      "Fold: 10  Epoch: 20  Training loss = 2.1322  Validation loss = 4.2340  \n",
      "\n",
      "Fold: 10  Epoch: 21  Training loss = 2.1188  Validation loss = 4.0198  \n",
      "\n",
      "Fold: 10  Epoch: 22  Training loss = 2.0977  Validation loss = 4.1851  \n",
      "\n",
      "Fold: 10  Epoch: 23  Training loss = 2.1092  Validation loss = 4.1158  \n",
      "\n",
      "Fold: 10  Epoch: 24  Training loss = 2.1184  Validation loss = 4.1961  \n",
      "\n",
      "Fold: 10  Epoch: 25  Training loss = 2.0835  Validation loss = 3.9929  \n",
      "\n",
      "Fold: 10  Epoch: 26  Training loss = 2.0835  Validation loss = 3.8293  \n",
      "\n",
      "Fold: 10  Epoch: 27  Training loss = 2.0626  Validation loss = 3.9126  \n",
      "\n",
      "Fold: 10  Epoch: 28  Training loss = 2.0874  Validation loss = 4.2876  \n",
      "\n",
      "Fold: 10  Epoch: 29  Training loss = 2.0762  Validation loss = 3.7763  \n",
      "\n",
      "Fold: 10  Epoch: 30  Training loss = 2.0578  Validation loss = 4.1250  \n",
      "\n",
      "Fold: 10  Epoch: 31  Training loss = 2.0677  Validation loss = 4.0459  \n",
      "\n",
      "Fold: 10  Epoch: 32  Training loss = 2.0940  Validation loss = 4.0747  \n",
      "\n",
      "Fold: 10  Epoch: 33  Training loss = 2.0333  Validation loss = 4.0539  \n",
      "\n",
      "Fold: 10  Epoch: 34  Training loss = 2.0403  Validation loss = 4.0662  \n",
      "\n",
      "Fold: 10  Epoch: 35  Training loss = 2.0352  Validation loss = 3.9582  \n",
      "\n",
      "Fold: 10  Epoch: 36  Training loss = 2.0272  Validation loss = 4.0343  \n",
      "\n",
      "Fold: 10  Epoch: 37  Training loss = 2.0259  Validation loss = 4.0149  \n",
      "\n",
      "Fold: 10  Epoch: 38  Training loss = 2.0094  Validation loss = 4.2732  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 29  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 2.1267  Validation loss = 2.3071  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 2.0836  Validation loss = 2.2591  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 2.0667  Validation loss = 2.1881  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 2.0941  Validation loss = 2.1815  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 2.0552  Validation loss = 2.2913  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 2.0362  Validation loss = 2.4327  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 2.0144  Validation loss = 2.4142  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 2.0189  Validation loss = 2.3586  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 1.9991  Validation loss = 2.2533  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 1.9874  Validation loss = 2.2702  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 1.9784  Validation loss = 2.2697  \n",
      "\n",
      "Fold: 11  Epoch: 12  Training loss = 1.9794  Validation loss = 2.5963  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 4  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 2.0171  Validation loss = 1.1191  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 2.0041  Validation loss = 1.0632  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 1.9929  Validation loss = 0.9908  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 2.0227  Validation loss = 0.9765  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 2.1734  Validation loss = 1.0651  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 1.9874  Validation loss = 0.9501  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 1.9950  Validation loss = 0.9972  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 1.9727  Validation loss = 0.9658  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 1.9895  Validation loss = 0.9766  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 1.9833  Validation loss = 0.9463  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 1.9679  Validation loss = 0.9342  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 1.9901  Validation loss = 0.9555  \n",
      "\n",
      "Fold: 12  Epoch: 13  Training loss = 1.9678  Validation loss = 0.9553  \n",
      "\n",
      "Fold: 12  Epoch: 14  Training loss = 1.9399  Validation loss = 0.9199  \n",
      "\n",
      "Fold: 12  Epoch: 15  Training loss = 1.9494  Validation loss = 0.8809  \n",
      "\n",
      "Fold: 12  Epoch: 16  Training loss = 1.9339  Validation loss = 0.8936  \n",
      "\n",
      "Fold: 12  Epoch: 17  Training loss = 1.9316  Validation loss = 0.9198  \n",
      "\n",
      "Fold: 12  Epoch: 18  Training loss = 1.9395  Validation loss = 0.8427  \n",
      "\n",
      "Fold: 12  Epoch: 19  Training loss = 1.9078  Validation loss = 0.9072  \n",
      "\n",
      "Fold: 12  Epoch: 20  Training loss = 1.9491  Validation loss = 0.8652  \n",
      "\n",
      "Fold: 12  Epoch: 21  Training loss = 1.9042  Validation loss = 0.8864  \n",
      "\n",
      "Fold: 12  Epoch: 22  Training loss = 1.9148  Validation loss = 0.8952  \n",
      "\n",
      "Fold: 12  Epoch: 23  Training loss = 1.8924  Validation loss = 0.8574  \n",
      "\n",
      "Fold: 12  Epoch: 24  Training loss = 1.8833  Validation loss = 0.9137  \n",
      "\n",
      "Fold: 12  Epoch: 25  Training loss = 1.8678  Validation loss = 0.8710  \n",
      "\n",
      "Fold: 12  Epoch: 26  Training loss = 1.9108  Validation loss = 0.9504  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 18  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 1.8351  Validation loss = 3.4080  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 1.8257  Validation loss = 3.3244  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 1.8462  Validation loss = 3.4689  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 1.9735  Validation loss = 3.5132  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 1.8460  Validation loss = 3.0862  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 1.7915  Validation loss = 3.2103  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 1.8863  Validation loss = 3.3455  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 1.8435  Validation loss = 3.1052  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 1.8131  Validation loss = 3.0160  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 1.8011  Validation loss = 3.1119  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 1.7895  Validation loss = 3.1597  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 1.7634  Validation loss = 3.2992  \n",
      "\n",
      "Fold: 13  Epoch: 13  Training loss = 1.7676  Validation loss = 3.1045  \n",
      "\n",
      "Fold: 13  Epoch: 14  Training loss = 1.7690  Validation loss = 3.2354  \n",
      "\n",
      "Fold: 13  Epoch: 15  Training loss = 1.8693  Validation loss = 3.5257  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 9  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 1.9308  Validation loss = 6.3332  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 1.8984  Validation loss = 6.7077  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 1.9008  Validation loss = 6.1385  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 1.8968  Validation loss = 6.7254  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 1.8576  Validation loss = 6.3349  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 1.8927  Validation loss = 6.6379  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 1.8470  Validation loss = 6.4453  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 1.8808  Validation loss = 5.6664  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 1.8454  Validation loss = 6.0751  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 1.8610  Validation loss = 5.9444  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 1.8210  Validation loss = 6.3851  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 1.8700  Validation loss = 6.0827  \n",
      "\n",
      "Fold: 14  Epoch: 13  Training loss = 1.8063  Validation loss = 6.3161  \n",
      "\n",
      "Fold: 14  Epoch: 14  Training loss = 1.8650  Validation loss = 5.8933  \n",
      "\n",
      "Fold: 14  Epoch: 15  Training loss = 1.8262  Validation loss = 6.0002  \n",
      "\n",
      "Fold: 14  Epoch: 16  Training loss = 1.8091  Validation loss = 5.7532  \n",
      "\n",
      "Fold: 14  Epoch: 17  Training loss = 1.8490  Validation loss = 6.5539  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 8  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 2.2284  Validation loss = 5.7098  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 2.1855  Validation loss = 6.2619  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 2.2414  Validation loss = 5.5815  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 2.2346  Validation loss = 6.4140  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 2.3079  Validation loss = 6.7571  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 2.1809  Validation loss = 6.1518  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 2.1348  Validation loss = 6.2269  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 2.1786  Validation loss = 5.8744  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 2.1607  Validation loss = 6.0033  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 2.1801  Validation loss = 5.5673  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 2.1699  Validation loss = 5.7686  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 2.0859  Validation loss = 6.2811  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 2.2658  Validation loss = 6.5032  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 2.0474  Validation loss = 5.8280  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 2.0771  Validation loss = 5.5834  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 2.1823  Validation loss = 6.2198  \n",
      "\n",
      "Fold: 15  Epoch: 17  Training loss = 2.0895  Validation loss = 6.1308  \n",
      "\n",
      "Fold: 15  Epoch: 18  Training loss = 1.9973  Validation loss = 5.7499  \n",
      "\n",
      "Fold: 15  Epoch: 19  Training loss = 2.1900  Validation loss = 6.0654  \n",
      "\n",
      "Fold: 15  Epoch: 20  Training loss = 2.0243  Validation loss = 5.4849  \n",
      "\n",
      "Fold: 15  Epoch: 21  Training loss = 2.0958  Validation loss = 5.6601  \n",
      "\n",
      "Fold: 15  Epoch: 22  Training loss = 1.9498  Validation loss = 5.3734  \n",
      "\n",
      "Fold: 15  Epoch: 23  Training loss = 2.0547  Validation loss = 5.9895  \n",
      "\n",
      "Fold: 15  Epoch: 24  Training loss = 1.9727  Validation loss = 5.0549  \n",
      "\n",
      "Fold: 15  Epoch: 25  Training loss = 2.0721  Validation loss = 4.9732  \n",
      "\n",
      "Fold: 15  Epoch: 26  Training loss = 1.9828  Validation loss = 5.0235  \n",
      "\n",
      "Fold: 15  Epoch: 27  Training loss = 1.9939  Validation loss = 5.2045  \n",
      "\n",
      "Fold: 15  Epoch: 28  Training loss = 2.1258  Validation loss = 4.9150  \n",
      "\n",
      "Fold: 15  Epoch: 29  Training loss = 1.8877  Validation loss = 5.2756  \n",
      "\n",
      "Fold: 15  Epoch: 30  Training loss = 1.8660  Validation loss = 5.4844  \n",
      "\n",
      "Fold: 15  Epoch: 31  Training loss = 1.9048  Validation loss = 5.5167  \n",
      "\n",
      "Fold: 15  Epoch: 32  Training loss = 1.9898  Validation loss = 4.7449  \n",
      "\n",
      "Fold: 15  Epoch: 33  Training loss = 1.9032  Validation loss = 5.6508  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 32  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 2.3440  Validation loss = 5.3556  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 2.1943  Validation loss = 5.4413  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 2.4987  Validation loss = 5.6742  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 2.2428  Validation loss = 5.4233  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 2.2201  Validation loss = 4.8799  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 2.3618  Validation loss = 5.7111  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 2.2350  Validation loss = 5.5630  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 2.3750  Validation loss = 5.6778  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 2.1675  Validation loss = 5.6877  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 2.4505  Validation loss = 5.8679  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 2.2383  Validation loss = 5.4173  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 2.2945  Validation loss = 4.9640  \n",
      "\n",
      "Fold: 16  Epoch: 13  Training loss = 2.1394  Validation loss = 5.2064  \n",
      "\n",
      "Fold: 16  Epoch: 14  Training loss = 2.0818  Validation loss = 5.3159  \n",
      "\n",
      "Fold: 16  Epoch: 15  Training loss = 2.1406  Validation loss = 5.6082  \n",
      "\n",
      "Fold: 16  Epoch: 16  Training loss = 2.4327  Validation loss = 5.4536  \n",
      "\n",
      "Fold: 16  Epoch: 17  Training loss = 2.2963  Validation loss = 5.4185  \n",
      "\n",
      "Fold: 16  Epoch: 18  Training loss = 2.1664  Validation loss = 5.5157  \n",
      "\n",
      "Fold: 16  Epoch: 19  Training loss = 2.0718  Validation loss = 5.1724  \n",
      "\n",
      "Fold: 16  Epoch: 20  Training loss = 2.0455  Validation loss = 5.3620  \n",
      "\n",
      "Fold: 16  Epoch: 21  Training loss = 2.0235  Validation loss = 5.4682  \n",
      "\n",
      "Fold: 16  Epoch: 22  Training loss = 2.0012  Validation loss = 5.1593  \n",
      "\n",
      "Fold: 16  Epoch: 23  Training loss = 1.8950  Validation loss = 5.1544  \n",
      "\n",
      "Fold: 16  Epoch: 24  Training loss = 2.0086  Validation loss = 5.0007  \n",
      "\n",
      "Fold: 16  Epoch: 25  Training loss = 1.9857  Validation loss = 4.6365  \n",
      "\n",
      "Fold: 16  Epoch: 26  Training loss = 1.8709  Validation loss = 6.1190  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 25  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 2.1666  Validation loss = 4.0158  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 2.1223  Validation loss = 4.0373  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 2.1942  Validation loss = 4.0476  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 2.0915  Validation loss = 4.0506  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 2.1860  Validation loss = 4.0901  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 2.0395  Validation loss = 4.0707  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 2.1062  Validation loss = 4.0924  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 2.1287  Validation loss = 4.1060  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 2.3369  Validation loss = 4.0244  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 1.9407  Validation loss = 3.9873  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 2.1284  Validation loss = 4.1302  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 10  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 2.4464  Validation loss = 3.8417  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 2.2054  Validation loss = 3.4314  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 2.2365  Validation loss = 3.8246  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 2.1461  Validation loss = 3.6516  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 2.0125  Validation loss = 3.4313  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 2.0209  Validation loss = 3.0866  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 2.0032  Validation loss = 3.4030  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 2.0088  Validation loss = 3.3925  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 2.0194  Validation loss = 3.4226  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 2.0122  Validation loss = 3.4303  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 1.9803  Validation loss = 3.3777  \n",
      "\n",
      "Fold: 18  Epoch: 12  Training loss = 1.9641  Validation loss = 3.2814  \n",
      "\n",
      "Fold: 18  Epoch: 13  Training loss = 1.9825  Validation loss = 3.4880  \n",
      "\n",
      "Fold: 18  Epoch: 14  Training loss = 2.0378  Validation loss = 3.1927  \n",
      "\n",
      "Fold: 18  Epoch: 15  Training loss = 2.1309  Validation loss = 3.0058  \n",
      "\n",
      "Fold: 18  Epoch: 16  Training loss = 2.9405  Validation loss = 3.2302  \n",
      "\n",
      "Fold: 18  Epoch: 17  Training loss = 2.0285  Validation loss = 3.5871  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 15  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 2.0245  Validation loss = 3.1293  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 1.8882  Validation loss = 2.7636  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 1.9393  Validation loss = 2.5384  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 1.8853  Validation loss = 2.5906  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 1.8384  Validation loss = 2.2709  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 1.8120  Validation loss = 2.1355  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 1.8173  Validation loss = 2.5126  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 1.7820  Validation loss = 2.6235  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 2.0055  Validation loss = 2.2673  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 1.7372  Validation loss = 2.3152  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 1.7563  Validation loss = 2.3859  \n",
      "\n",
      "Fold: 19  Epoch: 12  Training loss = 1.7240  Validation loss = 2.4106  \n",
      "\n",
      "Fold: 19  Epoch: 13  Training loss = 1.7911  Validation loss = 2.4273  \n",
      "\n",
      "Fold: 19  Epoch: 14  Training loss = 1.6793  Validation loss = 2.1085  \n",
      "\n",
      "Fold: 19  Epoch: 15  Training loss = 1.8209  Validation loss = 2.2171  \n",
      "\n",
      "Fold: 19  Epoch: 16  Training loss = 1.7085  Validation loss = 2.2314  \n",
      "\n",
      "Fold: 19  Epoch: 17  Training loss = 1.8024  Validation loss = 2.2772  \n",
      "\n",
      "Fold: 19  Epoch: 18  Training loss = 1.9088  Validation loss = 2.2241  \n",
      "\n",
      "Fold: 19  Epoch: 19  Training loss = 1.7170  Validation loss = 2.1731  \n",
      "\n",
      "Fold: 19  Epoch: 20  Training loss = 1.7178  Validation loss = 2.1375  \n",
      "\n",
      "Fold: 19  Epoch: 21  Training loss = 1.7424  Validation loss = 2.1737  \n",
      "\n",
      "Fold: 19  Epoch: 22  Training loss = 1.6977  Validation loss = 2.2494  \n",
      "\n",
      "Fold: 19  Epoch: 23  Training loss = 1.7728  Validation loss = 2.0274  \n",
      "\n",
      "Fold: 19  Epoch: 24  Training loss = 1.6316  Validation loss = 2.0362  \n",
      "\n",
      "Fold: 19  Epoch: 25  Training loss = 1.6476  Validation loss = 1.9817  \n",
      "\n",
      "Fold: 19  Epoch: 26  Training loss = 1.7331  Validation loss = 2.2406  \n",
      "\n",
      "Fold: 19  Epoch: 27  Training loss = 1.6136  Validation loss = 2.1643  \n",
      "\n",
      "Fold: 19  Epoch: 28  Training loss = 1.6876  Validation loss = 2.0923  \n",
      "\n",
      "Fold: 19  Epoch: 29  Training loss = 1.6357  Validation loss = 2.1997  \n",
      "\n",
      "Fold: 19  Epoch: 30  Training loss = 1.6092  Validation loss = 2.1179  \n",
      "\n",
      "Fold: 19  Epoch: 31  Training loss = 1.6442  Validation loss = 2.1365  \n",
      "\n",
      "Fold: 19  Epoch: 32  Training loss = 1.5809  Validation loss = 2.1182  \n",
      "\n",
      "Fold: 19  Epoch: 33  Training loss = 1.5934  Validation loss = 2.0483  \n",
      "\n",
      "Fold: 19  Epoch: 34  Training loss = 1.5399  Validation loss = 2.1371  \n",
      "\n",
      "Fold: 19  Epoch: 35  Training loss = 1.5481  Validation loss = 2.1944  \n",
      "\n",
      "Fold: 19  Epoch: 36  Training loss = 1.6257  Validation loss = 2.0216  \n",
      "\n",
      "Fold: 19  Epoch: 37  Training loss = 1.5662  Validation loss = 2.1769  \n",
      "\n",
      "Fold: 19  Epoch: 38  Training loss = 1.5130  Validation loss = 2.2995  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 25  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 1.5589  Validation loss = 3.3166  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 1.5833  Validation loss = 3.0164  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 1.5473  Validation loss = 3.1747  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 1.5802  Validation loss = 3.0211  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 1.5886  Validation loss = 3.4705  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 1.5469  Validation loss = 3.5094  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 1.5040  Validation loss = 3.4861  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 1.5586  Validation loss = 3.5688  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 1.4849  Validation loss = 3.5242  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 1.5007  Validation loss = 3.5251  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 1.4701  Validation loss = 3.4373  \n",
      "\n",
      "Fold: 20  Epoch: 12  Training loss = 1.4844  Validation loss = 3.4456  \n",
      "\n",
      "Fold: 20  Epoch: 13  Training loss = 1.4922  Validation loss = 3.4273  \n",
      "\n",
      "Fold: 20  Epoch: 14  Training loss = 1.5633  Validation loss = 3.2002  \n",
      "\n",
      "Fold: 20  Epoch: 15  Training loss = 1.4649  Validation loss = 3.4280  \n",
      "\n",
      "Fold: 20  Epoch: 16  Training loss = 1.6982  Validation loss = 3.8666  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 2  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 1.6078  Validation loss = 3.6235  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 1.6045  Validation loss = 3.2027  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 1.6021  Validation loss = 3.2070  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 1.5287  Validation loss = 3.0349  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 1.5591  Validation loss = 2.8847  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 1.5081  Validation loss = 2.7515  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 1.5547  Validation loss = 3.2606  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 1.4642  Validation loss = 2.8542  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 1.4752  Validation loss = 2.9749  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 1.4729  Validation loss = 2.9155  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 1.5324  Validation loss = 3.0315  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 1.4783  Validation loss = 2.7527  \n",
      "\n",
      "Fold: 21  Epoch: 13  Training loss = 1.4562  Validation loss = 3.0080  \n",
      "\n",
      "Fold: 21  Epoch: 14  Training loss = 1.4466  Validation loss = 2.8187  \n",
      "\n",
      "Fold: 21  Epoch: 15  Training loss = 1.4825  Validation loss = 2.5520  \n",
      "\n",
      "Fold: 21  Epoch: 16  Training loss = 1.4417  Validation loss = 2.6644  \n",
      "\n",
      "Fold: 21  Epoch: 17  Training loss = 1.4461  Validation loss = 2.5415  \n",
      "\n",
      "Fold: 21  Epoch: 18  Training loss = 1.4718  Validation loss = 2.4862  \n",
      "\n",
      "Fold: 21  Epoch: 19  Training loss = 1.3909  Validation loss = 2.5549  \n",
      "\n",
      "Fold: 21  Epoch: 20  Training loss = 1.3756  Validation loss = 2.5123  \n",
      "\n",
      "Fold: 21  Epoch: 21  Training loss = 1.4119  Validation loss = 2.3845  \n",
      "\n",
      "Fold: 21  Epoch: 22  Training loss = 1.4595  Validation loss = 2.4985  \n",
      "\n",
      "Fold: 21  Epoch: 23  Training loss = 1.3935  Validation loss = 2.4586  \n",
      "\n",
      "Fold: 21  Epoch: 24  Training loss = 1.4641  Validation loss = 2.8572  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 21  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 1.4924  Validation loss = 1.3651  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 1.4497  Validation loss = 1.2044  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 1.4602  Validation loss = 1.1379  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 1.4277  Validation loss = 1.5941  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 1.4482  Validation loss = 1.4690  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 1.3974  Validation loss = 1.2452  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 1.4521  Validation loss = 1.4386  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 1.3892  Validation loss = 1.1858  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 1.3764  Validation loss = 1.0486  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 1.3783  Validation loss = 1.1287  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 1.3617  Validation loss = 0.9690  \n",
      "\n",
      "Fold: 22  Epoch: 12  Training loss = 1.3431  Validation loss = 1.2591  \n",
      "\n",
      "Fold: 22  Epoch: 13  Training loss = 1.3562  Validation loss = 1.0068  \n",
      "\n",
      "Fold: 22  Epoch: 14  Training loss = 1.3592  Validation loss = 1.5118  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 11  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 1.2534  Validation loss = 3.0291  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 1.2371  Validation loss = 2.8077  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 1.2378  Validation loss = 2.8355  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 1.2635  Validation loss = 2.7478  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 1.2640  Validation loss = 2.5366  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 1.2461  Validation loss = 2.1504  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 1.2733  Validation loss = 3.0406  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 1.2088  Validation loss = 2.7902  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 1.2037  Validation loss = 2.4937  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 1.1816  Validation loss = 3.1332  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 1.1901  Validation loss = 3.2562  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 6  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 1.8414  Validation loss = 0.9668  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 1.3475  Validation loss = 1.0115  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 1.3296  Validation loss = 0.8629  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 1.3632  Validation loss = 0.9070  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 1.8534  Validation loss = 1.1636  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 1.7991  Validation loss = 1.4554  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 1.3707  Validation loss = 0.6983  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 1.4050  Validation loss = 1.1434  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 1.2637  Validation loss = 0.7492  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 1.2675  Validation loss = 0.8794  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 1.3998  Validation loss = 1.1856  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 1.2703  Validation loss = 0.8837  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 1.2233  Validation loss = 0.9208  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 1.2910  Validation loss = 1.0079  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 1.3409  Validation loss = 1.1786  \n",
      "\n",
      "Fold: 24  Epoch: 16  Training loss = 1.2955  Validation loss = 1.0522  \n",
      "\n",
      "Fold: 24  Epoch: 17  Training loss = 1.2626  Validation loss = 0.8878  \n",
      "\n",
      "Fold: 24  Epoch: 18  Training loss = 1.2094  Validation loss = 0.8153  \n",
      "\n",
      "Fold: 24  Epoch: 19  Training loss = 1.1962  Validation loss = 0.8310  \n",
      "\n",
      "Fold: 24  Epoch: 20  Training loss = 1.3350  Validation loss = 1.4862  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 7  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 1.2281  Validation loss = 1.2672  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 1.1658  Validation loss = 1.3807  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 1.1746  Validation loss = 1.3394  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 1.2599  Validation loss = 1.4473  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 1.4902  Validation loss = 1.3888  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 1.5468  Validation loss = 2.3251  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 1.4738  Validation loss = 2.1878  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 1.4157  Validation loss = 1.9371  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 1.4260  Validation loss = 1.7727  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 1.3635  Validation loss = 1.7109  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 1.3216  Validation loss = 1.5988  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 1.3124  Validation loss = 1.7256  \n",
      "\n",
      "Fold: 25  Epoch: 13  Training loss = 1.4964  Validation loss = 2.1474  \n",
      "\n",
      "Fold: 25  Epoch: 14  Training loss = 1.2782  Validation loss = 1.7818  \n",
      "\n",
      "Fold: 25  Epoch: 15  Training loss = 1.2714  Validation loss = 1.7705  \n",
      "\n",
      "Fold: 25  Epoch: 16  Training loss = 1.2378  Validation loss = 1.7646  \n",
      "\n",
      "Fold: 25  Epoch: 17  Training loss = 1.2298  Validation loss = 1.7416  \n",
      "\n",
      "Fold: 25  Epoch: 18  Training loss = 1.2586  Validation loss = 1.8624  \n",
      "\n",
      "Fold: 25  Epoch: 19  Training loss = 1.4092  Validation loss = 2.7581  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 1  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 1.1974  Validation loss = 2.9533  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 1.4532  Validation loss = 4.5793  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 1.2011  Validation loss = 2.8111  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 1.1741  Validation loss = 2.8679  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 1.1745  Validation loss = 2.7654  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 1.1585  Validation loss = 2.8287  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 1.2201  Validation loss = 2.6131  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 1.1363  Validation loss = 2.8689  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 1.1266  Validation loss = 2.6488  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 1.1132  Validation loss = 2.9025  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 1.1373  Validation loss = 2.9818  \n",
      "\n",
      "Fold: 26  Epoch: 12  Training loss = 1.1033  Validation loss = 2.9287  \n",
      "\n",
      "Fold: 26  Epoch: 13  Training loss = 1.0924  Validation loss = 2.8547  \n",
      "\n",
      "Fold: 26  Epoch: 14  Training loss = 1.0748  Validation loss = 2.9051  \n",
      "\n",
      "Fold: 26  Epoch: 15  Training loss = 1.1350  Validation loss = 2.6457  \n",
      "\n",
      "Fold: 26  Epoch: 16  Training loss = 1.0593  Validation loss = 3.0062  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 7  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 1.2417  Validation loss = 2.3270  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 1.2197  Validation loss = 2.4459  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 1.1806  Validation loss = 2.6293  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 1.1890  Validation loss = 2.7862  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 1.1269  Validation loss = 2.6073  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 1.1648  Validation loss = 2.4925  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 1.0669  Validation loss = 1.8383  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 1.0541  Validation loss = 1.7743  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 1.0581  Validation loss = 1.9564  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 1.0169  Validation loss = 1.6212  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 1.0136  Validation loss = 1.8017  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 1.0180  Validation loss = 1.9272  \n",
      "\n",
      "Fold: 27  Epoch: 13  Training loss = 1.0587  Validation loss = 1.6848  \n",
      "\n",
      "Fold: 27  Epoch: 14  Training loss = 1.0059  Validation loss = 1.3218  \n",
      "\n",
      "Fold: 27  Epoch: 15  Training loss = 1.0504  Validation loss = 1.4090  \n",
      "\n",
      "Fold: 27  Epoch: 16  Training loss = 1.0269  Validation loss = 1.7915  \n",
      "\n",
      "Fold: 27  Epoch: 17  Training loss = 1.0346  Validation loss = 1.8470  \n",
      "\n",
      "Fold: 27  Epoch: 18  Training loss = 0.9586  Validation loss = 1.7727  \n",
      "\n",
      "Fold: 27  Epoch: 19  Training loss = 0.9606  Validation loss = 1.7881  \n",
      "\n",
      "Fold: 27  Epoch: 20  Training loss = 1.0351  Validation loss = 1.3711  \n",
      "\n",
      "Fold: 27  Epoch: 21  Training loss = 0.9752  Validation loss = 1.4614  \n",
      "\n",
      "Fold: 27  Epoch: 22  Training loss = 1.0165  Validation loss = 1.5881  \n",
      "\n",
      "Fold: 27  Epoch: 23  Training loss = 0.9588  Validation loss = 1.8073  \n",
      "\n",
      "Fold: 27  Epoch: 24  Training loss = 0.9530  Validation loss = 1.8790  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 14  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 1.2239  Validation loss = 3.3616  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 1.0615  Validation loss = 2.9388  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 1.0106  Validation loss = 2.6058  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 1.0251  Validation loss = 2.8292  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 1.0231  Validation loss = 2.9719  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 0.9858  Validation loss = 2.8017  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 0.9782  Validation loss = 2.4912  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 0.9760  Validation loss = 2.2974  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 0.9611  Validation loss = 2.3865  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 0.9263  Validation loss = 2.4444  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 0.9164  Validation loss = 2.5411  \n",
      "\n",
      "Fold: 28  Epoch: 12  Training loss = 0.9121  Validation loss = 2.2964  \n",
      "\n",
      "Fold: 28  Epoch: 13  Training loss = 0.9391  Validation loss = 2.6277  \n",
      "\n",
      "Fold: 28  Epoch: 14  Training loss = 0.9094  Validation loss = 2.5520  \n",
      "\n",
      "Fold: 28  Epoch: 15  Training loss = 0.8993  Validation loss = 2.4016  \n",
      "\n",
      "Fold: 28  Epoch: 16  Training loss = 0.9010  Validation loss = 2.0007  \n",
      "\n",
      "Fold: 28  Epoch: 17  Training loss = 0.9352  Validation loss = 2.6347  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 16  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 1.0170  Validation loss = 1.4018  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 1.0748  Validation loss = 2.3412  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 1.0104  Validation loss = 2.0676  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 0.9928  Validation loss = 2.3836  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 1.1060  Validation loss = 1.1238  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 0.9501  Validation loss = 2.0193  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 0.9735  Validation loss = 2.2869  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 0.9316  Validation loss = 1.5566  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 0.9086  Validation loss = 1.9120  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 0.9837  Validation loss = 1.3058  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 0.9404  Validation loss = 1.5999  \n",
      "\n",
      "Fold: 29  Epoch: 12  Training loss = 0.8590  Validation loss = 1.9341  \n",
      "\n",
      "Fold: 29  Epoch: 13  Training loss = 0.8888  Validation loss = 2.2116  \n",
      "\n",
      "Fold: 29  Epoch: 14  Training loss = 0.9204  Validation loss = 1.4473  \n",
      "\n",
      "Fold: 29  Epoch: 15  Training loss = 0.8663  Validation loss = 1.7970  \n",
      "\n",
      "Fold: 29  Epoch: 16  Training loss = 0.9437  Validation loss = 2.0639  \n",
      "\n",
      "Fold: 29  Epoch: 17  Training loss = 0.9492  Validation loss = 1.2450  \n",
      "\n",
      "Fold: 29  Epoch: 18  Training loss = 0.8392  Validation loss = 2.1524  \n",
      "\n",
      "Fold: 29  Epoch: 19  Training loss = 0.9808  Validation loss = 1.4133  \n",
      "\n",
      "Fold: 29  Epoch: 20  Training loss = 0.8749  Validation loss = 2.3337  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 5  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 0.9922  Validation loss = 2.2233  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 1.1722  Validation loss = 1.9458  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 0.9293  Validation loss = 1.6368  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 0.9620  Validation loss = 1.4902  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 0.9035  Validation loss = 2.0352  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 1.0122  Validation loss = 2.4262  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 1.1667  Validation loss = 2.1359  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 0.9048  Validation loss = 1.8633  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 0.9300  Validation loss = 2.0697  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 1.2983  Validation loss = 1.7247  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 1.1404  Validation loss = 1.6788  \n",
      "\n",
      "Fold: 30  Epoch: 12  Training loss = 0.9499  Validation loss = 1.5842  \n",
      "\n",
      "Fold: 30  Epoch: 13  Training loss = 0.9810  Validation loss = 1.7966  \n",
      "\n",
      "Fold: 30  Epoch: 14  Training loss = 1.0151  Validation loss = 1.6733  \n",
      "\n",
      "Fold: 30  Epoch: 15  Training loss = 1.3721  Validation loss = 2.3287  \n",
      "\n",
      "Fold: 30  Epoch: 16  Training loss = 1.0305  Validation loss = 1.8797  \n",
      "\n",
      "Fold: 30  Epoch: 17  Training loss = 0.9653  Validation loss = 2.0793  \n",
      "\n",
      "Fold: 30  Epoch: 18  Training loss = 1.0534  Validation loss = 1.8584  \n",
      "\n",
      "Fold: 30  Epoch: 19  Training loss = 0.9559  Validation loss = 2.1253  \n",
      "\n",
      "Fold: 30  Epoch: 20  Training loss = 1.0721  Validation loss = 2.0287  \n",
      "\n",
      "Fold: 30  Epoch: 21  Training loss = 0.9284  Validation loss = 1.7463  \n",
      "\n",
      "Fold: 30  Epoch: 22  Training loss = 0.8599  Validation loss = 1.6925  \n",
      "\n",
      "Fold: 30  Epoch: 23  Training loss = 0.9204  Validation loss = 1.9178  \n",
      "\n",
      "Fold: 30  Epoch: 24  Training loss = 0.8065  Validation loss = 2.0803  \n",
      "\n",
      "Fold: 30  Epoch: 25  Training loss = 0.7796  Validation loss = 1.6408  \n",
      "\n",
      "Fold: 30  Epoch: 26  Training loss = 0.8806  Validation loss = 2.1189  \n",
      "\n",
      "Fold: 30  Epoch: 27  Training loss = 0.8072  Validation loss = 1.7698  \n",
      "\n",
      "Fold: 30  Epoch: 28  Training loss = 0.8177  Validation loss = 1.3410  \n",
      "\n",
      "Fold: 30  Epoch: 29  Training loss = 0.7785  Validation loss = 1.8480  \n",
      "\n",
      "Fold: 30  Epoch: 30  Training loss = 0.7624  Validation loss = 1.7286  \n",
      "\n",
      "Fold: 30  Epoch: 31  Training loss = 0.7536  Validation loss = 1.9900  \n",
      "\n",
      "Fold: 30  Epoch: 32  Training loss = 0.8276  Validation loss = 2.0781  \n",
      "\n",
      "Fold: 30  Epoch: 33  Training loss = 0.7668  Validation loss = 1.4791  \n",
      "\n",
      "Fold: 30  Epoch: 34  Training loss = 1.2767  Validation loss = 1.3748  \n",
      "\n",
      "Fold: 30  Epoch: 35  Training loss = 0.8975  Validation loss = 1.5628  \n",
      "\n",
      "Fold: 30  Epoch: 36  Training loss = 0.8468  Validation loss = 2.0495  \n",
      "\n",
      "Fold: 30  Epoch: 37  Training loss = 0.8647  Validation loss = 2.0836  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 28  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 1.0046  Validation loss = 1.6027  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 1.1410  Validation loss = 1.1095  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 0.9817  Validation loss = 1.9581  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 1.2116  Validation loss = 1.3684  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 0.9759  Validation loss = 1.6718  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 1.4194  Validation loss = 1.8290  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 1.0971  Validation loss = 1.2554  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 1.1722  Validation loss = 1.5953  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 1.0648  Validation loss = 1.5368  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 1.0095  Validation loss = 1.6551  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 0.9851  Validation loss = 1.9871  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 2  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 1.0102  Validation loss = 2.8841  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 1.1953  Validation loss = 3.3352  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 0.9500  Validation loss = 3.5450  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 1.1419  Validation loss = 2.5398  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 1.0734  Validation loss = 3.7303  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 1.0256  Validation loss = 3.7758  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 1.0237  Validation loss = 3.8269  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 1.0426  Validation loss = 3.6998  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 1.0380  Validation loss = 3.8709  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 1.0001  Validation loss = 3.3402  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 0.8809  Validation loss = 3.0221  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 0.8620  Validation loss = 2.4974  \n",
      "\n",
      "Fold: 32  Epoch: 13  Training loss = 1.0088  Validation loss = 3.5665  \n",
      "\n",
      "Fold: 32  Epoch: 14  Training loss = 0.8607  Validation loss = 2.8867  \n",
      "\n",
      "Fold: 32  Epoch: 15  Training loss = 1.0478  Validation loss = 3.4646  \n",
      "\n",
      "Fold: 32  Epoch: 16  Training loss = 0.9385  Validation loss = 3.4665  \n",
      "\n",
      "Fold: 32  Epoch: 17  Training loss = 0.9417  Validation loss = 3.7198  \n",
      "\n",
      "Fold: 32  Epoch: 18  Training loss = 0.9075  Validation loss = 3.4542  \n",
      "\n",
      "Fold: 32  Epoch: 19  Training loss = 0.8855  Validation loss = 3.2793  \n",
      "\n",
      "Fold: 32  Epoch: 20  Training loss = 0.8344  Validation loss = 2.6798  \n",
      "\n",
      "Fold: 32  Epoch: 21  Training loss = 0.8880  Validation loss = 2.3747  \n",
      "\n",
      "Fold: 32  Epoch: 22  Training loss = 0.7871  Validation loss = 2.6699  \n",
      "\n",
      "Fold: 32  Epoch: 23  Training loss = 0.8232  Validation loss = 2.5837  \n",
      "\n",
      "Fold: 32  Epoch: 24  Training loss = 0.7625  Validation loss = 2.5494  \n",
      "\n",
      "Fold: 32  Epoch: 25  Training loss = 0.7637  Validation loss = 2.5483  \n",
      "\n",
      "Fold: 32  Epoch: 26  Training loss = 0.7427  Validation loss = 2.5104  \n",
      "\n",
      "Fold: 32  Epoch: 27  Training loss = 0.7498  Validation loss = 2.7852  \n",
      "\n",
      "Fold: 32  Epoch: 28  Training loss = 0.7612  Validation loss = 2.7506  \n",
      "\n",
      "Fold: 32  Epoch: 29  Training loss = 0.7236  Validation loss = 2.5999  \n",
      "\n",
      "Fold: 32  Epoch: 30  Training loss = 0.7693  Validation loss = 2.6872  \n",
      "\n",
      "Fold: 32  Epoch: 31  Training loss = 0.8492  Validation loss = 3.3897  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 21  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    # Load model with lowest validation error for each fold\n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 16\n",
      "Average validation error: 3.25974\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 0.9074  Test loss = 3.4602  \n",
      "\n",
      "Epoch: 2  Training loss = 0.8755  Test loss = 3.4080  \n",
      "\n",
      "Epoch: 3  Training loss = 0.8623  Test loss = 3.3891  \n",
      "\n",
      "Epoch: 4  Training loss = 0.8509  Test loss = 3.3753  \n",
      "\n",
      "Epoch: 5  Training loss = 0.8409  Test loss = 3.3637  \n",
      "\n",
      "Epoch: 6  Training loss = 0.8331  Test loss = 3.3560  \n",
      "\n",
      "Epoch: 7  Training loss = 0.8268  Test loss = 3.3523  \n",
      "\n",
      "Epoch: 8  Training loss = 0.8212  Test loss = 3.3488  \n",
      "\n",
      "Epoch: 9  Training loss = 0.8161  Test loss = 3.3449  \n",
      "\n",
      "Epoch: 10  Training loss = 0.8113  Test loss = 3.3408  \n",
      "\n",
      "Epoch: 11  Training loss = 0.8067  Test loss = 3.3366  \n",
      "\n",
      "Epoch: 12  Training loss = 0.8024  Test loss = 3.3323  \n",
      "\n",
      "Epoch: 13  Training loss = 0.7983  Test loss = 3.3279  \n",
      "\n",
      "Epoch: 14  Training loss = 0.7943  Test loss = 3.3236  \n",
      "\n",
      "Epoch: 15  Training loss = 0.7906  Test loss = 3.3193  \n",
      "\n",
      "Epoch: 16  Training loss = 0.7869  Test loss = 3.3151  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyagAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc054ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2fASC0d5ugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xce1a240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 3.31507007702 \n",
      "Fixed scheme MAE:  2.356399347\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 0.7869  Test loss = 2.3636  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 0.8397  Test loss = 2.9753  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 0.9132  Test loss = 2.0000  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 0.9461  Test loss = 1.7480  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 0.6324  Test loss = 2.4465  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 0.6990  Test loss = 0.2915  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 0.6938  Test loss = 1.7188  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 0.7229  Test loss = 0.5210  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 0.5796  Test loss = 2.6085  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 0.6633  Test loss = 1.3326  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 0.6833  Test loss = 1.9106  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 0.7231  Test loss = 1.3888  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 0.6479  Test loss = 2.1967  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 0.7023  Test loss = 4.9090  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 0.9292  Test loss = 4.7077  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 1.0968  Test loss = 9.8459  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 1.2406  Test loss = 1.5373  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 1.2551  Test loss = 1.6322  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 1.2711  Test loss = 1.0056  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 1.2620  Test loss = 0.0452  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 0.8107  Test loss = 2.2287  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 0.8565  Test loss = 2.1173  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 0.8912  Test loss = 1.5627  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 0.9104  Test loss = 0.3639  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 0.8026  Test loss = 1.0612  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 0.8106  Test loss = 3.3475  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 0.9103  Test loss = 0.7743  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 0.9062  Test loss = 1.7818  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 0.8434  Test loss = 0.8466  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 0.8499  Test loss = 0.4096  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 0.8507  Test loss = 4.4182  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 1.0115  Test loss = 1.7053  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 0.9348  Test loss = 1.0357  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 0.9436  Test loss = 0.3386  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 0.9426  Test loss = 0.5880  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 0.9446  Test loss = 2.2678  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 0.8575  Test loss = 3.0002  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 0.9148  Test loss = 1.1610  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 0.9165  Test loss = 0.7079  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 0.9206  Test loss = 3.2663  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 0.8616  Test loss = 1.0097  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 0.8656  Test loss = 2.7015  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 0.9272  Test loss = 3.8457  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 1.0426  Test loss = 12.2006  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 1.9512  Test loss = 7.2593  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.1489  Test loss = 1.2802  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.1537  Test loss = 0.4069  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.1543  Test loss = 0.2997  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 1.3968  Test loss = 3.4345  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 1.4602  Test loss = 4.4321  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 1.5577  Test loss = 2.4409  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 1.5865  Test loss = 0.6340  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 1.3543  Test loss = 0.7675  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 1.3569  Test loss = 1.5874  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 1.3711  Test loss = 0.8909  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 1.3753  Test loss = 0.2489  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 1.2070  Test loss = 2.2514  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 1.2339  Test loss = 1.1538  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 1.2402  Test loss = 0.1267  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 1.2402  Test loss = 0.0752  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.1836  Test loss = 1.1146  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.1909  Test loss = 3.3796  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 1.2616  Test loss = 0.7851  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 1.2654  Test loss = 0.9571  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 1.1739  Test loss = 1.5069  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 1.1805  Test loss = 1.5944  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.1918  Test loss = 2.4377  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 1.2281  Test loss = 3.3203  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 1.1632  Test loss = 3.5938  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 1.2457  Test loss = 1.3235  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 1.2547  Test loss = 0.3581  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 1.2550  Test loss = 0.0684  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 1.0791  Test loss = 1.4294  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 1.0888  Test loss = 0.9593  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 1.0949  Test loss = 3.3400  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 1.1656  Test loss = 2.0080  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 1.0497  Test loss = 0.4169  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd675048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.84330604966 \n",
      "Updating scheme MAE:  2.02347361153\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_test_fixed.csv\", yhat_test_fixed)\n",
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_update.csv\", yhat_update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
