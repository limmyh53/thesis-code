{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"4Q/32_units/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 500\n",
    "learning_rate = 1e-4\n",
    "batch_size = 5\n",
    "early_stop_iters = 10\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 32 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag4',\n",
    "                                       'inflation.lag5',\n",
    "                                       'inflation.lag6',\n",
    "                                       'inflation.lag7']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag4',\n",
    "                                   'unemp.lag5',\n",
    "                                   'unemp.lag6',\n",
    "                                   'unemp.lag7']])\n",
    "train_4lag_oil = np.array(train[['oil.lag4',\n",
    "                                 'oil.lag5',\n",
    "                                 'oil.lag6',\n",
    "                                 'oil.lag7']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag4',\n",
    "                                     'inflation.lag5',\n",
    "                                     'inflation.lag6',\n",
    "                                     'inflation.lag7']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag4',\n",
    "                                 'unemp.lag5',\n",
    "                                 'unemp.lag6',\n",
    "                                 'unemp.lag7']])\n",
    "test_4lag_oil = np.array(test[['oil.lag4',\n",
    "                               'oil.lag5',\n",
    "                               'oil.lag6',\n",
    "                               'oil.lag7']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 32 \n",
      "Learning rate = 0.0001 \n",
      "Epochs = 500 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 10 \n",
      "Learning rate = 0.0001\n",
      "Fold: 1  Epoch: 1  Training loss = 3.4987  Validation loss = 3.8348  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 3.4747  Validation loss = 3.7962  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 3.4509  Validation loss = 3.7566  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 3.4273  Validation loss = 3.7169  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 3.4066  Validation loss = 3.6815  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 3.3905  Validation loss = 3.6525  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 3.3683  Validation loss = 3.6119  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 3.3526  Validation loss = 3.5830  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 3.3311  Validation loss = 3.5431  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 3.3075  Validation loss = 3.5004  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 3.2890  Validation loss = 3.4662  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 3.2627  Validation loss = 3.4174  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 3.2461  Validation loss = 3.3855  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 3.2224  Validation loss = 3.3397  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 3.2044  Validation loss = 3.3063  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 3.1817  Validation loss = 3.2632  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 3.1654  Validation loss = 3.2311  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 3.1425  Validation loss = 3.1868  \n",
      "\n",
      "Fold: 1  Epoch: 19  Training loss = 3.1219  Validation loss = 3.1466  \n",
      "\n",
      "Fold: 1  Epoch: 20  Training loss = 3.1044  Validation loss = 3.1121  \n",
      "\n",
      "Fold: 1  Epoch: 21  Training loss = 3.0902  Validation loss = 3.0839  \n",
      "\n",
      "Fold: 1  Epoch: 22  Training loss = 3.0734  Validation loss = 3.0518  \n",
      "\n",
      "Fold: 1  Epoch: 23  Training loss = 3.0511  Validation loss = 3.0098  \n",
      "\n",
      "Fold: 1  Epoch: 24  Training loss = 3.0394  Validation loss = 2.9860  \n",
      "\n",
      "Fold: 1  Epoch: 25  Training loss = 3.0308  Validation loss = 2.9690  \n",
      "\n",
      "Fold: 1  Epoch: 26  Training loss = 3.0132  Validation loss = 2.9350  \n",
      "\n",
      "Fold: 1  Epoch: 27  Training loss = 3.0048  Validation loss = 2.9181  \n",
      "\n",
      "Fold: 1  Epoch: 28  Training loss = 2.9943  Validation loss = 2.8956  \n",
      "\n",
      "Fold: 1  Epoch: 29  Training loss = 2.9824  Validation loss = 2.8709  \n",
      "\n",
      "Fold: 1  Epoch: 30  Training loss = 2.9687  Validation loss = 2.8442  \n",
      "\n",
      "Fold: 1  Epoch: 31  Training loss = 2.9568  Validation loss = 2.8203  \n",
      "\n",
      "Fold: 1  Epoch: 32  Training loss = 2.9463  Validation loss = 2.7979  \n",
      "\n",
      "Fold: 1  Epoch: 33  Training loss = 2.9388  Validation loss = 2.7816  \n",
      "\n",
      "Fold: 1  Epoch: 34  Training loss = 2.9264  Validation loss = 2.7546  \n",
      "\n",
      "Fold: 1  Epoch: 35  Training loss = 2.9156  Validation loss = 2.7311  \n",
      "\n",
      "Fold: 1  Epoch: 36  Training loss = 2.9065  Validation loss = 2.7107  \n",
      "\n",
      "Fold: 1  Epoch: 37  Training loss = 2.8992  Validation loss = 2.6960  \n",
      "\n",
      "Fold: 1  Epoch: 38  Training loss = 2.8927  Validation loss = 2.6819  \n",
      "\n",
      "Fold: 1  Epoch: 39  Training loss = 2.8864  Validation loss = 2.6667  \n",
      "\n",
      "Fold: 1  Epoch: 40  Training loss = 2.8737  Validation loss = 2.6395  \n",
      "\n",
      "Fold: 1  Epoch: 41  Training loss = 2.8656  Validation loss = 2.6206  \n",
      "\n",
      "Fold: 1  Epoch: 42  Training loss = 2.8569  Validation loss = 2.5998  \n",
      "\n",
      "Fold: 1  Epoch: 43  Training loss = 2.8516  Validation loss = 2.5867  \n",
      "\n",
      "Fold: 1  Epoch: 44  Training loss = 2.8451  Validation loss = 2.5712  \n",
      "\n",
      "Fold: 1  Epoch: 45  Training loss = 2.8368  Validation loss = 2.5514  \n",
      "\n",
      "Fold: 1  Epoch: 46  Training loss = 2.8300  Validation loss = 2.5355  \n",
      "\n",
      "Fold: 1  Epoch: 47  Training loss = 2.8239  Validation loss = 2.5207  \n",
      "\n",
      "Fold: 1  Epoch: 48  Training loss = 2.8147  Validation loss = 2.4979  \n",
      "\n",
      "Fold: 1  Epoch: 49  Training loss = 2.8101  Validation loss = 2.4860  \n",
      "\n",
      "Fold: 1  Epoch: 50  Training loss = 2.8073  Validation loss = 2.4774  \n",
      "\n",
      "Fold: 1  Epoch: 51  Training loss = 2.8009  Validation loss = 2.4607  \n",
      "\n",
      "Fold: 1  Epoch: 52  Training loss = 2.7939  Validation loss = 2.4434  \n",
      "\n",
      "Fold: 1  Epoch: 53  Training loss = 2.7911  Validation loss = 2.4343  \n",
      "\n",
      "Fold: 1  Epoch: 54  Training loss = 2.7880  Validation loss = 2.4255  \n",
      "\n",
      "Fold: 1  Epoch: 55  Training loss = 2.7815  Validation loss = 2.4080  \n",
      "\n",
      "Fold: 1  Epoch: 56  Training loss = 2.7771  Validation loss = 2.3966  \n",
      "\n",
      "Fold: 1  Epoch: 57  Training loss = 2.7725  Validation loss = 2.3828  \n",
      "\n",
      "Fold: 1  Epoch: 58  Training loss = 2.7688  Validation loss = 2.3728  \n",
      "\n",
      "Fold: 1  Epoch: 59  Training loss = 2.7621  Validation loss = 2.3543  \n",
      "\n",
      "Fold: 1  Epoch: 60  Training loss = 2.7587  Validation loss = 2.3456  \n",
      "\n",
      "Fold: 1  Epoch: 61  Training loss = 2.7519  Validation loss = 2.3272  \n",
      "\n",
      "Fold: 1  Epoch: 62  Training loss = 2.7459  Validation loss = 2.3104  \n",
      "\n",
      "Fold: 1  Epoch: 63  Training loss = 2.7402  Validation loss = 2.2935  \n",
      "\n",
      "Fold: 1  Epoch: 64  Training loss = 2.7363  Validation loss = 2.2821  \n",
      "\n",
      "Fold: 1  Epoch: 65  Training loss = 2.7293  Validation loss = 2.2609  \n",
      "\n",
      "Fold: 1  Epoch: 66  Training loss = 2.7222  Validation loss = 2.2410  \n",
      "\n",
      "Fold: 1  Epoch: 67  Training loss = 2.7195  Validation loss = 2.2323  \n",
      "\n",
      "Fold: 1  Epoch: 68  Training loss = 2.7137  Validation loss = 2.2153  \n",
      "\n",
      "Fold: 1  Epoch: 69  Training loss = 2.7103  Validation loss = 2.2054  \n",
      "\n",
      "Fold: 1  Epoch: 70  Training loss = 2.7058  Validation loss = 2.1913  \n",
      "\n",
      "Fold: 1  Epoch: 71  Training loss = 2.7005  Validation loss = 2.1730  \n",
      "\n",
      "Fold: 1  Epoch: 72  Training loss = 2.6942  Validation loss = 2.1524  \n",
      "\n",
      "Fold: 1  Epoch: 73  Training loss = 2.6886  Validation loss = 2.1333  \n",
      "\n",
      "Fold: 1  Epoch: 74  Training loss = 2.6837  Validation loss = 2.1170  \n",
      "\n",
      "Fold: 1  Epoch: 75  Training loss = 2.6816  Validation loss = 2.1084  \n",
      "\n",
      "Fold: 1  Epoch: 76  Training loss = 2.6775  Validation loss = 2.0946  \n",
      "\n",
      "Fold: 1  Epoch: 77  Training loss = 2.6749  Validation loss = 2.0837  \n",
      "\n",
      "Fold: 1  Epoch: 78  Training loss = 2.6716  Validation loss = 2.0716  \n",
      "\n",
      "Fold: 1  Epoch: 79  Training loss = 2.6707  Validation loss = 2.0688  \n",
      "\n",
      "Fold: 1  Epoch: 80  Training loss = 2.6659  Validation loss = 2.0504  \n",
      "\n",
      "Fold: 1  Epoch: 81  Training loss = 2.6636  Validation loss = 2.0434  \n",
      "\n",
      "Fold: 1  Epoch: 82  Training loss = 2.6607  Validation loss = 2.0308  \n",
      "\n",
      "Fold: 1  Epoch: 83  Training loss = 2.6600  Validation loss = 2.0288  \n",
      "\n",
      "Fold: 1  Epoch: 84  Training loss = 2.6587  Validation loss = 2.0249  \n",
      "\n",
      "Fold: 1  Epoch: 85  Training loss = 2.6570  Validation loss = 2.0195  \n",
      "\n",
      "Fold: 1  Epoch: 86  Training loss = 2.6538  Validation loss = 2.0081  \n",
      "\n",
      "Fold: 1  Epoch: 87  Training loss = 2.6514  Validation loss = 1.9982  \n",
      "\n",
      "Fold: 1  Epoch: 88  Training loss = 2.6480  Validation loss = 1.9850  \n",
      "\n",
      "Fold: 1  Epoch: 89  Training loss = 2.6458  Validation loss = 1.9770  \n",
      "\n",
      "Fold: 1  Epoch: 90  Training loss = 2.6416  Validation loss = 1.9596  \n",
      "\n",
      "Fold: 1  Epoch: 91  Training loss = 2.6393  Validation loss = 1.9502  \n",
      "\n",
      "Fold: 1  Epoch: 92  Training loss = 2.6335  Validation loss = 1.9270  \n",
      "\n",
      "Fold: 1  Epoch: 93  Training loss = 2.6299  Validation loss = 1.9106  \n",
      "\n",
      "Fold: 1  Epoch: 94  Training loss = 2.6253  Validation loss = 1.8887  \n",
      "\n",
      "Fold: 1  Epoch: 95  Training loss = 2.6231  Validation loss = 1.8786  \n",
      "\n",
      "Fold: 1  Epoch: 96  Training loss = 2.6214  Validation loss = 1.8714  \n",
      "\n",
      "Fold: 1  Epoch: 97  Training loss = 2.6189  Validation loss = 1.8599  \n",
      "\n",
      "Fold: 1  Epoch: 98  Training loss = 2.6182  Validation loss = 1.8597  \n",
      "\n",
      "Fold: 1  Epoch: 99  Training loss = 2.6159  Validation loss = 1.8490  \n",
      "\n",
      "Fold: 1  Epoch: 100  Training loss = 2.6135  Validation loss = 1.8380  \n",
      "\n",
      "Fold: 1  Epoch: 101  Training loss = 2.6115  Validation loss = 1.8278  \n",
      "\n",
      "Fold: 1  Epoch: 102  Training loss = 2.6103  Validation loss = 1.8241  \n",
      "\n",
      "Fold: 1  Epoch: 103  Training loss = 2.6090  Validation loss = 1.8188  \n",
      "\n",
      "Fold: 1  Epoch: 104  Training loss = 2.6072  Validation loss = 1.8098  \n",
      "\n",
      "Fold: 1  Epoch: 105  Training loss = 2.6063  Validation loss = 1.8058  \n",
      "\n",
      "Fold: 1  Epoch: 106  Training loss = 2.6049  Validation loss = 1.7989  \n",
      "\n",
      "Fold: 1  Epoch: 107  Training loss = 2.6044  Validation loss = 1.7988  \n",
      "\n",
      "Fold: 1  Epoch: 108  Training loss = 2.6018  Validation loss = 1.7863  \n",
      "\n",
      "Fold: 1  Epoch: 109  Training loss = 2.6022  Validation loss = 1.7896  \n",
      "\n",
      "Fold: 1  Epoch: 110  Training loss = 2.6006  Validation loss = 1.7834  \n",
      "\n",
      "Fold: 1  Epoch: 111  Training loss = 2.5980  Validation loss = 1.7716  \n",
      "\n",
      "Fold: 1  Epoch: 112  Training loss = 2.5953  Validation loss = 1.7592  \n",
      "\n",
      "Fold: 1  Epoch: 113  Training loss = 2.5948  Validation loss = 1.7562  \n",
      "\n",
      "Fold: 1  Epoch: 114  Training loss = 2.5931  Validation loss = 1.7469  \n",
      "\n",
      "Fold: 1  Epoch: 115  Training loss = 2.5922  Validation loss = 1.7433  \n",
      "\n",
      "Fold: 1  Epoch: 116  Training loss = 2.5912  Validation loss = 1.7392  \n",
      "\n",
      "Fold: 1  Epoch: 117  Training loss = 2.5907  Validation loss = 1.7370  \n",
      "\n",
      "Fold: 1  Epoch: 118  Training loss = 2.5886  Validation loss = 1.7268  \n",
      "\n",
      "Fold: 1  Epoch: 119  Training loss = 2.5870  Validation loss = 1.7172  \n",
      "\n",
      "Fold: 1  Epoch: 120  Training loss = 2.5849  Validation loss = 1.7061  \n",
      "\n",
      "Fold: 1  Epoch: 121  Training loss = 2.5834  Validation loss = 1.6971  \n",
      "\n",
      "Fold: 1  Epoch: 122  Training loss = 2.5818  Validation loss = 1.6907  \n",
      "\n",
      "Fold: 1  Epoch: 123  Training loss = 2.5804  Validation loss = 1.6826  \n",
      "\n",
      "Fold: 1  Epoch: 124  Training loss = 2.5794  Validation loss = 1.6791  \n",
      "\n",
      "Fold: 1  Epoch: 125  Training loss = 2.5790  Validation loss = 1.6767  \n",
      "\n",
      "Fold: 1  Epoch: 126  Training loss = 2.5779  Validation loss = 1.6675  \n",
      "\n",
      "Fold: 1  Epoch: 127  Training loss = 2.5767  Validation loss = 1.6605  \n",
      "\n",
      "Fold: 1  Epoch: 128  Training loss = 2.5753  Validation loss = 1.6514  \n",
      "\n",
      "Fold: 1  Epoch: 129  Training loss = 2.5741  Validation loss = 1.6456  \n",
      "\n",
      "Fold: 1  Epoch: 130  Training loss = 2.5729  Validation loss = 1.6405  \n",
      "\n",
      "Fold: 1  Epoch: 131  Training loss = 2.5716  Validation loss = 1.6313  \n",
      "\n",
      "Fold: 1  Epoch: 132  Training loss = 2.5709  Validation loss = 1.6280  \n",
      "\n",
      "Fold: 1  Epoch: 133  Training loss = 2.5701  Validation loss = 1.6235  \n",
      "\n",
      "Fold: 1  Epoch: 134  Training loss = 2.5699  Validation loss = 1.6249  \n",
      "\n",
      "Fold: 1  Epoch: 135  Training loss = 2.5688  Validation loss = 1.6179  \n",
      "\n",
      "Fold: 1  Epoch: 136  Training loss = 2.5678  Validation loss = 1.6143  \n",
      "\n",
      "Fold: 1  Epoch: 137  Training loss = 2.5674  Validation loss = 1.6182  \n",
      "\n",
      "Fold: 1  Epoch: 138  Training loss = 2.5668  Validation loss = 1.6190  \n",
      "\n",
      "Fold: 1  Epoch: 139  Training loss = 2.5665  Validation loss = 1.6191  \n",
      "\n",
      "Fold: 1  Epoch: 140  Training loss = 2.5657  Validation loss = 1.6172  \n",
      "\n",
      "Fold: 1  Epoch: 141  Training loss = 2.5638  Validation loss = 1.6047  \n",
      "\n",
      "Fold: 1  Epoch: 142  Training loss = 2.5625  Validation loss = 1.5991  \n",
      "\n",
      "Fold: 1  Epoch: 143  Training loss = 2.5613  Validation loss = 1.5911  \n",
      "\n",
      "Fold: 1  Epoch: 144  Training loss = 2.5611  Validation loss = 1.5945  \n",
      "\n",
      "Fold: 1  Epoch: 145  Training loss = 2.5607  Validation loss = 1.5911  \n",
      "\n",
      "Fold: 1  Epoch: 146  Training loss = 2.5605  Validation loss = 1.5956  \n",
      "\n",
      "Fold: 1  Epoch: 147  Training loss = 2.5600  Validation loss = 1.5935  \n",
      "\n",
      "Fold: 1  Epoch: 148  Training loss = 2.5600  Validation loss = 1.5983  \n",
      "\n",
      "Fold: 1  Epoch: 149  Training loss = 2.5590  Validation loss = 1.5953  \n",
      "\n",
      "Fold: 1  Epoch: 150  Training loss = 2.5591  Validation loss = 1.5961  \n",
      "\n",
      "Fold: 1  Epoch: 151  Training loss = 2.5584  Validation loss = 1.5930  \n",
      "\n",
      "Fold: 1  Epoch: 152  Training loss = 2.5573  Validation loss = 1.5861  \n",
      "\n",
      "Fold: 1  Epoch: 153  Training loss = 2.5576  Validation loss = 1.5938  \n",
      "\n",
      "Fold: 1  Epoch: 154  Training loss = 2.5569  Validation loss = 1.5925  \n",
      "\n",
      "Fold: 1  Epoch: 155  Training loss = 2.5556  Validation loss = 1.5826  \n",
      "\n",
      "Fold: 1  Epoch: 156  Training loss = 2.5554  Validation loss = 1.5834  \n",
      "\n",
      "Fold: 1  Epoch: 157  Training loss = 2.5553  Validation loss = 1.5866  \n",
      "\n",
      "Fold: 1  Epoch: 158  Training loss = 2.5550  Validation loss = 1.5874  \n",
      "\n",
      "Fold: 1  Epoch: 159  Training loss = 2.5542  Validation loss = 1.5827  \n",
      "\n",
      "Fold: 1  Epoch: 160  Training loss = 2.5521  Validation loss = 1.5653  \n",
      "\n",
      "Fold: 1  Epoch: 161  Training loss = 2.5500  Validation loss = 1.5443  \n",
      "\n",
      "Fold: 1  Epoch: 162  Training loss = 2.5496  Validation loss = 1.5461  \n",
      "\n",
      "Fold: 1  Epoch: 163  Training loss = 2.5493  Validation loss = 1.5467  \n",
      "\n",
      "Fold: 1  Epoch: 164  Training loss = 2.5483  Validation loss = 1.5429  \n",
      "\n",
      "Fold: 1  Epoch: 165  Training loss = 2.5474  Validation loss = 1.5360  \n",
      "\n",
      "Fold: 1  Epoch: 166  Training loss = 2.5474  Validation loss = 1.5400  \n",
      "\n",
      "Fold: 1  Epoch: 167  Training loss = 2.5467  Validation loss = 1.5365  \n",
      "\n",
      "Fold: 1  Epoch: 168  Training loss = 2.5464  Validation loss = 1.5362  \n",
      "\n",
      "Fold: 1  Epoch: 169  Training loss = 2.5463  Validation loss = 1.5394  \n",
      "\n",
      "Fold: 1  Epoch: 170  Training loss = 2.5458  Validation loss = 1.5386  \n",
      "\n",
      "Fold: 1  Epoch: 171  Training loss = 2.5459  Validation loss = 1.5421  \n",
      "\n",
      "Fold: 1  Epoch: 172  Training loss = 2.5451  Validation loss = 1.5365  \n",
      "\n",
      "Fold: 1  Epoch: 173  Training loss = 2.5444  Validation loss = 1.5342  \n",
      "\n",
      "Fold: 1  Epoch: 174  Training loss = 2.5438  Validation loss = 1.5301  \n",
      "\n",
      "Fold: 1  Epoch: 175  Training loss = 2.5432  Validation loss = 1.5250  \n",
      "\n",
      "Fold: 1  Epoch: 176  Training loss = 2.5425  Validation loss = 1.5244  \n",
      "\n",
      "Fold: 1  Epoch: 177  Training loss = 2.5422  Validation loss = 1.5292  \n",
      "\n",
      "Fold: 1  Epoch: 178  Training loss = 2.5421  Validation loss = 1.5324  \n",
      "\n",
      "Fold: 1  Epoch: 179  Training loss = 2.5420  Validation loss = 1.5352  \n",
      "\n",
      "Fold: 1  Epoch: 180  Training loss = 2.5414  Validation loss = 1.5369  \n",
      "\n",
      "Fold: 1  Epoch: 181  Training loss = 2.5411  Validation loss = 1.5331  \n",
      "\n",
      "Fold: 1  Epoch: 182  Training loss = 2.5401  Validation loss = 1.5230  \n",
      "\n",
      "Fold: 1  Epoch: 183  Training loss = 2.5395  Validation loss = 1.5195  \n",
      "\n",
      "Fold: 1  Epoch: 184  Training loss = 2.5393  Validation loss = 1.5241  \n",
      "\n",
      "Fold: 1  Epoch: 185  Training loss = 2.5390  Validation loss = 1.5267  \n",
      "\n",
      "Fold: 1  Epoch: 186  Training loss = 2.5381  Validation loss = 1.5213  \n",
      "\n",
      "Fold: 1  Epoch: 187  Training loss = 2.5374  Validation loss = 1.5166  \n",
      "\n",
      "Fold: 1  Epoch: 188  Training loss = 2.5372  Validation loss = 1.5197  \n",
      "\n",
      "Fold: 1  Epoch: 189  Training loss = 2.5371  Validation loss = 1.5238  \n",
      "\n",
      "Fold: 1  Epoch: 190  Training loss = 2.5365  Validation loss = 1.5255  \n",
      "\n",
      "Fold: 1  Epoch: 191  Training loss = 2.5353  Validation loss = 1.5160  \n",
      "\n",
      "Fold: 1  Epoch: 192  Training loss = 2.5352  Validation loss = 1.5136  \n",
      "\n",
      "Fold: 1  Epoch: 193  Training loss = 2.5351  Validation loss = 1.5177  \n",
      "\n",
      "Fold: 1  Epoch: 194  Training loss = 2.5341  Validation loss = 1.5116  \n",
      "\n",
      "Fold: 1  Epoch: 195  Training loss = 2.5327  Validation loss = 1.5003  \n",
      "\n",
      "Fold: 1  Epoch: 196  Training loss = 2.5311  Validation loss = 1.4820  \n",
      "\n",
      "Fold: 1  Epoch: 197  Training loss = 2.5306  Validation loss = 1.4858  \n",
      "\n",
      "Fold: 1  Epoch: 198  Training loss = 2.5301  Validation loss = 1.4910  \n",
      "\n",
      "Fold: 1  Epoch: 199  Training loss = 2.5297  Validation loss = 1.4901  \n",
      "\n",
      "Fold: 1  Epoch: 200  Training loss = 2.5293  Validation loss = 1.4966  \n",
      "\n",
      "Fold: 1  Epoch: 201  Training loss = 2.5292  Validation loss = 1.5032  \n",
      "\n",
      "Fold: 1  Epoch: 202  Training loss = 2.5287  Validation loss = 1.5029  \n",
      "\n",
      "Fold: 1  Epoch: 203  Training loss = 2.5287  Validation loss = 1.5058  \n",
      "\n",
      "Fold: 1  Epoch: 204  Training loss = 2.5286  Validation loss = 1.5060  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 196  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.4409  Validation loss = 2.1049  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.4403  Validation loss = 2.1052  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.4398  Validation loss = 2.1026  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.4386  Validation loss = 2.0943  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.4376  Validation loss = 2.0874  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.4376  Validation loss = 2.0887  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.4371  Validation loss = 2.0849  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.4365  Validation loss = 2.0849  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.4367  Validation loss = 2.0913  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.4363  Validation loss = 2.0889  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.4362  Validation loss = 2.0869  \n",
      "\n",
      "Fold: 2  Epoch: 12  Training loss = 2.4365  Validation loss = 2.0941  \n",
      "\n",
      "Fold: 2  Epoch: 13  Training loss = 2.4365  Validation loss = 2.0980  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 8  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.5012  Validation loss = 3.4896  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.5004  Validation loss = 3.4920  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.4998  Validation loss = 3.4935  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.4995  Validation loss = 3.4918  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.4989  Validation loss = 3.4899  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.4986  Validation loss = 3.4884  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.4980  Validation loss = 3.4917  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.4976  Validation loss = 3.4933  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.4971  Validation loss = 3.4945  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.4965  Validation loss = 3.4970  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.4961  Validation loss = 3.4971  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 6  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.6310  Validation loss = 4.7354  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.6302  Validation loss = 4.7300  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.6295  Validation loss = 4.7244  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.6276  Validation loss = 4.7099  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.6274  Validation loss = 4.7090  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.6268  Validation loss = 4.7112  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.6267  Validation loss = 4.7135  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.6250  Validation loss = 4.7052  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.6244  Validation loss = 4.7052  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 1.6235  Validation loss = 4.6981  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.6222  Validation loss = 4.6883  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 1.6211  Validation loss = 4.6832  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.6206  Validation loss = 4.6847  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 1.6199  Validation loss = 4.6852  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.6192  Validation loss = 4.6843  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 1.6182  Validation loss = 4.6777  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 1.6171  Validation loss = 4.6723  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 1.6164  Validation loss = 4.6657  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 1.6150  Validation loss = 4.6581  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 1.6142  Validation loss = 4.6491  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 1.6135  Validation loss = 4.6492  \n",
      "\n",
      "Fold: 4  Epoch: 22  Training loss = 1.6125  Validation loss = 4.6433  \n",
      "\n",
      "Fold: 4  Epoch: 23  Training loss = 1.6119  Validation loss = 4.6413  \n",
      "\n",
      "Fold: 4  Epoch: 24  Training loss = 1.6113  Validation loss = 4.6360  \n",
      "\n",
      "Fold: 4  Epoch: 25  Training loss = 1.6104  Validation loss = 4.6345  \n",
      "\n",
      "Fold: 4  Epoch: 26  Training loss = 1.6098  Validation loss = 4.6313  \n",
      "\n",
      "Fold: 4  Epoch: 27  Training loss = 1.6094  Validation loss = 4.6323  \n",
      "\n",
      "Fold: 4  Epoch: 28  Training loss = 1.6084  Validation loss = 4.6240  \n",
      "\n",
      "Fold: 4  Epoch: 29  Training loss = 1.6073  Validation loss = 4.6175  \n",
      "\n",
      "Fold: 4  Epoch: 30  Training loss = 1.6067  Validation loss = 4.6159  \n",
      "\n",
      "Fold: 4  Epoch: 31  Training loss = 1.6063  Validation loss = 4.6172  \n",
      "\n",
      "Fold: 4  Epoch: 32  Training loss = 1.6057  Validation loss = 4.6118  \n",
      "\n",
      "Fold: 4  Epoch: 33  Training loss = 1.6058  Validation loss = 4.6163  \n",
      "\n",
      "Fold: 4  Epoch: 34  Training loss = 1.6052  Validation loss = 4.6146  \n",
      "\n",
      "Fold: 4  Epoch: 35  Training loss = 1.6047  Validation loss = 4.6156  \n",
      "\n",
      "Fold: 4  Epoch: 36  Training loss = 1.6042  Validation loss = 4.6109  \n",
      "\n",
      "Fold: 4  Epoch: 37  Training loss = 1.6034  Validation loss = 4.6071  \n",
      "\n",
      "Fold: 4  Epoch: 38  Training loss = 1.6028  Validation loss = 4.6083  \n",
      "\n",
      "Fold: 4  Epoch: 39  Training loss = 1.6023  Validation loss = 4.6093  \n",
      "\n",
      "Fold: 4  Epoch: 40  Training loss = 1.6023  Validation loss = 4.6146  \n",
      "\n",
      "Fold: 4  Epoch: 41  Training loss = 1.6016  Validation loss = 4.6070  \n",
      "\n",
      "Fold: 4  Epoch: 42  Training loss = 1.6008  Validation loss = 4.6038  \n",
      "\n",
      "Fold: 4  Epoch: 43  Training loss = 1.6008  Validation loss = 4.6052  \n",
      "\n",
      "Fold: 4  Epoch: 44  Training loss = 1.5999  Validation loss = 4.5988  \n",
      "\n",
      "Fold: 4  Epoch: 45  Training loss = 1.5997  Validation loss = 4.6053  \n",
      "\n",
      "Fold: 4  Epoch: 46  Training loss = 1.5995  Validation loss = 4.6056  \n",
      "\n",
      "Fold: 4  Epoch: 47  Training loss = 1.5993  Validation loss = 4.6042  \n",
      "\n",
      "Fold: 4  Epoch: 48  Training loss = 1.5983  Validation loss = 4.5936  \n",
      "\n",
      "Fold: 4  Epoch: 49  Training loss = 1.5979  Validation loss = 4.5948  \n",
      "\n",
      "Fold: 4  Epoch: 50  Training loss = 1.5976  Validation loss = 4.5980  \n",
      "\n",
      "Fold: 4  Epoch: 51  Training loss = 1.5971  Validation loss = 4.5975  \n",
      "\n",
      "Fold: 4  Epoch: 52  Training loss = 1.5961  Validation loss = 4.5920  \n",
      "\n",
      "Fold: 4  Epoch: 53  Training loss = 1.5956  Validation loss = 4.5902  \n",
      "\n",
      "Fold: 4  Epoch: 54  Training loss = 1.5950  Validation loss = 4.5850  \n",
      "\n",
      "Fold: 4  Epoch: 55  Training loss = 1.5945  Validation loss = 4.5837  \n",
      "\n",
      "Fold: 4  Epoch: 56  Training loss = 1.5940  Validation loss = 4.5832  \n",
      "\n",
      "Fold: 4  Epoch: 57  Training loss = 1.5937  Validation loss = 4.5820  \n",
      "\n",
      "Fold: 4  Epoch: 58  Training loss = 1.5932  Validation loss = 4.5786  \n",
      "\n",
      "Fold: 4  Epoch: 59  Training loss = 1.5923  Validation loss = 4.5716  \n",
      "\n",
      "Fold: 4  Epoch: 60  Training loss = 1.5916  Validation loss = 4.5690  \n",
      "\n",
      "Fold: 4  Epoch: 61  Training loss = 1.5909  Validation loss = 4.5634  \n",
      "\n",
      "Fold: 4  Epoch: 62  Training loss = 1.5906  Validation loss = 4.5595  \n",
      "\n",
      "Fold: 4  Epoch: 63  Training loss = 1.5902  Validation loss = 4.5595  \n",
      "\n",
      "Fold: 4  Epoch: 64  Training loss = 1.5900  Validation loss = 4.5627  \n",
      "\n",
      "Fold: 4  Epoch: 65  Training loss = 1.5899  Validation loss = 4.5679  \n",
      "\n",
      "Fold: 4  Epoch: 66  Training loss = 1.5893  Validation loss = 4.5638  \n",
      "\n",
      "Fold: 4  Epoch: 67  Training loss = 1.5889  Validation loss = 4.5631  \n",
      "\n",
      "Fold: 4  Epoch: 68  Training loss = 1.5885  Validation loss = 4.5629  \n",
      "\n",
      "Fold: 4  Epoch: 69  Training loss = 1.5883  Validation loss = 4.5640  \n",
      "\n",
      "Fold: 4  Epoch: 70  Training loss = 1.5875  Validation loss = 4.5562  \n",
      "\n",
      "Fold: 4  Epoch: 71  Training loss = 1.5874  Validation loss = 4.5585  \n",
      "\n",
      "Fold: 4  Epoch: 72  Training loss = 1.5870  Validation loss = 4.5548  \n",
      "\n",
      "Fold: 4  Epoch: 73  Training loss = 1.5865  Validation loss = 4.5507  \n",
      "\n",
      "Fold: 4  Epoch: 74  Training loss = 1.5860  Validation loss = 4.5459  \n",
      "\n",
      "Fold: 4  Epoch: 75  Training loss = 1.5860  Validation loss = 4.5497  \n",
      "\n",
      "Fold: 4  Epoch: 76  Training loss = 1.5852  Validation loss = 4.5383  \n",
      "\n",
      "Fold: 4  Epoch: 77  Training loss = 1.5847  Validation loss = 4.5345  \n",
      "\n",
      "Fold: 4  Epoch: 78  Training loss = 1.5844  Validation loss = 4.5305  \n",
      "\n",
      "Fold: 4  Epoch: 79  Training loss = 1.5841  Validation loss = 4.5311  \n",
      "\n",
      "Fold: 4  Epoch: 80  Training loss = 1.5836  Validation loss = 4.5288  \n",
      "\n",
      "Fold: 4  Epoch: 81  Training loss = 1.5831  Validation loss = 4.5220  \n",
      "\n",
      "Fold: 4  Epoch: 82  Training loss = 1.5827  Validation loss = 4.5198  \n",
      "\n",
      "Fold: 4  Epoch: 83  Training loss = 1.5823  Validation loss = 4.5206  \n",
      "\n",
      "Fold: 4  Epoch: 84  Training loss = 1.5819  Validation loss = 4.5200  \n",
      "\n",
      "Fold: 4  Epoch: 85  Training loss = 1.5818  Validation loss = 4.5211  \n",
      "\n",
      "Fold: 4  Epoch: 86  Training loss = 1.5819  Validation loss = 4.5239  \n",
      "\n",
      "Fold: 4  Epoch: 87  Training loss = 1.5808  Validation loss = 4.5142  \n",
      "\n",
      "Fold: 4  Epoch: 88  Training loss = 1.5804  Validation loss = 4.5112  \n",
      "\n",
      "Fold: 4  Epoch: 89  Training loss = 1.5800  Validation loss = 4.5102  \n",
      "\n",
      "Fold: 4  Epoch: 90  Training loss = 1.5797  Validation loss = 4.5115  \n",
      "\n",
      "Fold: 4  Epoch: 91  Training loss = 1.5795  Validation loss = 4.5107  \n",
      "\n",
      "Fold: 4  Epoch: 92  Training loss = 1.5791  Validation loss = 4.5134  \n",
      "\n",
      "Fold: 4  Epoch: 93  Training loss = 1.5787  Validation loss = 4.5097  \n",
      "\n",
      "Fold: 4  Epoch: 94  Training loss = 1.5783  Validation loss = 4.5070  \n",
      "\n",
      "Fold: 4  Epoch: 95  Training loss = 1.5779  Validation loss = 4.5085  \n",
      "\n",
      "Fold: 4  Epoch: 96  Training loss = 1.5778  Validation loss = 4.5113  \n",
      "\n",
      "Fold: 4  Epoch: 97  Training loss = 1.5774  Validation loss = 4.5089  \n",
      "\n",
      "Fold: 4  Epoch: 98  Training loss = 1.5774  Validation loss = 4.5110  \n",
      "\n",
      "Fold: 4  Epoch: 99  Training loss = 1.5769  Validation loss = 4.5071  \n",
      "\n",
      "Fold: 4  Epoch: 100  Training loss = 1.5766  Validation loss = 4.5042  \n",
      "\n",
      "Fold: 4  Epoch: 101  Training loss = 1.5765  Validation loss = 4.5081  \n",
      "\n",
      "Fold: 4  Epoch: 102  Training loss = 1.5762  Validation loss = 4.5062  \n",
      "\n",
      "Fold: 4  Epoch: 103  Training loss = 1.5755  Validation loss = 4.5033  \n",
      "\n",
      "Fold: 4  Epoch: 104  Training loss = 1.5753  Validation loss = 4.5041  \n",
      "\n",
      "Fold: 4  Epoch: 105  Training loss = 1.5749  Validation loss = 4.5052  \n",
      "\n",
      "Fold: 4  Epoch: 106  Training loss = 1.5743  Validation loss = 4.4992  \n",
      "\n",
      "Fold: 4  Epoch: 107  Training loss = 1.5738  Validation loss = 4.4950  \n",
      "\n",
      "Fold: 4  Epoch: 108  Training loss = 1.5737  Validation loss = 4.4954  \n",
      "\n",
      "Fold: 4  Epoch: 109  Training loss = 1.5734  Validation loss = 4.4940  \n",
      "\n",
      "Fold: 4  Epoch: 110  Training loss = 1.5732  Validation loss = 4.4931  \n",
      "\n",
      "Fold: 4  Epoch: 111  Training loss = 1.5729  Validation loss = 4.4935  \n",
      "\n",
      "Fold: 4  Epoch: 112  Training loss = 1.5725  Validation loss = 4.4900  \n",
      "\n",
      "Fold: 4  Epoch: 113  Training loss = 1.5721  Validation loss = 4.4864  \n",
      "\n",
      "Fold: 4  Epoch: 114  Training loss = 1.5717  Validation loss = 4.4815  \n",
      "\n",
      "Fold: 4  Epoch: 115  Training loss = 1.5711  Validation loss = 4.4717  \n",
      "\n",
      "Fold: 4  Epoch: 116  Training loss = 1.5709  Validation loss = 4.4747  \n",
      "\n",
      "Fold: 4  Epoch: 117  Training loss = 1.5705  Validation loss = 4.4680  \n",
      "\n",
      "Fold: 4  Epoch: 118  Training loss = 1.5701  Validation loss = 4.4659  \n",
      "\n",
      "Fold: 4  Epoch: 119  Training loss = 1.5700  Validation loss = 4.4615  \n",
      "\n",
      "Fold: 4  Epoch: 120  Training loss = 1.5694  Validation loss = 4.4584  \n",
      "\n",
      "Fold: 4  Epoch: 121  Training loss = 1.5692  Validation loss = 4.4595  \n",
      "\n",
      "Fold: 4  Epoch: 122  Training loss = 1.5690  Validation loss = 4.4617  \n",
      "\n",
      "Fold: 4  Epoch: 123  Training loss = 1.5688  Validation loss = 4.4626  \n",
      "\n",
      "Fold: 4  Epoch: 124  Training loss = 1.5686  Validation loss = 4.4621  \n",
      "\n",
      "Fold: 4  Epoch: 125  Training loss = 1.5683  Validation loss = 4.4597  \n",
      "\n",
      "Fold: 4  Epoch: 126  Training loss = 1.5682  Validation loss = 4.4584  \n",
      "\n",
      "Fold: 4  Epoch: 127  Training loss = 1.5681  Validation loss = 4.4581  \n",
      "\n",
      "Fold: 4  Epoch: 128  Training loss = 1.5678  Validation loss = 4.4540  \n",
      "\n",
      "Fold: 4  Epoch: 129  Training loss = 1.5675  Validation loss = 4.4498  \n",
      "\n",
      "Fold: 4  Epoch: 130  Training loss = 1.5670  Validation loss = 4.4448  \n",
      "\n",
      "Fold: 4  Epoch: 131  Training loss = 1.5667  Validation loss = 4.4444  \n",
      "\n",
      "Fold: 4  Epoch: 132  Training loss = 1.5664  Validation loss = 4.4401  \n",
      "\n",
      "Fold: 4  Epoch: 133  Training loss = 1.5661  Validation loss = 4.4398  \n",
      "\n",
      "Fold: 4  Epoch: 134  Training loss = 1.5660  Validation loss = 4.4458  \n",
      "\n",
      "Fold: 4  Epoch: 135  Training loss = 1.5659  Validation loss = 4.4464  \n",
      "\n",
      "Fold: 4  Epoch: 136  Training loss = 1.5657  Validation loss = 4.4452  \n",
      "\n",
      "Fold: 4  Epoch: 137  Training loss = 1.5654  Validation loss = 4.4403  \n",
      "\n",
      "Fold: 4  Epoch: 138  Training loss = 1.5652  Validation loss = 4.4421  \n",
      "\n",
      "Fold: 4  Epoch: 139  Training loss = 1.5650  Validation loss = 4.4417  \n",
      "\n",
      "Fold: 4  Epoch: 140  Training loss = 1.5646  Validation loss = 4.4390  \n",
      "\n",
      "Fold: 4  Epoch: 141  Training loss = 1.5642  Validation loss = 4.4333  \n",
      "\n",
      "Fold: 4  Epoch: 142  Training loss = 1.5640  Validation loss = 4.4305  \n",
      "\n",
      "Fold: 4  Epoch: 143  Training loss = 1.5637  Validation loss = 4.4301  \n",
      "\n",
      "Fold: 4  Epoch: 144  Training loss = 1.5635  Validation loss = 4.4308  \n",
      "\n",
      "Fold: 4  Epoch: 145  Training loss = 1.5630  Validation loss = 4.4205  \n",
      "\n",
      "Fold: 4  Epoch: 146  Training loss = 1.5627  Validation loss = 4.4212  \n",
      "\n",
      "Fold: 4  Epoch: 147  Training loss = 1.5625  Validation loss = 4.4259  \n",
      "\n",
      "Fold: 4  Epoch: 148  Training loss = 1.5623  Validation loss = 4.4245  \n",
      "\n",
      "Fold: 4  Epoch: 149  Training loss = 1.5620  Validation loss = 4.4206  \n",
      "\n",
      "Fold: 4  Epoch: 150  Training loss = 1.5619  Validation loss = 4.4183  \n",
      "\n",
      "Fold: 4  Epoch: 151  Training loss = 1.5616  Validation loss = 4.4175  \n",
      "\n",
      "Fold: 4  Epoch: 152  Training loss = 1.5613  Validation loss = 4.4170  \n",
      "\n",
      "Fold: 4  Epoch: 153  Training loss = 1.5611  Validation loss = 4.4225  \n",
      "\n",
      "Fold: 4  Epoch: 154  Training loss = 1.5608  Validation loss = 4.4176  \n",
      "\n",
      "Fold: 4  Epoch: 155  Training loss = 1.5606  Validation loss = 4.4155  \n",
      "\n",
      "Fold: 4  Epoch: 156  Training loss = 1.5605  Validation loss = 4.4163  \n",
      "\n",
      "Fold: 4  Epoch: 157  Training loss = 1.5601  Validation loss = 4.4168  \n",
      "\n",
      "Fold: 4  Epoch: 158  Training loss = 1.5601  Validation loss = 4.4210  \n",
      "\n",
      "Fold: 4  Epoch: 159  Training loss = 1.5598  Validation loss = 4.4183  \n",
      "\n",
      "Fold: 4  Epoch: 160  Training loss = 1.5598  Validation loss = 4.4193  \n",
      "\n",
      "Fold: 4  Epoch: 161  Training loss = 1.5596  Validation loss = 4.4207  \n",
      "\n",
      "Fold: 4  Epoch: 162  Training loss = 1.5591  Validation loss = 4.4130  \n",
      "\n",
      "Fold: 4  Epoch: 163  Training loss = 1.5588  Validation loss = 4.4111  \n",
      "\n",
      "Fold: 4  Epoch: 164  Training loss = 1.5586  Validation loss = 4.4088  \n",
      "\n",
      "Fold: 4  Epoch: 165  Training loss = 1.5583  Validation loss = 4.4086  \n",
      "\n",
      "Fold: 4  Epoch: 166  Training loss = 1.5580  Validation loss = 4.4016  \n",
      "\n",
      "Fold: 4  Epoch: 167  Training loss = 1.5577  Validation loss = 4.3994  \n",
      "\n",
      "Fold: 4  Epoch: 168  Training loss = 1.5576  Validation loss = 4.3979  \n",
      "\n",
      "Fold: 4  Epoch: 169  Training loss = 1.5572  Validation loss = 4.3974  \n",
      "\n",
      "Fold: 4  Epoch: 170  Training loss = 1.5570  Validation loss = 4.3963  \n",
      "\n",
      "Fold: 4  Epoch: 171  Training loss = 1.5567  Validation loss = 4.3952  \n",
      "\n",
      "Fold: 4  Epoch: 172  Training loss = 1.5565  Validation loss = 4.3921  \n",
      "\n",
      "Fold: 4  Epoch: 173  Training loss = 1.5562  Validation loss = 4.3894  \n",
      "\n",
      "Fold: 4  Epoch: 174  Training loss = 1.5559  Validation loss = 4.3876  \n",
      "\n",
      "Fold: 4  Epoch: 175  Training loss = 1.5557  Validation loss = 4.3874  \n",
      "\n",
      "Fold: 4  Epoch: 176  Training loss = 1.5553  Validation loss = 4.3826  \n",
      "\n",
      "Fold: 4  Epoch: 177  Training loss = 1.5550  Validation loss = 4.3773  \n",
      "\n",
      "Fold: 4  Epoch: 178  Training loss = 1.5548  Validation loss = 4.3789  \n",
      "\n",
      "Fold: 4  Epoch: 179  Training loss = 1.5543  Validation loss = 4.3766  \n",
      "\n",
      "Fold: 4  Epoch: 180  Training loss = 1.5541  Validation loss = 4.3745  \n",
      "\n",
      "Fold: 4  Epoch: 181  Training loss = 1.5538  Validation loss = 4.3738  \n",
      "\n",
      "Fold: 4  Epoch: 182  Training loss = 1.5535  Validation loss = 4.3711  \n",
      "\n",
      "Fold: 4  Epoch: 183  Training loss = 1.5533  Validation loss = 4.3749  \n",
      "\n",
      "Fold: 4  Epoch: 184  Training loss = 1.5530  Validation loss = 4.3722  \n",
      "\n",
      "Fold: 4  Epoch: 185  Training loss = 1.5528  Validation loss = 4.3684  \n",
      "\n",
      "Fold: 4  Epoch: 186  Training loss = 1.5526  Validation loss = 4.3665  \n",
      "\n",
      "Fold: 4  Epoch: 187  Training loss = 1.5523  Validation loss = 4.3677  \n",
      "\n",
      "Fold: 4  Epoch: 188  Training loss = 1.5521  Validation loss = 4.3722  \n",
      "\n",
      "Fold: 4  Epoch: 189  Training loss = 1.5517  Validation loss = 4.3641  \n",
      "\n",
      "Fold: 4  Epoch: 190  Training loss = 1.5516  Validation loss = 4.3662  \n",
      "\n",
      "Fold: 4  Epoch: 191  Training loss = 1.5512  Validation loss = 4.3602  \n",
      "\n",
      "Fold: 4  Epoch: 192  Training loss = 1.5509  Validation loss = 4.3602  \n",
      "\n",
      "Fold: 4  Epoch: 193  Training loss = 1.5507  Validation loss = 4.3514  \n",
      "\n",
      "Fold: 4  Epoch: 194  Training loss = 1.5505  Validation loss = 4.3531  \n",
      "\n",
      "Fold: 4  Epoch: 195  Training loss = 1.5504  Validation loss = 4.3558  \n",
      "\n",
      "Fold: 4  Epoch: 196  Training loss = 1.5501  Validation loss = 4.3555  \n",
      "\n",
      "Fold: 4  Epoch: 197  Training loss = 1.5500  Validation loss = 4.3600  \n",
      "\n",
      "Fold: 4  Epoch: 198  Training loss = 1.5498  Validation loss = 4.3600  \n",
      "\n",
      "Fold: 4  Epoch: 199  Training loss = 1.5496  Validation loss = 4.3576  \n",
      "\n",
      "Fold: 4  Epoch: 200  Training loss = 1.5493  Validation loss = 4.3548  \n",
      "\n",
      "Fold: 4  Epoch: 201  Training loss = 1.5491  Validation loss = 4.3529  \n",
      "\n",
      "Fold: 4  Epoch: 202  Training loss = 1.5489  Validation loss = 4.3459  \n",
      "\n",
      "Fold: 4  Epoch: 203  Training loss = 1.5487  Validation loss = 4.3403  \n",
      "\n",
      "Fold: 4  Epoch: 204  Training loss = 1.5484  Validation loss = 4.3413  \n",
      "\n",
      "Fold: 4  Epoch: 205  Training loss = 1.5484  Validation loss = 4.3434  \n",
      "\n",
      "Fold: 4  Epoch: 206  Training loss = 1.5482  Validation loss = 4.3432  \n",
      "\n",
      "Fold: 4  Epoch: 207  Training loss = 1.5480  Validation loss = 4.3511  \n",
      "\n",
      "Fold: 4  Epoch: 208  Training loss = 1.5479  Validation loss = 4.3521  \n",
      "\n",
      "Fold: 4  Epoch: 209  Training loss = 1.5477  Validation loss = 4.3472  \n",
      "\n",
      "Fold: 4  Epoch: 210  Training loss = 1.5475  Validation loss = 4.3458  \n",
      "\n",
      "Fold: 4  Epoch: 211  Training loss = 1.5472  Validation loss = 4.3418  \n",
      "\n",
      "Fold: 4  Epoch: 212  Training loss = 1.5469  Validation loss = 4.3382  \n",
      "\n",
      "Fold: 4  Epoch: 213  Training loss = 1.5467  Validation loss = 4.3323  \n",
      "\n",
      "Fold: 4  Epoch: 214  Training loss = 1.5464  Validation loss = 4.3282  \n",
      "\n",
      "Fold: 4  Epoch: 215  Training loss = 1.5461  Validation loss = 4.3292  \n",
      "\n",
      "Fold: 4  Epoch: 216  Training loss = 1.5459  Validation loss = 4.3266  \n",
      "\n",
      "Fold: 4  Epoch: 217  Training loss = 1.5457  Validation loss = 4.3266  \n",
      "\n",
      "Fold: 4  Epoch: 218  Training loss = 1.5455  Validation loss = 4.3220  \n",
      "\n",
      "Fold: 4  Epoch: 219  Training loss = 1.5452  Validation loss = 4.3235  \n",
      "\n",
      "Fold: 4  Epoch: 220  Training loss = 1.5448  Validation loss = 4.3198  \n",
      "\n",
      "Fold: 4  Epoch: 221  Training loss = 1.5446  Validation loss = 4.3217  \n",
      "\n",
      "Fold: 4  Epoch: 222  Training loss = 1.5445  Validation loss = 4.3241  \n",
      "\n",
      "Fold: 4  Epoch: 223  Training loss = 1.5443  Validation loss = 4.3248  \n",
      "\n",
      "Fold: 4  Epoch: 224  Training loss = 1.5440  Validation loss = 4.3219  \n",
      "\n",
      "Fold: 4  Epoch: 225  Training loss = 1.5437  Validation loss = 4.3191  \n",
      "\n",
      "Fold: 4  Epoch: 226  Training loss = 1.5436  Validation loss = 4.3181  \n",
      "\n",
      "Fold: 4  Epoch: 227  Training loss = 1.5434  Validation loss = 4.3099  \n",
      "\n",
      "Fold: 4  Epoch: 228  Training loss = 1.5432  Validation loss = 4.3104  \n",
      "\n",
      "Fold: 4  Epoch: 229  Training loss = 1.5429  Validation loss = 4.3075  \n",
      "\n",
      "Fold: 4  Epoch: 230  Training loss = 1.5427  Validation loss = 4.3136  \n",
      "\n",
      "Fold: 4  Epoch: 231  Training loss = 1.5426  Validation loss = 4.3136  \n",
      "\n",
      "Fold: 4  Epoch: 232  Training loss = 1.5424  Validation loss = 4.3152  \n",
      "\n",
      "Fold: 4  Epoch: 233  Training loss = 1.5424  Validation loss = 4.3149  \n",
      "\n",
      "Fold: 4  Epoch: 234  Training loss = 1.5422  Validation loss = 4.3136  \n",
      "\n",
      "Fold: 4  Epoch: 235  Training loss = 1.5419  Validation loss = 4.3035  \n",
      "\n",
      "Fold: 4  Epoch: 236  Training loss = 1.5418  Validation loss = 4.3053  \n",
      "\n",
      "Fold: 4  Epoch: 237  Training loss = 1.5415  Validation loss = 4.3025  \n",
      "\n",
      "Fold: 4  Epoch: 238  Training loss = 1.5413  Validation loss = 4.2995  \n",
      "\n",
      "Fold: 4  Epoch: 239  Training loss = 1.5412  Validation loss = 4.3027  \n",
      "\n",
      "Fold: 4  Epoch: 240  Training loss = 1.5409  Validation loss = 4.2998  \n",
      "\n",
      "Fold: 4  Epoch: 241  Training loss = 1.5409  Validation loss = 4.3039  \n",
      "\n",
      "Fold: 4  Epoch: 242  Training loss = 1.5407  Validation loss = 4.3047  \n",
      "\n",
      "Fold: 4  Epoch: 243  Training loss = 1.5406  Validation loss = 4.3131  \n",
      "\n",
      "Fold: 4  Epoch: 244  Training loss = 1.5404  Validation loss = 4.3072  \n",
      "\n",
      "Fold: 4  Epoch: 245  Training loss = 1.5403  Validation loss = 4.3096  \n",
      "\n",
      "Fold: 4  Epoch: 246  Training loss = 1.5401  Validation loss = 4.3108  \n",
      "\n",
      "Fold: 4  Epoch: 247  Training loss = 1.5399  Validation loss = 4.3124  \n",
      "\n",
      "Fold: 4  Epoch: 248  Training loss = 1.5398  Validation loss = 4.3143  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 238  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.8303  Validation loss = 4.5725  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.8280  Validation loss = 4.5598  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.8276  Validation loss = 4.5596  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.8255  Validation loss = 4.5494  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.8255  Validation loss = 4.5505  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 1.8245  Validation loss = 4.5480  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 1.8237  Validation loss = 4.5440  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.8221  Validation loss = 4.5370  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 1.8218  Validation loss = 4.5376  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.8195  Validation loss = 4.5260  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 1.8171  Validation loss = 4.5122  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 1.8146  Validation loss = 4.4994  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 1.8137  Validation loss = 4.4960  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.8126  Validation loss = 4.4927  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 1.8103  Validation loss = 4.4813  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 1.8091  Validation loss = 4.4773  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 1.8085  Validation loss = 4.4785  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 1.8069  Validation loss = 4.4731  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 1.8051  Validation loss = 4.4606  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 1.8044  Validation loss = 4.4639  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 1.8039  Validation loss = 4.4633  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 1.8033  Validation loss = 4.4627  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 1.8026  Validation loss = 4.4613  \n",
      "\n",
      "Fold: 5  Epoch: 24  Training loss = 1.8014  Validation loss = 4.4556  \n",
      "\n",
      "Fold: 5  Epoch: 25  Training loss = 1.7998  Validation loss = 4.4492  \n",
      "\n",
      "Fold: 5  Epoch: 26  Training loss = 1.7982  Validation loss = 4.4415  \n",
      "\n",
      "Fold: 5  Epoch: 27  Training loss = 1.7971  Validation loss = 4.4348  \n",
      "\n",
      "Fold: 5  Epoch: 28  Training loss = 1.7956  Validation loss = 4.4238  \n",
      "\n",
      "Fold: 5  Epoch: 29  Training loss = 1.7940  Validation loss = 4.4155  \n",
      "\n",
      "Fold: 5  Epoch: 30  Training loss = 1.7931  Validation loss = 4.4128  \n",
      "\n",
      "Fold: 5  Epoch: 31  Training loss = 1.7916  Validation loss = 4.4015  \n",
      "\n",
      "Fold: 5  Epoch: 32  Training loss = 1.7896  Validation loss = 4.3887  \n",
      "\n",
      "Fold: 5  Epoch: 33  Training loss = 1.7891  Validation loss = 4.3863  \n",
      "\n",
      "Fold: 5  Epoch: 34  Training loss = 1.7887  Validation loss = 4.3845  \n",
      "\n",
      "Fold: 5  Epoch: 35  Training loss = 1.7879  Validation loss = 4.3853  \n",
      "\n",
      "Fold: 5  Epoch: 36  Training loss = 1.7870  Validation loss = 4.3777  \n",
      "\n",
      "Fold: 5  Epoch: 37  Training loss = 1.7863  Validation loss = 4.3766  \n",
      "\n",
      "Fold: 5  Epoch: 38  Training loss = 1.7858  Validation loss = 4.3756  \n",
      "\n",
      "Fold: 5  Epoch: 39  Training loss = 1.7849  Validation loss = 4.3725  \n",
      "\n",
      "Fold: 5  Epoch: 40  Training loss = 1.7840  Validation loss = 4.3679  \n",
      "\n",
      "Fold: 5  Epoch: 41  Training loss = 1.7838  Validation loss = 4.3759  \n",
      "\n",
      "Fold: 5  Epoch: 42  Training loss = 1.7832  Validation loss = 4.3750  \n",
      "\n",
      "Fold: 5  Epoch: 43  Training loss = 1.7822  Validation loss = 4.3701  \n",
      "\n",
      "Fold: 5  Epoch: 44  Training loss = 1.7813  Validation loss = 4.3653  \n",
      "\n",
      "Fold: 5  Epoch: 45  Training loss = 1.7802  Validation loss = 4.3550  \n",
      "\n",
      "Fold: 5  Epoch: 46  Training loss = 1.7786  Validation loss = 4.3376  \n",
      "\n",
      "Fold: 5  Epoch: 47  Training loss = 1.7782  Validation loss = 4.3348  \n",
      "\n",
      "Fold: 5  Epoch: 48  Training loss = 1.7778  Validation loss = 4.3347  \n",
      "\n",
      "Fold: 5  Epoch: 49  Training loss = 1.7770  Validation loss = 4.3289  \n",
      "\n",
      "Fold: 5  Epoch: 50  Training loss = 1.7768  Validation loss = 4.3327  \n",
      "\n",
      "Fold: 5  Epoch: 51  Training loss = 1.7768  Validation loss = 4.3357  \n",
      "\n",
      "Fold: 5  Epoch: 52  Training loss = 1.7763  Validation loss = 4.3309  \n",
      "\n",
      "Fold: 5  Epoch: 53  Training loss = 1.7749  Validation loss = 4.3215  \n",
      "\n",
      "Fold: 5  Epoch: 54  Training loss = 1.7738  Validation loss = 4.3130  \n",
      "\n",
      "Fold: 5  Epoch: 55  Training loss = 1.7727  Validation loss = 4.3053  \n",
      "\n",
      "Fold: 5  Epoch: 56  Training loss = 1.7718  Validation loss = 4.3005  \n",
      "\n",
      "Fold: 5  Epoch: 57  Training loss = 1.7715  Validation loss = 4.3021  \n",
      "\n",
      "Fold: 5  Epoch: 58  Training loss = 1.7706  Validation loss = 4.2990  \n",
      "\n",
      "Fold: 5  Epoch: 59  Training loss = 1.7696  Validation loss = 4.2893  \n",
      "\n",
      "Fold: 5  Epoch: 60  Training loss = 1.7688  Validation loss = 4.2873  \n",
      "\n",
      "Fold: 5  Epoch: 61  Training loss = 1.7685  Validation loss = 4.2919  \n",
      "\n",
      "Fold: 5  Epoch: 62  Training loss = 1.7678  Validation loss = 4.2868  \n",
      "\n",
      "Fold: 5  Epoch: 63  Training loss = 1.7671  Validation loss = 4.2865  \n",
      "\n",
      "Fold: 5  Epoch: 64  Training loss = 1.7667  Validation loss = 4.2852  \n",
      "\n",
      "Fold: 5  Epoch: 65  Training loss = 1.7664  Validation loss = 4.2869  \n",
      "\n",
      "Fold: 5  Epoch: 66  Training loss = 1.7660  Validation loss = 4.2864  \n",
      "\n",
      "Fold: 5  Epoch: 67  Training loss = 1.7657  Validation loss = 4.2858  \n",
      "\n",
      "Fold: 5  Epoch: 68  Training loss = 1.7653  Validation loss = 4.2895  \n",
      "\n",
      "Fold: 5  Epoch: 69  Training loss = 1.7644  Validation loss = 4.2824  \n",
      "\n",
      "Fold: 5  Epoch: 70  Training loss = 1.7632  Validation loss = 4.2793  \n",
      "\n",
      "Fold: 5  Epoch: 71  Training loss = 1.7622  Validation loss = 4.2694  \n",
      "\n",
      "Fold: 5  Epoch: 72  Training loss = 1.7617  Validation loss = 4.2677  \n",
      "\n",
      "Fold: 5  Epoch: 73  Training loss = 1.7607  Validation loss = 4.2593  \n",
      "\n",
      "Fold: 5  Epoch: 74  Training loss = 1.7594  Validation loss = 4.2502  \n",
      "\n",
      "Fold: 5  Epoch: 75  Training loss = 1.7587  Validation loss = 4.2473  \n",
      "\n",
      "Fold: 5  Epoch: 76  Training loss = 1.7582  Validation loss = 4.2432  \n",
      "\n",
      "Fold: 5  Epoch: 77  Training loss = 1.7572  Validation loss = 4.2319  \n",
      "\n",
      "Fold: 5  Epoch: 78  Training loss = 1.7566  Validation loss = 4.2274  \n",
      "\n",
      "Fold: 5  Epoch: 79  Training loss = 1.7562  Validation loss = 4.2306  \n",
      "\n",
      "Fold: 5  Epoch: 80  Training loss = 1.7555  Validation loss = 4.2297  \n",
      "\n",
      "Fold: 5  Epoch: 81  Training loss = 1.7551  Validation loss = 4.2367  \n",
      "\n",
      "Fold: 5  Epoch: 82  Training loss = 1.7545  Validation loss = 4.2328  \n",
      "\n",
      "Fold: 5  Epoch: 83  Training loss = 1.7542  Validation loss = 4.2360  \n",
      "\n",
      "Fold: 5  Epoch: 84  Training loss = 1.7537  Validation loss = 4.2354  \n",
      "\n",
      "Fold: 5  Epoch: 85  Training loss = 1.7529  Validation loss = 4.2307  \n",
      "\n",
      "Fold: 5  Epoch: 86  Training loss = 1.7526  Validation loss = 4.2356  \n",
      "\n",
      "Fold: 5  Epoch: 87  Training loss = 1.7524  Validation loss = 4.2373  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 78  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 1.9962  Validation loss = 1.9937  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 1.9950  Validation loss = 1.9908  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 1.9894  Validation loss = 1.9718  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 1.9824  Validation loss = 1.9468  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 1.9802  Validation loss = 1.9402  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 1.9756  Validation loss = 1.9233  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 1.9733  Validation loss = 1.9173  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 1.9685  Validation loss = 1.8985  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 1.9639  Validation loss = 1.8800  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 1.9605  Validation loss = 1.8667  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 1.9574  Validation loss = 1.8532  \n",
      "\n",
      "Fold: 6  Epoch: 12  Training loss = 1.9555  Validation loss = 1.8467  \n",
      "\n",
      "Fold: 6  Epoch: 13  Training loss = 1.9537  Validation loss = 1.8411  \n",
      "\n",
      "Fold: 6  Epoch: 14  Training loss = 1.9516  Validation loss = 1.8320  \n",
      "\n",
      "Fold: 6  Epoch: 15  Training loss = 1.9509  Validation loss = 1.8298  \n",
      "\n",
      "Fold: 6  Epoch: 16  Training loss = 1.9490  Validation loss = 1.8226  \n",
      "\n",
      "Fold: 6  Epoch: 17  Training loss = 1.9478  Validation loss = 1.8193  \n",
      "\n",
      "Fold: 6  Epoch: 18  Training loss = 1.9466  Validation loss = 1.8150  \n",
      "\n",
      "Fold: 6  Epoch: 19  Training loss = 1.9432  Validation loss = 1.7983  \n",
      "\n",
      "Fold: 6  Epoch: 20  Training loss = 1.9418  Validation loss = 1.7913  \n",
      "\n",
      "Fold: 6  Epoch: 21  Training loss = 1.9393  Validation loss = 1.7801  \n",
      "\n",
      "Fold: 6  Epoch: 22  Training loss = 1.9370  Validation loss = 1.7692  \n",
      "\n",
      "Fold: 6  Epoch: 23  Training loss = 1.9368  Validation loss = 1.7736  \n",
      "\n",
      "Fold: 6  Epoch: 24  Training loss = 1.9352  Validation loss = 1.7656  \n",
      "\n",
      "Fold: 6  Epoch: 25  Training loss = 1.9323  Validation loss = 1.7486  \n",
      "\n",
      "Fold: 6  Epoch: 26  Training loss = 1.9301  Validation loss = 1.7373  \n",
      "\n",
      "Fold: 6  Epoch: 27  Training loss = 1.9286  Validation loss = 1.7304  \n",
      "\n",
      "Fold: 6  Epoch: 28  Training loss = 1.9255  Validation loss = 1.7076  \n",
      "\n",
      "Fold: 6  Epoch: 29  Training loss = 1.9240  Validation loss = 1.6998  \n",
      "\n",
      "Fold: 6  Epoch: 30  Training loss = 1.9231  Validation loss = 1.7001  \n",
      "\n",
      "Fold: 6  Epoch: 31  Training loss = 1.9220  Validation loss = 1.6950  \n",
      "\n",
      "Fold: 6  Epoch: 32  Training loss = 1.9208  Validation loss = 1.6937  \n",
      "\n",
      "Fold: 6  Epoch: 33  Training loss = 1.9184  Validation loss = 1.6758  \n",
      "\n",
      "Fold: 6  Epoch: 34  Training loss = 1.9172  Validation loss = 1.6710  \n",
      "\n",
      "Fold: 6  Epoch: 35  Training loss = 1.9166  Validation loss = 1.6723  \n",
      "\n",
      "Fold: 6  Epoch: 36  Training loss = 1.9147  Validation loss = 1.6634  \n",
      "\n",
      "Fold: 6  Epoch: 37  Training loss = 1.9135  Validation loss = 1.6584  \n",
      "\n",
      "Fold: 6  Epoch: 38  Training loss = 1.9121  Validation loss = 1.6510  \n",
      "\n",
      "Fold: 6  Epoch: 39  Training loss = 1.9123  Validation loss = 1.6601  \n",
      "\n",
      "Fold: 6  Epoch: 40  Training loss = 1.9110  Validation loss = 1.6524  \n",
      "\n",
      "Fold: 6  Epoch: 41  Training loss = 1.9099  Validation loss = 1.6477  \n",
      "\n",
      "Fold: 6  Epoch: 42  Training loss = 1.9090  Validation loss = 1.6477  \n",
      "\n",
      "Fold: 6  Epoch: 43  Training loss = 1.9076  Validation loss = 1.6394  \n",
      "\n",
      "Fold: 6  Epoch: 44  Training loss = 1.9062  Validation loss = 1.6350  \n",
      "\n",
      "Fold: 6  Epoch: 45  Training loss = 1.9046  Validation loss = 1.6248  \n",
      "\n",
      "Fold: 6  Epoch: 46  Training loss = 1.9034  Validation loss = 1.6179  \n",
      "\n",
      "Fold: 6  Epoch: 47  Training loss = 1.9025  Validation loss = 1.6138  \n",
      "\n",
      "Fold: 6  Epoch: 48  Training loss = 1.9019  Validation loss = 1.6124  \n",
      "\n",
      "Fold: 6  Epoch: 49  Training loss = 1.9008  Validation loss = 1.6065  \n",
      "\n",
      "Fold: 6  Epoch: 50  Training loss = 1.9004  Validation loss = 1.6096  \n",
      "\n",
      "Fold: 6  Epoch: 51  Training loss = 1.8993  Validation loss = 1.6069  \n",
      "\n",
      "Fold: 6  Epoch: 52  Training loss = 1.8980  Validation loss = 1.6031  \n",
      "\n",
      "Fold: 6  Epoch: 53  Training loss = 1.8975  Validation loss = 1.6095  \n",
      "\n",
      "Fold: 6  Epoch: 54  Training loss = 1.8961  Validation loss = 1.6013  \n",
      "\n",
      "Fold: 6  Epoch: 55  Training loss = 1.8941  Validation loss = 1.5816  \n",
      "\n",
      "Fold: 6  Epoch: 56  Training loss = 1.8933  Validation loss = 1.5794  \n",
      "\n",
      "Fold: 6  Epoch: 57  Training loss = 1.8926  Validation loss = 1.5757  \n",
      "\n",
      "Fold: 6  Epoch: 58  Training loss = 1.8916  Validation loss = 1.5729  \n",
      "\n",
      "Fold: 6  Epoch: 59  Training loss = 1.8903  Validation loss = 1.5650  \n",
      "\n",
      "Fold: 6  Epoch: 60  Training loss = 1.8887  Validation loss = 1.5521  \n",
      "\n",
      "Fold: 6  Epoch: 61  Training loss = 1.8869  Validation loss = 1.5359  \n",
      "\n",
      "Fold: 6  Epoch: 62  Training loss = 1.8863  Validation loss = 1.5422  \n",
      "\n",
      "Fold: 6  Epoch: 63  Training loss = 1.8852  Validation loss = 1.5380  \n",
      "\n",
      "Fold: 6  Epoch: 64  Training loss = 1.8846  Validation loss = 1.5460  \n",
      "\n",
      "Fold: 6  Epoch: 65  Training loss = 1.8836  Validation loss = 1.5452  \n",
      "\n",
      "Fold: 6  Epoch: 66  Training loss = 1.8832  Validation loss = 1.5500  \n",
      "\n",
      "Fold: 6  Epoch: 67  Training loss = 1.8819  Validation loss = 1.5461  \n",
      "\n",
      "Fold: 6  Epoch: 68  Training loss = 1.8808  Validation loss = 1.5435  \n",
      "\n",
      "Fold: 6  Epoch: 69  Training loss = 1.8801  Validation loss = 1.5486  \n",
      "\n",
      "Fold: 6  Epoch: 70  Training loss = 1.8792  Validation loss = 1.5475  \n",
      "\n",
      "Fold: 6  Epoch: 71  Training loss = 1.8787  Validation loss = 1.5557  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 61  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 1.8639  Validation loss = 1.2523  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 1.8604  Validation loss = 1.2390  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 1.8578  Validation loss = 1.2269  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.8557  Validation loss = 1.2198  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 1.8536  Validation loss = 1.2113  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 1.8513  Validation loss = 1.2036  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 1.8496  Validation loss = 1.1969  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 1.8476  Validation loss = 1.1878  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 1.8479  Validation loss = 1.1968  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 1.8461  Validation loss = 1.1931  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 1.8451  Validation loss = 1.1895  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 1.8430  Validation loss = 1.1795  \n",
      "\n",
      "Fold: 7  Epoch: 13  Training loss = 1.8390  Validation loss = 1.1593  \n",
      "\n",
      "Fold: 7  Epoch: 14  Training loss = 1.8372  Validation loss = 1.1515  \n",
      "\n",
      "Fold: 7  Epoch: 15  Training loss = 1.8347  Validation loss = 1.1391  \n",
      "\n",
      "Fold: 7  Epoch: 16  Training loss = 1.8328  Validation loss = 1.1318  \n",
      "\n",
      "Fold: 7  Epoch: 17  Training loss = 1.8309  Validation loss = 1.1224  \n",
      "\n",
      "Fold: 7  Epoch: 18  Training loss = 1.8292  Validation loss = 1.1163  \n",
      "\n",
      "Fold: 7  Epoch: 19  Training loss = 1.8284  Validation loss = 1.1195  \n",
      "\n",
      "Fold: 7  Epoch: 20  Training loss = 1.8270  Validation loss = 1.1144  \n",
      "\n",
      "Fold: 7  Epoch: 21  Training loss = 1.8257  Validation loss = 1.1087  \n",
      "\n",
      "Fold: 7  Epoch: 22  Training loss = 1.8237  Validation loss = 1.0985  \n",
      "\n",
      "Fold: 7  Epoch: 23  Training loss = 1.8219  Validation loss = 1.0919  \n",
      "\n",
      "Fold: 7  Epoch: 24  Training loss = 1.8214  Validation loss = 1.0943  \n",
      "\n",
      "Fold: 7  Epoch: 25  Training loss = 1.8203  Validation loss = 1.0931  \n",
      "\n",
      "Fold: 7  Epoch: 26  Training loss = 1.8184  Validation loss = 1.0837  \n",
      "\n",
      "Fold: 7  Epoch: 27  Training loss = 1.8163  Validation loss = 1.0720  \n",
      "\n",
      "Fold: 7  Epoch: 28  Training loss = 1.8151  Validation loss = 1.0663  \n",
      "\n",
      "Fold: 7  Epoch: 29  Training loss = 1.8139  Validation loss = 1.0631  \n",
      "\n",
      "Fold: 7  Epoch: 30  Training loss = 1.8111  Validation loss = 1.0431  \n",
      "\n",
      "Fold: 7  Epoch: 31  Training loss = 1.8089  Validation loss = 1.0292  \n",
      "\n",
      "Fold: 7  Epoch: 32  Training loss = 1.8076  Validation loss = 1.0242  \n",
      "\n",
      "Fold: 7  Epoch: 33  Training loss = 1.8064  Validation loss = 1.0193  \n",
      "\n",
      "Fold: 7  Epoch: 34  Training loss = 1.8050  Validation loss = 1.0135  \n",
      "\n",
      "Fold: 7  Epoch: 35  Training loss = 1.8030  Validation loss = 0.9967  \n",
      "\n",
      "Fold: 7  Epoch: 36  Training loss = 1.8024  Validation loss = 1.0007  \n",
      "\n",
      "Fold: 7  Epoch: 37  Training loss = 1.8012  Validation loss = 0.9974  \n",
      "\n",
      "Fold: 7  Epoch: 38  Training loss = 1.7991  Validation loss = 0.9813  \n",
      "\n",
      "Fold: 7  Epoch: 39  Training loss = 1.7981  Validation loss = 0.9807  \n",
      "\n",
      "Fold: 7  Epoch: 40  Training loss = 1.7967  Validation loss = 0.9756  \n",
      "\n",
      "Fold: 7  Epoch: 41  Training loss = 1.7958  Validation loss = 0.9778  \n",
      "\n",
      "Fold: 7  Epoch: 42  Training loss = 1.7937  Validation loss = 0.9623  \n",
      "\n",
      "Fold: 7  Epoch: 43  Training loss = 1.7928  Validation loss = 0.9597  \n",
      "\n",
      "Fold: 7  Epoch: 44  Training loss = 1.7910  Validation loss = 0.9555  \n",
      "\n",
      "Fold: 7  Epoch: 45  Training loss = 1.7903  Validation loss = 0.9588  \n",
      "\n",
      "Fold: 7  Epoch: 46  Training loss = 1.7892  Validation loss = 0.9558  \n",
      "\n",
      "Fold: 7  Epoch: 47  Training loss = 1.7881  Validation loss = 0.9551  \n",
      "\n",
      "Fold: 7  Epoch: 48  Training loss = 1.7872  Validation loss = 0.9541  \n",
      "\n",
      "Fold: 7  Epoch: 49  Training loss = 1.7859  Validation loss = 0.9503  \n",
      "\n",
      "Fold: 7  Epoch: 50  Training loss = 1.7846  Validation loss = 0.9460  \n",
      "\n",
      "Fold: 7  Epoch: 51  Training loss = 1.7837  Validation loss = 0.9467  \n",
      "\n",
      "Fold: 7  Epoch: 52  Training loss = 1.7818  Validation loss = 0.9358  \n",
      "\n",
      "Fold: 7  Epoch: 53  Training loss = 1.7805  Validation loss = 0.9303  \n",
      "\n",
      "Fold: 7  Epoch: 54  Training loss = 1.7793  Validation loss = 0.9249  \n",
      "\n",
      "Fold: 7  Epoch: 55  Training loss = 1.7778  Validation loss = 0.9175  \n",
      "\n",
      "Fold: 7  Epoch: 56  Training loss = 1.7766  Validation loss = 0.9186  \n",
      "\n",
      "Fold: 7  Epoch: 57  Training loss = 1.7754  Validation loss = 0.9164  \n",
      "\n",
      "Fold: 7  Epoch: 58  Training loss = 1.7738  Validation loss = 0.9086  \n",
      "\n",
      "Fold: 7  Epoch: 59  Training loss = 1.7727  Validation loss = 0.9076  \n",
      "\n",
      "Fold: 7  Epoch: 60  Training loss = 1.7708  Validation loss = 0.8951  \n",
      "\n",
      "Fold: 7  Epoch: 61  Training loss = 1.7698  Validation loss = 0.8915  \n",
      "\n",
      "Fold: 7  Epoch: 62  Training loss = 1.7684  Validation loss = 0.8853  \n",
      "\n",
      "Fold: 7  Epoch: 63  Training loss = 1.7669  Validation loss = 0.8823  \n",
      "\n",
      "Fold: 7  Epoch: 64  Training loss = 1.7655  Validation loss = 0.8756  \n",
      "\n",
      "Fold: 7  Epoch: 65  Training loss = 1.7641  Validation loss = 0.8710  \n",
      "\n",
      "Fold: 7  Epoch: 66  Training loss = 1.7629  Validation loss = 0.8674  \n",
      "\n",
      "Fold: 7  Epoch: 67  Training loss = 1.7615  Validation loss = 0.8568  \n",
      "\n",
      "Fold: 7  Epoch: 68  Training loss = 1.7601  Validation loss = 0.8497  \n",
      "\n",
      "Fold: 7  Epoch: 69  Training loss = 1.7593  Validation loss = 0.8495  \n",
      "\n",
      "Fold: 7  Epoch: 70  Training loss = 1.7582  Validation loss = 0.8419  \n",
      "\n",
      "Fold: 7  Epoch: 71  Training loss = 1.7572  Validation loss = 0.8454  \n",
      "\n",
      "Fold: 7  Epoch: 72  Training loss = 1.7559  Validation loss = 0.8490  \n",
      "\n",
      "Fold: 7  Epoch: 73  Training loss = 1.7545  Validation loss = 0.8546  \n",
      "\n",
      "Fold: 7  Epoch: 74  Training loss = 1.7529  Validation loss = 0.8471  \n",
      "\n",
      "Fold: 7  Epoch: 75  Training loss = 1.7518  Validation loss = 0.8476  \n",
      "\n",
      "Fold: 7  Epoch: 76  Training loss = 1.7510  Validation loss = 0.8513  \n",
      "\n",
      "Fold: 7  Epoch: 77  Training loss = 1.7498  Validation loss = 0.8508  \n",
      "\n",
      "Fold: 7  Epoch: 78  Training loss = 1.7486  Validation loss = 0.8490  \n",
      "\n",
      "Fold: 7  Epoch: 79  Training loss = 1.7476  Validation loss = 0.8531  \n",
      "\n",
      "Fold: 7  Epoch: 80  Training loss = 1.7465  Validation loss = 0.8515  \n",
      "\n",
      "Fold: 7  Epoch: 81  Training loss = 1.7457  Validation loss = 0.8535  \n",
      "\n",
      "Fold: 7  Epoch: 82  Training loss = 1.7443  Validation loss = 0.8470  \n",
      "\n",
      "Fold: 7  Epoch: 83  Training loss = 1.7426  Validation loss = 0.8380  \n",
      "\n",
      "Fold: 7  Epoch: 84  Training loss = 1.7413  Validation loss = 0.8312  \n",
      "\n",
      "Fold: 7  Epoch: 85  Training loss = 1.7401  Validation loss = 0.8249  \n",
      "\n",
      "Fold: 7  Epoch: 86  Training loss = 1.7392  Validation loss = 0.8280  \n",
      "\n",
      "Fold: 7  Epoch: 87  Training loss = 1.7377  Validation loss = 0.8192  \n",
      "\n",
      "Fold: 7  Epoch: 88  Training loss = 1.7371  Validation loss = 0.8187  \n",
      "\n",
      "Fold: 7  Epoch: 89  Training loss = 1.7358  Validation loss = 0.8103  \n",
      "\n",
      "Fold: 7  Epoch: 90  Training loss = 1.7346  Validation loss = 0.8098  \n",
      "\n",
      "Fold: 7  Epoch: 91  Training loss = 1.7334  Validation loss = 0.8146  \n",
      "\n",
      "Fold: 7  Epoch: 92  Training loss = 1.7322  Validation loss = 0.8153  \n",
      "\n",
      "Fold: 7  Epoch: 93  Training loss = 1.7309  Validation loss = 0.8084  \n",
      "\n",
      "Fold: 7  Epoch: 94  Training loss = 1.7299  Validation loss = 0.8081  \n",
      "\n",
      "Fold: 7  Epoch: 95  Training loss = 1.7284  Validation loss = 0.8029  \n",
      "\n",
      "Fold: 7  Epoch: 96  Training loss = 1.7273  Validation loss = 0.8028  \n",
      "\n",
      "Fold: 7  Epoch: 97  Training loss = 1.7262  Validation loss = 0.8046  \n",
      "\n",
      "Fold: 7  Epoch: 98  Training loss = 1.7252  Validation loss = 0.8054  \n",
      "\n",
      "Fold: 7  Epoch: 99  Training loss = 1.7236  Validation loss = 0.7977  \n",
      "\n",
      "Fold: 7  Epoch: 100  Training loss = 1.7225  Validation loss = 0.7965  \n",
      "\n",
      "Fold: 7  Epoch: 101  Training loss = 1.7212  Validation loss = 0.7907  \n",
      "\n",
      "Fold: 7  Epoch: 102  Training loss = 1.7198  Validation loss = 0.7869  \n",
      "\n",
      "Fold: 7  Epoch: 103  Training loss = 1.7183  Validation loss = 0.7866  \n",
      "\n",
      "Fold: 7  Epoch: 104  Training loss = 1.7173  Validation loss = 0.7841  \n",
      "\n",
      "Fold: 7  Epoch: 105  Training loss = 1.7156  Validation loss = 0.7715  \n",
      "\n",
      "Fold: 7  Epoch: 106  Training loss = 1.7139  Validation loss = 0.7560  \n",
      "\n",
      "Fold: 7  Epoch: 107  Training loss = 1.7129  Validation loss = 0.7582  \n",
      "\n",
      "Fold: 7  Epoch: 108  Training loss = 1.7115  Validation loss = 0.7562  \n",
      "\n",
      "Fold: 7  Epoch: 109  Training loss = 1.7100  Validation loss = 0.7560  \n",
      "\n",
      "Fold: 7  Epoch: 110  Training loss = 1.7085  Validation loss = 0.7507  \n",
      "\n",
      "Fold: 7  Epoch: 111  Training loss = 1.7073  Validation loss = 0.7493  \n",
      "\n",
      "Fold: 7  Epoch: 112  Training loss = 1.7061  Validation loss = 0.7476  \n",
      "\n",
      "Fold: 7  Epoch: 113  Training loss = 1.7053  Validation loss = 0.7469  \n",
      "\n",
      "Fold: 7  Epoch: 114  Training loss = 1.7047  Validation loss = 0.7489  \n",
      "\n",
      "Fold: 7  Epoch: 115  Training loss = 1.7035  Validation loss = 0.7455  \n",
      "\n",
      "Fold: 7  Epoch: 116  Training loss = 1.7023  Validation loss = 0.7393  \n",
      "\n",
      "Fold: 7  Epoch: 117  Training loss = 1.7006  Validation loss = 0.7331  \n",
      "\n",
      "Fold: 7  Epoch: 118  Training loss = 1.6989  Validation loss = 0.7246  \n",
      "\n",
      "Fold: 7  Epoch: 119  Training loss = 1.6980  Validation loss = 0.7272  \n",
      "\n",
      "Fold: 7  Epoch: 120  Training loss = 1.6968  Validation loss = 0.7255  \n",
      "\n",
      "Fold: 7  Epoch: 121  Training loss = 1.6951  Validation loss = 0.7271  \n",
      "\n",
      "Fold: 7  Epoch: 122  Training loss = 1.6939  Validation loss = 0.7256  \n",
      "\n",
      "Fold: 7  Epoch: 123  Training loss = 1.6929  Validation loss = 0.7249  \n",
      "\n",
      "Fold: 7  Epoch: 124  Training loss = 1.6917  Validation loss = 0.7265  \n",
      "\n",
      "Fold: 7  Epoch: 125  Training loss = 1.6906  Validation loss = 0.7295  \n",
      "\n",
      "Fold: 7  Epoch: 126  Training loss = 1.6897  Validation loss = 0.7217  \n",
      "\n",
      "Fold: 7  Epoch: 127  Training loss = 1.6883  Validation loss = 0.7128  \n",
      "\n",
      "Fold: 7  Epoch: 128  Training loss = 1.6871  Validation loss = 0.7099  \n",
      "\n",
      "Fold: 7  Epoch: 129  Training loss = 1.6857  Validation loss = 0.7083  \n",
      "\n",
      "Fold: 7  Epoch: 130  Training loss = 1.6841  Validation loss = 0.7037  \n",
      "\n",
      "Fold: 7  Epoch: 131  Training loss = 1.6828  Validation loss = 0.7023  \n",
      "\n",
      "Fold: 7  Epoch: 132  Training loss = 1.6817  Validation loss = 0.7004  \n",
      "\n",
      "Fold: 7  Epoch: 133  Training loss = 1.6808  Validation loss = 0.7061  \n",
      "\n",
      "Fold: 7  Epoch: 134  Training loss = 1.6788  Validation loss = 0.6975  \n",
      "\n",
      "Fold: 7  Epoch: 135  Training loss = 1.6774  Validation loss = 0.6979  \n",
      "\n",
      "Fold: 7  Epoch: 136  Training loss = 1.6760  Validation loss = 0.7023  \n",
      "\n",
      "Fold: 7  Epoch: 137  Training loss = 1.6748  Validation loss = 0.6988  \n",
      "\n",
      "Fold: 7  Epoch: 138  Training loss = 1.6738  Validation loss = 0.7064  \n",
      "\n",
      "Fold: 7  Epoch: 139  Training loss = 1.6725  Validation loss = 0.7024  \n",
      "\n",
      "Fold: 7  Epoch: 140  Training loss = 1.6712  Validation loss = 0.7038  \n",
      "\n",
      "Fold: 7  Epoch: 141  Training loss = 1.6699  Validation loss = 0.6989  \n",
      "\n",
      "Fold: 7  Epoch: 142  Training loss = 1.6684  Validation loss = 0.7040  \n",
      "\n",
      "Fold: 7  Epoch: 143  Training loss = 1.6674  Validation loss = 0.7082  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 134  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 1.6177  Validation loss = 5.8633  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 1.6163  Validation loss = 5.8645  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 1.6153  Validation loss = 5.8666  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 1.6139  Validation loss = 5.8683  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 1.6129  Validation loss = 5.8732  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 1.6107  Validation loss = 5.8713  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 1.6091  Validation loss = 5.8616  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 1.6079  Validation loss = 5.8610  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 1.6065  Validation loss = 5.8531  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 1.6052  Validation loss = 5.8487  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 1.6035  Validation loss = 5.8435  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 1.6021  Validation loss = 5.8462  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 1.6010  Validation loss = 5.8424  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 1.5998  Validation loss = 5.8366  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 1.5985  Validation loss = 5.8307  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 1.5978  Validation loss = 5.8322  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 1.5965  Validation loss = 5.8357  \n",
      "\n",
      "Fold: 8  Epoch: 18  Training loss = 1.5956  Validation loss = 5.8363  \n",
      "\n",
      "Fold: 8  Epoch: 19  Training loss = 1.5940  Validation loss = 5.8394  \n",
      "\n",
      "Fold: 8  Epoch: 20  Training loss = 1.5925  Validation loss = 5.8316  \n",
      "\n",
      "Fold: 8  Epoch: 21  Training loss = 1.5914  Validation loss = 5.8331  \n",
      "\n",
      "Fold: 8  Epoch: 22  Training loss = 1.5899  Validation loss = 5.8282  \n",
      "\n",
      "Fold: 8  Epoch: 23  Training loss = 1.5888  Validation loss = 5.8395  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 22  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 2.1269  Validation loss = 9.0459  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 2.1233  Validation loss = 9.0295  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 2.1199  Validation loss = 9.0127  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 2.1182  Validation loss = 9.0054  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 2.1117  Validation loss = 8.9690  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 2.1097  Validation loss = 8.9610  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 2.1078  Validation loss = 8.9524  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 2.1054  Validation loss = 8.9391  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 2.1012  Validation loss = 8.9135  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 2.1003  Validation loss = 8.9110  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 2.0959  Validation loss = 8.8839  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 2.0961  Validation loss = 8.8909  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 2.0933  Validation loss = 8.8716  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 2.0924  Validation loss = 8.8693  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 2.0888  Validation loss = 8.8450  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 2.0895  Validation loss = 8.8559  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 2.0868  Validation loss = 8.8382  \n",
      "\n",
      "Fold: 9  Epoch: 18  Training loss = 2.0851  Validation loss = 8.8296  \n",
      "\n",
      "Fold: 9  Epoch: 19  Training loss = 2.0837  Validation loss = 8.8251  \n",
      "\n",
      "Fold: 9  Epoch: 20  Training loss = 2.0823  Validation loss = 8.8155  \n",
      "\n",
      "Fold: 9  Epoch: 21  Training loss = 2.0818  Validation loss = 8.8175  \n",
      "\n",
      "Fold: 9  Epoch: 22  Training loss = 2.0804  Validation loss = 8.8093  \n",
      "\n",
      "Fold: 9  Epoch: 23  Training loss = 2.0786  Validation loss = 8.8015  \n",
      "\n",
      "Fold: 9  Epoch: 24  Training loss = 2.0772  Validation loss = 8.7936  \n",
      "\n",
      "Fold: 9  Epoch: 25  Training loss = 2.0748  Validation loss = 8.7771  \n",
      "\n",
      "Fold: 9  Epoch: 26  Training loss = 2.0751  Validation loss = 8.7917  \n",
      "\n",
      "Fold: 9  Epoch: 27  Training loss = 2.0737  Validation loss = 8.7863  \n",
      "\n",
      "Fold: 9  Epoch: 28  Training loss = 2.0731  Validation loss = 8.7878  \n",
      "\n",
      "Fold: 9  Epoch: 29  Training loss = 2.0724  Validation loss = 8.7902  \n",
      "\n",
      "Fold: 9  Epoch: 30  Training loss = 2.0716  Validation loss = 8.7896  \n",
      "\n",
      "Fold: 9  Epoch: 31  Training loss = 2.0706  Validation loss = 8.7871  \n",
      "\n",
      "Fold: 9  Epoch: 32  Training loss = 2.0691  Validation loss = 8.7777  \n",
      "\n",
      "Fold: 9  Epoch: 33  Training loss = 2.0659  Validation loss = 8.7495  \n",
      "\n",
      "Fold: 9  Epoch: 34  Training loss = 2.0630  Validation loss = 8.7216  \n",
      "\n",
      "Fold: 9  Epoch: 35  Training loss = 2.0619  Validation loss = 8.7188  \n",
      "\n",
      "Fold: 9  Epoch: 36  Training loss = 2.0604  Validation loss = 8.7098  \n",
      "\n",
      "Fold: 9  Epoch: 37  Training loss = 2.0585  Validation loss = 8.6899  \n",
      "\n",
      "Fold: 9  Epoch: 38  Training loss = 2.0568  Validation loss = 8.6767  \n",
      "\n",
      "Fold: 9  Epoch: 39  Training loss = 2.0554  Validation loss = 8.6676  \n",
      "\n",
      "Fold: 9  Epoch: 40  Training loss = 2.0538  Validation loss = 8.6533  \n",
      "\n",
      "Fold: 9  Epoch: 41  Training loss = 2.0526  Validation loss = 8.6427  \n",
      "\n",
      "Fold: 9  Epoch: 42  Training loss = 2.0514  Validation loss = 8.6396  \n",
      "\n",
      "Fold: 9  Epoch: 43  Training loss = 2.0498  Validation loss = 8.6343  \n",
      "\n",
      "Fold: 9  Epoch: 44  Training loss = 2.0486  Validation loss = 8.6319  \n",
      "\n",
      "Fold: 9  Epoch: 45  Training loss = 2.0473  Validation loss = 8.6243  \n",
      "\n",
      "Fold: 9  Epoch: 46  Training loss = 2.0461  Validation loss = 8.6212  \n",
      "\n",
      "Fold: 9  Epoch: 47  Training loss = 2.0447  Validation loss = 8.6129  \n",
      "\n",
      "Fold: 9  Epoch: 48  Training loss = 2.0439  Validation loss = 8.6194  \n",
      "\n",
      "Fold: 9  Epoch: 49  Training loss = 2.0428  Validation loss = 8.6230  \n",
      "\n",
      "Fold: 9  Epoch: 50  Training loss = 2.0417  Validation loss = 8.6171  \n",
      "\n",
      "Fold: 9  Epoch: 51  Training loss = 2.0406  Validation loss = 8.6213  \n",
      "\n",
      "Fold: 9  Epoch: 52  Training loss = 2.0399  Validation loss = 8.6279  \n",
      "\n",
      "Fold: 9  Epoch: 53  Training loss = 2.0385  Validation loss = 8.6165  \n",
      "\n",
      "Fold: 9  Epoch: 54  Training loss = 2.0372  Validation loss = 8.6130  \n",
      "\n",
      "Fold: 9  Epoch: 55  Training loss = 2.0355  Validation loss = 8.5942  \n",
      "\n",
      "Fold: 9  Epoch: 56  Training loss = 2.0344  Validation loss = 8.5842  \n",
      "\n",
      "Fold: 9  Epoch: 57  Training loss = 2.0327  Validation loss = 8.5668  \n",
      "\n",
      "Fold: 9  Epoch: 58  Training loss = 2.0317  Validation loss = 8.5737  \n",
      "\n",
      "Fold: 9  Epoch: 59  Training loss = 2.0309  Validation loss = 8.5679  \n",
      "\n",
      "Fold: 9  Epoch: 60  Training loss = 2.0298  Validation loss = 8.5590  \n",
      "\n",
      "Fold: 9  Epoch: 61  Training loss = 2.0289  Validation loss = 8.5365  \n",
      "\n",
      "Fold: 9  Epoch: 62  Training loss = 2.0277  Validation loss = 8.5535  \n",
      "\n",
      "Fold: 9  Epoch: 63  Training loss = 2.0266  Validation loss = 8.5361  \n",
      "\n",
      "Fold: 9  Epoch: 64  Training loss = 2.0252  Validation loss = 8.5239  \n",
      "\n",
      "Fold: 9  Epoch: 65  Training loss = 2.0239  Validation loss = 8.5133  \n",
      "\n",
      "Fold: 9  Epoch: 66  Training loss = 2.0230  Validation loss = 8.5142  \n",
      "\n",
      "Fold: 9  Epoch: 67  Training loss = 2.0218  Validation loss = 8.5109  \n",
      "\n",
      "Fold: 9  Epoch: 68  Training loss = 2.0207  Validation loss = 8.5058  \n",
      "\n",
      "Fold: 9  Epoch: 69  Training loss = 2.0194  Validation loss = 8.5059  \n",
      "\n",
      "Fold: 9  Epoch: 70  Training loss = 2.0180  Validation loss = 8.5086  \n",
      "\n",
      "Fold: 9  Epoch: 71  Training loss = 2.0167  Validation loss = 8.4924  \n",
      "\n",
      "Fold: 9  Epoch: 72  Training loss = 2.0154  Validation loss = 8.4967  \n",
      "\n",
      "Fold: 9  Epoch: 73  Training loss = 2.0141  Validation loss = 8.4912  \n",
      "\n",
      "Fold: 9  Epoch: 74  Training loss = 2.0128  Validation loss = 8.4825  \n",
      "\n",
      "Fold: 9  Epoch: 75  Training loss = 2.0114  Validation loss = 8.4918  \n",
      "\n",
      "Fold: 9  Epoch: 76  Training loss = 2.0104  Validation loss = 8.4923  \n",
      "\n",
      "Fold: 9  Epoch: 77  Training loss = 2.0093  Validation loss = 8.4897  \n",
      "\n",
      "Fold: 9  Epoch: 78  Training loss = 2.0082  Validation loss = 8.4845  \n",
      "\n",
      "Fold: 9  Epoch: 79  Training loss = 2.0072  Validation loss = 8.4619  \n",
      "\n",
      "Fold: 9  Epoch: 80  Training loss = 2.0062  Validation loss = 8.4586  \n",
      "\n",
      "Fold: 9  Epoch: 81  Training loss = 2.0053  Validation loss = 8.4743  \n",
      "\n",
      "Fold: 9  Epoch: 82  Training loss = 2.0043  Validation loss = 8.4778  \n",
      "\n",
      "Fold: 9  Epoch: 83  Training loss = 2.0030  Validation loss = 8.4777  \n",
      "\n",
      "Fold: 9  Epoch: 84  Training loss = 2.0022  Validation loss = 8.4852  \n",
      "\n",
      "Fold: 9  Epoch: 85  Training loss = 2.0003  Validation loss = 8.4668  \n",
      "\n",
      "Fold: 9  Epoch: 86  Training loss = 1.9996  Validation loss = 8.4714  \n",
      "\n",
      "Fold: 9  Epoch: 87  Training loss = 1.9985  Validation loss = 8.4669  \n",
      "\n",
      "Fold: 9  Epoch: 88  Training loss = 1.9973  Validation loss = 8.4576  \n",
      "\n",
      "Fold: 9  Epoch: 89  Training loss = 1.9955  Validation loss = 8.4408  \n",
      "\n",
      "Fold: 9  Epoch: 90  Training loss = 1.9944  Validation loss = 8.4349  \n",
      "\n",
      "Fold: 9  Epoch: 91  Training loss = 1.9936  Validation loss = 8.4344  \n",
      "\n",
      "Fold: 9  Epoch: 92  Training loss = 1.9925  Validation loss = 8.4192  \n",
      "\n",
      "Fold: 9  Epoch: 93  Training loss = 1.9914  Validation loss = 8.4172  \n",
      "\n",
      "Fold: 9  Epoch: 94  Training loss = 1.9902  Validation loss = 8.4027  \n",
      "\n",
      "Fold: 9  Epoch: 95  Training loss = 1.9886  Validation loss = 8.4141  \n",
      "\n",
      "Fold: 9  Epoch: 96  Training loss = 1.9876  Validation loss = 8.4225  \n",
      "\n",
      "Fold: 9  Epoch: 97  Training loss = 1.9859  Validation loss = 8.4100  \n",
      "\n",
      "Fold: 9  Epoch: 98  Training loss = 1.9848  Validation loss = 8.4058  \n",
      "\n",
      "Fold: 9  Epoch: 99  Training loss = 1.9837  Validation loss = 8.4163  \n",
      "\n",
      "Fold: 9  Epoch: 100  Training loss = 1.9829  Validation loss = 8.4251  \n",
      "\n",
      "Fold: 9  Epoch: 101  Training loss = 1.9818  Validation loss = 8.4283  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 94  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 2.8353  Validation loss = 4.9499  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 2.8270  Validation loss = 4.9328  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 2.8238  Validation loss = 4.9281  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 2.8202  Validation loss = 4.9102  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 2.8147  Validation loss = 4.8918  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 2.8078  Validation loss = 4.8673  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 2.8068  Validation loss = 4.8562  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 2.7997  Validation loss = 4.8110  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 2.7927  Validation loss = 4.7643  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 2.7886  Validation loss = 4.7463  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 2.7850  Validation loss = 4.7293  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 2.7828  Validation loss = 4.7251  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 2.7775  Validation loss = 4.6832  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 2.7718  Validation loss = 4.6656  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 2.7655  Validation loss = 4.6377  \n",
      "\n",
      "Fold: 10  Epoch: 16  Training loss = 2.7631  Validation loss = 4.6308  \n",
      "\n",
      "Fold: 10  Epoch: 17  Training loss = 2.7658  Validation loss = 4.6406  \n",
      "\n",
      "Fold: 10  Epoch: 18  Training loss = 2.7634  Validation loss = 4.6338  \n",
      "\n",
      "Fold: 10  Epoch: 19  Training loss = 2.7586  Validation loss = 4.6231  \n",
      "\n",
      "Fold: 10  Epoch: 20  Training loss = 2.7546  Validation loss = 4.6151  \n",
      "\n",
      "Fold: 10  Epoch: 21  Training loss = 2.7500  Validation loss = 4.6028  \n",
      "\n",
      "Fold: 10  Epoch: 22  Training loss = 2.7451  Validation loss = 4.5796  \n",
      "\n",
      "Fold: 10  Epoch: 23  Training loss = 2.7431  Validation loss = 4.5762  \n",
      "\n",
      "Fold: 10  Epoch: 24  Training loss = 2.7369  Validation loss = 4.5473  \n",
      "\n",
      "Fold: 10  Epoch: 25  Training loss = 2.7331  Validation loss = 4.5361  \n",
      "\n",
      "Fold: 10  Epoch: 26  Training loss = 2.7273  Validation loss = 4.5087  \n",
      "\n",
      "Fold: 10  Epoch: 27  Training loss = 2.7215  Validation loss = 4.4847  \n",
      "\n",
      "Fold: 10  Epoch: 28  Training loss = 2.7169  Validation loss = 4.4704  \n",
      "\n",
      "Fold: 10  Epoch: 29  Training loss = 2.7148  Validation loss = 4.4711  \n",
      "\n",
      "Fold: 10  Epoch: 30  Training loss = 2.7116  Validation loss = 4.4631  \n",
      "\n",
      "Fold: 10  Epoch: 31  Training loss = 2.7090  Validation loss = 4.4642  \n",
      "\n",
      "Fold: 10  Epoch: 32  Training loss = 2.7055  Validation loss = 4.4549  \n",
      "\n",
      "Fold: 10  Epoch: 33  Training loss = 2.7026  Validation loss = 4.4483  \n",
      "\n",
      "Fold: 10  Epoch: 34  Training loss = 2.7006  Validation loss = 4.4413  \n",
      "\n",
      "Fold: 10  Epoch: 35  Training loss = 2.6962  Validation loss = 4.4319  \n",
      "\n",
      "Fold: 10  Epoch: 36  Training loss = 2.6927  Validation loss = 4.4210  \n",
      "\n",
      "Fold: 10  Epoch: 37  Training loss = 2.6884  Validation loss = 4.4065  \n",
      "\n",
      "Fold: 10  Epoch: 38  Training loss = 2.6833  Validation loss = 4.3856  \n",
      "\n",
      "Fold: 10  Epoch: 39  Training loss = 2.6798  Validation loss = 4.3722  \n",
      "\n",
      "Fold: 10  Epoch: 40  Training loss = 2.6768  Validation loss = 4.3638  \n",
      "\n",
      "Fold: 10  Epoch: 41  Training loss = 2.6751  Validation loss = 4.3696  \n",
      "\n",
      "Fold: 10  Epoch: 42  Training loss = 2.6738  Validation loss = 4.3704  \n",
      "\n",
      "Fold: 10  Epoch: 43  Training loss = 2.6702  Validation loss = 4.3676  \n",
      "\n",
      "Fold: 10  Epoch: 44  Training loss = 2.6684  Validation loss = 4.3642  \n",
      "\n",
      "Fold: 10  Epoch: 45  Training loss = 2.6657  Validation loss = 4.3611  \n",
      "\n",
      "Fold: 10  Epoch: 46  Training loss = 2.6611  Validation loss = 4.3509  \n",
      "\n",
      "Fold: 10  Epoch: 47  Training loss = 2.6578  Validation loss = 4.3383  \n",
      "\n",
      "Fold: 10  Epoch: 48  Training loss = 2.6547  Validation loss = 4.3340  \n",
      "\n",
      "Fold: 10  Epoch: 49  Training loss = 2.6529  Validation loss = 4.3353  \n",
      "\n",
      "Fold: 10  Epoch: 50  Training loss = 2.6509  Validation loss = 4.3327  \n",
      "\n",
      "Fold: 10  Epoch: 51  Training loss = 2.6488  Validation loss = 4.3353  \n",
      "\n",
      "Fold: 10  Epoch: 52  Training loss = 2.6472  Validation loss = 4.3417  \n",
      "\n",
      "Fold: 10  Epoch: 53  Training loss = 2.6465  Validation loss = 4.3431  \n",
      "\n",
      "Fold: 10  Epoch: 54  Training loss = 2.6431  Validation loss = 4.3375  \n",
      "\n",
      "Fold: 10  Epoch: 55  Training loss = 2.6405  Validation loss = 4.3319  \n",
      "\n",
      "Fold: 10  Epoch: 56  Training loss = 2.6392  Validation loss = 4.3352  \n",
      "\n",
      "Fold: 10  Epoch: 57  Training loss = 2.6375  Validation loss = 4.3364  \n",
      "\n",
      "Fold: 10  Epoch: 58  Training loss = 2.6336  Validation loss = 4.3239  \n",
      "\n",
      "Fold: 10  Epoch: 59  Training loss = 2.6305  Validation loss = 4.3191  \n",
      "\n",
      "Fold: 10  Epoch: 60  Training loss = 2.6272  Validation loss = 4.3113  \n",
      "\n",
      "Fold: 10  Epoch: 61  Training loss = 2.6238  Validation loss = 4.3091  \n",
      "\n",
      "Fold: 10  Epoch: 62  Training loss = 2.6212  Validation loss = 4.3038  \n",
      "\n",
      "Fold: 10  Epoch: 63  Training loss = 2.6181  Validation loss = 4.2928  \n",
      "\n",
      "Fold: 10  Epoch: 64  Training loss = 2.6160  Validation loss = 4.2919  \n",
      "\n",
      "Fold: 10  Epoch: 65  Training loss = 2.6146  Validation loss = 4.2945  \n",
      "\n",
      "Fold: 10  Epoch: 66  Training loss = 2.6109  Validation loss = 4.2835  \n",
      "\n",
      "Fold: 10  Epoch: 67  Training loss = 2.6082  Validation loss = 4.2794  \n",
      "\n",
      "Fold: 10  Epoch: 68  Training loss = 2.6065  Validation loss = 4.2850  \n",
      "\n",
      "Fold: 10  Epoch: 69  Training loss = 2.6053  Validation loss = 4.2845  \n",
      "\n",
      "Fold: 10  Epoch: 70  Training loss = 2.6061  Validation loss = 4.2953  \n",
      "\n",
      "Fold: 10  Epoch: 71  Training loss = 2.6037  Validation loss = 4.2947  \n",
      "\n",
      "Fold: 10  Epoch: 72  Training loss = 2.5992  Validation loss = 4.2867  \n",
      "\n",
      "Fold: 10  Epoch: 73  Training loss = 2.5966  Validation loss = 4.2835  \n",
      "\n",
      "Fold: 10  Epoch: 74  Training loss = 2.5938  Validation loss = 4.2745  \n",
      "\n",
      "Fold: 10  Epoch: 75  Training loss = 2.5920  Validation loss = 4.2742  \n",
      "\n",
      "Fold: 10  Epoch: 76  Training loss = 2.5901  Validation loss = 4.2750  \n",
      "\n",
      "Fold: 10  Epoch: 77  Training loss = 2.5864  Validation loss = 4.2678  \n",
      "\n",
      "Fold: 10  Epoch: 78  Training loss = 2.5843  Validation loss = 4.2678  \n",
      "\n",
      "Fold: 10  Epoch: 79  Training loss = 2.5828  Validation loss = 4.2648  \n",
      "\n",
      "Fold: 10  Epoch: 80  Training loss = 2.5785  Validation loss = 4.2496  \n",
      "\n",
      "Fold: 10  Epoch: 81  Training loss = 2.5754  Validation loss = 4.2423  \n",
      "\n",
      "Fold: 10  Epoch: 82  Training loss = 2.5725  Validation loss = 4.2419  \n",
      "\n",
      "Fold: 10  Epoch: 83  Training loss = 2.5713  Validation loss = 4.2362  \n",
      "\n",
      "Fold: 10  Epoch: 84  Training loss = 2.5691  Validation loss = 4.2429  \n",
      "\n",
      "Fold: 10  Epoch: 85  Training loss = 2.5653  Validation loss = 4.2320  \n",
      "\n",
      "Fold: 10  Epoch: 86  Training loss = 2.5629  Validation loss = 4.2352  \n",
      "\n",
      "Fold: 10  Epoch: 87  Training loss = 2.5593  Validation loss = 4.2223  \n",
      "\n",
      "Fold: 10  Epoch: 88  Training loss = 2.5563  Validation loss = 4.2137  \n",
      "\n",
      "Fold: 10  Epoch: 89  Training loss = 2.5535  Validation loss = 4.2034  \n",
      "\n",
      "Fold: 10  Epoch: 90  Training loss = 2.5512  Validation loss = 4.2071  \n",
      "\n",
      "Fold: 10  Epoch: 91  Training loss = 2.5492  Validation loss = 4.2055  \n",
      "\n",
      "Fold: 10  Epoch: 92  Training loss = 2.5474  Validation loss = 4.2094  \n",
      "\n",
      "Fold: 10  Epoch: 93  Training loss = 2.5443  Validation loss = 4.2040  \n",
      "\n",
      "Fold: 10  Epoch: 94  Training loss = 2.5412  Validation loss = 4.1993  \n",
      "\n",
      "Fold: 10  Epoch: 95  Training loss = 2.5377  Validation loss = 4.1817  \n",
      "\n",
      "Fold: 10  Epoch: 96  Training loss = 2.5349  Validation loss = 4.1859  \n",
      "\n",
      "Fold: 10  Epoch: 97  Training loss = 2.5320  Validation loss = 4.1803  \n",
      "\n",
      "Fold: 10  Epoch: 98  Training loss = 2.5290  Validation loss = 4.1718  \n",
      "\n",
      "Fold: 10  Epoch: 99  Training loss = 2.5278  Validation loss = 4.1782  \n",
      "\n",
      "Fold: 10  Epoch: 100  Training loss = 2.5247  Validation loss = 4.1729  \n",
      "\n",
      "Fold: 10  Epoch: 101  Training loss = 2.5232  Validation loss = 4.1774  \n",
      "\n",
      "Fold: 10  Epoch: 102  Training loss = 2.5223  Validation loss = 4.1791  \n",
      "\n",
      "Fold: 10  Epoch: 103  Training loss = 2.5198  Validation loss = 4.1719  \n",
      "\n",
      "Fold: 10  Epoch: 104  Training loss = 2.5143  Validation loss = 4.1656  \n",
      "\n",
      "Fold: 10  Epoch: 105  Training loss = 2.5116  Validation loss = 4.1659  \n",
      "\n",
      "Fold: 10  Epoch: 106  Training loss = 2.5089  Validation loss = 4.1642  \n",
      "\n",
      "Fold: 10  Epoch: 107  Training loss = 2.5072  Validation loss = 4.1662  \n",
      "\n",
      "Fold: 10  Epoch: 108  Training loss = 2.5049  Validation loss = 4.1638  \n",
      "\n",
      "Fold: 10  Epoch: 109  Training loss = 2.5027  Validation loss = 4.1649  \n",
      "\n",
      "Fold: 10  Epoch: 110  Training loss = 2.5013  Validation loss = 4.1709  \n",
      "\n",
      "Fold: 10  Epoch: 111  Training loss = 2.4980  Validation loss = 4.1611  \n",
      "\n",
      "Fold: 10  Epoch: 112  Training loss = 2.4968  Validation loss = 4.1502  \n",
      "\n",
      "Fold: 10  Epoch: 113  Training loss = 2.4949  Validation loss = 4.1516  \n",
      "\n",
      "Fold: 10  Epoch: 114  Training loss = 2.4927  Validation loss = 4.1411  \n",
      "\n",
      "Fold: 10  Epoch: 115  Training loss = 2.4893  Validation loss = 4.1348  \n",
      "\n",
      "Fold: 10  Epoch: 116  Training loss = 2.4866  Validation loss = 4.1379  \n",
      "\n",
      "Fold: 10  Epoch: 117  Training loss = 2.4849  Validation loss = 4.1368  \n",
      "\n",
      "Fold: 10  Epoch: 118  Training loss = 2.4825  Validation loss = 4.1343  \n",
      "\n",
      "Fold: 10  Epoch: 119  Training loss = 2.4798  Validation loss = 4.1200  \n",
      "\n",
      "Fold: 10  Epoch: 120  Training loss = 2.4778  Validation loss = 4.1304  \n",
      "\n",
      "Fold: 10  Epoch: 121  Training loss = 2.4765  Validation loss = 4.1302  \n",
      "\n",
      "Fold: 10  Epoch: 122  Training loss = 2.4747  Validation loss = 4.1309  \n",
      "\n",
      "Fold: 10  Epoch: 123  Training loss = 2.4717  Validation loss = 4.1219  \n",
      "\n",
      "Fold: 10  Epoch: 124  Training loss = 2.4693  Validation loss = 4.1191  \n",
      "\n",
      "Fold: 10  Epoch: 125  Training loss = 2.4682  Validation loss = 4.1106  \n",
      "\n",
      "Fold: 10  Epoch: 126  Training loss = 2.4668  Validation loss = 4.0979  \n",
      "\n",
      "Fold: 10  Epoch: 127  Training loss = 2.4628  Validation loss = 4.0996  \n",
      "\n",
      "Fold: 10  Epoch: 128  Training loss = 2.4596  Validation loss = 4.0956  \n",
      "\n",
      "Fold: 10  Epoch: 129  Training loss = 2.4567  Validation loss = 4.0858  \n",
      "\n",
      "Fold: 10  Epoch: 130  Training loss = 2.4538  Validation loss = 4.0744  \n",
      "\n",
      "Fold: 10  Epoch: 131  Training loss = 2.4526  Validation loss = 4.0731  \n",
      "\n",
      "Fold: 10  Epoch: 132  Training loss = 2.4523  Validation loss = 4.0822  \n",
      "\n",
      "Fold: 10  Epoch: 133  Training loss = 2.4525  Validation loss = 4.0892  \n",
      "\n",
      "Fold: 10  Epoch: 134  Training loss = 2.4513  Validation loss = 4.0910  \n",
      "\n",
      "Fold: 10  Epoch: 135  Training loss = 2.4471  Validation loss = 4.0834  \n",
      "\n",
      "Fold: 10  Epoch: 136  Training loss = 2.4433  Validation loss = 4.0688  \n",
      "\n",
      "Fold: 10  Epoch: 137  Training loss = 2.4397  Validation loss = 4.0527  \n",
      "\n",
      "Fold: 10  Epoch: 138  Training loss = 2.4373  Validation loss = 4.0515  \n",
      "\n",
      "Fold: 10  Epoch: 139  Training loss = 2.4352  Validation loss = 4.0491  \n",
      "\n",
      "Fold: 10  Epoch: 140  Training loss = 2.4347  Validation loss = 4.0500  \n",
      "\n",
      "Fold: 10  Epoch: 141  Training loss = 2.4334  Validation loss = 4.0490  \n",
      "\n",
      "Fold: 10  Epoch: 142  Training loss = 2.4296  Validation loss = 4.0461  \n",
      "\n",
      "Fold: 10  Epoch: 143  Training loss = 2.4261  Validation loss = 4.0331  \n",
      "\n",
      "Fold: 10  Epoch: 144  Training loss = 2.4242  Validation loss = 4.0293  \n",
      "\n",
      "Fold: 10  Epoch: 145  Training loss = 2.4223  Validation loss = 4.0256  \n",
      "\n",
      "Fold: 10  Epoch: 146  Training loss = 2.4206  Validation loss = 4.0361  \n",
      "\n",
      "Fold: 10  Epoch: 147  Training loss = 2.4190  Validation loss = 4.0274  \n",
      "\n",
      "Fold: 10  Epoch: 148  Training loss = 2.4172  Validation loss = 4.0224  \n",
      "\n",
      "Fold: 10  Epoch: 149  Training loss = 2.4152  Validation loss = 4.0105  \n",
      "\n",
      "Fold: 10  Epoch: 150  Training loss = 2.4120  Validation loss = 4.0199  \n",
      "\n",
      "Fold: 10  Epoch: 151  Training loss = 2.4100  Validation loss = 4.0143  \n",
      "\n",
      "Fold: 10  Epoch: 152  Training loss = 2.4083  Validation loss = 4.0110  \n",
      "\n",
      "Fold: 10  Epoch: 153  Training loss = 2.4059  Validation loss = 4.0103  \n",
      "\n",
      "Fold: 10  Epoch: 154  Training loss = 2.4039  Validation loss = 4.0173  \n",
      "\n",
      "Fold: 10  Epoch: 155  Training loss = 2.4030  Validation loss = 4.0181  \n",
      "\n",
      "Fold: 10  Epoch: 156  Training loss = 2.4048  Validation loss = 4.0230  \n",
      "\n",
      "Fold: 10  Epoch: 157  Training loss = 2.3996  Validation loss = 4.0143  \n",
      "\n",
      "Fold: 10  Epoch: 158  Training loss = 2.3977  Validation loss = 4.0145  \n",
      "\n",
      "Fold: 10  Epoch: 159  Training loss = 2.3924  Validation loss = 3.9971  \n",
      "\n",
      "Fold: 10  Epoch: 160  Training loss = 2.3901  Validation loss = 3.9996  \n",
      "\n",
      "Fold: 10  Epoch: 161  Training loss = 2.3891  Validation loss = 4.0066  \n",
      "\n",
      "Fold: 10  Epoch: 162  Training loss = 2.3859  Validation loss = 3.9938  \n",
      "\n",
      "Fold: 10  Epoch: 163  Training loss = 2.3834  Validation loss = 3.9928  \n",
      "\n",
      "Fold: 10  Epoch: 164  Training loss = 2.3815  Validation loss = 3.9899  \n",
      "\n",
      "Fold: 10  Epoch: 165  Training loss = 2.3806  Validation loss = 3.9956  \n",
      "\n",
      "Fold: 10  Epoch: 166  Training loss = 2.3800  Validation loss = 3.9966  \n",
      "\n",
      "Fold: 10  Epoch: 167  Training loss = 2.3788  Validation loss = 4.0055  \n",
      "\n",
      "Fold: 10  Epoch: 168  Training loss = 2.3796  Validation loss = 4.0109  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 164  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 2.4859  Validation loss = 1.8229  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 2.4812  Validation loss = 1.8226  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 2.4771  Validation loss = 1.8244  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 2.4676  Validation loss = 1.8345  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 2.4640  Validation loss = 1.8357  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 2.4619  Validation loss = 1.8363  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 2.4559  Validation loss = 1.8432  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 2.4564  Validation loss = 1.8348  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 2.4478  Validation loss = 1.8484  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 2.4446  Validation loss = 1.8520  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 2.4423  Validation loss = 1.8534  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 2  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 2.4627  Validation loss = 1.8666  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 2.4598  Validation loss = 1.8562  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 2.4561  Validation loss = 1.8432  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 2.4518  Validation loss = 1.8253  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 2.4490  Validation loss = 1.8211  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 2.4459  Validation loss = 1.8025  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 2.4454  Validation loss = 1.8120  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 2.4457  Validation loss = 1.8169  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 2.4443  Validation loss = 1.8122  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 2.4410  Validation loss = 1.7993  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 2.4377  Validation loss = 1.7884  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 2.4335  Validation loss = 1.7748  \n",
      "\n",
      "Fold: 12  Epoch: 13  Training loss = 2.4315  Validation loss = 1.7655  \n",
      "\n",
      "Fold: 12  Epoch: 14  Training loss = 2.4316  Validation loss = 1.7745  \n",
      "\n",
      "Fold: 12  Epoch: 15  Training loss = 2.4269  Validation loss = 1.7487  \n",
      "\n",
      "Fold: 12  Epoch: 16  Training loss = 2.4252  Validation loss = 1.7401  \n",
      "\n",
      "Fold: 12  Epoch: 17  Training loss = 2.4235  Validation loss = 1.7457  \n",
      "\n",
      "Fold: 12  Epoch: 18  Training loss = 2.4213  Validation loss = 1.7521  \n",
      "\n",
      "Fold: 12  Epoch: 19  Training loss = 2.4159  Validation loss = 1.7325  \n",
      "\n",
      "Fold: 12  Epoch: 20  Training loss = 2.4170  Validation loss = 1.7440  \n",
      "\n",
      "Fold: 12  Epoch: 21  Training loss = 2.4142  Validation loss = 1.7304  \n",
      "\n",
      "Fold: 12  Epoch: 22  Training loss = 2.4114  Validation loss = 1.7201  \n",
      "\n",
      "Fold: 12  Epoch: 23  Training loss = 2.4088  Validation loss = 1.7062  \n",
      "\n",
      "Fold: 12  Epoch: 24  Training loss = 2.4068  Validation loss = 1.7024  \n",
      "\n",
      "Fold: 12  Epoch: 25  Training loss = 2.4036  Validation loss = 1.6855  \n",
      "\n",
      "Fold: 12  Epoch: 26  Training loss = 2.4025  Validation loss = 1.6734  \n",
      "\n",
      "Fold: 12  Epoch: 27  Training loss = 2.4001  Validation loss = 1.6725  \n",
      "\n",
      "Fold: 12  Epoch: 28  Training loss = 2.3979  Validation loss = 1.6616  \n",
      "\n",
      "Fold: 12  Epoch: 29  Training loss = 2.3961  Validation loss = 1.6616  \n",
      "\n",
      "Fold: 12  Epoch: 30  Training loss = 2.3950  Validation loss = 1.6684  \n",
      "\n",
      "Fold: 12  Epoch: 31  Training loss = 2.3929  Validation loss = 1.6618  \n",
      "\n",
      "Fold: 12  Epoch: 32  Training loss = 2.3914  Validation loss = 1.6567  \n",
      "\n",
      "Fold: 12  Epoch: 33  Training loss = 2.3900  Validation loss = 1.6552  \n",
      "\n",
      "Fold: 12  Epoch: 34  Training loss = 2.3876  Validation loss = 1.6525  \n",
      "\n",
      "Fold: 12  Epoch: 35  Training loss = 2.3738  Validation loss = 1.6326  \n",
      "\n",
      "Fold: 12  Epoch: 36  Training loss = 2.3701  Validation loss = 1.6141  \n",
      "\n",
      "Fold: 12  Epoch: 37  Training loss = 2.3693  Validation loss = 1.6157  \n",
      "\n",
      "Fold: 12  Epoch: 38  Training loss = 2.3673  Validation loss = 1.6051  \n",
      "\n",
      "Fold: 12  Epoch: 39  Training loss = 2.3657  Validation loss = 1.5970  \n",
      "\n",
      "Fold: 12  Epoch: 40  Training loss = 2.3661  Validation loss = 1.6031  \n",
      "\n",
      "Fold: 12  Epoch: 41  Training loss = 2.3665  Validation loss = 1.5989  \n",
      "\n",
      "Fold: 12  Epoch: 42  Training loss = 2.3627  Validation loss = 1.5859  \n",
      "\n",
      "Fold: 12  Epoch: 43  Training loss = 2.3586  Validation loss = 1.5740  \n",
      "\n",
      "Fold: 12  Epoch: 44  Training loss = 2.3558  Validation loss = 1.5628  \n",
      "\n",
      "Fold: 12  Epoch: 45  Training loss = 2.3532  Validation loss = 1.5474  \n",
      "\n",
      "Fold: 12  Epoch: 46  Training loss = 2.3518  Validation loss = 1.5401  \n",
      "\n",
      "Fold: 12  Epoch: 47  Training loss = 2.3508  Validation loss = 1.5371  \n",
      "\n",
      "Fold: 12  Epoch: 48  Training loss = 2.3497  Validation loss = 1.5299  \n",
      "\n",
      "Fold: 12  Epoch: 49  Training loss = 2.3473  Validation loss = 1.5238  \n",
      "\n",
      "Fold: 12  Epoch: 50  Training loss = 2.3451  Validation loss = 1.5187  \n",
      "\n",
      "Fold: 12  Epoch: 51  Training loss = 2.3434  Validation loss = 1.5110  \n",
      "\n",
      "Fold: 12  Epoch: 52  Training loss = 2.3416  Validation loss = 1.5032  \n",
      "\n",
      "Fold: 12  Epoch: 53  Training loss = 2.3397  Validation loss = 1.4976  \n",
      "\n",
      "Fold: 12  Epoch: 54  Training loss = 2.3379  Validation loss = 1.4808  \n",
      "\n",
      "Fold: 12  Epoch: 55  Training loss = 2.3368  Validation loss = 1.4718  \n",
      "\n",
      "Fold: 12  Epoch: 56  Training loss = 2.3344  Validation loss = 1.4778  \n",
      "\n",
      "Fold: 12  Epoch: 57  Training loss = 2.3341  Validation loss = 1.4852  \n",
      "\n",
      "Fold: 12  Epoch: 58  Training loss = 2.3329  Validation loss = 1.4798  \n",
      "\n",
      "Fold: 12  Epoch: 59  Training loss = 2.3321  Validation loss = 1.4761  \n",
      "\n",
      "Fold: 12  Epoch: 60  Training loss = 2.3297  Validation loss = 1.4669  \n",
      "\n",
      "Fold: 12  Epoch: 61  Training loss = 2.3289  Validation loss = 1.4639  \n",
      "\n",
      "Fold: 12  Epoch: 62  Training loss = 2.3294  Validation loss = 1.4729  \n",
      "\n",
      "Fold: 12  Epoch: 63  Training loss = 2.3306  Validation loss = 1.4711  \n",
      "\n",
      "Fold: 12  Epoch: 64  Training loss = 2.3261  Validation loss = 1.4593  \n",
      "\n",
      "Fold: 12  Epoch: 65  Training loss = 2.3166  Validation loss = 1.4501  \n",
      "\n",
      "Fold: 12  Epoch: 66  Training loss = 2.3146  Validation loss = 1.4462  \n",
      "\n",
      "Fold: 12  Epoch: 67  Training loss = 2.3132  Validation loss = 1.4375  \n",
      "\n",
      "Fold: 12  Epoch: 68  Training loss = 2.3074  Validation loss = 1.4243  \n",
      "\n",
      "Fold: 12  Epoch: 69  Training loss = 2.3069  Validation loss = 1.4197  \n",
      "\n",
      "Fold: 12  Epoch: 70  Training loss = 2.3081  Validation loss = 1.4024  \n",
      "\n",
      "Fold: 12  Epoch: 71  Training loss = 2.3053  Validation loss = 1.4114  \n",
      "\n",
      "Fold: 12  Epoch: 72  Training loss = 2.3022  Validation loss = 1.4082  \n",
      "\n",
      "Fold: 12  Epoch: 73  Training loss = 2.3003  Validation loss = 1.4157  \n",
      "\n",
      "Fold: 12  Epoch: 74  Training loss = 2.2995  Validation loss = 1.4187  \n",
      "\n",
      "Fold: 12  Epoch: 75  Training loss = 2.2990  Validation loss = 1.4219  \n",
      "\n",
      "Fold: 12  Epoch: 76  Training loss = 2.2992  Validation loss = 1.4241  \n",
      "\n",
      "Fold: 12  Epoch: 77  Training loss = 2.2976  Validation loss = 1.4207  \n",
      "\n",
      "Fold: 12  Epoch: 78  Training loss = 2.2973  Validation loss = 1.4241  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 70  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 2.2741  Validation loss = 3.6705  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 2.2715  Validation loss = 3.6689  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 2.2691  Validation loss = 3.6484  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 2.2657  Validation loss = 3.6127  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 2.2634  Validation loss = 3.5826  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 2.2624  Validation loss = 3.5678  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 2.2618  Validation loss = 3.5706  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 2.2617  Validation loss = 3.5644  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 2.2643  Validation loss = 3.5856  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 2.2622  Validation loss = 3.5637  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 2.2619  Validation loss = 3.5624  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 2.2610  Validation loss = 3.5625  \n",
      "\n",
      "Fold: 13  Epoch: 13  Training loss = 2.2633  Validation loss = 3.5762  \n",
      "\n",
      "Fold: 13  Epoch: 14  Training loss = 2.2615  Validation loss = 3.5690  \n",
      "\n",
      "Fold: 13  Epoch: 15  Training loss = 2.2559  Validation loss = 3.5348  \n",
      "\n",
      "Fold: 13  Epoch: 16  Training loss = 2.2547  Validation loss = 3.5271  \n",
      "\n",
      "Fold: 13  Epoch: 17  Training loss = 2.2528  Validation loss = 3.5096  \n",
      "\n",
      "Fold: 13  Epoch: 18  Training loss = 2.2528  Validation loss = 3.5202  \n",
      "\n",
      "Fold: 13  Epoch: 19  Training loss = 2.2513  Validation loss = 3.5023  \n",
      "\n",
      "Fold: 13  Epoch: 20  Training loss = 2.2505  Validation loss = 3.4942  \n",
      "\n",
      "Fold: 13  Epoch: 21  Training loss = 2.2508  Validation loss = 3.5050  \n",
      "\n",
      "Fold: 13  Epoch: 22  Training loss = 2.2509  Validation loss = 3.5053  \n",
      "\n",
      "Fold: 13  Epoch: 23  Training loss = 2.2490  Validation loss = 3.4991  \n",
      "\n",
      "Fold: 13  Epoch: 24  Training loss = 2.2461  Validation loss = 3.4693  \n",
      "\n",
      "Fold: 13  Epoch: 25  Training loss = 2.2449  Validation loss = 3.4599  \n",
      "\n",
      "Fold: 13  Epoch: 26  Training loss = 2.2445  Validation loss = 3.4621  \n",
      "\n",
      "Fold: 13  Epoch: 27  Training loss = 2.2432  Validation loss = 3.4535  \n",
      "\n",
      "Fold: 13  Epoch: 28  Training loss = 2.2441  Validation loss = 3.4599  \n",
      "\n",
      "Fold: 13  Epoch: 29  Training loss = 2.2425  Validation loss = 3.4482  \n",
      "\n",
      "Fold: 13  Epoch: 30  Training loss = 2.2385  Validation loss = 3.4354  \n",
      "\n",
      "Fold: 13  Epoch: 31  Training loss = 2.2375  Validation loss = 3.4246  \n",
      "\n",
      "Fold: 13  Epoch: 32  Training loss = 2.2292  Validation loss = 3.4143  \n",
      "\n",
      "Fold: 13  Epoch: 33  Training loss = 2.2276  Validation loss = 3.3989  \n",
      "\n",
      "Fold: 13  Epoch: 34  Training loss = 2.2272  Validation loss = 3.3982  \n",
      "\n",
      "Fold: 13  Epoch: 35  Training loss = 2.2263  Validation loss = 3.3806  \n",
      "\n",
      "Fold: 13  Epoch: 36  Training loss = 2.2256  Validation loss = 3.3747  \n",
      "\n",
      "Fold: 13  Epoch: 37  Training loss = 2.2251  Validation loss = 3.3853  \n",
      "\n",
      "Fold: 13  Epoch: 38  Training loss = 2.2246  Validation loss = 3.3856  \n",
      "\n",
      "Fold: 13  Epoch: 39  Training loss = 2.2234  Validation loss = 3.3739  \n",
      "\n",
      "Fold: 13  Epoch: 40  Training loss = 2.2219  Validation loss = 3.3597  \n",
      "\n",
      "Fold: 13  Epoch: 41  Training loss = 2.2215  Validation loss = 3.3571  \n",
      "\n",
      "Fold: 13  Epoch: 42  Training loss = 2.2199  Validation loss = 3.3486  \n",
      "\n",
      "Fold: 13  Epoch: 43  Training loss = 2.2184  Validation loss = 3.3272  \n",
      "\n",
      "Fold: 13  Epoch: 44  Training loss = 2.2189  Validation loss = 3.3398  \n",
      "\n",
      "Fold: 13  Epoch: 45  Training loss = 2.2172  Validation loss = 3.3301  \n",
      "\n",
      "Fold: 13  Epoch: 46  Training loss = 2.2162  Validation loss = 3.3227  \n",
      "\n",
      "Fold: 13  Epoch: 47  Training loss = 2.2152  Validation loss = 3.3126  \n",
      "\n",
      "Fold: 13  Epoch: 48  Training loss = 2.2137  Validation loss = 3.2854  \n",
      "\n",
      "Fold: 13  Epoch: 49  Training loss = 2.2130  Validation loss = 3.2754  \n",
      "\n",
      "Fold: 13  Epoch: 50  Training loss = 2.2146  Validation loss = 3.2490  \n",
      "\n",
      "Fold: 13  Epoch: 51  Training loss = 2.2136  Validation loss = 3.2401  \n",
      "\n",
      "Fold: 13  Epoch: 52  Training loss = 2.2131  Validation loss = 3.2410  \n",
      "\n",
      "Fold: 13  Epoch: 53  Training loss = 2.2115  Validation loss = 3.2461  \n",
      "\n",
      "Fold: 13  Epoch: 54  Training loss = 2.2113  Validation loss = 3.2453  \n",
      "\n",
      "Fold: 13  Epoch: 55  Training loss = 2.2097  Validation loss = 3.2520  \n",
      "\n",
      "Fold: 13  Epoch: 56  Training loss = 2.2086  Validation loss = 3.2557  \n",
      "\n",
      "Fold: 13  Epoch: 57  Training loss = 2.2073  Validation loss = 3.2488  \n",
      "\n",
      "Fold: 13  Epoch: 58  Training loss = 2.2094  Validation loss = 3.2227  \n",
      "\n",
      "Fold: 13  Epoch: 59  Training loss = 2.2069  Validation loss = 3.2192  \n",
      "\n",
      "Fold: 13  Epoch: 60  Training loss = 2.2043  Validation loss = 3.2195  \n",
      "\n",
      "Fold: 13  Epoch: 61  Training loss = 2.2032  Validation loss = 3.2247  \n",
      "\n",
      "Fold: 13  Epoch: 62  Training loss = 2.2025  Validation loss = 3.2353  \n",
      "\n",
      "Fold: 13  Epoch: 63  Training loss = 2.2017  Validation loss = 3.2282  \n",
      "\n",
      "Fold: 13  Epoch: 64  Training loss = 2.2010  Validation loss = 3.2277  \n",
      "\n",
      "Fold: 13  Epoch: 65  Training loss = 2.2004  Validation loss = 3.2229  \n",
      "\n",
      "Fold: 13  Epoch: 66  Training loss = 2.1996  Validation loss = 3.2212  \n",
      "\n",
      "Fold: 13  Epoch: 67  Training loss = 2.1994  Validation loss = 3.2251  \n",
      "\n",
      "Fold: 13  Epoch: 68  Training loss = 2.1990  Validation loss = 3.2185  \n",
      "\n",
      "Fold: 13  Epoch: 69  Training loss = 2.1978  Validation loss = 3.2180  \n",
      "\n",
      "Fold: 13  Epoch: 70  Training loss = 2.1983  Validation loss = 3.2165  \n",
      "\n",
      "Fold: 13  Epoch: 71  Training loss = 2.1981  Validation loss = 3.2138  \n",
      "\n",
      "Fold: 13  Epoch: 72  Training loss = 2.1967  Validation loss = 3.2080  \n",
      "\n",
      "Fold: 13  Epoch: 73  Training loss = 2.1957  Validation loss = 3.2009  \n",
      "\n",
      "Fold: 13  Epoch: 74  Training loss = 2.1948  Validation loss = 3.2069  \n",
      "\n",
      "Fold: 13  Epoch: 75  Training loss = 2.1943  Validation loss = 3.2048  \n",
      "\n",
      "Fold: 13  Epoch: 76  Training loss = 2.1935  Validation loss = 3.1933  \n",
      "\n",
      "Fold: 13  Epoch: 77  Training loss = 2.1942  Validation loss = 3.1823  \n",
      "\n",
      "Fold: 13  Epoch: 78  Training loss = 2.1927  Validation loss = 3.1909  \n",
      "\n",
      "Fold: 13  Epoch: 79  Training loss = 2.1917  Validation loss = 3.1904  \n",
      "\n",
      "Fold: 13  Epoch: 80  Training loss = 2.1912  Validation loss = 3.1944  \n",
      "\n",
      "Fold: 13  Epoch: 81  Training loss = 2.1910  Validation loss = 3.2100  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 77  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 2.3204  Validation loss = 6.2824  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 2.3170  Validation loss = 6.2693  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 2.3167  Validation loss = 6.2624  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 2.3146  Validation loss = 6.2482  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 2.3118  Validation loss = 6.2295  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 2.3082  Validation loss = 6.2160  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 2.3070  Validation loss = 6.2172  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 2.3055  Validation loss = 6.2187  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 2.3032  Validation loss = 6.2062  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 2.3028  Validation loss = 6.2101  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 2.2997  Validation loss = 6.1901  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 2.2959  Validation loss = 6.1797  \n",
      "\n",
      "Fold: 14  Epoch: 13  Training loss = 2.2942  Validation loss = 6.1707  \n",
      "\n",
      "Fold: 14  Epoch: 14  Training loss = 2.2938  Validation loss = 6.1783  \n",
      "\n",
      "Fold: 14  Epoch: 15  Training loss = 2.2914  Validation loss = 6.1557  \n",
      "\n",
      "Fold: 14  Epoch: 16  Training loss = 2.2901  Validation loss = 6.1509  \n",
      "\n",
      "Fold: 14  Epoch: 17  Training loss = 2.2888  Validation loss = 6.1443  \n",
      "\n",
      "Fold: 14  Epoch: 18  Training loss = 2.2877  Validation loss = 6.1413  \n",
      "\n",
      "Fold: 14  Epoch: 19  Training loss = 2.2865  Validation loss = 6.1309  \n",
      "\n",
      "Fold: 14  Epoch: 20  Training loss = 2.2856  Validation loss = 6.1264  \n",
      "\n",
      "Fold: 14  Epoch: 21  Training loss = 2.2838  Validation loss = 6.1085  \n",
      "\n",
      "Fold: 14  Epoch: 22  Training loss = 2.2827  Validation loss = 6.0989  \n",
      "\n",
      "Fold: 14  Epoch: 23  Training loss = 2.2826  Validation loss = 6.1150  \n",
      "\n",
      "Fold: 14  Epoch: 24  Training loss = 2.2823  Validation loss = 6.1226  \n",
      "\n",
      "Fold: 14  Epoch: 25  Training loss = 2.2811  Validation loss = 6.1105  \n",
      "\n",
      "Fold: 14  Epoch: 26  Training loss = 2.2807  Validation loss = 6.1195  \n",
      "\n",
      "Fold: 14  Epoch: 27  Training loss = 2.2791  Validation loss = 6.1039  \n",
      "\n",
      "Fold: 14  Epoch: 28  Training loss = 2.2777  Validation loss = 6.1019  \n",
      "\n",
      "Fold: 14  Epoch: 29  Training loss = 2.2768  Validation loss = 6.1037  \n",
      "\n",
      "Fold: 14  Epoch: 30  Training loss = 2.2754  Validation loss = 6.0903  \n",
      "\n",
      "Fold: 14  Epoch: 31  Training loss = 2.2738  Validation loss = 6.0745  \n",
      "\n",
      "Fold: 14  Epoch: 32  Training loss = 2.2729  Validation loss = 6.0711  \n",
      "\n",
      "Fold: 14  Epoch: 33  Training loss = 2.2717  Validation loss = 6.0698  \n",
      "\n",
      "Fold: 14  Epoch: 34  Training loss = 2.2705  Validation loss = 6.0676  \n",
      "\n",
      "Fold: 14  Epoch: 35  Training loss = 2.2693  Validation loss = 6.0697  \n",
      "\n",
      "Fold: 14  Epoch: 36  Training loss = 2.2695  Validation loss = 6.0850  \n",
      "\n",
      "Fold: 14  Epoch: 37  Training loss = 2.2686  Validation loss = 6.0736  \n",
      "\n",
      "Fold: 14  Epoch: 38  Training loss = 2.2670  Validation loss = 6.0639  \n",
      "\n",
      "Fold: 14  Epoch: 39  Training loss = 2.2660  Validation loss = 6.0696  \n",
      "\n",
      "Fold: 14  Epoch: 40  Training loss = 2.2648  Validation loss = 6.0569  \n",
      "\n",
      "Fold: 14  Epoch: 41  Training loss = 2.2641  Validation loss = 6.0488  \n",
      "\n",
      "Fold: 14  Epoch: 42  Training loss = 2.2626  Validation loss = 6.0307  \n",
      "\n",
      "Fold: 14  Epoch: 43  Training loss = 2.2614  Validation loss = 5.9970  \n",
      "\n",
      "Fold: 14  Epoch: 44  Training loss = 2.2600  Validation loss = 6.0098  \n",
      "\n",
      "Fold: 14  Epoch: 45  Training loss = 2.2590  Validation loss = 6.0239  \n",
      "\n",
      "Fold: 14  Epoch: 46  Training loss = 2.2577  Validation loss = 6.0176  \n",
      "\n",
      "Fold: 14  Epoch: 47  Training loss = 2.2572  Validation loss = 6.0244  \n",
      "\n",
      "Fold: 14  Epoch: 48  Training loss = 2.2561  Validation loss = 6.0171  \n",
      "\n",
      "Fold: 14  Epoch: 49  Training loss = 2.2552  Validation loss = 6.0247  \n",
      "\n",
      "Fold: 14  Epoch: 50  Training loss = 2.2544  Validation loss = 6.0245  \n",
      "\n",
      "Fold: 14  Epoch: 51  Training loss = 2.2539  Validation loss = 6.0431  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 43  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 2.6975  Validation loss = 6.9106  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 2.6907  Validation loss = 6.8836  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 2.6843  Validation loss = 6.8587  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 2.6814  Validation loss = 6.8479  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 2.6748  Validation loss = 6.8159  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 2.6682  Validation loss = 6.8152  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 2.6623  Validation loss = 6.7850  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 2.6624  Validation loss = 6.8008  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 2.6583  Validation loss = 6.7839  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 2.6530  Validation loss = 6.7601  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 2.6530  Validation loss = 6.7706  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 2.6496  Validation loss = 6.7548  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 2.6466  Validation loss = 6.7379  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 2.6451  Validation loss = 6.7438  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 2.6433  Validation loss = 6.7425  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 2.6444  Validation loss = 6.7721  \n",
      "\n",
      "Fold: 15  Epoch: 17  Training loss = 2.6403  Validation loss = 6.7479  \n",
      "\n",
      "Fold: 15  Epoch: 18  Training loss = 2.6374  Validation loss = 6.7296  \n",
      "\n",
      "Fold: 15  Epoch: 19  Training loss = 2.6335  Validation loss = 6.7147  \n",
      "\n",
      "Fold: 15  Epoch: 20  Training loss = 2.6307  Validation loss = 6.7006  \n",
      "\n",
      "Fold: 15  Epoch: 21  Training loss = 2.6302  Validation loss = 6.7081  \n",
      "\n",
      "Fold: 15  Epoch: 22  Training loss = 2.6276  Validation loss = 6.7001  \n",
      "\n",
      "Fold: 15  Epoch: 23  Training loss = 2.6255  Validation loss = 6.6964  \n",
      "\n",
      "Fold: 15  Epoch: 24  Training loss = 2.6243  Validation loss = 6.7068  \n",
      "\n",
      "Fold: 15  Epoch: 25  Training loss = 2.6203  Validation loss = 6.6730  \n",
      "\n",
      "Fold: 15  Epoch: 26  Training loss = 2.6192  Validation loss = 6.6832  \n",
      "\n",
      "Fold: 15  Epoch: 27  Training loss = 2.6163  Validation loss = 6.6550  \n",
      "\n",
      "Fold: 15  Epoch: 28  Training loss = 2.6145  Validation loss = 6.6480  \n",
      "\n",
      "Fold: 15  Epoch: 29  Training loss = 2.6129  Validation loss = 6.6534  \n",
      "\n",
      "Fold: 15  Epoch: 30  Training loss = 2.6106  Validation loss = 6.6590  \n",
      "\n",
      "Fold: 15  Epoch: 31  Training loss = 2.6075  Validation loss = 6.6354  \n",
      "\n",
      "Fold: 15  Epoch: 32  Training loss = 2.6049  Validation loss = 6.6116  \n",
      "\n",
      "Fold: 15  Epoch: 33  Training loss = 2.6040  Validation loss = 6.6316  \n",
      "\n",
      "Fold: 15  Epoch: 34  Training loss = 2.6035  Validation loss = 6.6468  \n",
      "\n",
      "Fold: 15  Epoch: 35  Training loss = 2.6016  Validation loss = 6.6384  \n",
      "\n",
      "Fold: 15  Epoch: 36  Training loss = 2.5988  Validation loss = 6.6126  \n",
      "\n",
      "Fold: 15  Epoch: 37  Training loss = 2.5976  Validation loss = 6.5982  \n",
      "\n",
      "Fold: 15  Epoch: 38  Training loss = 2.5957  Validation loss = 6.5987  \n",
      "\n",
      "Fold: 15  Epoch: 39  Training loss = 2.5943  Validation loss = 6.5978  \n",
      "\n",
      "Fold: 15  Epoch: 40  Training loss = 2.5920  Validation loss = 6.5720  \n",
      "\n",
      "Fold: 15  Epoch: 41  Training loss = 2.5908  Validation loss = 6.5586  \n",
      "\n",
      "Fold: 15  Epoch: 42  Training loss = 2.5894  Validation loss = 6.5647  \n",
      "\n",
      "Fold: 15  Epoch: 43  Training loss = 2.5881  Validation loss = 6.5692  \n",
      "\n",
      "Fold: 15  Epoch: 44  Training loss = 2.5875  Validation loss = 6.5686  \n",
      "\n",
      "Fold: 15  Epoch: 45  Training loss = 2.5863  Validation loss = 6.5649  \n",
      "\n",
      "Fold: 15  Epoch: 46  Training loss = 2.5847  Validation loss = 6.5742  \n",
      "\n",
      "Fold: 15  Epoch: 47  Training loss = 2.5827  Validation loss = 6.5541  \n",
      "\n",
      "Fold: 15  Epoch: 48  Training loss = 2.5837  Validation loss = 6.5133  \n",
      "\n",
      "Fold: 15  Epoch: 49  Training loss = 2.5795  Validation loss = 6.5031  \n",
      "\n",
      "Fold: 15  Epoch: 50  Training loss = 2.5787  Validation loss = 6.5141  \n",
      "\n",
      "Fold: 15  Epoch: 51  Training loss = 2.5774  Validation loss = 6.5336  \n",
      "\n",
      "Fold: 15  Epoch: 52  Training loss = 2.5785  Validation loss = 6.5071  \n",
      "\n",
      "Fold: 15  Epoch: 53  Training loss = 2.5790  Validation loss = 6.5082  \n",
      "\n",
      "Fold: 15  Epoch: 54  Training loss = 2.5797  Validation loss = 6.4991  \n",
      "\n",
      "Fold: 15  Epoch: 55  Training loss = 2.5739  Validation loss = 6.5048  \n",
      "\n",
      "Fold: 15  Epoch: 56  Training loss = 2.5728  Validation loss = 6.5034  \n",
      "\n",
      "Fold: 15  Epoch: 57  Training loss = 2.5717  Validation loss = 6.4905  \n",
      "\n",
      "Fold: 15  Epoch: 58  Training loss = 2.5697  Validation loss = 6.5136  \n",
      "\n",
      "Fold: 15  Epoch: 59  Training loss = 2.5690  Validation loss = 6.5235  \n",
      "\n",
      "Fold: 15  Epoch: 60  Training loss = 2.5679  Validation loss = 6.5335  \n",
      "\n",
      "Fold: 15  Epoch: 61  Training loss = 2.5660  Validation loss = 6.5486  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 57  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 3.0154  Validation loss = 5.4179  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 3.0104  Validation loss = 5.4154  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 3.0112  Validation loss = 5.4262  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 3.0007  Validation loss = 5.3130  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 2.9972  Validation loss = 5.2971  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 2.9877  Validation loss = 5.2766  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 2.9840  Validation loss = 5.2639  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 2.9822  Validation loss = 5.2559  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 2.9822  Validation loss = 5.2505  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 2.9811  Validation loss = 5.2332  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 2.9815  Validation loss = 5.2346  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 2.9796  Validation loss = 5.2354  \n",
      "\n",
      "Fold: 16  Epoch: 13  Training loss = 2.9746  Validation loss = 5.2135  \n",
      "\n",
      "Fold: 16  Epoch: 14  Training loss = 2.9703  Validation loss = 5.1887  \n",
      "\n",
      "Fold: 16  Epoch: 15  Training loss = 2.9674  Validation loss = 5.1816  \n",
      "\n",
      "Fold: 16  Epoch: 16  Training loss = 2.9672  Validation loss = 5.1805  \n",
      "\n",
      "Fold: 16  Epoch: 17  Training loss = 2.9637  Validation loss = 5.1697  \n",
      "\n",
      "Fold: 16  Epoch: 18  Training loss = 2.9605  Validation loss = 5.1568  \n",
      "\n",
      "Fold: 16  Epoch: 19  Training loss = 2.9596  Validation loss = 5.1601  \n",
      "\n",
      "Fold: 16  Epoch: 20  Training loss = 2.9571  Validation loss = 5.1560  \n",
      "\n",
      "Fold: 16  Epoch: 21  Training loss = 2.9558  Validation loss = 5.1458  \n",
      "\n",
      "Fold: 16  Epoch: 22  Training loss = 2.9521  Validation loss = 5.1344  \n",
      "\n",
      "Fold: 16  Epoch: 23  Training loss = 2.9502  Validation loss = 5.1280  \n",
      "\n",
      "Fold: 16  Epoch: 24  Training loss = 2.9483  Validation loss = 5.1205  \n",
      "\n",
      "Fold: 16  Epoch: 25  Training loss = 2.9464  Validation loss = 5.1147  \n",
      "\n",
      "Fold: 16  Epoch: 26  Training loss = 2.9462  Validation loss = 5.1237  \n",
      "\n",
      "Fold: 16  Epoch: 27  Training loss = 2.9448  Validation loss = 5.1294  \n",
      "\n",
      "Fold: 16  Epoch: 28  Training loss = 2.9442  Validation loss = 5.1306  \n",
      "\n",
      "Fold: 16  Epoch: 29  Training loss = 2.9422  Validation loss = 5.1199  \n",
      "\n",
      "Fold: 16  Epoch: 30  Training loss = 2.9403  Validation loss = 5.1424  \n",
      "\n",
      "Fold: 16  Epoch: 31  Training loss = 2.9404  Validation loss = 5.1474  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 25  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 3.1928  Validation loss = 3.1826  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 3.1806  Validation loss = 3.1847  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 3.1725  Validation loss = 3.1841  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 3.1620  Validation loss = 3.1967  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 3.1539  Validation loss = 3.2042  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 3.1498  Validation loss = 3.1997  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 3.1441  Validation loss = 3.1963  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 3.1411  Validation loss = 3.1973  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 3.1350  Validation loss = 3.2018  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 3.1298  Validation loss = 3.2038  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 3.1257  Validation loss = 3.2045  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 1  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 3.2109  Validation loss = 2.7011  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 3.2049  Validation loss = 2.6960  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 3.2012  Validation loss = 2.6880  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 3.1969  Validation loss = 2.6866  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 3.1969  Validation loss = 2.6871  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 3.1935  Validation loss = 2.6856  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 3.1849  Validation loss = 2.6809  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 3.1829  Validation loss = 2.6802  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 3.1776  Validation loss = 2.6754  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 3.1750  Validation loss = 2.6697  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 3.1710  Validation loss = 2.6709  \n",
      "\n",
      "Fold: 18  Epoch: 12  Training loss = 3.1711  Validation loss = 2.6742  \n",
      "\n",
      "Fold: 18  Epoch: 13  Training loss = 3.1718  Validation loss = 2.6660  \n",
      "\n",
      "Fold: 18  Epoch: 14  Training loss = 3.1649  Validation loss = 2.6581  \n",
      "\n",
      "Fold: 18  Epoch: 15  Training loss = 3.1622  Validation loss = 2.6567  \n",
      "\n",
      "Fold: 18  Epoch: 16  Training loss = 3.1534  Validation loss = 2.6485  \n",
      "\n",
      "Fold: 18  Epoch: 17  Training loss = 3.1498  Validation loss = 2.6478  \n",
      "\n",
      "Fold: 18  Epoch: 18  Training loss = 3.1479  Validation loss = 2.6504  \n",
      "\n",
      "Fold: 18  Epoch: 19  Training loss = 3.1475  Validation loss = 2.6530  \n",
      "\n",
      "Fold: 18  Epoch: 20  Training loss = 3.1418  Validation loss = 2.6399  \n",
      "\n",
      "Fold: 18  Epoch: 21  Training loss = 3.1393  Validation loss = 2.6330  \n",
      "\n",
      "Fold: 18  Epoch: 22  Training loss = 3.1338  Validation loss = 2.6277  \n",
      "\n",
      "Fold: 18  Epoch: 23  Training loss = 3.1323  Validation loss = 2.6258  \n",
      "\n",
      "Fold: 18  Epoch: 24  Training loss = 3.1282  Validation loss = 2.6294  \n",
      "\n",
      "Fold: 18  Epoch: 25  Training loss = 3.1294  Validation loss = 2.6072  \n",
      "\n",
      "Fold: 18  Epoch: 26  Training loss = 3.1237  Validation loss = 2.6003  \n",
      "\n",
      "Fold: 18  Epoch: 27  Training loss = 3.1210  Validation loss = 2.5945  \n",
      "\n",
      "Fold: 18  Epoch: 28  Training loss = 3.1191  Validation loss = 2.5906  \n",
      "\n",
      "Fold: 18  Epoch: 29  Training loss = 3.1130  Validation loss = 2.5922  \n",
      "\n",
      "Fold: 18  Epoch: 30  Training loss = 3.1051  Validation loss = 2.5953  \n",
      "\n",
      "Fold: 18  Epoch: 31  Training loss = 3.1046  Validation loss = 2.5968  \n",
      "\n",
      "Fold: 18  Epoch: 32  Training loss = 3.1027  Validation loss = 2.6032  \n",
      "\n",
      "Fold: 18  Epoch: 33  Training loss = 3.0965  Validation loss = 2.6068  \n",
      "\n",
      "Fold: 18  Epoch: 34  Training loss = 3.0949  Validation loss = 2.6070  \n",
      "\n",
      "Fold: 18  Epoch: 35  Training loss = 3.1058  Validation loss = 2.6084  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 28  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 3.0587  Validation loss = 1.9264  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 3.0538  Validation loss = 1.9148  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 3.0465  Validation loss = 1.8950  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 3.0421  Validation loss = 1.8675  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 3.0366  Validation loss = 1.8420  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 3.0321  Validation loss = 1.8309  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 3.0271  Validation loss = 1.7985  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 3.0222  Validation loss = 1.7864  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 3.0189  Validation loss = 1.7641  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 3.0150  Validation loss = 1.7259  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 3.0127  Validation loss = 1.7336  \n",
      "\n",
      "Fold: 19  Epoch: 12  Training loss = 3.0052  Validation loss = 1.6875  \n",
      "\n",
      "Fold: 19  Epoch: 13  Training loss = 3.0026  Validation loss = 1.6832  \n",
      "\n",
      "Fold: 19  Epoch: 14  Training loss = 3.0005  Validation loss = 1.6818  \n",
      "\n",
      "Fold: 19  Epoch: 15  Training loss = 2.9963  Validation loss = 1.6699  \n",
      "\n",
      "Fold: 19  Epoch: 16  Training loss = 2.9947  Validation loss = 1.6600  \n",
      "\n",
      "Fold: 19  Epoch: 17  Training loss = 2.9891  Validation loss = 1.6087  \n",
      "\n",
      "Fold: 19  Epoch: 18  Training loss = 2.9863  Validation loss = 1.5931  \n",
      "\n",
      "Fold: 19  Epoch: 19  Training loss = 2.9850  Validation loss = 1.5785  \n",
      "\n",
      "Fold: 19  Epoch: 20  Training loss = 2.9789  Validation loss = 1.5354  \n",
      "\n",
      "Fold: 19  Epoch: 21  Training loss = 2.9755  Validation loss = 1.5143  \n",
      "\n",
      "Fold: 19  Epoch: 22  Training loss = 2.9712  Validation loss = 1.5009  \n",
      "\n",
      "Fold: 19  Epoch: 23  Training loss = 2.9667  Validation loss = 1.4691  \n",
      "\n",
      "Fold: 19  Epoch: 24  Training loss = 2.9650  Validation loss = 1.4744  \n",
      "\n",
      "Fold: 19  Epoch: 25  Training loss = 2.9690  Validation loss = 1.4855  \n",
      "\n",
      "Fold: 19  Epoch: 26  Training loss = 2.9676  Validation loss = 1.4868  \n",
      "\n",
      "Fold: 19  Epoch: 27  Training loss = 2.9605  Validation loss = 1.4751  \n",
      "\n",
      "Fold: 19  Epoch: 28  Training loss = 2.9576  Validation loss = 1.4570  \n",
      "\n",
      "Fold: 19  Epoch: 29  Training loss = 2.9553  Validation loss = 1.4565  \n",
      "\n",
      "Fold: 19  Epoch: 30  Training loss = 2.9547  Validation loss = 1.4300  \n",
      "\n",
      "Fold: 19  Epoch: 31  Training loss = 2.9524  Validation loss = 1.4053  \n",
      "\n",
      "Fold: 19  Epoch: 32  Training loss = 2.9489  Validation loss = 1.4013  \n",
      "\n",
      "Fold: 19  Epoch: 33  Training loss = 2.9457  Validation loss = 1.3862  \n",
      "\n",
      "Fold: 19  Epoch: 34  Training loss = 2.9428  Validation loss = 1.3626  \n",
      "\n",
      "Fold: 19  Epoch: 35  Training loss = 2.9406  Validation loss = 1.3561  \n",
      "\n",
      "Fold: 19  Epoch: 36  Training loss = 2.9445  Validation loss = 1.3250  \n",
      "\n",
      "Fold: 19  Epoch: 37  Training loss = 2.9383  Validation loss = 1.2966  \n",
      "\n",
      "Fold: 19  Epoch: 38  Training loss = 2.9474  Validation loss = 1.2613  \n",
      "\n",
      "Fold: 19  Epoch: 39  Training loss = 2.9346  Validation loss = 1.2712  \n",
      "\n",
      "Fold: 19  Epoch: 40  Training loss = 2.9288  Validation loss = 1.2899  \n",
      "\n",
      "Fold: 19  Epoch: 41  Training loss = 2.9260  Validation loss = 1.2943  \n",
      "\n",
      "Fold: 19  Epoch: 42  Training loss = 2.9247  Validation loss = 1.2877  \n",
      "\n",
      "Fold: 19  Epoch: 43  Training loss = 2.9259  Validation loss = 1.2552  \n",
      "\n",
      "Fold: 19  Epoch: 44  Training loss = 2.9224  Validation loss = 1.2505  \n",
      "\n",
      "Fold: 19  Epoch: 45  Training loss = 2.9205  Validation loss = 1.2154  \n",
      "\n",
      "Fold: 19  Epoch: 46  Training loss = 2.9250  Validation loss = 1.1904  \n",
      "\n",
      "Fold: 19  Epoch: 47  Training loss = 2.9262  Validation loss = 1.1765  \n",
      "\n",
      "Fold: 19  Epoch: 48  Training loss = 2.9098  Validation loss = 1.1999  \n",
      "\n",
      "Fold: 19  Epoch: 49  Training loss = 2.9066  Validation loss = 1.2024  \n",
      "\n",
      "Fold: 19  Epoch: 50  Training loss = 2.9063  Validation loss = 1.1766  \n",
      "\n",
      "Fold: 19  Epoch: 51  Training loss = 2.9064  Validation loss = 1.1600  \n",
      "\n",
      "Fold: 19  Epoch: 52  Training loss = 2.9024  Validation loss = 1.1538  \n",
      "\n",
      "Fold: 19  Epoch: 53  Training loss = 2.9002  Validation loss = 1.1625  \n",
      "\n",
      "Fold: 19  Epoch: 54  Training loss = 2.8969  Validation loss = 1.1792  \n",
      "\n",
      "Fold: 19  Epoch: 55  Training loss = 2.8988  Validation loss = 1.1555  \n",
      "\n",
      "Fold: 19  Epoch: 56  Training loss = 2.8928  Validation loss = 1.1410  \n",
      "\n",
      "Fold: 19  Epoch: 57  Training loss = 2.8965  Validation loss = 1.1275  \n",
      "\n",
      "Fold: 19  Epoch: 58  Training loss = 2.8892  Validation loss = 1.1225  \n",
      "\n",
      "Fold: 19  Epoch: 59  Training loss = 2.8886  Validation loss = 1.1149  \n",
      "\n",
      "Fold: 19  Epoch: 60  Training loss = 2.8900  Validation loss = 1.0885  \n",
      "\n",
      "Fold: 19  Epoch: 61  Training loss = 2.8848  Validation loss = 1.0847  \n",
      "\n",
      "Fold: 19  Epoch: 62  Training loss = 2.8867  Validation loss = 1.0808  \n",
      "\n",
      "Fold: 19  Epoch: 63  Training loss = 2.8816  Validation loss = 1.0852  \n",
      "\n",
      "Fold: 19  Epoch: 64  Training loss = 2.8755  Validation loss = 1.0863  \n",
      "\n",
      "Fold: 19  Epoch: 65  Training loss = 2.8764  Validation loss = 1.0701  \n",
      "\n",
      "Fold: 19  Epoch: 66  Training loss = 2.8688  Validation loss = 1.0818  \n",
      "\n",
      "Fold: 19  Epoch: 67  Training loss = 2.8656  Validation loss = 1.0770  \n",
      "\n",
      "Fold: 19  Epoch: 68  Training loss = 2.8676  Validation loss = 1.0503  \n",
      "\n",
      "Fold: 19  Epoch: 69  Training loss = 2.8626  Validation loss = 1.0516  \n",
      "\n",
      "Fold: 19  Epoch: 70  Training loss = 2.8667  Validation loss = 1.0327  \n",
      "\n",
      "Fold: 19  Epoch: 71  Training loss = 2.8582  Validation loss = 1.0487  \n",
      "\n",
      "Fold: 19  Epoch: 72  Training loss = 2.8555  Validation loss = 1.0394  \n",
      "\n",
      "Fold: 19  Epoch: 73  Training loss = 2.8561  Validation loss = 1.0822  \n",
      "\n",
      "Fold: 19  Epoch: 74  Training loss = 2.8639  Validation loss = 1.0855  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 70  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 2.8269  Validation loss = 0.8937  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 2.8215  Validation loss = 0.9365  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 2.8217  Validation loss = 0.9618  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 2.8209  Validation loss = 1.0133  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 2.8155  Validation loss = 0.9276  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 2.8112  Validation loss = 0.9813  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 2.8089  Validation loss = 0.9967  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 2.8064  Validation loss = 0.9977  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 2.8045  Validation loss = 1.0057  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 2.8027  Validation loss = 1.0154  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 2.8026  Validation loss = 0.9961  \n",
      "\n",
      "Fold: 20  Epoch: 12  Training loss = 2.7976  Validation loss = 0.9865  \n",
      "\n",
      "Fold: 20  Epoch: 13  Training loss = 2.7939  Validation loss = 1.0027  \n",
      "\n",
      "Fold: 20  Epoch: 14  Training loss = 2.7923  Validation loss = 1.0174  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 1  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 2.7890  Validation loss = 2.9370  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 2.7885  Validation loss = 2.9365  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 2.7879  Validation loss = 2.9153  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 2.7874  Validation loss = 2.9218  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 2.7811  Validation loss = 2.9476  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 2.7788  Validation loss = 2.9467  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 2.7803  Validation loss = 2.9625  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 2.7795  Validation loss = 2.9681  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 2.7725  Validation loss = 2.9506  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 2.7724  Validation loss = 2.9474  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 2.7687  Validation loss = 2.9631  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 2.7647  Validation loss = 2.9475  \n",
      "\n",
      "Fold: 21  Epoch: 13  Training loss = 2.7611  Validation loss = 2.9205  \n",
      "\n",
      "Fold: 21  Epoch: 14  Training loss = 2.7619  Validation loss = 2.9036  \n",
      "\n",
      "Fold: 21  Epoch: 15  Training loss = 2.7601  Validation loss = 2.8964  \n",
      "\n",
      "Fold: 21  Epoch: 16  Training loss = 2.7575  Validation loss = 2.8952  \n",
      "\n",
      "Fold: 21  Epoch: 17  Training loss = 2.7591  Validation loss = 2.8769  \n",
      "\n",
      "Fold: 21  Epoch: 18  Training loss = 2.7527  Validation loss = 2.9052  \n",
      "\n",
      "Fold: 21  Epoch: 19  Training loss = 2.7517  Validation loss = 2.9157  \n",
      "\n",
      "Fold: 21  Epoch: 20  Training loss = 2.7553  Validation loss = 2.8612  \n",
      "\n",
      "Fold: 21  Epoch: 21  Training loss = 2.7515  Validation loss = 2.8545  \n",
      "\n",
      "Fold: 21  Epoch: 22  Training loss = 2.7506  Validation loss = 2.8471  \n",
      "\n",
      "Fold: 21  Epoch: 23  Training loss = 2.7561  Validation loss = 2.8249  \n",
      "\n",
      "Fold: 21  Epoch: 24  Training loss = 2.7465  Validation loss = 2.8342  \n",
      "\n",
      "Fold: 21  Epoch: 25  Training loss = 2.7411  Validation loss = 2.8415  \n",
      "\n",
      "Fold: 21  Epoch: 26  Training loss = 2.7412  Validation loss = 2.8643  \n",
      "\n",
      "Fold: 21  Epoch: 27  Training loss = 2.7363  Validation loss = 2.8648  \n",
      "\n",
      "Fold: 21  Epoch: 28  Training loss = 2.7342  Validation loss = 2.8508  \n",
      "\n",
      "Fold: 21  Epoch: 29  Training loss = 2.7345  Validation loss = 2.8385  \n",
      "\n",
      "Fold: 21  Epoch: 30  Training loss = 2.7315  Validation loss = 2.8436  \n",
      "\n",
      "Fold: 21  Epoch: 31  Training loss = 2.7287  Validation loss = 2.8439  \n",
      "\n",
      "Fold: 21  Epoch: 32  Training loss = 2.7278  Validation loss = 2.8428  \n",
      "\n",
      "Fold: 21  Epoch: 33  Training loss = 2.7266  Validation loss = 2.8537  \n",
      "\n",
      "Fold: 21  Epoch: 34  Training loss = 2.7228  Validation loss = 2.8412  \n",
      "\n",
      "Fold: 21  Epoch: 35  Training loss = 2.7209  Validation loss = 2.8576  \n",
      "\n",
      "Fold: 21  Epoch: 36  Training loss = 2.7202  Validation loss = 2.8692  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 23  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 2.7722  Validation loss = 0.9564  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 2.7674  Validation loss = 1.0089  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 2.7765  Validation loss = 0.9796  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 2.7617  Validation loss = 1.0116  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 2.7610  Validation loss = 1.0311  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 2.7608  Validation loss = 1.0382  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 2.7561  Validation loss = 1.0234  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 2.7879  Validation loss = 1.0746  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 2.7535  Validation loss = 1.0206  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 2.7428  Validation loss = 0.9904  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 2.7421  Validation loss = 0.9659  \n",
      "\n",
      "Fold: 22  Epoch: 12  Training loss = 2.7374  Validation loss = 0.9818  \n",
      "\n",
      "Fold: 22  Epoch: 13  Training loss = 2.7405  Validation loss = 0.9632  \n",
      "\n",
      "Fold: 22  Epoch: 14  Training loss = 2.7524  Validation loss = 0.9374  \n",
      "\n",
      "Fold: 22  Epoch: 15  Training loss = 2.7339  Validation loss = 0.9465  \n",
      "\n",
      "Fold: 22  Epoch: 16  Training loss = 2.7243  Validation loss = 0.9751  \n",
      "\n",
      "Fold: 22  Epoch: 17  Training loss = 2.7224  Validation loss = 0.9581  \n",
      "\n",
      "Fold: 22  Epoch: 18  Training loss = 2.7182  Validation loss = 1.0036  \n",
      "\n",
      "Fold: 22  Epoch: 19  Training loss = 2.7247  Validation loss = 1.0435  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 14  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 2.5645  Validation loss = 1.5356  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 2.5591  Validation loss = 1.4937  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 2.5546  Validation loss = 1.4267  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 2.5521  Validation loss = 1.4823  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 2.5463  Validation loss = 1.4470  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 2.5427  Validation loss = 1.4922  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 2.5377  Validation loss = 1.5270  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 2.5321  Validation loss = 1.4887  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 2.5312  Validation loss = 1.4901  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 2.5254  Validation loss = 1.5337  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 2.5255  Validation loss = 1.5980  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 3  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 2.4477  Validation loss = 1.6558  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 2.4309  Validation loss = 1.7286  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 2.4390  Validation loss = 1.6935  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 2.4243  Validation loss = 1.7262  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 2.4419  Validation loss = 1.7307  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 2.4327  Validation loss = 1.6609  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 2.4137  Validation loss = 1.7075  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 2.4082  Validation loss = 1.7441  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 2.4062  Validation loss = 1.7704  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 2.4124  Validation loss = 1.7976  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 2.4188  Validation loss = 1.7223  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 2.4109  Validation loss = 1.7398  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 2.3972  Validation loss = 1.7391  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 2.4109  Validation loss = 1.6981  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 2.3914  Validation loss = 1.7127  \n",
      "\n",
      "Fold: 24  Epoch: 16  Training loss = 2.3894  Validation loss = 1.7187  \n",
      "\n",
      "Fold: 24  Epoch: 17  Training loss = 2.3873  Validation loss = 1.7285  \n",
      "\n",
      "Fold: 24  Epoch: 18  Training loss = 2.3792  Validation loss = 1.7108  \n",
      "\n",
      "Fold: 24  Epoch: 19  Training loss = 2.3755  Validation loss = 1.6921  \n",
      "\n",
      "Fold: 24  Epoch: 20  Training loss = 2.3728  Validation loss = 1.6681  \n",
      "\n",
      "Fold: 24  Epoch: 21  Training loss = 2.3862  Validation loss = 1.6735  \n",
      "\n",
      "Fold: 24  Epoch: 22  Training loss = 2.3687  Validation loss = 1.7187  \n",
      "\n",
      "Fold: 24  Epoch: 23  Training loss = 2.3754  Validation loss = 1.7095  \n",
      "\n",
      "Fold: 24  Epoch: 24  Training loss = 2.3776  Validation loss = 1.7110  \n",
      "\n",
      "Fold: 24  Epoch: 25  Training loss = 2.3594  Validation loss = 1.6934  \n",
      "\n",
      "Fold: 24  Epoch: 26  Training loss = 2.3634  Validation loss = 1.6967  \n",
      "\n",
      "Fold: 24  Epoch: 27  Training loss = 2.3692  Validation loss = 1.7123  \n",
      "\n",
      "Fold: 24  Epoch: 28  Training loss = 2.3599  Validation loss = 1.6984  \n",
      "\n",
      "Fold: 24  Epoch: 29  Training loss = 2.3532  Validation loss = 1.6873  \n",
      "\n",
      "Fold: 24  Epoch: 30  Training loss = 2.3656  Validation loss = 1.6374  \n",
      "\n",
      "Fold: 24  Epoch: 31  Training loss = 2.3487  Validation loss = 1.6351  \n",
      "\n",
      "Fold: 24  Epoch: 32  Training loss = 2.3411  Validation loss = 1.6738  \n",
      "\n",
      "Fold: 24  Epoch: 33  Training loss = 2.3413  Validation loss = 1.6936  \n",
      "\n",
      "Fold: 24  Epoch: 34  Training loss = 2.3382  Validation loss = 1.6582  \n",
      "\n",
      "Fold: 24  Epoch: 35  Training loss = 2.3348  Validation loss = 1.6661  \n",
      "\n",
      "Fold: 24  Epoch: 36  Training loss = 2.3305  Validation loss = 1.6616  \n",
      "\n",
      "Fold: 24  Epoch: 37  Training loss = 2.3675  Validation loss = 1.6818  \n",
      "\n",
      "Fold: 24  Epoch: 38  Training loss = 2.3251  Validation loss = 1.7058  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 31  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 2.3338  Validation loss = 2.3814  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 2.3417  Validation loss = 2.2364  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 2.3541  Validation loss = 2.4393  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 2.3389  Validation loss = 2.1442  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 2.3239  Validation loss = 2.3250  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 2.3192  Validation loss = 2.2506  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 2.3138  Validation loss = 2.3233  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 2.3111  Validation loss = 2.3538  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 2.3306  Validation loss = 2.3804  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 2.3038  Validation loss = 2.3299  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 2.2990  Validation loss = 2.2213  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 2.3581  Validation loss = 2.2988  \n",
      "\n",
      "Fold: 25  Epoch: 13  Training loss = 2.3508  Validation loss = 2.2652  \n",
      "\n",
      "Fold: 25  Epoch: 14  Training loss = 2.3516  Validation loss = 2.2368  \n",
      "\n",
      "Fold: 25  Epoch: 15  Training loss = 2.3368  Validation loss = 2.2005  \n",
      "\n",
      "Fold: 25  Epoch: 16  Training loss = 2.3353  Validation loss = 2.1978  \n",
      "\n",
      "Fold: 25  Epoch: 17  Training loss = 2.3305  Validation loss = 2.2143  \n",
      "\n",
      "Fold: 25  Epoch: 18  Training loss = 2.3249  Validation loss = 2.2053  \n",
      "\n",
      "Fold: 25  Epoch: 19  Training loss = 2.3312  Validation loss = 2.2629  \n",
      "\n",
      "Fold: 25  Epoch: 20  Training loss = 2.3229  Validation loss = 2.1900  \n",
      "\n",
      "Fold: 25  Epoch: 21  Training loss = 2.3161  Validation loss = 2.1935  \n",
      "\n",
      "Fold: 25  Epoch: 22  Training loss = 2.3135  Validation loss = 2.2227  \n",
      "\n",
      "Fold: 25  Epoch: 23  Training loss = 2.3129  Validation loss = 2.2227  \n",
      "\n",
      "Fold: 25  Epoch: 24  Training loss = 2.3090  Validation loss = 2.2352  \n",
      "\n",
      "Fold: 25  Epoch: 25  Training loss = 2.3052  Validation loss = 2.2064  \n",
      "\n",
      "Fold: 25  Epoch: 26  Training loss = 2.3022  Validation loss = 2.1813  \n",
      "\n",
      "Fold: 25  Epoch: 27  Training loss = 2.2981  Validation loss = 2.1662  \n",
      "\n",
      "Fold: 25  Epoch: 28  Training loss = 2.2936  Validation loss = 2.1814  \n",
      "\n",
      "Fold: 25  Epoch: 29  Training loss = 2.2905  Validation loss = 2.1600  \n",
      "\n",
      "Fold: 25  Epoch: 30  Training loss = 2.2888  Validation loss = 2.1309  \n",
      "\n",
      "Fold: 25  Epoch: 31  Training loss = 2.2872  Validation loss = 2.1450  \n",
      "\n",
      "Fold: 25  Epoch: 32  Training loss = 2.2800  Validation loss = 2.1433  \n",
      "\n",
      "Fold: 25  Epoch: 33  Training loss = 2.2771  Validation loss = 2.1542  \n",
      "\n",
      "Fold: 25  Epoch: 34  Training loss = 2.2729  Validation loss = 2.1212  \n",
      "\n",
      "Fold: 25  Epoch: 35  Training loss = 2.2712  Validation loss = 2.1295  \n",
      "\n",
      "Fold: 25  Epoch: 36  Training loss = 2.2688  Validation loss = 2.1297  \n",
      "\n",
      "Fold: 25  Epoch: 37  Training loss = 2.2682  Validation loss = 2.1366  \n",
      "\n",
      "Fold: 25  Epoch: 38  Training loss = 2.2676  Validation loss = 2.0824  \n",
      "\n",
      "Fold: 25  Epoch: 39  Training loss = 2.2623  Validation loss = 2.1149  \n",
      "\n",
      "Fold: 25  Epoch: 40  Training loss = 2.2622  Validation loss = 2.0815  \n",
      "\n",
      "Fold: 25  Epoch: 41  Training loss = 2.2570  Validation loss = 2.0710  \n",
      "\n",
      "Fold: 25  Epoch: 42  Training loss = 2.2599  Validation loss = 2.0770  \n",
      "\n",
      "Fold: 25  Epoch: 43  Training loss = 2.2515  Validation loss = 2.0646  \n",
      "\n",
      "Fold: 25  Epoch: 44  Training loss = 2.2477  Validation loss = 2.0929  \n",
      "\n",
      "Fold: 25  Epoch: 45  Training loss = 2.2468  Validation loss = 2.1083  \n",
      "\n",
      "Fold: 25  Epoch: 46  Training loss = 2.2440  Validation loss = 2.1103  \n",
      "\n",
      "Fold: 25  Epoch: 47  Training loss = 2.2437  Validation loss = 2.1093  \n",
      "\n",
      "Fold: 25  Epoch: 48  Training loss = 2.2474  Validation loss = 2.0493  \n",
      "\n",
      "Fold: 25  Epoch: 49  Training loss = 2.2374  Validation loss = 2.0820  \n",
      "\n",
      "Fold: 25  Epoch: 50  Training loss = 2.2532  Validation loss = 2.1818  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 48  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 2.1859  Validation loss = 2.3677  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 2.1776  Validation loss = 2.4649  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 2.1740  Validation loss = 2.4953  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 2.1719  Validation loss = 2.4901  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 2.1685  Validation loss = 2.5332  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 2.1672  Validation loss = 2.5820  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 2.1634  Validation loss = 2.5648  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 2.1641  Validation loss = 2.5143  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 2.1672  Validation loss = 2.4916  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 2.1753  Validation loss = 2.4155  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 2.1679  Validation loss = 2.4818  \n",
      "\n",
      "Fold: 26  Epoch: 12  Training loss = 2.1652  Validation loss = 2.6330  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 1  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 2.1834  Validation loss = 2.1753  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 2.1827  Validation loss = 2.0631  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 2.1758  Validation loss = 2.1162  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 2.1711  Validation loss = 2.0408  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 2.1681  Validation loss = 2.0405  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 2.1667  Validation loss = 2.0520  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 2.1645  Validation loss = 1.9656  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 2.1635  Validation loss = 1.9643  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 2.1556  Validation loss = 1.9394  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 2.1638  Validation loss = 1.8095  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 2.1590  Validation loss = 2.1388  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 10  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 1.9927  Validation loss = 1.6255  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 2.0880  Validation loss = 1.6112  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 1.9790  Validation loss = 1.6439  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 2.0099  Validation loss = 1.6478  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 2.0282  Validation loss = 1.5967  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 1.9801  Validation loss = 1.6317  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 1.9704  Validation loss = 1.6653  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 1.9675  Validation loss = 1.6340  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 1.9657  Validation loss = 1.6308  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 1.9643  Validation loss = 1.5739  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 1.9555  Validation loss = 1.5305  \n",
      "\n",
      "Fold: 28  Epoch: 12  Training loss = 1.9511  Validation loss = 1.5267  \n",
      "\n",
      "Fold: 28  Epoch: 13  Training loss = 1.9444  Validation loss = 1.5997  \n",
      "\n",
      "Fold: 28  Epoch: 14  Training loss = 1.9860  Validation loss = 1.5565  \n",
      "\n",
      "Fold: 28  Epoch: 15  Training loss = 1.9340  Validation loss = 1.5694  \n",
      "\n",
      "Fold: 28  Epoch: 16  Training loss = 1.9315  Validation loss = 1.6027  \n",
      "\n",
      "Fold: 28  Epoch: 17  Training loss = 1.9226  Validation loss = 1.5773  \n",
      "\n",
      "Fold: 28  Epoch: 18  Training loss = 1.9095  Validation loss = 1.5903  \n",
      "\n",
      "Fold: 28  Epoch: 19  Training loss = 1.8991  Validation loss = 1.5508  \n",
      "\n",
      "Fold: 28  Epoch: 20  Training loss = 1.8958  Validation loss = 1.4785  \n",
      "\n",
      "Fold: 28  Epoch: 21  Training loss = 1.8836  Validation loss = 1.4826  \n",
      "\n",
      "Fold: 28  Epoch: 22  Training loss = 1.8837  Validation loss = 1.4296  \n",
      "\n",
      "Fold: 28  Epoch: 23  Training loss = 1.8794  Validation loss = 1.4913  \n",
      "\n",
      "Fold: 28  Epoch: 24  Training loss = 1.8803  Validation loss = 1.5285  \n",
      "\n",
      "Fold: 28  Epoch: 25  Training loss = 1.8816  Validation loss = 1.5217  \n",
      "\n",
      "Fold: 28  Epoch: 26  Training loss = 1.8791  Validation loss = 1.4919  \n",
      "\n",
      "Fold: 28  Epoch: 27  Training loss = 1.8867  Validation loss = 1.5202  \n",
      "\n",
      "Fold: 28  Epoch: 28  Training loss = 1.8758  Validation loss = 1.5662  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 22  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 1.8318  Validation loss = 1.6659  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 1.8298  Validation loss = 1.6317  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 1.8259  Validation loss = 1.6358  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 1.8195  Validation loss = 1.6491  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 1.8223  Validation loss = 1.6188  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 1.8137  Validation loss = 1.6329  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 1.8135  Validation loss = 1.5893  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 1.8070  Validation loss = 1.6573  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 1.8026  Validation loss = 1.6467  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 1.8011  Validation loss = 1.6316  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 1.8051  Validation loss = 1.6074  \n",
      "\n",
      "Fold: 29  Epoch: 12  Training loss = 1.8002  Validation loss = 1.6043  \n",
      "\n",
      "Fold: 29  Epoch: 13  Training loss = 1.7967  Validation loss = 1.5786  \n",
      "\n",
      "Fold: 29  Epoch: 14  Training loss = 1.7940  Validation loss = 1.6069  \n",
      "\n",
      "Fold: 29  Epoch: 15  Training loss = 1.7913  Validation loss = 1.5672  \n",
      "\n",
      "Fold: 29  Epoch: 16  Training loss = 1.7885  Validation loss = 1.5950  \n",
      "\n",
      "Fold: 29  Epoch: 17  Training loss = 1.7866  Validation loss = 1.5990  \n",
      "\n",
      "Fold: 29  Epoch: 18  Training loss = 1.7895  Validation loss = 1.6290  \n",
      "\n",
      "Fold: 29  Epoch: 19  Training loss = 1.7844  Validation loss = 1.6069  \n",
      "\n",
      "Fold: 29  Epoch: 20  Training loss = 1.7832  Validation loss = 1.5635  \n",
      "\n",
      "Fold: 29  Epoch: 21  Training loss = 1.7786  Validation loss = 1.5811  \n",
      "\n",
      "Fold: 29  Epoch: 22  Training loss = 1.7792  Validation loss = 1.6344  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 20  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 1.8117  Validation loss = 1.1537  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 1.8138  Validation loss = 1.1276  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 1.8130  Validation loss = 1.1527  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 1.8096  Validation loss = 1.1329  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 1.8086  Validation loss = 1.1367  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 1.8063  Validation loss = 1.1132  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 1.7955  Validation loss = 1.1622  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 1.7958  Validation loss = 1.1282  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 1.7922  Validation loss = 1.1163  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 1.7921  Validation loss = 1.1427  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 1.7874  Validation loss = 1.1768  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 6  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 1.7658  Validation loss = 0.8104  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 1.7621  Validation loss = 0.7703  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 1.7566  Validation loss = 0.7573  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 1.7558  Validation loss = 0.6434  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 1.7557  Validation loss = 0.6857  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 1.7532  Validation loss = 0.6183  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 1.7542  Validation loss = 0.6285  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 1.7636  Validation loss = 0.5181  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 1.7430  Validation loss = 0.4922  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 1.7379  Validation loss = 0.3983  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 1.7384  Validation loss = 0.3446  \n",
      "\n",
      "Fold: 31  Epoch: 12  Training loss = 1.7297  Validation loss = 0.3989  \n",
      "\n",
      "Fold: 31  Epoch: 13  Training loss = 1.7197  Validation loss = 0.3297  \n",
      "\n",
      "Fold: 31  Epoch: 14  Training loss = 1.7192  Validation loss = 0.3337  \n",
      "\n",
      "Fold: 31  Epoch: 15  Training loss = 1.7231  Validation loss = 0.2311  \n",
      "\n",
      "Fold: 31  Epoch: 16  Training loss = 1.7162  Validation loss = 0.2352  \n",
      "\n",
      "Fold: 31  Epoch: 17  Training loss = 1.7119  Validation loss = 0.2505  \n",
      "\n",
      "Fold: 31  Epoch: 18  Training loss = 1.7148  Validation loss = 0.2457  \n",
      "\n",
      "Fold: 31  Epoch: 19  Training loss = 1.7068  Validation loss = 0.3498  \n",
      "\n",
      "Fold: 31  Epoch: 20  Training loss = 1.7117  Validation loss = 0.4445  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 15  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 1.5699  Validation loss = 2.4713  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 1.5702  Validation loss = 2.6203  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 1.6025  Validation loss = 2.3626  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 1.5633  Validation loss = 2.4618  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 1.5651  Validation loss = 2.4564  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 1.5680  Validation loss = 2.6173  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 1.5726  Validation loss = 2.3851  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 1.5680  Validation loss = 2.6351  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 1.5689  Validation loss = 2.6635  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 1.5613  Validation loss = 2.6684  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 1.5577  Validation loss = 2.4185  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 1.5486  Validation loss = 2.5127  \n",
      "\n",
      "Fold: 32  Epoch: 13  Training loss = 1.5566  Validation loss = 2.5901  \n",
      "\n",
      "Fold: 32  Epoch: 14  Training loss = 1.5522  Validation loss = 2.4820  \n",
      "\n",
      "Fold: 32  Epoch: 15  Training loss = 1.5494  Validation loss = 2.5321  \n",
      "\n",
      "Fold: 32  Epoch: 16  Training loss = 1.5621  Validation loss = 2.4722  \n",
      "\n",
      "Fold: 32  Epoch: 17  Training loss = 1.5748  Validation loss = 2.4217  \n",
      "\n",
      "Fold: 32  Epoch: 18  Training loss = 1.5405  Validation loss = 2.4613  \n",
      "\n",
      "Fold: 32  Epoch: 19  Training loss = 1.5470  Validation loss = 2.4344  \n",
      "\n",
      "Fold: 32  Epoch: 20  Training loss = 1.5513  Validation loss = 2.6266  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 3  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    # Load model with lowest validation error for each fold\n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 49\n",
      "Average validation error: 2.81113\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 1.4898  Test loss = 2.9536  \n",
      "\n",
      "Epoch: 2  Training loss = 1.4806  Test loss = 2.9275  \n",
      "\n",
      "Epoch: 3  Training loss = 1.4753  Test loss = 2.9075  \n",
      "\n",
      "Epoch: 4  Training loss = 1.4721  Test loss = 2.8919  \n",
      "\n",
      "Epoch: 5  Training loss = 1.4700  Test loss = 2.8799  \n",
      "\n",
      "Epoch: 6  Training loss = 1.4686  Test loss = 2.8708  \n",
      "\n",
      "Epoch: 7  Training loss = 1.4674  Test loss = 2.8639  \n",
      "\n",
      "Epoch: 8  Training loss = 1.4665  Test loss = 2.8586  \n",
      "\n",
      "Epoch: 9  Training loss = 1.4656  Test loss = 2.8545  \n",
      "\n",
      "Epoch: 10  Training loss = 1.4648  Test loss = 2.8512  \n",
      "\n",
      "Epoch: 11  Training loss = 1.4641  Test loss = 2.8486  \n",
      "\n",
      "Epoch: 12  Training loss = 1.4634  Test loss = 2.8463  \n",
      "\n",
      "Epoch: 13  Training loss = 1.4627  Test loss = 2.8443  \n",
      "\n",
      "Epoch: 14  Training loss = 1.4621  Test loss = 2.8426  \n",
      "\n",
      "Epoch: 15  Training loss = 1.4615  Test loss = 2.8410  \n",
      "\n",
      "Epoch: 16  Training loss = 1.4609  Test loss = 2.8395  \n",
      "\n",
      "Epoch: 17  Training loss = 1.4603  Test loss = 2.8382  \n",
      "\n",
      "Epoch: 18  Training loss = 1.4597  Test loss = 2.8369  \n",
      "\n",
      "Epoch: 19  Training loss = 1.4592  Test loss = 2.8357  \n",
      "\n",
      "Epoch: 20  Training loss = 1.4587  Test loss = 2.8346  \n",
      "\n",
      "Epoch: 21  Training loss = 1.4582  Test loss = 2.8336  \n",
      "\n",
      "Epoch: 22  Training loss = 1.4576  Test loss = 2.8326  \n",
      "\n",
      "Epoch: 23  Training loss = 1.4572  Test loss = 2.8316  \n",
      "\n",
      "Epoch: 24  Training loss = 1.4567  Test loss = 2.8307  \n",
      "\n",
      "Epoch: 25  Training loss = 1.4562  Test loss = 2.8299  \n",
      "\n",
      "Epoch: 26  Training loss = 1.4557  Test loss = 2.8290  \n",
      "\n",
      "Epoch: 27  Training loss = 1.4553  Test loss = 2.8283  \n",
      "\n",
      "Epoch: 28  Training loss = 1.4548  Test loss = 2.8275  \n",
      "\n",
      "Epoch: 29  Training loss = 1.4544  Test loss = 2.8268  \n",
      "\n",
      "Epoch: 30  Training loss = 1.4539  Test loss = 2.8261  \n",
      "\n",
      "Epoch: 31  Training loss = 1.4535  Test loss = 2.8254  \n",
      "\n",
      "Epoch: 32  Training loss = 1.4531  Test loss = 2.8247  \n",
      "\n",
      "Epoch: 33  Training loss = 1.4527  Test loss = 2.8241  \n",
      "\n",
      "Epoch: 34  Training loss = 1.4523  Test loss = 2.8234  \n",
      "\n",
      "Epoch: 35  Training loss = 1.4518  Test loss = 2.8228  \n",
      "\n",
      "Epoch: 36  Training loss = 1.4514  Test loss = 2.8222  \n",
      "\n",
      "Epoch: 37  Training loss = 1.4510  Test loss = 2.8216  \n",
      "\n",
      "Epoch: 38  Training loss = 1.4507  Test loss = 2.8210  \n",
      "\n",
      "Epoch: 39  Training loss = 1.4503  Test loss = 2.8204  \n",
      "\n",
      "Epoch: 40  Training loss = 1.4499  Test loss = 2.8198  \n",
      "\n",
      "Epoch: 41  Training loss = 1.4495  Test loss = 2.8193  \n",
      "\n",
      "Epoch: 42  Training loss = 1.4491  Test loss = 2.8187  \n",
      "\n",
      "Epoch: 43  Training loss = 1.4488  Test loss = 2.8181  \n",
      "\n",
      "Epoch: 44  Training loss = 1.4484  Test loss = 2.8176  \n",
      "\n",
      "Epoch: 45  Training loss = 1.4480  Test loss = 2.8170  \n",
      "\n",
      "Epoch: 46  Training loss = 1.4477  Test loss = 2.8165  \n",
      "\n",
      "Epoch: 47  Training loss = 1.4473  Test loss = 2.8159  \n",
      "\n",
      "Epoch: 48  Training loss = 1.4469  Test loss = 2.8154  \n",
      "\n",
      "Epoch: 49  Training loss = 1.4466  Test loss = 2.8149  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc1d8080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc8cef98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 2.81487107612 \n",
      "Fixed scheme MAE:  1.92286175986\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 1.4466  Test loss = 3.1509  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 1.4985  Test loss = 2.0978  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 1.5080  Test loss = 1.5059  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 1.4969  Test loss = 0.9661  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 1.3159  Test loss = 1.4143  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 1.2877  Test loss = 0.2856  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 1.2447  Test loss = 0.5741  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 1.2407  Test loss = 1.7562  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 1.1798  Test loss = 2.3756  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 1.2096  Test loss = 1.1447  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 1.1787  Test loss = 1.0318  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 1.1840  Test loss = 1.6989  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 1.1195  Test loss = 1.2464  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 1.1290  Test loss = 2.6435  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 1.1737  Test loss = 3.9727  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 1.2714  Test loss = 6.2029  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 1.3234  Test loss = 2.9735  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 1.3737  Test loss = 0.8873  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 1.3768  Test loss = 1.1023  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 1.3167  Test loss = 0.8147  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 1.2001  Test loss = 2.3062  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 1.2310  Test loss = 3.3758  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 1.2882  Test loss = 0.7297  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 1.2876  Test loss = 0.1983  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 1.2266  Test loss = 0.9269  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 1.2310  Test loss = 1.5819  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 1.2463  Test loss = 0.1628  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 1.2327  Test loss = 2.1684  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 1.1787  Test loss = 0.2289  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 1.1689  Test loss = 0.1718  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 1.1630  Test loss = 3.6966  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 1.2253  Test loss = 1.2331  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 1.1806  Test loss = 0.4196  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 1.1761  Test loss = 1.1083  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 1.1579  Test loss = 1.1695  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 1.1663  Test loss = 3.7899  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 1.1800  Test loss = 1.4774  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 1.1573  Test loss = 1.0974  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 1.1650  Test loss = 0.2076  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 1.1620  Test loss = 3.1447  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 1.1929  Test loss = 1.7958  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 1.2130  Test loss = 2.0154  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 1.2385  Test loss = 3.5977  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 1.3164  Test loss = 12.6524  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 2.0288  Test loss = 6.3630  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.1767  Test loss = 1.8410  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.1886  Test loss = 0.3742  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.1890  Test loss = 0.0151  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 1.9055  Test loss = 3.0417  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 1.9401  Test loss = 2.8700  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 1.9715  Test loss = 0.8211  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 1.9730  Test loss = 1.2043  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 1.8835  Test loss = 0.7427  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 1.8856  Test loss = 2.4824  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 1.9104  Test loss = 0.4805  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 1.9090  Test loss = 0.2295  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 1.8358  Test loss = 1.1753  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 1.8416  Test loss = 0.2727  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 1.8416  Test loss = 1.2022  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 1.8462  Test loss = 0.4240  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.8045  Test loss = 1.4111  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.8113  Test loss = 3.0496  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 1.8483  Test loss = 0.4391  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 1.8482  Test loss = 1.3968  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 1.8058  Test loss = 0.3423  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 1.8042  Test loss = 0.1990  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.7897  Test loss = 0.7452  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 1.7865  Test loss = 2.8993  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 1.7891  Test loss = 3.5982  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 1.8420  Test loss = 0.8627  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 1.8402  Test loss = 0.1184  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 1.8389  Test loss = 2.2108  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 1.8068  Test loss = 2.4891  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 1.8326  Test loss = 0.5579  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 1.8231  Test loss = 0.6096  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 1.8175  Test loss = 0.7939  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 1.7566  Test loss = 0.6652  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd23ef28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.50313163941 \n",
      "Updating scheme MAE:  1.72765486479\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_test_fixed.csv\", yhat_test_fixed)\n",
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_update.csv\", yhat_update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
