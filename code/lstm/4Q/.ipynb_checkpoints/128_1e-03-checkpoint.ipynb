{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"4Q/128_units/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 500\n",
    "learning_rate = 1e-3\n",
    "batch_size = 5\n",
    "early_stop_iters = 10\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 128 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag4',\n",
    "                                       'inflation.lag5',\n",
    "                                       'inflation.lag6',\n",
    "                                       'inflation.lag7']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag4',\n",
    "                                   'unemp.lag5',\n",
    "                                   'unemp.lag6',\n",
    "                                   'unemp.lag7']])\n",
    "train_4lag_oil = np.array(train[['oil.lag4',\n",
    "                                 'oil.lag5',\n",
    "                                 'oil.lag6',\n",
    "                                 'oil.lag7']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag4',\n",
    "                                     'inflation.lag5',\n",
    "                                     'inflation.lag6',\n",
    "                                     'inflation.lag7']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag4',\n",
    "                                 'unemp.lag5',\n",
    "                                 'unemp.lag6',\n",
    "                                 'unemp.lag7']])\n",
    "test_4lag_oil = np.array(test[['oil.lag4',\n",
    "                               'oil.lag5',\n",
    "                               'oil.lag6',\n",
    "                               'oil.lag7']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 128 \n",
      "Learning rate = 0.001 \n",
      "Epochs = 500 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 10 \n",
      "Learning rate = 0.001\n",
      "Fold: 1  Epoch: 1  Training loss = 3.1588  Validation loss = 3.1849  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 2.8159  Validation loss = 2.5218  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 2.7222  Validation loss = 2.2797  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 2.6631  Validation loss = 2.0933  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 2.6145  Validation loss = 1.8996  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 2.5889  Validation loss = 1.8087  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 2.5646  Validation loss = 1.6865  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 2.5463  Validation loss = 1.5760  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 2.5433  Validation loss = 1.5464  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 2.5364  Validation loss = 1.4677  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 2.5328  Validation loss = 1.5392  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 2.5227  Validation loss = 1.4915  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 2.5198  Validation loss = 1.5753  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 2.5124  Validation loss = 1.4929  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 2.5113  Validation loss = 1.3502  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 2.5061  Validation loss = 1.4473  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 2.4991  Validation loss = 1.5536  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 2.4946  Validation loss = 1.4733  \n",
      "\n",
      "Fold: 1  Epoch: 19  Training loss = 2.4908  Validation loss = 1.4560  \n",
      "\n",
      "Fold: 1  Epoch: 20  Training loss = 2.4908  Validation loss = 1.6657  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 15  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.4124  Validation loss = 2.0886  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.4184  Validation loss = 1.9553  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.4112  Validation loss = 2.0000  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.4069  Validation loss = 2.0597  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.4035  Validation loss = 2.0467  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.4022  Validation loss = 2.0261  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.3977  Validation loss = 2.0547  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.3949  Validation loss = 2.1350  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.3964  Validation loss = 2.0328  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.3920  Validation loss = 2.1194  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.3913  Validation loss = 2.0399  \n",
      "\n",
      "Fold: 2  Epoch: 12  Training loss = 2.3937  Validation loss = 2.0014  \n",
      "\n",
      "Fold: 2  Epoch: 13  Training loss = 2.3837  Validation loss = 2.0877  \n",
      "\n",
      "Fold: 2  Epoch: 14  Training loss = 2.3799  Validation loss = 2.1088  \n",
      "\n",
      "Fold: 2  Epoch: 15  Training loss = 2.3776  Validation loss = 2.1532  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 2  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.5395  Validation loss = 3.3469  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.5258  Validation loss = 3.4204  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.5187  Validation loss = 3.5018  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.5122  Validation loss = 3.4943  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.5060  Validation loss = 3.4498  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.4979  Validation loss = 3.4047  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.4967  Validation loss = 3.2497  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.4958  Validation loss = 3.1939  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.4848  Validation loss = 3.2699  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.4806  Validation loss = 3.2770  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.4776  Validation loss = 3.2814  \n",
      "\n",
      "Fold: 3  Epoch: 12  Training loss = 1.4772  Validation loss = 3.2506  \n",
      "\n",
      "Fold: 3  Epoch: 13  Training loss = 1.4725  Validation loss = 3.2622  \n",
      "\n",
      "Fold: 3  Epoch: 14  Training loss = 1.4727  Validation loss = 3.1474  \n",
      "\n",
      "Fold: 3  Epoch: 15  Training loss = 1.4686  Validation loss = 3.1627  \n",
      "\n",
      "Fold: 3  Epoch: 16  Training loss = 1.4657  Validation loss = 3.1827  \n",
      "\n",
      "Fold: 3  Epoch: 17  Training loss = 1.4605  Validation loss = 3.2359  \n",
      "\n",
      "Fold: 3  Epoch: 18  Training loss = 1.4583  Validation loss = 3.2300  \n",
      "\n",
      "Fold: 3  Epoch: 19  Training loss = 1.4572  Validation loss = 3.2484  \n",
      "\n",
      "Fold: 3  Epoch: 20  Training loss = 1.4556  Validation loss = 3.2517  \n",
      "\n",
      "Fold: 3  Epoch: 21  Training loss = 1.4593  Validation loss = 3.3613  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 14  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.5690  Validation loss = 4.5024  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.5673  Validation loss = 4.4971  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.5598  Validation loss = 4.4294  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.5650  Validation loss = 4.4825  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.5671  Validation loss = 4.5053  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.5485  Validation loss = 4.3476  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.5414  Validation loss = 4.1825  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.5398  Validation loss = 4.1974  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.5363  Validation loss = 4.1348  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 1.5316  Validation loss = 4.2186  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.5296  Validation loss = 4.1338  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 1.5318  Validation loss = 3.9984  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.5235  Validation loss = 4.0811  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 1.5185  Validation loss = 4.1540  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.5138  Validation loss = 4.0605  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 1.5117  Validation loss = 4.0569  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 1.5090  Validation loss = 4.0107  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 1.5138  Validation loss = 3.8543  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 1.5091  Validation loss = 3.8596  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 1.5034  Validation loss = 3.9159  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 1.4983  Validation loss = 3.9425  \n",
      "\n",
      "Fold: 4  Epoch: 22  Training loss = 1.4959  Validation loss = 3.9541  \n",
      "\n",
      "Fold: 4  Epoch: 23  Training loss = 1.4950  Validation loss = 3.9322  \n",
      "\n",
      "Fold: 4  Epoch: 24  Training loss = 1.4942  Validation loss = 3.8909  \n",
      "\n",
      "Fold: 4  Epoch: 25  Training loss = 1.4961  Validation loss = 3.7955  \n",
      "\n",
      "Fold: 4  Epoch: 26  Training loss = 1.4881  Validation loss = 3.9629  \n",
      "\n",
      "Fold: 4  Epoch: 27  Training loss = 1.4847  Validation loss = 3.8451  \n",
      "\n",
      "Fold: 4  Epoch: 28  Training loss = 1.4824  Validation loss = 3.8291  \n",
      "\n",
      "Fold: 4  Epoch: 29  Training loss = 1.4879  Validation loss = 3.7422  \n",
      "\n",
      "Fold: 4  Epoch: 30  Training loss = 1.4790  Validation loss = 3.7768  \n",
      "\n",
      "Fold: 4  Epoch: 31  Training loss = 1.4750  Validation loss = 3.7897  \n",
      "\n",
      "Fold: 4  Epoch: 32  Training loss = 1.4712  Validation loss = 3.8249  \n",
      "\n",
      "Fold: 4  Epoch: 33  Training loss = 1.4696  Validation loss = 3.8700  \n",
      "\n",
      "Fold: 4  Epoch: 34  Training loss = 1.4679  Validation loss = 3.8474  \n",
      "\n",
      "Fold: 4  Epoch: 35  Training loss = 1.4643  Validation loss = 3.7816  \n",
      "\n",
      "Fold: 4  Epoch: 36  Training loss = 1.4628  Validation loss = 3.8077  \n",
      "\n",
      "Fold: 4  Epoch: 37  Training loss = 1.4608  Validation loss = 3.7301  \n",
      "\n",
      "Fold: 4  Epoch: 38  Training loss = 1.4589  Validation loss = 3.7744  \n",
      "\n",
      "Fold: 4  Epoch: 39  Training loss = 1.4568  Validation loss = 3.7178  \n",
      "\n",
      "Fold: 4  Epoch: 40  Training loss = 1.4540  Validation loss = 3.6988  \n",
      "\n",
      "Fold: 4  Epoch: 41  Training loss = 1.4539  Validation loss = 3.6417  \n",
      "\n",
      "Fold: 4  Epoch: 42  Training loss = 1.4504  Validation loss = 3.7009  \n",
      "\n",
      "Fold: 4  Epoch: 43  Training loss = 1.4495  Validation loss = 3.7680  \n",
      "\n",
      "Fold: 4  Epoch: 44  Training loss = 1.4478  Validation loss = 3.7590  \n",
      "\n",
      "Fold: 4  Epoch: 45  Training loss = 1.4461  Validation loss = 3.7380  \n",
      "\n",
      "Fold: 4  Epoch: 46  Training loss = 1.4453  Validation loss = 3.6406  \n",
      "\n",
      "Fold: 4  Epoch: 47  Training loss = 1.4473  Validation loss = 3.5616  \n",
      "\n",
      "Fold: 4  Epoch: 48  Training loss = 1.4444  Validation loss = 3.5535  \n",
      "\n",
      "Fold: 4  Epoch: 49  Training loss = 1.4404  Validation loss = 3.5818  \n",
      "\n",
      "Fold: 4  Epoch: 50  Training loss = 1.4398  Validation loss = 3.5454  \n",
      "\n",
      "Fold: 4  Epoch: 51  Training loss = 1.4373  Validation loss = 3.5372  \n",
      "\n",
      "Fold: 4  Epoch: 52  Training loss = 1.4358  Validation loss = 3.4856  \n",
      "\n",
      "Fold: 4  Epoch: 53  Training loss = 1.4275  Validation loss = 3.5718  \n",
      "\n",
      "Fold: 4  Epoch: 54  Training loss = 1.4250  Validation loss = 3.5805  \n",
      "\n",
      "Fold: 4  Epoch: 55  Training loss = 1.4279  Validation loss = 3.6556  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 52  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.6048  Validation loss = 3.7431  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.6041  Validation loss = 3.7526  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.5637  Validation loss = 3.5555  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.5548  Validation loss = 3.5324  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.5499  Validation loss = 3.5481  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 1.5415  Validation loss = 3.4691  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 1.5303  Validation loss = 3.4760  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.5228  Validation loss = 3.4870  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 1.5049  Validation loss = 3.3376  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.4983  Validation loss = 3.3058  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 1.4922  Validation loss = 3.1739  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 1.4819  Validation loss = 3.2185  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 1.4792  Validation loss = 3.2738  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.4711  Validation loss = 3.3056  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 1.4598  Validation loss = 3.2241  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 1.4556  Validation loss = 3.2571  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 1.4482  Validation loss = 3.2371  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 1.4446  Validation loss = 3.3029  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 1.4233  Validation loss = 3.1596  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 1.4188  Validation loss = 3.1237  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 1.4104  Validation loss = 3.0624  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 1.4093  Validation loss = 3.0523  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 1.3986  Validation loss = 2.9835  \n",
      "\n",
      "Fold: 5  Epoch: 24  Training loss = 1.3939  Validation loss = 3.0227  \n",
      "\n",
      "Fold: 5  Epoch: 25  Training loss = 1.3855  Validation loss = 2.9105  \n",
      "\n",
      "Fold: 5  Epoch: 26  Training loss = 1.3823  Validation loss = 2.9846  \n",
      "\n",
      "Fold: 5  Epoch: 27  Training loss = 1.3808  Validation loss = 3.0056  \n",
      "\n",
      "Fold: 5  Epoch: 28  Training loss = 1.3750  Validation loss = 2.9694  \n",
      "\n",
      "Fold: 5  Epoch: 29  Training loss = 1.3635  Validation loss = 2.8279  \n",
      "\n",
      "Fold: 5  Epoch: 30  Training loss = 1.3560  Validation loss = 2.7496  \n",
      "\n",
      "Fold: 5  Epoch: 31  Training loss = 1.3486  Validation loss = 2.7698  \n",
      "\n",
      "Fold: 5  Epoch: 32  Training loss = 1.3433  Validation loss = 2.6921  \n",
      "\n",
      "Fold: 5  Epoch: 33  Training loss = 1.3356  Validation loss = 2.7586  \n",
      "\n",
      "Fold: 5  Epoch: 34  Training loss = 1.3283  Validation loss = 2.6847  \n",
      "\n",
      "Fold: 5  Epoch: 35  Training loss = 1.3250  Validation loss = 2.7482  \n",
      "\n",
      "Fold: 5  Epoch: 36  Training loss = 1.3210  Validation loss = 2.7257  \n",
      "\n",
      "Fold: 5  Epoch: 37  Training loss = 1.3174  Validation loss = 2.7295  \n",
      "\n",
      "Fold: 5  Epoch: 38  Training loss = 1.3188  Validation loss = 2.7724  \n",
      "\n",
      "Fold: 5  Epoch: 39  Training loss = 1.3108  Validation loss = 2.7303  \n",
      "\n",
      "Fold: 5  Epoch: 40  Training loss = 1.3130  Validation loss = 2.6117  \n",
      "\n",
      "Fold: 5  Epoch: 41  Training loss = 1.3008  Validation loss = 2.5810  \n",
      "\n",
      "Fold: 5  Epoch: 42  Training loss = 1.3012  Validation loss = 2.6823  \n",
      "\n",
      "Fold: 5  Epoch: 43  Training loss = 1.2967  Validation loss = 2.6840  \n",
      "\n",
      "Fold: 5  Epoch: 44  Training loss = 1.2899  Validation loss = 2.5988  \n",
      "\n",
      "Fold: 5  Epoch: 45  Training loss = 1.2886  Validation loss = 2.5152  \n",
      "\n",
      "Fold: 5  Epoch: 46  Training loss = 1.2814  Validation loss = 2.5526  \n",
      "\n",
      "Fold: 5  Epoch: 47  Training loss = 1.2908  Validation loss = 2.5196  \n",
      "\n",
      "Fold: 5  Epoch: 48  Training loss = 1.2977  Validation loss = 2.4914  \n",
      "\n",
      "Fold: 5  Epoch: 49  Training loss = 1.2941  Validation loss = 2.4543  \n",
      "\n",
      "Fold: 5  Epoch: 50  Training loss = 1.2745  Validation loss = 2.4934  \n",
      "\n",
      "Fold: 5  Epoch: 51  Training loss = 1.2706  Validation loss = 2.6503  \n",
      "\n",
      "Fold: 5  Epoch: 52  Training loss = 1.2606  Validation loss = 2.5445  \n",
      "\n",
      "Fold: 5  Epoch: 53  Training loss = 1.2578  Validation loss = 2.4583  \n",
      "\n",
      "Fold: 5  Epoch: 54  Training loss = 1.2564  Validation loss = 2.4351  \n",
      "\n",
      "Fold: 5  Epoch: 55  Training loss = 1.2522  Validation loss = 2.3249  \n",
      "\n",
      "Fold: 5  Epoch: 56  Training loss = 1.2479  Validation loss = 2.3022  \n",
      "\n",
      "Fold: 5  Epoch: 57  Training loss = 1.2535  Validation loss = 2.2855  \n",
      "\n",
      "Fold: 5  Epoch: 58  Training loss = 1.2423  Validation loss = 2.2959  \n",
      "\n",
      "Fold: 5  Epoch: 59  Training loss = 1.2412  Validation loss = 2.4186  \n",
      "\n",
      "Fold: 5  Epoch: 60  Training loss = 1.2358  Validation loss = 2.3678  \n",
      "\n",
      "Fold: 5  Epoch: 61  Training loss = 1.2393  Validation loss = 2.4896  \n",
      "\n",
      "Fold: 5  Epoch: 62  Training loss = 1.2309  Validation loss = 2.3980  \n",
      "\n",
      "Fold: 5  Epoch: 63  Training loss = 1.2274  Validation loss = 2.3884  \n",
      "\n",
      "Fold: 5  Epoch: 64  Training loss = 1.2267  Validation loss = 2.3420  \n",
      "\n",
      "Fold: 5  Epoch: 65  Training loss = 1.2275  Validation loss = 2.2716  \n",
      "\n",
      "Fold: 5  Epoch: 66  Training loss = 1.2226  Validation loss = 2.3471  \n",
      "\n",
      "Fold: 5  Epoch: 67  Training loss = 1.2200  Validation loss = 2.2443  \n",
      "\n",
      "Fold: 5  Epoch: 68  Training loss = 1.2166  Validation loss = 2.2424  \n",
      "\n",
      "Fold: 5  Epoch: 69  Training loss = 1.2152  Validation loss = 2.3163  \n",
      "\n",
      "Fold: 5  Epoch: 70  Training loss = 1.2126  Validation loss = 2.3206  \n",
      "\n",
      "Fold: 5  Epoch: 71  Training loss = 1.2141  Validation loss = 2.4058  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 68  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 1.2656  Validation loss = 1.0263  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 1.2529  Validation loss = 1.0442  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 1.2406  Validation loss = 1.0870  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 1.2243  Validation loss = 1.1807  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 1.2236  Validation loss = 1.1834  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 1.2178  Validation loss = 1.2602  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 1.2151  Validation loss = 1.1406  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 1.2092  Validation loss = 1.1603  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 1.2074  Validation loss = 1.2302  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 1.2069  Validation loss = 1.0899  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 1.1920  Validation loss = 1.2706  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 1  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 1.2054  Validation loss = 1.2972  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 1.2041  Validation loss = 1.3228  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 1.2016  Validation loss = 1.3574  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.1968  Validation loss = 1.3418  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 1.1928  Validation loss = 1.3642  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 1.1914  Validation loss = 1.4003  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 1.1876  Validation loss = 1.3561  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 1.1841  Validation loss = 1.3768  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 1.1803  Validation loss = 1.3926  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 1.1784  Validation loss = 1.4355  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 1.1749  Validation loss = 1.3964  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 1.1722  Validation loss = 1.4053  \n",
      "\n",
      "Fold: 7  Epoch: 13  Training loss = 1.1697  Validation loss = 1.4533  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 1  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 1.1494  Validation loss = 6.3993  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 1.1463  Validation loss = 6.4207  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 1.1447  Validation loss = 6.4398  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 1.1481  Validation loss = 6.3996  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 1.1423  Validation loss = 6.5152  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 1.1387  Validation loss = 6.4961  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 1.1338  Validation loss = 6.3241  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 1.1329  Validation loss = 6.3275  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 1.1602  Validation loss = 6.6030  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 1.1320  Validation loss = 6.5329  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 1.1362  Validation loss = 6.3318  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 1.1289  Validation loss = 6.3256  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 1.1243  Validation loss = 6.4756  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 1.1279  Validation loss = 6.5733  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 1.1366  Validation loss = 6.2939  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 1.1184  Validation loss = 6.4162  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 1.1163  Validation loss = 6.4593  \n",
      "\n",
      "Fold: 8  Epoch: 18  Training loss = 1.1177  Validation loss = 6.4756  \n",
      "\n",
      "Fold: 8  Epoch: 19  Training loss = 1.1214  Validation loss = 6.3864  \n",
      "\n",
      "Fold: 8  Epoch: 20  Training loss = 1.1098  Validation loss = 6.4336  \n",
      "\n",
      "Fold: 8  Epoch: 21  Training loss = 1.1197  Validation loss = 6.5649  \n",
      "\n",
      "Fold: 8  Epoch: 22  Training loss = 1.1254  Validation loss = 6.6531  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 15  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 1.8963  Validation loss = 8.1267  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 1.9239  Validation loss = 8.3317  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 1.8659  Validation loss = 8.1750  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 1.8435  Validation loss = 7.7949  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 1.8377  Validation loss = 7.8672  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 1.8528  Validation loss = 8.2673  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 1.8232  Validation loss = 7.7112  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 1.8148  Validation loss = 7.8955  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 1.8073  Validation loss = 7.7194  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 1.8038  Validation loss = 7.6730  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 1.7986  Validation loss = 7.6631  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 1.7923  Validation loss = 7.8855  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 1.7841  Validation loss = 7.8861  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 1.7889  Validation loss = 7.9924  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 1.7708  Validation loss = 7.6885  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 1.7645  Validation loss = 7.7976  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 1.7591  Validation loss = 7.9281  \n",
      "\n",
      "Fold: 9  Epoch: 18  Training loss = 1.7587  Validation loss = 8.0437  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 11  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 2.4787  Validation loss = 5.5759  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 2.4233  Validation loss = 5.4913  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 2.3639  Validation loss = 5.1549  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 2.3425  Validation loss = 5.3143  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 2.3112  Validation loss = 5.2594  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 2.2673  Validation loss = 4.7056  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 2.2654  Validation loss = 4.7015  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 2.2451  Validation loss = 4.8429  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 2.2525  Validation loss = 4.8857  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 2.2591  Validation loss = 5.0985  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 2.2531  Validation loss = 5.1523  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 2.2376  Validation loss = 4.5639  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 2.2204  Validation loss = 4.4514  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 2.1750  Validation loss = 4.2685  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 2.1794  Validation loss = 4.1011  \n",
      "\n",
      "Fold: 10  Epoch: 16  Training loss = 2.1654  Validation loss = 4.1470  \n",
      "\n",
      "Fold: 10  Epoch: 17  Training loss = 2.1549  Validation loss = 4.0819  \n",
      "\n",
      "Fold: 10  Epoch: 18  Training loss = 2.1751  Validation loss = 4.1329  \n",
      "\n",
      "Fold: 10  Epoch: 19  Training loss = 2.1459  Validation loss = 4.3333  \n",
      "\n",
      "Fold: 10  Epoch: 20  Training loss = 2.1374  Validation loss = 4.1314  \n",
      "\n",
      "Fold: 10  Epoch: 21  Training loss = 2.1511  Validation loss = 4.4600  \n",
      "\n",
      "Fold: 10  Epoch: 22  Training loss = 2.1148  Validation loss = 4.0506  \n",
      "\n",
      "Fold: 10  Epoch: 23  Training loss = 2.1055  Validation loss = 3.9242  \n",
      "\n",
      "Fold: 10  Epoch: 24  Training loss = 2.1265  Validation loss = 4.3017  \n",
      "\n",
      "Fold: 10  Epoch: 25  Training loss = 2.1130  Validation loss = 3.8646  \n",
      "\n",
      "Fold: 10  Epoch: 26  Training loss = 2.0849  Validation loss = 3.7282  \n",
      "\n",
      "Fold: 10  Epoch: 27  Training loss = 2.0936  Validation loss = 3.8508  \n",
      "\n",
      "Fold: 10  Epoch: 28  Training loss = 2.1441  Validation loss = 4.1232  \n",
      "\n",
      "Fold: 10  Epoch: 29  Training loss = 2.0997  Validation loss = 3.7769  \n",
      "\n",
      "Fold: 10  Epoch: 30  Training loss = 2.0694  Validation loss = 3.4379  \n",
      "\n",
      "Fold: 10  Epoch: 31  Training loss = 2.0561  Validation loss = 3.0358  \n",
      "\n",
      "Fold: 10  Epoch: 32  Training loss = 2.0415  Validation loss = 3.3812  \n",
      "\n",
      "Fold: 10  Epoch: 33  Training loss = 2.0325  Validation loss = 3.2543  \n",
      "\n",
      "Fold: 10  Epoch: 34  Training loss = 2.0313  Validation loss = 3.4314  \n",
      "\n",
      "Fold: 10  Epoch: 35  Training loss = 2.0286  Validation loss = 3.3585  \n",
      "\n",
      "Fold: 10  Epoch: 36  Training loss = 2.0056  Validation loss = 3.2567  \n",
      "\n",
      "Fold: 10  Epoch: 37  Training loss = 2.0543  Validation loss = 3.3472  \n",
      "\n",
      "Fold: 10  Epoch: 38  Training loss = 2.0178  Validation loss = 3.2469  \n",
      "\n",
      "Fold: 10  Epoch: 39  Training loss = 2.0307  Validation loss = 3.3978  \n",
      "\n",
      "Fold: 10  Epoch: 40  Training loss = 2.0166  Validation loss = 3.1289  \n",
      "\n",
      "Fold: 10  Epoch: 41  Training loss = 1.9869  Validation loss = 3.2882  \n",
      "\n",
      "Fold: 10  Epoch: 42  Training loss = 1.9777  Validation loss = 3.6071  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 31  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 2.0504  Validation loss = 3.0846  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 2.0427  Validation loss = 2.8731  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 1.9751  Validation loss = 2.8455  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 1.9851  Validation loss = 2.8474  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 1.9456  Validation loss = 2.9747  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 1.9494  Validation loss = 2.7034  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 1.9347  Validation loss = 3.1868  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 1.9195  Validation loss = 2.8028  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 1.9177  Validation loss = 2.8504  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 1.9078  Validation loss = 2.7929  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 1.8841  Validation loss = 3.0045  \n",
      "\n",
      "Fold: 11  Epoch: 12  Training loss = 1.9477  Validation loss = 2.5858  \n",
      "\n",
      "Fold: 11  Epoch: 13  Training loss = 1.8968  Validation loss = 2.4336  \n",
      "\n",
      "Fold: 11  Epoch: 14  Training loss = 1.9322  Validation loss = 2.6932  \n",
      "\n",
      "Fold: 11  Epoch: 15  Training loss = 1.8759  Validation loss = 3.0097  \n",
      "\n",
      "Fold: 11  Epoch: 16  Training loss = 1.9154  Validation loss = 3.1541  \n",
      "\n",
      "Fold: 11  Epoch: 17  Training loss = 1.8493  Validation loss = 2.7324  \n",
      "\n",
      "Fold: 11  Epoch: 18  Training loss = 1.8430  Validation loss = 2.9871  \n",
      "\n",
      "Fold: 11  Epoch: 19  Training loss = 1.8589  Validation loss = 2.6978  \n",
      "\n",
      "Fold: 11  Epoch: 20  Training loss = 1.8263  Validation loss = 3.2671  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 13  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 1.9711  Validation loss = 1.5788  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 1.9261  Validation loss = 1.2564  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 1.9154  Validation loss = 1.2826  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 1.9034  Validation loss = 1.6489  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 1.8875  Validation loss = 0.9204  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 1.8870  Validation loss = 1.2059  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 1.9215  Validation loss = 1.5138  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 1.8668  Validation loss = 1.0704  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 1.8663  Validation loss = 1.2161  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 1.8760  Validation loss = 0.9270  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 1.8757  Validation loss = 1.1285  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 1.8755  Validation loss = 1.3684  \n",
      "\n",
      "Fold: 12  Epoch: 13  Training loss = 1.8661  Validation loss = 1.6686  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 5  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 1.8285  Validation loss = 3.1949  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 1.8232  Validation loss = 2.8389  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 1.7906  Validation loss = 3.2589  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 1.7757  Validation loss = 3.2911  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 1.7829  Validation loss = 3.5078  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 1.7717  Validation loss = 3.7035  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 1.7522  Validation loss = 3.5924  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 1.8118  Validation loss = 3.0496  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 1.7650  Validation loss = 3.1360  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 1.7470  Validation loss = 3.4390  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 1.7729  Validation loss = 3.7734  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 2  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 1.8654  Validation loss = 6.2730  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 1.8786  Validation loss = 6.3328  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 1.8470  Validation loss = 6.2458  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 1.8665  Validation loss = 5.9020  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 1.8419  Validation loss = 6.5383  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 1.8438  Validation loss = 6.0223  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 1.8526  Validation loss = 6.0176  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 1.8188  Validation loss = 5.8869  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 1.7674  Validation loss = 6.2153  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 1.7986  Validation loss = 6.2570  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 1.7835  Validation loss = 6.1426  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 1.7748  Validation loss = 6.4408  \n",
      "\n",
      "Fold: 14  Epoch: 13  Training loss = 1.7611  Validation loss = 6.0913  \n",
      "\n",
      "Fold: 14  Epoch: 14  Training loss = 1.7961  Validation loss = 6.7202  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 8  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 2.3243  Validation loss = 5.9392  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 2.3257  Validation loss = 6.4604  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 2.2453  Validation loss = 6.3121  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 2.2287  Validation loss = 6.1011  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 2.1902  Validation loss = 5.7093  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 2.2027  Validation loss = 5.3832  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 2.2008  Validation loss = 5.5841  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 2.1979  Validation loss = 5.3077  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 2.2287  Validation loss = 5.1635  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 2.1299  Validation loss = 5.5450  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 2.1198  Validation loss = 5.3497  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 2.1173  Validation loss = 5.3982  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 2.1653  Validation loss = 5.6633  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 2.0877  Validation loss = 5.2701  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 2.1160  Validation loss = 5.5547  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 2.0951  Validation loss = 5.5403  \n",
      "\n",
      "Fold: 15  Epoch: 17  Training loss = 2.0614  Validation loss = 5.1622  \n",
      "\n",
      "Fold: 15  Epoch: 18  Training loss = 2.0320  Validation loss = 5.2688  \n",
      "\n",
      "Fold: 15  Epoch: 19  Training loss = 2.0367  Validation loss = 5.3718  \n",
      "\n",
      "Fold: 15  Epoch: 20  Training loss = 2.0054  Validation loss = 5.7760  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 17  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 2.3916  Validation loss = 5.6687  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 2.3960  Validation loss = 4.9536  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 2.3181  Validation loss = 4.8491  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 2.2997  Validation loss = 5.1175  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 2.2771  Validation loss = 4.8059  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 2.2980  Validation loss = 4.0925  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 2.5411  Validation loss = 3.7332  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 2.2843  Validation loss = 4.5674  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 2.2883  Validation loss = 5.2156  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 2.1761  Validation loss = 4.9964  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 2.2317  Validation loss = 5.0302  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 2.2189  Validation loss = 5.1934  \n",
      "\n",
      "Fold: 16  Epoch: 13  Training loss = 2.1703  Validation loss = 5.1444  \n",
      "\n",
      "Fold: 16  Epoch: 14  Training loss = 2.1383  Validation loss = 4.8580  \n",
      "\n",
      "Fold: 16  Epoch: 15  Training loss = 2.1778  Validation loss = 5.0736  \n",
      "\n",
      "Fold: 16  Epoch: 16  Training loss = 2.3151  Validation loss = 4.5973  \n",
      "\n",
      "Fold: 16  Epoch: 17  Training loss = 2.1497  Validation loss = 4.8265  \n",
      "\n",
      "Fold: 16  Epoch: 18  Training loss = 2.0298  Validation loss = 5.3115  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 7  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 2.4173  Validation loss = 3.8398  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 2.1940  Validation loss = 3.9481  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 2.1228  Validation loss = 4.0592  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 2.5539  Validation loss = 4.2298  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 2.1444  Validation loss = 4.1227  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 2.0299  Validation loss = 4.0651  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 2.1096  Validation loss = 4.1312  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 2.0978  Validation loss = 3.9019  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 2.0036  Validation loss = 3.9573  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 2.0553  Validation loss = 3.8679  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 2.0687  Validation loss = 4.0006  \n",
      "\n",
      "Fold: 17  Epoch: 12  Training loss = 1.9572  Validation loss = 3.8433  \n",
      "\n",
      "Fold: 17  Epoch: 13  Training loss = 1.9566  Validation loss = 3.8957  \n",
      "\n",
      "Fold: 17  Epoch: 14  Training loss = 2.1152  Validation loss = 4.0753  \n",
      "\n",
      "Fold: 17  Epoch: 15  Training loss = 1.9446  Validation loss = 3.9591  \n",
      "\n",
      "Fold: 17  Epoch: 16  Training loss = 1.9342  Validation loss = 4.0708  \n",
      "\n",
      "Fold: 17  Epoch: 17  Training loss = 1.9478  Validation loss = 3.8685  \n",
      "\n",
      "Fold: 17  Epoch: 18  Training loss = 1.9188  Validation loss = 3.9433  \n",
      "\n",
      "Fold: 17  Epoch: 19  Training loss = 1.8519  Validation loss = 3.9579  \n",
      "\n",
      "Fold: 17  Epoch: 20  Training loss = 1.8376  Validation loss = 3.9624  \n",
      "\n",
      "Fold: 17  Epoch: 21  Training loss = 2.0037  Validation loss = 3.8923  \n",
      "\n",
      "Fold: 17  Epoch: 22  Training loss = 2.2215  Validation loss = 3.9547  \n",
      "\n",
      "Fold: 17  Epoch: 23  Training loss = 2.1083  Validation loss = 3.8630  \n",
      "\n",
      "Fold: 17  Epoch: 24  Training loss = 1.8544  Validation loss = 4.0569  \n",
      "\n",
      "Fold: 17  Epoch: 25  Training loss = 1.9400  Validation loss = 3.9956  \n",
      "\n",
      "Fold: 17  Epoch: 26  Training loss = 1.9361  Validation loss = 4.2129  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 1  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 1.9338  Validation loss = 4.3676  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 1.9069  Validation loss = 4.1363  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 1.9343  Validation loss = 3.7437  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 1.8740  Validation loss = 3.8740  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 1.8209  Validation loss = 3.8730  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 1.8449  Validation loss = 3.8664  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 1.8776  Validation loss = 3.8463  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 1.9368  Validation loss = 3.6635  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 1.8294  Validation loss = 3.8064  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 1.8594  Validation loss = 3.8932  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 1.7336  Validation loss = 3.8275  \n",
      "\n",
      "Fold: 18  Epoch: 12  Training loss = 2.1017  Validation loss = 3.7733  \n",
      "\n",
      "Fold: 18  Epoch: 13  Training loss = 1.7804  Validation loss = 3.6319  \n",
      "\n",
      "Fold: 18  Epoch: 14  Training loss = 1.7966  Validation loss = 3.7421  \n",
      "\n",
      "Fold: 18  Epoch: 15  Training loss = 1.7908  Validation loss = 3.6809  \n",
      "\n",
      "Fold: 18  Epoch: 16  Training loss = 1.7960  Validation loss = 3.5721  \n",
      "\n",
      "Fold: 18  Epoch: 17  Training loss = 1.7163  Validation loss = 3.7579  \n",
      "\n",
      "Fold: 18  Epoch: 18  Training loss = 1.7647  Validation loss = 3.5290  \n",
      "\n",
      "Fold: 18  Epoch: 19  Training loss = 1.6220  Validation loss = 3.7983  \n",
      "\n",
      "Fold: 18  Epoch: 20  Training loss = 1.8752  Validation loss = 3.5069  \n",
      "\n",
      "Fold: 18  Epoch: 21  Training loss = 1.6223  Validation loss = 3.4795  \n",
      "\n",
      "Fold: 18  Epoch: 22  Training loss = 1.6219  Validation loss = 3.3833  \n",
      "\n",
      "Fold: 18  Epoch: 23  Training loss = 1.6036  Validation loss = 3.6957  \n",
      "\n",
      "Fold: 18  Epoch: 24  Training loss = 1.5550  Validation loss = 3.4170  \n",
      "\n",
      "Fold: 18  Epoch: 25  Training loss = 1.6732  Validation loss = 3.1202  \n",
      "\n",
      "Fold: 18  Epoch: 26  Training loss = 1.7620  Validation loss = 3.3998  \n",
      "\n",
      "Fold: 18  Epoch: 27  Training loss = 1.7068  Validation loss = 3.2082  \n",
      "\n",
      "Fold: 18  Epoch: 28  Training loss = 1.8076  Validation loss = 3.1885  \n",
      "\n",
      "Fold: 18  Epoch: 29  Training loss = 1.5637  Validation loss = 3.1278  \n",
      "\n",
      "Fold: 18  Epoch: 30  Training loss = 1.7026  Validation loss = 3.1938  \n",
      "\n",
      "Fold: 18  Epoch: 31  Training loss = 1.7234  Validation loss = 3.2924  \n",
      "\n",
      "Fold: 18  Epoch: 32  Training loss = 1.7020  Validation loss = 3.2330  \n",
      "\n",
      "Fold: 18  Epoch: 33  Training loss = 1.8380  Validation loss = 3.2722  \n",
      "\n",
      "Fold: 18  Epoch: 34  Training loss = 1.5650  Validation loss = 3.4154  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 25  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 1.8830  Validation loss = 2.5721  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 1.8741  Validation loss = 2.4191  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 1.5031  Validation loss = 2.4213  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 1.7531  Validation loss = 2.1160  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 1.4759  Validation loss = 2.0973  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 1.4827  Validation loss = 2.3858  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 1.4139  Validation loss = 2.3690  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 1.4052  Validation loss = 2.2968  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 1.4141  Validation loss = 2.3556  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 1.4003  Validation loss = 1.9214  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 1.3995  Validation loss = 1.8849  \n",
      "\n",
      "Fold: 19  Epoch: 12  Training loss = 1.3269  Validation loss = 1.9772  \n",
      "\n",
      "Fold: 19  Epoch: 13  Training loss = 1.3441  Validation loss = 1.8644  \n",
      "\n",
      "Fold: 19  Epoch: 14  Training loss = 1.2717  Validation loss = 2.0190  \n",
      "\n",
      "Fold: 19  Epoch: 15  Training loss = 1.4208  Validation loss = 2.4834  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 13  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 1.4255  Validation loss = 3.4806  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 1.4083  Validation loss = 3.4667  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 1.3155  Validation loss = 3.4358  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 1.4340  Validation loss = 3.8883  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 1.3034  Validation loss = 3.4192  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 1.2770  Validation loss = 3.3629  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 1.4587  Validation loss = 3.7286  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 1.2996  Validation loss = 3.1077  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 1.2598  Validation loss = 3.2368  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 1.2478  Validation loss = 3.4266  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 1.3402  Validation loss = 3.6595  \n",
      "\n",
      "Fold: 20  Epoch: 12  Training loss = 1.2768  Validation loss = 3.2885  \n",
      "\n",
      "Fold: 20  Epoch: 13  Training loss = 1.6792  Validation loss = 4.0129  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 8  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 1.4336  Validation loss = 3.9867  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 1.4616  Validation loss = 3.6305  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 1.3659  Validation loss = 3.6965  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 1.2612  Validation loss = 3.5757  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 1.2858  Validation loss = 3.7301  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 1.3234  Validation loss = 3.8371  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 1.3751  Validation loss = 3.4318  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 1.5679  Validation loss = 3.2907  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 1.2382  Validation loss = 3.3688  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 1.2802  Validation loss = 3.4970  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 1.5601  Validation loss = 3.1039  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 1.2138  Validation loss = 2.7647  \n",
      "\n",
      "Fold: 21  Epoch: 13  Training loss = 1.3417  Validation loss = 2.8480  \n",
      "\n",
      "Fold: 21  Epoch: 14  Training loss = 1.5642  Validation loss = 2.8704  \n",
      "\n",
      "Fold: 21  Epoch: 15  Training loss = 1.2573  Validation loss = 2.8111  \n",
      "\n",
      "Fold: 21  Epoch: 16  Training loss = 1.2568  Validation loss = 2.8711  \n",
      "\n",
      "Fold: 21  Epoch: 17  Training loss = 1.1762  Validation loss = 2.7156  \n",
      "\n",
      "Fold: 21  Epoch: 18  Training loss = 1.0839  Validation loss = 2.8495  \n",
      "\n",
      "Fold: 21  Epoch: 19  Training loss = 1.0975  Validation loss = 2.8638  \n",
      "\n",
      "Fold: 21  Epoch: 20  Training loss = 1.2417  Validation loss = 2.8631  \n",
      "\n",
      "Fold: 21  Epoch: 21  Training loss = 1.3293  Validation loss = 2.7504  \n",
      "\n",
      "Fold: 21  Epoch: 22  Training loss = 1.1099  Validation loss = 2.9194  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 17  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 1.3128  Validation loss = 1.9158  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 1.2155  Validation loss = 1.7758  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 1.2321  Validation loss = 1.8675  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 1.2513  Validation loss = 1.7737  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 1.1889  Validation loss = 1.8291  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 1.2196  Validation loss = 1.5231  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 1.1210  Validation loss = 1.8871  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 1.1195  Validation loss = 1.8404  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 1.1466  Validation loss = 1.4553  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 1.1272  Validation loss = 1.2550  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 1.1279  Validation loss = 1.1455  \n",
      "\n",
      "Fold: 22  Epoch: 12  Training loss = 1.0656  Validation loss = 1.4635  \n",
      "\n",
      "Fold: 22  Epoch: 13  Training loss = 1.0712  Validation loss = 1.5753  \n",
      "\n",
      "Fold: 22  Epoch: 14  Training loss = 1.1095  Validation loss = 1.6807  \n",
      "\n",
      "Fold: 22  Epoch: 15  Training loss = 1.0552  Validation loss = 1.6282  \n",
      "\n",
      "Fold: 22  Epoch: 16  Training loss = 1.1232  Validation loss = 1.7433  \n",
      "\n",
      "Fold: 22  Epoch: 17  Training loss = 1.0350  Validation loss = 1.8363  \n",
      "\n",
      "Fold: 22  Epoch: 18  Training loss = 1.0269  Validation loss = 1.3843  \n",
      "\n",
      "Fold: 22  Epoch: 19  Training loss = 1.0406  Validation loss = 1.5593  \n",
      "\n",
      "Fold: 22  Epoch: 20  Training loss = 1.0179  Validation loss = 1.5367  \n",
      "\n",
      "Fold: 22  Epoch: 21  Training loss = 1.0632  Validation loss = 1.3734  \n",
      "\n",
      "Fold: 22  Epoch: 22  Training loss = 1.0983  Validation loss = 1.8132  \n",
      "\n",
      "Fold: 22  Epoch: 23  Training loss = 0.9871  Validation loss = 1.4028  \n",
      "\n",
      "Fold: 22  Epoch: 24  Training loss = 0.9795  Validation loss = 1.3167  \n",
      "\n",
      "Fold: 22  Epoch: 25  Training loss = 0.9423  Validation loss = 1.3064  \n",
      "\n",
      "Fold: 22  Epoch: 26  Training loss = 1.0005  Validation loss = 1.7334  \n",
      "\n",
      "Fold: 22  Epoch: 27  Training loss = 1.1523  Validation loss = 1.8609  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 11  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 0.9259  Validation loss = 2.9463  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 0.9296  Validation loss = 3.1648  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 0.9221  Validation loss = 3.4843  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 0.8774  Validation loss = 3.0576  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 0.8531  Validation loss = 2.9012  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 0.8722  Validation loss = 3.0888  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 0.9212  Validation loss = 2.7531  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 0.9193  Validation loss = 3.0612  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 0.8752  Validation loss = 2.5307  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 0.8651  Validation loss = 2.6135  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 0.9017  Validation loss = 2.3802  \n",
      "\n",
      "Fold: 23  Epoch: 12  Training loss = 0.8538  Validation loss = 2.7384  \n",
      "\n",
      "Fold: 23  Epoch: 13  Training loss = 0.8107  Validation loss = 2.8342  \n",
      "\n",
      "Fold: 23  Epoch: 14  Training loss = 0.8186  Validation loss = 2.9811  \n",
      "\n",
      "Fold: 23  Epoch: 15  Training loss = 0.8124  Validation loss = 2.9499  \n",
      "\n",
      "Fold: 23  Epoch: 16  Training loss = 0.8875  Validation loss = 3.3929  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 11  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 0.9481  Validation loss = 1.9434  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 0.9257  Validation loss = 2.2312  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 0.9040  Validation loss = 2.0619  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 1.1852  Validation loss = 2.1093  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 1.0617  Validation loss = 1.9782  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 1.0917  Validation loss = 2.5723  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 0.9679  Validation loss = 2.3425  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 0.9086  Validation loss = 1.8690  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 0.9174  Validation loss = 1.7650  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 0.8525  Validation loss = 1.8134  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 1.0252  Validation loss = 1.6854  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 1.2125  Validation loss = 2.2472  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 0.9333  Validation loss = 1.7142  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 0.9382  Validation loss = 2.0772  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 0.8610  Validation loss = 1.7273  \n",
      "\n",
      "Fold: 24  Epoch: 16  Training loss = 0.9463  Validation loss = 1.8724  \n",
      "\n",
      "Fold: 24  Epoch: 17  Training loss = 0.8829  Validation loss = 1.8628  \n",
      "\n",
      "Fold: 24  Epoch: 18  Training loss = 0.8704  Validation loss = 1.6095  \n",
      "\n",
      "Fold: 24  Epoch: 19  Training loss = 0.9345  Validation loss = 2.1945  \n",
      "\n",
      "Fold: 24  Epoch: 20  Training loss = 0.8375  Validation loss = 1.7642  \n",
      "\n",
      "Fold: 24  Epoch: 21  Training loss = 0.8096  Validation loss = 1.8287  \n",
      "\n",
      "Fold: 24  Epoch: 22  Training loss = 0.9305  Validation loss = 1.8194  \n",
      "\n",
      "Fold: 24  Epoch: 23  Training loss = 0.8006  Validation loss = 1.7064  \n",
      "\n",
      "Fold: 24  Epoch: 24  Training loss = 0.9824  Validation loss = 2.0095  \n",
      "\n",
      "Fold: 24  Epoch: 25  Training loss = 0.8376  Validation loss = 1.7500  \n",
      "\n",
      "Fold: 24  Epoch: 26  Training loss = 0.7858  Validation loss = 1.4943  \n",
      "\n",
      "Fold: 24  Epoch: 27  Training loss = 0.7766  Validation loss = 1.6890  \n",
      "\n",
      "Fold: 24  Epoch: 28  Training loss = 0.8257  Validation loss = 1.6684  \n",
      "\n",
      "Fold: 24  Epoch: 29  Training loss = 0.7979  Validation loss = 1.7297  \n",
      "\n",
      "Fold: 24  Epoch: 30  Training loss = 0.8060  Validation loss = 1.5739  \n",
      "\n",
      "Fold: 24  Epoch: 31  Training loss = 0.8340  Validation loss = 1.3641  \n",
      "\n",
      "Fold: 24  Epoch: 32  Training loss = 0.8627  Validation loss = 1.4849  \n",
      "\n",
      "Fold: 24  Epoch: 33  Training loss = 0.7721  Validation loss = 1.6393  \n",
      "\n",
      "Fold: 24  Epoch: 34  Training loss = 0.8355  Validation loss = 1.8305  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 31  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 0.8584  Validation loss = 1.2151  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 0.8685  Validation loss = 1.3530  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 0.9074  Validation loss = 1.6023  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 0.7888  Validation loss = 1.3741  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 0.9228  Validation loss = 1.1620  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 0.7831  Validation loss = 1.2239  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 0.7773  Validation loss = 1.2855  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 0.8896  Validation loss = 1.4544  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 0.7932  Validation loss = 1.3195  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 0.7438  Validation loss = 1.2032  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 0.7173  Validation loss = 1.2018  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 0.7182  Validation loss = 1.2585  \n",
      "\n",
      "Fold: 25  Epoch: 13  Training loss = 0.8301  Validation loss = 1.2582  \n",
      "\n",
      "Fold: 25  Epoch: 14  Training loss = 0.7596  Validation loss = 1.3318  \n",
      "\n",
      "Fold: 25  Epoch: 15  Training loss = 0.7389  Validation loss = 1.2161  \n",
      "\n",
      "Fold: 25  Epoch: 16  Training loss = 0.9051  Validation loss = 1.3403  \n",
      "\n",
      "Fold: 25  Epoch: 17  Training loss = 0.7800  Validation loss = 1.3571  \n",
      "\n",
      "Fold: 25  Epoch: 18  Training loss = 0.7539  Validation loss = 1.4266  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 5  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 0.7159  Validation loss = 2.9592  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 0.6522  Validation loss = 3.5438  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 0.6894  Validation loss = 2.8462  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 0.7357  Validation loss = 3.2265  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 0.6993  Validation loss = 3.7636  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 0.5945  Validation loss = 3.4843  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 0.6142  Validation loss = 3.1183  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 0.6093  Validation loss = 3.2133  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 0.6121  Validation loss = 3.6258  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 0.6724  Validation loss = 3.9974  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 0.5657  Validation loss = 3.4257  \n",
      "\n",
      "Fold: 26  Epoch: 12  Training loss = 0.7169  Validation loss = 2.8078  \n",
      "\n",
      "Fold: 26  Epoch: 13  Training loss = 0.5916  Validation loss = 3.6189  \n",
      "\n",
      "Fold: 26  Epoch: 14  Training loss = 0.6242  Validation loss = 3.8858  \n",
      "\n",
      "Fold: 26  Epoch: 15  Training loss = 0.5743  Validation loss = 3.1729  \n",
      "\n",
      "Fold: 26  Epoch: 16  Training loss = 0.5714  Validation loss = 3.4661  \n",
      "\n",
      "Fold: 26  Epoch: 17  Training loss = 0.5215  Validation loss = 3.2918  \n",
      "\n",
      "Fold: 26  Epoch: 18  Training loss = 0.4934  Validation loss = 3.4932  \n",
      "\n",
      "Fold: 26  Epoch: 19  Training loss = 0.5330  Validation loss = 3.6956  \n",
      "\n",
      "Fold: 26  Epoch: 20  Training loss = 0.7051  Validation loss = 4.0990  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 12  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 1.0165  Validation loss = 2.6150  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 1.0318  Validation loss = 3.2492  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 0.8623  Validation loss = 2.6002  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 0.8865  Validation loss = 1.9644  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 1.1296  Validation loss = 1.7514  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 0.9803  Validation loss = 1.5771  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 0.8690  Validation loss = 1.6839  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 0.8850  Validation loss = 2.0049  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 0.8100  Validation loss = 3.0543  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 0.7402  Validation loss = 2.7379  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 0.7390  Validation loss = 2.6572  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 0.7516  Validation loss = 2.4013  \n",
      "\n",
      "Fold: 27  Epoch: 13  Training loss = 0.8424  Validation loss = 3.2272  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 6  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 0.8671  Validation loss = 2.7779  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 0.8156  Validation loss = 2.6913  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 0.8885  Validation loss = 2.7161  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 0.8513  Validation loss = 2.4938  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 0.8962  Validation loss = 3.0423  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 0.8039  Validation loss = 2.6637  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 0.8294  Validation loss = 2.4491  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 0.7570  Validation loss = 2.6404  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 0.8537  Validation loss = 2.6805  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 0.7710  Validation loss = 2.4975  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 0.8235  Validation loss = 3.0215  \n",
      "\n",
      "Fold: 28  Epoch: 12  Training loss = 0.7580  Validation loss = 2.9094  \n",
      "\n",
      "Fold: 28  Epoch: 13  Training loss = 0.8191  Validation loss = 2.3866  \n",
      "\n",
      "Fold: 28  Epoch: 14  Training loss = 0.6917  Validation loss = 2.3011  \n",
      "\n",
      "Fold: 28  Epoch: 15  Training loss = 0.6892  Validation loss = 2.4824  \n",
      "\n",
      "Fold: 28  Epoch: 16  Training loss = 0.6669  Validation loss = 2.4090  \n",
      "\n",
      "Fold: 28  Epoch: 17  Training loss = 0.6745  Validation loss = 2.6011  \n",
      "\n",
      "Fold: 28  Epoch: 18  Training loss = 0.6690  Validation loss = 2.6031  \n",
      "\n",
      "Fold: 28  Epoch: 19  Training loss = 0.6533  Validation loss = 2.7202  \n",
      "\n",
      "Fold: 28  Epoch: 20  Training loss = 0.7788  Validation loss = 2.7086  \n",
      "\n",
      "Fold: 28  Epoch: 21  Training loss = 0.6724  Validation loss = 2.1251  \n",
      "\n",
      "Fold: 28  Epoch: 22  Training loss = 0.6633  Validation loss = 2.2876  \n",
      "\n",
      "Fold: 28  Epoch: 23  Training loss = 0.6388  Validation loss = 2.3223  \n",
      "\n",
      "Fold: 28  Epoch: 24  Training loss = 0.6247  Validation loss = 2.4639  \n",
      "\n",
      "Fold: 28  Epoch: 25  Training loss = 0.6595  Validation loss = 2.4707  \n",
      "\n",
      "Fold: 28  Epoch: 26  Training loss = 0.7370  Validation loss = 2.8056  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 21  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 0.9443  Validation loss = 1.8259  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 0.7708  Validation loss = 2.0219  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 0.9444  Validation loss = 1.9661  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 0.8511  Validation loss = 2.2133  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 0.8516  Validation loss = 2.1142  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 0.7493  Validation loss = 1.6468  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 0.7541  Validation loss = 2.1061  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 0.7390  Validation loss = 2.0798  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 0.7468  Validation loss = 2.1078  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 0.8415  Validation loss = 2.2281  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 0.7067  Validation loss = 2.1128  \n",
      "\n",
      "Fold: 29  Epoch: 12  Training loss = 0.7750  Validation loss = 1.9939  \n",
      "\n",
      "Fold: 29  Epoch: 13  Training loss = 0.7067  Validation loss = 1.8985  \n",
      "\n",
      "Fold: 29  Epoch: 14  Training loss = 0.8014  Validation loss = 2.0173  \n",
      "\n",
      "Fold: 29  Epoch: 15  Training loss = 0.8841  Validation loss = 1.7388  \n",
      "\n",
      "Fold: 29  Epoch: 16  Training loss = 0.6811  Validation loss = 1.9207  \n",
      "\n",
      "Fold: 29  Epoch: 17  Training loss = 0.7614  Validation loss = 1.8167  \n",
      "\n",
      "Fold: 29  Epoch: 18  Training loss = 0.6962  Validation loss = 2.0798  \n",
      "\n",
      "Fold: 29  Epoch: 19  Training loss = 0.6192  Validation loss = 2.0361  \n",
      "\n",
      "Fold: 29  Epoch: 20  Training loss = 0.6271  Validation loss = 1.8041  \n",
      "\n",
      "Fold: 29  Epoch: 21  Training loss = 0.6142  Validation loss = 1.9383  \n",
      "\n",
      "Fold: 29  Epoch: 22  Training loss = 0.7762  Validation loss = 1.9082  \n",
      "\n",
      "Fold: 29  Epoch: 23  Training loss = 0.6595  Validation loss = 2.1493  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 6  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 0.7542  Validation loss = 1.3272  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 0.9792  Validation loss = 1.5309  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 0.7841  Validation loss = 1.1676  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 0.7096  Validation loss = 1.3455  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 0.7509  Validation loss = 1.2832  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 0.7481  Validation loss = 1.3058  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 0.7769  Validation loss = 0.8621  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 0.7119  Validation loss = 1.1523  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 0.6562  Validation loss = 1.1815  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 0.6587  Validation loss = 1.2395  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 0.6030  Validation loss = 1.1125  \n",
      "\n",
      "Fold: 30  Epoch: 12  Training loss = 0.6404  Validation loss = 1.2910  \n",
      "\n",
      "Fold: 30  Epoch: 13  Training loss = 0.6836  Validation loss = 1.2147  \n",
      "\n",
      "Fold: 30  Epoch: 14  Training loss = 0.6633  Validation loss = 1.1087  \n",
      "\n",
      "Fold: 30  Epoch: 15  Training loss = 0.7615  Validation loss = 1.2156  \n",
      "\n",
      "Fold: 30  Epoch: 16  Training loss = 0.7522  Validation loss = 1.2380  \n",
      "\n",
      "Fold: 30  Epoch: 17  Training loss = 0.6598  Validation loss = 1.3725  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 7  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 0.6573  Validation loss = 1.3018  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 0.7366  Validation loss = 1.5146  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 0.6320  Validation loss = 1.2697  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 0.6454  Validation loss = 1.1638  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 0.6651  Validation loss = 1.1638  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 0.6973  Validation loss = 1.4294  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 0.7125  Validation loss = 1.0430  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 0.8405  Validation loss = 0.9125  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 0.7048  Validation loss = 1.4832  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 0.6773  Validation loss = 1.2160  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 0.6287  Validation loss = 1.1100  \n",
      "\n",
      "Fold: 31  Epoch: 12  Training loss = 0.7939  Validation loss = 0.9771  \n",
      "\n",
      "Fold: 31  Epoch: 13  Training loss = 0.6932  Validation loss = 1.0435  \n",
      "\n",
      "Fold: 31  Epoch: 14  Training loss = 0.6180  Validation loss = 1.0125  \n",
      "\n",
      "Fold: 31  Epoch: 15  Training loss = 0.7062  Validation loss = 1.4122  \n",
      "\n",
      "Fold: 31  Epoch: 16  Training loss = 0.7717  Validation loss = 1.3022  \n",
      "\n",
      "Fold: 31  Epoch: 17  Training loss = 0.5904  Validation loss = 1.0188  \n",
      "\n",
      "Fold: 31  Epoch: 18  Training loss = 0.5817  Validation loss = 0.9678  \n",
      "\n",
      "Fold: 31  Epoch: 19  Training loss = 0.7071  Validation loss = 1.2346  \n",
      "\n",
      "Fold: 31  Epoch: 20  Training loss = 0.6677  Validation loss = 1.1403  \n",
      "\n",
      "Fold: 31  Epoch: 21  Training loss = 0.5739  Validation loss = 1.0561  \n",
      "\n",
      "Fold: 31  Epoch: 22  Training loss = 0.6262  Validation loss = 1.2295  \n",
      "\n",
      "Fold: 31  Epoch: 23  Training loss = 0.6304  Validation loss = 0.8683  \n",
      "\n",
      "Fold: 31  Epoch: 24  Training loss = 0.5499  Validation loss = 0.8245  \n",
      "\n",
      "Fold: 31  Epoch: 25  Training loss = 0.5214  Validation loss = 1.0385  \n",
      "\n",
      "Fold: 31  Epoch: 26  Training loss = 0.5893  Validation loss = 0.9443  \n",
      "\n",
      "Fold: 31  Epoch: 27  Training loss = 0.7594  Validation loss = 1.5684  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 24  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 0.7739  Validation loss = 2.0251  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 0.5697  Validation loss = 2.1548  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 0.5876  Validation loss = 2.4085  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 0.5554  Validation loss = 2.2262  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 0.7778  Validation loss = 2.5003  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 0.7347  Validation loss = 2.1630  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 0.6912  Validation loss = 2.3292  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 0.6295  Validation loss = 2.2287  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 0.6467  Validation loss = 2.1356  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 0.7001  Validation loss = 2.7339  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 0.5883  Validation loss = 2.3558  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 0.6540  Validation loss = 2.6236  \n",
      "\n",
      "Fold: 32  Epoch: 13  Training loss = 0.6726  Validation loss = 2.2037  \n",
      "\n",
      "Fold: 32  Epoch: 14  Training loss = 0.6405  Validation loss = 2.4464  \n",
      "\n",
      "Fold: 32  Epoch: 15  Training loss = 0.8611  Validation loss = 2.2625  \n",
      "\n",
      "Fold: 32  Epoch: 16  Training loss = 0.7456  Validation loss = 2.2531  \n",
      "\n",
      "Fold: 32  Epoch: 17  Training loss = 0.5874  Validation loss = 2.4154  \n",
      "\n",
      "Fold: 32  Epoch: 18  Training loss = 0.5394  Validation loss = 2.4887  \n",
      "\n",
      "Fold: 32  Epoch: 19  Training loss = 0.6033  Validation loss = 2.7227  \n",
      "\n",
      "Fold: 32  Epoch: 20  Training loss = 0.5800  Validation loss = 2.2672  \n",
      "\n",
      "Fold: 32  Epoch: 21  Training loss = 0.6628  Validation loss = 2.7962  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 1  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    # Load model with lowest validation error for each fold\n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 14\n",
      "Average validation error: 3.26123\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 0.8044  Test loss = 3.1652  \n",
      "\n",
      "Epoch: 2  Training loss = 0.7458  Test loss = 3.0629  \n",
      "\n",
      "Epoch: 3  Training loss = 0.7132  Test loss = 3.0063  \n",
      "\n",
      "Epoch: 4  Training loss = 0.6950  Test loss = 2.9788  \n",
      "\n",
      "Epoch: 5  Training loss = 0.6829  Test loss = 2.9618  \n",
      "\n",
      "Epoch: 6  Training loss = 0.6738  Test loss = 2.9486  \n",
      "\n",
      "Epoch: 7  Training loss = 0.6664  Test loss = 2.9373  \n",
      "\n",
      "Epoch: 8  Training loss = 0.6601  Test loss = 2.9274  \n",
      "\n",
      "Epoch: 9  Training loss = 0.6547  Test loss = 2.9188  \n",
      "\n",
      "Epoch: 10  Training loss = 0.6497  Test loss = 2.9113  \n",
      "\n",
      "Epoch: 11  Training loss = 0.6453  Test loss = 2.9048  \n",
      "\n",
      "Epoch: 12  Training loss = 0.6412  Test loss = 2.8991  \n",
      "\n",
      "Epoch: 13  Training loss = 0.6375  Test loss = 2.8942  \n",
      "\n",
      "Epoch: 14  Training loss = 0.6340  Test loss = 2.8898  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc1f7940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXd4FFXbxu+TTe+FJCAESAFCkYSAdJGmgIgK0lERQYoo\nfIhvsbz2Xl/EgiAKIoj0FxEQRIqC0hOSEBISCD2FVNKz2ef74+xstsy27CYbkvO7rlybzMzOnN3s\nzn2eehgRQSAQCATNDydHD0AgEAgEjkEIgEAgEDRThAAIBAJBM0UIgEAgEDRThAAIBAJBM0UIgEAg\nEDRThAAIBAJBM0UIgEAgEDRThAAIBAJBM8XZ0QMwRYsWLah9+/aOHoZAIBDcNpw8efImEQVbcmyj\nFoD27dvjxIkTjh6GQCAQ3DYwxi5ZeqxwAQkEAkEzRQiAQCAQNFOEAAgEAkEzRQiAQCAQNFOEAAgE\nAkEzRQiAQCAQNFNsFgDGWCfGWLzWTzFj7P/0jhnMGCvSOuYVW68rEAgEAtuwuQ6AiFIBxAIAY0wB\n4BqArTKH/kFED9h6vWbBzz8D4eFAt26OHolAIGjC2LsQbBiADCKyuBBBoIdKBUydCvj5AQkJQFCQ\no0ckEAiaKPaOAUwG8KORff0YYwmMsV2Msa52vm7T4dIloKQEuHYNmDULIHL0iAQCQRPFbgLAGHMF\n8CCAjTK7TwFoR0QxAJYC2GbiPLMZYycYYydyc3PtNbzbh+Rk/jhhArBtG7BsmWPHIxAImiz2tABG\nAThFRNn6O4iomIhK1L/vBODCGGshdxIiWk5EvYioV3CwRf2MmhaSAHz9NTBqFLBoEZCY6NgxCQSC\nJok9BWAKjLh/GGMtGWNM/Xtv9XXz7HjtpkNyMtC6NRAQAKxaBfj7A1OmAOXljh6ZQCBoYthFABhj\nXgDuBbBFa9tcxthc9Z/jASQxxhIAfAZgMpFwbsuSlAR0VYdIQkKA77/novDGG44dl6DhKCnhPwJB\nPWMXASCiUiIKIqIirW3LiGiZ+vfPiagrEcUQUV8iOmKP6zY5amqAlJRaAQCA++4Dxo8HvvkGqKpy\n3NgEDcfkyUBMDNAcY2CCBkVUAjcmLl4EKioM8/9nzgRu3gS2b3fMuAQNS3IycOEC8NBD/PMgENQT\nQgAaE1IAuKteluy99wJhYcDKlQ0/JkHDolQCV64AvXoBf/0FPPEErw0RCOoBIQCNiaQk/tili+52\nhYLfCH79ld8cBE2X69e5K/Cpp4APPgB++gl4+WVHj0rQRBEC0JhITgbatgV8fAz3zZjBi8JWrWrw\nYQkakEvqIvp27YDnnwdmzwbefRf47jvHjkvQJBEC0JhITjbe/yc8HBg6FPj2W+ESaMpkZvLH9u0B\nxoDPPwfuvhv4979FEoDA7ggBaCwolcC5c4b+f21mzuQ3iP37G2xYggZGsgDatuWPLi7AP/8J5OTw\nJoECgR0RAtBYyMjgMzxTAjB2LC8ME8HgpktmJhAaCnh41G4bOZIXB65Y4bBhCZomQgAaC1IA2FQL\naA8PYNo0YMsWoKCgYcZVFwoLgbIyR4/i9uTSJe7/18bZGXjySWDPnloXkUBgB4QANBaSk7nPt3Nn\n08fNnAlUVvIK4cbK8OG8h5FAlt27d2Pfvn3yO+UEAOD/d0AEgwV2RQhAYyE5mQd6PT1NH9ejBzBo\nEE8NPHeuYcZmDRUVwOnTwPnzjh5Jo0SlUmHGjBl466235HZyAWjf3nBfu3a8Kvzbb3maqEBgB4QA\nNBaSk037/7VZu5a7g8aOBYqL63dc1nLuHL+RZRs0hbUbTz75JP7zn//U2/nrkyNHjiArKwtVchk9\n2dk8DiRnAQC8NuDqVWD37vod5C+/AD8aW9ZD0JQQAtAYqKoCUlMtXwKyTRtgwwY+y25slaJSLCMn\np94usWfPHuzatavezl+fbN68GQDkBUA7BVSOMWN4g8D6DgZ/+CFPOxU0eYQANAbOn+dpoJZaAAAw\neDD/om7dCrz/fr0NzWqkdhZ5efw12RmVSoXs7GxkZGTY/dz1DRFhyxbeMFdWALSLwORwdeWCv2MH\ncONG/QwS4JbI5ctAfn79XUPQKBAC0Bgw1gPIHP/3f7xz5EsvAcaCig2NZAEQcRGwM3l5eVAqlSgs\nLES+dINSqW6LpTNPnDiBy5cvw9XVFdXV1YYHmBMAgC8TWlNTv8HgrCz+mJBQf9cQNAqEADQGkpMB\nJycgOtq65zHG20QHBzee7JDk5Noc9nqIA9zQmvmmp6fzG//gwTxNspGzadMmODs7Y9iwYcZdQIGB\n8q1AJDp0AAYMAP73v/oZZGUlT+MFhAA0A4QANAaSkoCoKMDd3frnenlxn3Fj6B1fUsJbWg8YwP+u\nhzhAljQ7BbgbaO9e4I8/gHXr6sXisBdEhM2bN2PYsGEIDQ017gIyNfuXiI3lwfb6sHq0/2fx8fY/\nv6BRIQSgMWBNBpAcwcGNQwBSUvjj0KH8sR4EQNsCyMjIAN55B/Dz44H09evtfj17cebMGWRkZOCR\nRx4x7gLKzDQeANamUyee/VUfgXbJanN1FQLQDBAC0Bi4coXXANSVxiIAkv+/AQQgMDAQOHIEOHgQ\neO01voJWI+6UunnzZjg5OeHhhx+Gi4uLoQVAZLkF0LEjf0xNtf9AJQtr4EDg7FnRgK6JIwTA0ZSW\n8rYJISF1P4ckAI4OhCYnA25uQM+evH1BPbmAfHx80K1bNwz5+28gKIjnxz/xBHDiRG1AvZGxadMm\nDBo0CMHBwfIWQF4e/xxYagEAQFqa3cepsQBGjgSqq7kICJosQgAcjTRzDw6u+zlatODBO0cvJJ6U\nxFtZODtzQaunIHCrVq0wODAQAwoKgIULeRxk6lR+3dWr7X5NW0lJSUFKSgoeeeQRAICrq6uhBSDV\nAFhiAYSFcaGtDwtA+p/ddx9/FG6gJo0QAEcjCYCtFgDA1w12JNrrGYSE1JsLqGXLlph44QJuASid\nMaP2evffD6xZUy/1B7YgFX+NHTsWAORdQJakgEooFDwbqD4sgKwsHlPp1o1nc4lMoCaN3QSAMZbJ\nGEtkjMUzxk7I7GeMsc8YY+mMsTOMsTh7Xfu2RrpJ2mIBSM91ZBygsJC3KZCC2fUkAFlZWYj18UHn\nxER8BeCCdlfUJ57gN7C9e+1+3bpCRFi/fj369++P1q1bA+AWABGhRrunj7kqYH06dqw/CyA0lItM\n9+7CAmji2NsCGEJEsUTUS2bfKAAd1D+zAXxl52vfnkg3SXtYAI4UAMlXLFkAoaH2FYCaGuDsWQy6\nfBnzz5wBXFzwKdS1ABKjR/OYQCMKBh87dgzJycmYPn26ZpurqysAvWrgS5d4/r+/v2Un7tSJryFh\nb2snOxto2ZL/HhvLBcDRsSVBvdGQLqCHAHxPnL8B+DPGWjXg9Rsn9ogBNAYBkDKAtC2A7Gz73Dye\nfx7w9QW6dsU3lZUIz8pC5b//jSxAtyWEqyuPBWzbxtdLqK4Gdu7kRWLvveeQLporV66Ep6cnJk+e\nrNnm4uICQEYApGUgLaFjR37zv3jRjqNFrQUAcAEoLORtIQRNEnsKAAHYwxg7yRibLbO/NYArWn9f\nVW9r3uTkcF+rl1fdz9EYBCA5mb8GyYcdEgKUl/MsJ1vZsgXo0AHZH36IrgDWf/01PF5/HUFBQboW\nAMDdQFVVwMMPA61acatgwwbghReABx+srXK1IyUlJSiVeZ0lJSX48ccfMXHiRPj6+mq2SxaATiZQ\nZqZl/n+J+soEysrSFQBAuIGaMPYUgIFEFAfu6pnPGBtUl5MwxmYzxk4wxk7kNobc9vomN5ffwC2d\n+cnh7c2zQhxtAXTpwltaALUuLXu4gfLygHvuQVqfPjgLoGWbNgCAyMhIw6ZwPXoAvXoBx48D994L\nbN/Om5otW8ZX1Ordu7ZgzU488sgjuPvuuw1SOzdu3IiSkhLMmjVLZ7tRF5Cl/n+gfmoBpDYQkgDc\neSf/XAoBaLLYTQCI6Jr6MQfAVgC99Q65BiBM6+826m3651lORL2IqFewLW6R24WcHNv8/wD/krZo\n4XgLQLudtXQTsVUAqqt51WtQkKYIrKXaRx0VFWUoAIzx4rDcXN7TfswY7hqaMwfYvx8oKgL69LFr\n87z4+HicPn0aH330kc72lStXolOnTujfv7/OdgMXUGEhH5c1FkBQEP+xpwUg/a+kGICXF8820heA\nvDzgmsFXV3AbYhcBYIx5McZ8pN8B3AcgSe+w7QAeV2cD9QVQRET12NP2NiE313YBALgV4ag00Js3\nue9Yu52F9JpsrQWQ+vu0aKHpA9SqFQ8dRUZG4tKlS4YplZ6e8i61gQN5sVhQEPD227aNS82tW7eQ\nk5MDDw8PvP7660hVz8hTUlJw+PBhzJw5E0zPujNwAVmTAqqNvTOBpCpgSbwB7gbSTgW9eROIi+PV\n3iI4fNtjLwsgFMCfjLEEAMcA/EJEuxljcxljc9XH7ARwAUA6gBUAnrbTtW9vcnJsCwBLOLIdhFR9\nq20B2MsFJAmA2gJwcXHhbSDABUClUuGSdAO1hLAw3qzuwgWLn5KcnIzzRpa4vKA+z/vvvw8PDw/M\nnj0bKpUK3377LZydnfH4448bPMfAApDGb40LCOBxAHtaAJJY6wvAxYtIP3GCB9GnTOFB4bQ0LqaC\n2xq7CAARXSCiGPVPVyJ6W719GREtU/9ORDSfiCKJ6E4iEp8eIvtaAI4WAG0LQBI1OwtAaGgonNRx\nhqioKAAwDASbIyKC91+Sa8imR0VFBYYPH445c+bI7peuPXDgQHz88cc4dOgQvvzyS3z//fcYM2YM\nQrVvpmoMYgDWVAFr07EjcP06cOuWdc8zhowAnFdbUu9NncrXof7tN+CjjwAXF+Cnn+xzXYHDEJXA\njqSkhC+ifrtbAElJvHq0tVZSl7s732ZHAcjKytK4fwBuAQCwfnWw8HC+iIwF6Y3fffcdsrKykGyk\nx5B07cjISMyYMQNDhw7FggULkJOTg5kzZ8o+R9YF5OFh/edAygQyYp1YjZ4L6OjRo3jg5ZcBAJPS\n03kq7ezZwOLFvFXEhg2NazlSgdUIAXAk9mgDIREczGeClZW2n8tapHbW+plM9ugHpGcBaAtAaGgo\nvLy86iYAgNkceqVSiQ8++AAAkJOTU7sCmRYZGRlo0aIFfH19wRjD8uXL4e7ujtatW2PEiBGy55V1\nAbVrZ30mmL0zgbKzuWi7u+Pw4cO49957URMcjBIvL9xLhPLu3YHPPuPHTprErai//7bPtQUOQQiA\nI7FHGwgJR9YCpKXJr2ZmYTuI8ePH46WXXpLfqScAUgYQADDGEBkZab0LyEIBWL9+PTIzMzXun3Pn\nzhkck56errFEAG4JbN26FWvWrIGzs7PseQ0sgLNnebaNtURFcdGwVxxAXQT2559/YsSIEWjVqhUO\nHjoEZVwccgAceOYZnm4MAA89xH8XbqDbGiEA+hDxIqKG6CppjzYQEi1a8MeGFoCKCu46kAtgWiAA\n1dXV+Pnnn7HXWP+evDzAzQ3VLi64efOmjgUAGEkFNUebNrxzqAkBUKlUePfdd9GtWzf84x//AMAz\ne/TJyMjQxCIkRowYgSFDhhg9t04MoLCQ1yX01s+atgB3d2452NMCCA3Fs88+i9DQUBw8eBCtW7eG\ny/ffoweAE1qrscHXFxg1Cti40SEV1gL7IARAn9RUvt7qsmX1fy17tIGQsGNH0Lfffhtbtmyx7GDJ\njy4nABb0A0pNTUVVVZXxWXxeHhAUhJzcXBCRgQBERkbiwoULUFnji1Yo+I3ThABs374dZ8+exQsv\nvID27dvD3d3dQAAqKytx5coVHQvAEnRcQMeP8419+1p1Dg32zATKykKJjw/i4+Px9NNPa6wtr/bt\n4RYebhgHmTQJuHED+PNP3e3l5bx2Q9DoEQKgz+7d/PHYsfpfY7YRuoCUSiXeeustrF271rInmOpi\nGRLCBcnEDDFeXWRUUFCAPLn3Wy0A+kVgEpGRkaisrMQ1awuTwsONpoISEd555x1ERERg4sSJUCgU\n6Nixo4ELKDMzEyqVymoB0HEBHT3K3Th33WXd+CWkWgB75ORnZyOtqAgAMG7cOJ1dXbt2xVn9xWEe\neIAHr7XdQBcu8Arizp1rPxuCRosQAH127+aFRCoVT3mrAzNmzMAPP/xg/sDcXF6w5OlZp+voYCcB\nOH/+PCoqKmQDnrKYSmEMCeE3JhNWSbxWlalsrr1aAPSLwCQk90udAsFGLIB9+/bh+PHj+Ne//qXx\n43fu3NnAApCuqe8CMoeOC+jvv/nN0s/PuvFLdOrEs8m03TN1Qd0G4vilS4iLi0O43hKlXbp0QWpq\nKpTa3Ue9vbkIbNrEG9OdOcNrLPLzuRVw7731s26xwG4IAdCmvJy3EZg5EwgIqLUGrODcuXNYtWoV\nXnnlFfNuCSNtIHJzc1FeXm7dhQMCuGvDRgHQnpFbxKVL3J9+xx2G+ywoBouPj0dAQAAA0wIgWQBy\nLiCgDrUA4eH8vZJZRW3JkiVo1aqVTgvn6OhoXLx4ERUVFZpt2img1qBxAVVWcgugTx/rxq6NvTKB\n1NlaJ65eNZj9A9wCkHXVTZrE38e33wYGDeKfhT//BHbs4O0iRo4U7qBGjBAAbQ4e5EHN0aN5nvPu\n3Vab1pLv/OLFi/j9999NHyw1gtNCqVQiNjYWCxYssOq6cHLiLQ5sFIAEddm/VRZAWBj/4utjRgCI\nCAkJCXjggQfg5OQkfxPXEwD9wqqwsDC4uLhYbAGUlpZi6dKlOCsJrIybIiMjA/369YOblPECbgEQ\nEdK0/O0ZGRnw8vJCiJVBfMkCcL9+nVtHdfX/A/brCqoWgCxAs3SlNl3VRX4GbqD77+eWwGuv8R5C\nhw/zpoD9+wObNwOJiTypoqKC/6SkcHFITLRtvAK7IARAm927eWbFoEF85pKVxc1aK9i8eTN69OiB\nwMBALF++XPaYiooKvhqUjAVw4MABXL9+HZs2bTJcONwcdigGkywAqwTAWAWrmYZw169fx82bN9G7\nd2+0bdvW0AIg4u4EtQsoKChIc/OUUCgUiIiIMNqqQaK8vByffvopIiIisGDBAnz16698h4wbqKio\nCP7aC7McO4ZYdfWxdhxASgHV7/VjDuk1BEpjtsUCaNOG++HtZAF4hYcjWialV9pmEAj28ADmzweG\nD+cz/7Zta/eNGsUX59m/n7fm9vTk4jBmDDB4cL205hZYh3yicnNl927+wfTwAKQinl27gJgYi56e\nmZmJU6dO4YMPPsCNGzfw+eefIzc3F9pdTQsLC9G9e3eMGzcO/83N5e2LtdiwYYPmuIMHD2L48OGW\nj98OHUElASgtLUVVVZXBDdeAS5e4r1cOMw3hpGvFxsaiQ4cOhjfx4mLuWw4Kwo2UFAP3j8Sdd96J\n06dPGx3i//73Pzz99NO4fv06hg4dipycHKRKRVgygeCioiL4ST75ggJgxAhEde0KxphOHCAjIwNd\nunQxel1jSC6gFunpPAak3ULDWpyceA2BjQJwKz0dPgB6jR4tu9/LywvhcplAAK8QNsa0aXyMv/7K\nW3BERvI2EpMn8+eZeq6g3hEWgMTFi/xLNHIk/7tVK37jtyIOILl/xo0bh6eeegrV1dVYrVdP8OKL\nL+LKlStY+c03ID0LQKlUYsuWLXj44Yfh6elpeSqmhI0dQbOyspCdnY2Oar+y2ThAZSXvRWOsiZm/\nP3cN6VsACQnAxYsaAejevbtGAEjb5WaiCEybuLg4ZGRkoNDIjPLZZ5+Fr68v9u/fj3379qFr1664\nVFrKb756FkB1dTVKS0trBeC994DCQijOnkV4+/YaAaipqcGFCxes9v8DtRZAaGYmX7vASMGYxdgh\nFTT14EEAwL2PPmr0GNlMIEuYMoVbAq+8wgVh4kT+uGQJryYWOAwhABKSS0ASAICbsIcPWxzE2rx5\nM2JiYhAZGYnOnTtjwIABWLFiheam9tdff2HZsmUYPnw4nEpLwaqrdWIA+/fvR15eHqZPn46RI0di\n27Zt1uW32+gCkvz/UhGTWTfQlSvcTWPMBeTkxMekLQA1NTy+Mm4c4k+fRmRkJHx9fREVFYWioiLd\nVFATfYC06dmzJwDg1KlTBvtu3LiBK1euYM6cORg8eDAAwN/fH4VFRbKZQMXq/7W/vz9/fUuW8KKn\nggL0jYjQuICuXbuGqqqqOgmAi4sL3ACEXL9eJ///hQsXNG2nAfAq7AsXbGoDcu3UKRQ7OaG7iYI0\n2UyguvLmmzzT7tVXbT+XoM4IAZDYvZvfEKSsCoCLgVIJmAvmgt9ojhw5ohNAmz17NtLS0nDo0CFU\nV1dj9uzZaN26NbZs2YLBnTsDAEhLADZs2ABvb2+MHDkS48aNw40bN3D06FHLX0NwMPeZ17EyU5qR\nSzdKsxaAkTbG7777Lt544w0ew9AvBvvjD/53fDw8/v4bMWr3Wgd1KwQdN5DamqHAQIM+QNqYEgDp\n/eutdWPz9/fn1oKMABSp8+D9/Px4YJMI+OQTAMDAwECkpqaipqamzimgABeAWACKmpo6+f8nT56M\nHj164NChQ3xDdDT/n1ubCqumsLAQVVeuoNLPz2Q8w2gmUF1o3x545hlecZ+kv3SIGc6cseg76TBu\n3NBdQ6ERIwQA4GvI7tvHb/jaX4B+/QAfH4vcQFu3bgWgm0Exfvx4+Pn5YcWKFfjkk0+QlJSEL774\nAj4+Ppih9rWmqm+y1dXV2Lp1Kx588EG4u7tj9OjRcHFxsc4NFBzMb1h1LGCLj49Hu3btNLNasxaA\nTBGYSqXCO++8g1dffRWDBw9Gua+vbgxg0ybA3R2q4GBMvnYNsep1Z2UFQP06ipydUVVVZdQFFBQU\nhHbt2uHkyZMG+44ePQpnZ2f00Iq1+Pn5oaqqCsq2bbkAaLmdJDdSm6Ii7rZ45hluCQLo7uKCiooK\nXL58uc4poADg5OSE/tLSmVYKQGFhIU6cOIHKykqMHj2aC5wUtJXpVWQJO3bsQAgR3My0ozaaCVRX\nXnyRf7/+/W/rnjd/PjB2LM8qamyoVDzr6b77bosFc4QAAMCRIzwfXNv9A/ClBIcNsygddPPmzYiO\njtYJCnp6euLRRx/Fpk2b8Prrr2Ps2LF48MEHAQAj4uIAAOvVSxNK7p+JEycC4LPUYcOGYevWrbp+\ncVPYWAyWkJCA2NhYzYIrZgXg0iXu5tFqA33p0iWUlJRg3LhxSExMxM9//40ySShUKr7A+6hRuPrw\nw7gffFYNAOHh4XBycpIVgCx1NpQxCwDgVoAxAYiJiYGHh4dmm5ThUxYayv/vWnETyQKI2bCBpze+\n+CKPB/n5IaKsDADvCZSeng4XFxeEhWmvcmo5fRlDgY+PfP2ECQ4dOgQiwg8//ICQkBCMHDkSZ6Qb\nYR0FYPv27Wjt7AwfM2JmNBOorgQFAS+8APzyC0/BtoSCAv59LS7mCRqNjZ9+4l0EcnLqHN9ISkrC\nypUr7TwweYQAAPyD5OICyDXwGjmS3+hMZFncvHkTBw8elM2ffuqpp1BZWQlnZ2d8JrXSBeChXsTj\nhz17UFBQgI0bN8LHx0enhfDYsWORkZGBREtzpo0IwKJFi7Bq1SqTTy0rK0NqaipiYmI0hVkWWQBt\n2vD3To001sWLF+PUqVOo8vcH5eTg448/Bv76i5vH48djb1QUygDc9ccfAHhgtF27drruhbw8gDFc\nU994TQlAXFwczp8/r7mBAzxQe+LECfTRm2VLAlAcFMQ3aLmBioqKMABA8JEjfGYaFMStwi5d0ELt\nyjp37hwyMjLQvn17KBQK0++REXqrVMg0YtGY4sCBA3B3d8fYsWPx+++/w8fHB8MeeghVLVvWWQAS\nEhLQkjEwE+8vYCYTqK4sWMAnEC+8YNnxe/bwiYSLC1/zuTFRUcFfh/S5knFJmuLWrVt4/vnnERsb\ni5dffhmlpaX1MEhdhAAAfIY/cCA3R/XRTgc1wvbt21FTUyNbQRkTE4OFCxdi+fLlaNOmTe0O9U36\nSmUlVq5ciS1btmjcPxIPPfQQGGOWu4FkOoJWV1fjiy++wNtvv23SkkhKSoJKpUJsbCz81L5gszGA\nzEwD/78kAN26dUNUVBQmL1wILwDvvPQSbn33HbeqHngAR9PTsc7NDV5bt2raGBikgublAQEBuKG+\n8RpzAQG1cQDtdNBz587h1q1bBgIgZfjkS5k+WgJQWFiINwEoQ0KAhQtrn9SlC1zOn0dwcDBSUlJk\nu4BaTE4O2hPhgvT/soL9+/ejX79+cHd3R7t27fD7779DoVDgTGVlnQSgqqoKV9LT4S3Fa8xQ50wg\nY3h4cKH96y/+Y46dO/kNdtYs4Oef7bcamj1YsoRPFlet4paxidRkbYgIGzZsQOfOnfHxxx/jySef\nRFJSErzk1rW2M0IArl3jQSV9948aatcOl9zdsfsf/0Dbtm0RExODIUOGYNy4cZg5cyaef/55fPbZ\nZ2jfvr2On1mb//73v5g8ebLuxpwcwNcXPfr0wauvvor8/HxMmDBB55DQ0FAMHDhQE18wi0xH0LS0\nNPhUV+Nqeromy0cO7Zx8hUIBf39/oxbAggUL8MUXX9QuZKJFYmIi2rVrB19fXwCAs9rFEUqEqh9/\n5L5RX18kJCTgQGwsz4T6/HMAMEwFNdMGQhu5QLAUADZmAeRIXzAtAWDnz2MIgMo5c3R7NHXpAuTm\nom9kpMYFpPH/L14MzJtnuc9XPa7z0kzRQvLz85GQkKAJ0gM8CD1mzBjEl5dzAbDS75yeno4WUqaZ\nBQJg10wgiSee4CnD6mC7UVQqPhEbMYKnkVZUANu3228ctpCTA7zzDi9ye+ABHpex0AJ49913MWnS\nJISEhOCvv/7C8uXLEWTlZ6OuCAGQArzqQJ8+V69exS8VFRgE4L4hQxAeHo6amhqcP38eu3fvxpdf\nfomEhATMmDHDuopQdRuIuXPnoqyszMD9IzF27FicOXPGsswLGQsg8cwZHAOwDMDGjRuNPjUhIQG+\nvr5or57RBwQEGBWA9evXY9WKFcDVqwYWQFJSEu68887aDeo6h3fvvhtBZWW4dNddUCqVOHPmDEL6\n9+cBsy8DZn2BAAAgAElEQVS/BEpLERUVheLiYtyUBEyrEZynpyd85Cw0NcHBwQgLC9OJAxw9ehR+\nfn6aALOEJAB5lZVcNLUEIPLQISgBuM6apXsBdWxnUHAwTp48iVu3bnEBUCqBFSt4+/Cvv5Yf3PXr\nvLHgsWP8Jv3bb6gGkK4WSUuR/P/6aw1ERUXhdEUFnw1b2RQuJSUFmtu+hRaA3TKBJLy9gTlzeHzI\n1CI9J0/yz/b99/MEjbCwxuMGeu01oLQUUK8gh7g4iy2AdevW4e6778bx48fR15a2IHVACMCuXdwH\n2a2b7O6jR49iHwDPmhp8M2cOtm3bhkOHDiExMRHXrl1DWVkZKisr8corr1h3XXUR2MSJExEUFIRx\n48bpuH8kxo4dCwCWWQEuLnwmpSUAN/ftQySASU5O+OWnn4y6geLj4xETE6MRscDAQFkXkFKpxM2b\nN5F/5gyfkWlZAFVVVUhNTZUVgAeuXEE1gIX79iEtLQ0VFRU8A2jxYh7Y++47w0wgLQugZcuWZgU2\nLi7OQAB69+6tWUReQnIBFUm1AFI1sFKJO0+fxq8KBVy0WxoAGgHo4eamaQgXFRXFb0q3bvE+OP/3\nf4BWd1MAPGDZtSuvlu7Th3f+/OwzpLq5odTK9XQl/39vvVz9qKgoaJw/VrqBzp07VysAFsQk7J4J\nJPHMM9xtsnSp8WN27eLxmBEj+LGTJ/P6HVvbtl+8WLsuQ11ISQGWLwfmzq3NyOrRg3sXzHRDvX79\nOpKTkzFmzJg6x5NswWYBYIyFMcb2M8bOMsaSGWMLZY4ZzBgrYozFq3+svFvaQHExrzzcts1wX3U1\nsHcvn1EYubkcPXoUR6Qgp5HcY7PtEuRQWwCenp6Ij4/H52o3iD7t27dH586da3O+zaFXDOanXqzD\nXaVCNyMBZZVKpckAkggMDJS1AHLVC7OESUKiZQGcO3cOSqVSVwDUs0pFWhquR0fjf4cO4T11+X9M\nTAxvGta1K/DzzyYFwJT7R6Jnz55IS0vDrVu3UFpaisTERAP3D1BrARjUAuzeDb/SUmyWm5mHhQHe\n3oiSWkhAnQK6fz//47ffuG964sRav/TevfzG36IFtzR37OAz1mXL8EqbNrVrAlvIgQMHMGDAAJ0m\ndYBtApCSkoIu6qC/JRaA3TOBJNq04e/dN98AWoF8HXbu5CIqWbpTpnALbPNm26799NPcBVzXlc1e\nfZVXlWsXtamz/MxZAb+pW87fd999dbu2jdjDAlACWExEXQD0BTCfMSbXIOUPIopV/7xhh+uap6KC\nr126cSOPzuvPfqV0MiPuH4ALQLu4OCA2ltcK2AutNhBt2rSBt7e30UN79eolW+Qki54AdM7MxIWA\nANS0aYNpkHcDXbhwAaWlpRYJQLY6p7+9tEFLACRx0REArUK31gsXIioqCmvWrIGLiws6d+7Mhbd7\ndyA1VZNVoy0AlT4+OH36tEUB1549e4KIcPr0aZw8eRIqlUpWADw9PeHs7FwrAJcv8y//ypUodHPD\nCbkbIWNA584IUc82GWO8Z/6BA1zAunblN/eMDD4T3LyZd5WNiuLFbyNG8L8nTwbmzEGmr69Vzf7y\n8vIM/P8SERERuA6g0tW1ThZAF8nfbIEA1Ecm0M2bN3HlyhVg0SIunnIpkLm53IV2//2122JjeRsM\nPTdQ9rp1uPDf/1p28bIyLuL5+YbWmyVcusT/13Pn6nb2lb5LZr63e/bsQUhIiO53pgGxWQCI6AYR\nnVL/fgtACoDWpp/VACiVfIZw4AAvGjl3jrd10GbXLt6HZdgwI6dQ4uTJk/wmMmwYFwxr+/TLoVLx\nQK2FK4HFxcXh+vXrmkVRTKIlALcyM9GjshLXYmOhmDYN9zGG33780cANJAWAY7Sa3hmLAUgCEA5A\nBfCZsZrExES4uLigk9SiGODdVX19AScnOD/yCN5//30A3JWgsZw6dQIuX4ZrTU1tKmhlJVBair/V\nqZ3PPPOM2ZcuBYJPnjxpNAAM8Ju3n58fdwFFRHBL8PRpYMcO7AkNhbc0I9anSxe4X7gAT09PtG7d\nGu4KBe+AKfnkBw0CXn8dWLcOGD+e9/k5cEDWteLq6mqVBSBZgHIC4OPjg9DQUFz39bVKAFQqFc6d\nO4dwLy/+P5JxQcoRExODw4cP8462dmDixIkYNGgQVHFxwN13A599xr+/2vz6K5/AaQsAY1xQDx7k\n7pabN4FHH0XotGlot2gRKi3p43XwYG0LjbosALV0KR+H/ufT359/tuQsgPR04IMPQC+9hHu2bMFP\n3t5wWrLE+mvbAbvGABhj7QH0ACDXv6AfYyyBMbaLMWZD+0MLIAJmz+ZunyVLgDVreIrnihW6x+3a\nxdM/jQTjkpKSUFZWxm8iQ4fyiuEjR2wfX2Eh/4Bb2EfeVKsDA7Q6gt5YvRoKgH9ppk6Fggg9MjIM\nZm/x8fFQKBQa/y5QGwPQ70UkiVDvli2RxRhIrwYgOjpa0+1SQ1gYv0kGB2Ps2LEYP368puANABcA\nIiA9vTYVVD3T3nHkCEaMGIFevXqZfemhoaFo3bo1Tp06haNHjyI8PFynE6s2Ou0gAH7jViqxwcen\nthGcPl26gN24gf6dO3Pr5fhxHvjTvim/8AIwYQKfdOzdyxfqkcHFxcUqAdi/fz88PDwM/P8SUVFR\nOO/kZJUAXLt2Db6lpehYUmKR/1/isccew9WrV7Fjxw6Ln2OMs2fPYv/+/cjMzMSRI0eA557js2r9\nmNfOnfz7op9pN2UK/+w8+yzQuTNowwa8xRjSANRMnmy0E60GqQV8x47WW/i3bnGX1fjxOhMhDT16\nyFsATz8N/OtfwHvvYUJ5OfplZQHPP2/7qm51gYjs8gPAG8BJAONk9vkC8Fb/fj+A8ybOMxvACQAn\n2rZtS1ajUhEtXkwEEL3ySu32OXOIPDyICgr431ev8mM++MDoqZYtW0YAKD09nai4mMjZmeiFF3QP\nqq4mGjKEaMkSy8eYksKv/cMPFh1eXFxMAOjNN980f/C//83HqVJRWu/elA3QxYwMIpWKqjt1oj8A\nekX7fSGi0aNHU7du3XS2ffzxxwSACgsLdba///77BIAuR0XRHwCdO3dOsy8sLIymTp0q/3qvXjU+\n5lOn+PuxcSM988wz5OPjQ6qEBCKAxgN06NAh869bzZgxY6hz584UFhZGkyZNMnpcz5496f777ydK\nT+fXBoj696cOHToYf97PPxMBdG3jRrp06RLR22/z5+XmWjw+iaFDh9LAgQMtPv7OO++k4cOHG90/\nffp0es/Xl4+npMT0ycrLiX78kXJ79iSl9Nrnz7d4LNXV1dSmTRuT47GUZ599llxdXcnDw4PmzZtH\npFQSRUYStW9PtG8fP0ipJAoIIJo+Xf4kPXrw19C3L/3ywQcEgPp6eVEFY0TDh/PnG6NDB6JRo4j+\n7/+I3N35e2MpS5fy6/71l/x+6fOh/R26coWIMaKXX6YP1WPN+v13ftxnn1l+bRMAOEEW3rftYgEw\nxlwAbAawlogMqpaIqJiIStS/7wTgwhiTrYIhouVE1IuIehmbvZkkPx/YsIH3C3nttdrtTz3F3TfS\nYudm0j8B7v8PCgpCREQEtyB69zYMBK9ezX2I777LLQRLkHz0FloAPj4+6Nixo2UWQHAwty4KCtDq\nzBnsc3ZGu/BwgDE4T5+OgQAOq9+D0tJSPP/889i1axf69euncxpj1cDZ2dnw9PREaHk5LoF3OAV4\nQPXKlSvyvszoaJ12EQZIaZqpqejQoQNu3bqFXPVMtvWdd+Luu+82/7rV9OzZEykpKbhy5Yqs+0dC\n4wJq25ZnlADAk0+isLBQdzEYbdSZQHcUFqJt27b8/969e21Q0gqscQHdvHkTiYmJBumf2kRGRuK4\n1LXWVGvo4mIeeJ8yBW4ZGXgXwM3DhzW1GJbg7OyMuXPn4rffftPtSmolpaWlWL16NSZMmIAxY8Zg\n48aNqFapeCEVY9ztOmECj+EVFOi6f7RZvpxb+X/+iR9On0ZwcDAmvPEGnibibp133pF/XkYGcP48\nvwcMG8ZjhjIW/rZt2/CqftdSlYp7F/r2Nd7RVW2tXPvlF+yTrIsffuCS+8QT2Pvbb+jSpQtChwzh\nn6P16y141+yMpUph7AcAA/A9gP+aOKYlAKb+vTeAy9Lfpn569uxZNwnMziaqqTHc3qMHUUwMtxLG\njSNq04b/boSuXbvSqFGjajf85z9ETk61il5RQdS2LVFQEFfwn36ybHybNvHjT5+2+CVNnjyZLLKI\nvv+en1v9+J8OHWr3XbhABNC/APr8888pPDycANDs2bMNZvrbtm0jAHTy5Emd7VOnTqUO4eGkcnam\nj9zcaPbs2URE9McffxAA2rFjh8WvSYfWrYkef5x++eUXAkBLBw8mAujwl19adZqff/6ZABAAOnz4\nsNHjxo0bR127duV/tG1L5OVFVFxMrq6u9M9//lP+SUoltyIXLeL/ew8PogULrBqfxJgxYyg2Ntai\nYzdt2mT29axbt466SrP5devkD6qsJBo2jEihIPrxR5o3Zw75+/uTysR3wBhZWVnk4uJCC+r4+omI\nvvnmGwJAf/zxh+bztnPnTr6zrIzojTf4ewzwMefnmzxfVVUV+fn50ZNPPkn5+fnk7uZGf3fowL+z\nv/9u+ITPP+fnTkvjFr5CQfTiiwaHjR8/ngBQampq7cbt2/lz1683PqAbN4gA+r5nT1IoFBR/+jRR\ndDTRwIFUXl5O7u7utHDhQn7su+/y82VmmnvbzIIGtgAGAHgMwFCtNM/7GWNzGWNz1ceMB5DEGEsA\n8BmAyeqB1g8hIbWzOm2eeoq3af3rLz4zGDXKaPpncXExzp49qzuLHDqUK7+Ukrl8Oc8gWbuWZ8N8\n9ZVl47PSAgD4zPby5cu1RVLGUFtNtHo1agDc0p7Zh4ejqlcvTAPwzDPPwMXFBQcPHsTXX39t4Pc2\n1hAuOzsbXQMCwJRKOEdFcb8tjGQAWUOnThoLAADOHDgAAOj3wANWnUaKl+h3ANVHEwMAgMcfB156\nCRVqv7zRGIBCwa2Zs2d5Rkp5uXz/KAtwdXW1OAvowIED8PT0NBkHiYqKQjoAMhYHUKmAJ5/kfu5v\nvgEmT0ZKaio6d+5s9ZKWAI+3TJgwAatWrUJJSYlmu1KpxKxZszB37lwTz+Z89dVX6NatGwYMGICR\nI0fC398f69at4zs9PID//Ifn2E+dyrNs1FbplStXZNOZDx06hKKiIjz44IMICAjA5ClT8ND166iJ\nigIee4w3/tNm924eqI2K4hZ+376ygWCpHmbZsmW1Gz/9lPv9Zfp/aWjZEmjVCgEXL6KmpgafTJnC\n/zfTp+PPP/9ERUVFbfrnpEn88aefzL5vdsVSpXDET50tAGMUFhJ5elJx+/ZcbbduNXrovn37CADt\n2rWrdmNFBfcTLlzI/awhIUSDB3MrQlLws2fNj+P11/mxlZUWD10az549e0wfeOIEEUAqxugQQEuX\nLtXdr571fDlvHpWb8HcmJiYSAPpJz6rp1q0bvThwIBFAax57jBhjVFhYSPPmzSM/P786zSaJiGje\nPCJ/f6qqrCSFQkEvSLPZsjKrT9WqVSsy99lZtGgReXt762y7ceMGAaAvvvjC+BOnTSMKC+OzU8aI\n8vKsHh8Rt+g6aFtnJrj77rvNxgvy8/MJAOUHBRFNnGh4wD/+wd/Pt9/WbAoNDaUZM2ZYNW5tDh8+\nTADoq6++IiIipVJJ06ZNIwDk4uJCpaWlRp97/PhxgtoSlZg1axZ5e3ubfB4R0YQJE8jX15eys7N1\nti9YsIDc3d2pRB0D+fvvvwkAbX3+ef7aX3659uDyciJPT93YxyuvcGtBihOq6dGjBwGggIAAKisr\nI4qP5+d7/32T4yQiUt1/PyU5OVHHjh3pc4CqnZ2JCgvpn//8J7m4uGjGSkREffpwL4WNoKFjALcN\nfn64NWoUfDIzUQXg8dWrjfow5RYSgZsbzxr6/Xee/pWTA7z9NrcinnySNzqzxArIzeVpYlYUkEmz\nWbmWxzqoLQBGhJ2QmZFPmAAoFJh38iTcV67kKWkymLIAotTjDh8yBESEo0ePIjExEd26davTbBIA\nz8IoLIRLURGioqLQMSgI5OHBZ4JW8sknn+DNN980eYy/vz9KSkp0etroLAZjjC5deJvf7dv5kqHq\n98larLEA8vPzEWLGWgwICEBAQACuensbdq79/HPgww959om662ZBQQGys7N5NlMd6devH3r06IEv\nvvgCKpUK8+bNw9q1azF69GhUV1fjsH7atRZfffUVvLy88Nhjj2m2TZkyBSUlJWazi9LS0lBcXKzj\nlycibN++HcOHD9c0UevduzdiY2Px2t69oKlT+XsgFf39+SevAdDuATZ8OLeU1NanRH5+PiIiInjX\n3rVrecaOpyf3KJjhVocO6KRS4R9PPYXHXVywmQgX8vKwd+9e9O/fX7fh25QpPG3UxvWdrcJSpXDE\nj90tACL6ZPx4IoDOt21L3t7epFAoaObMmZSVlaVz3EMPPSQ/Q5Nm+r6+RKNH6+6bOpVvN5eFMXEi\nzz6wkvDwcJowYYLpg0pLScpq6Q5QntwMdckSovBwzXEUEUG0caPOIWVlZQSA3nnnHc226upqYozR\nPrV/vig7mxhj9Nprr5Gfnx/NnTvX6tekYedOPpY//qD4+HgqkmI09cSSJUsIAN28eVOz7ejRowSA\nfv75Z+NP3Lq19n1btKjO1581axbdcccdFh3bqlUrmjlzptnj7rrrLtrQrh23UqUY2PHjPCtszBid\nbJgjR44QANq+fXtdhq9B8uOPHDmSANCLL75IxcXF3IrTz5hTk5+fTx4eHvTUU0/pbFcqldSqVSt6\n+OGHjV5PpVKRr68vubq6kpOTEyUlJRERUUJCAgGgFStW6BwvZfKd2r6dz/jHjeM7nnuOyNVV97ta\nWcljQXoZUb6+vrRgwQLq2rkz7Q4M5P/7lSsten/OvPYaEUAXJ00iAughd3fq168fAaC3tawxIiK6\ndo1bla+9ZtG5jQFhAchTUFCA/+zciQNRUYhauhQZGRl49tlnsWbNGowcORJl6r7zpJ7VymaRDB3K\nH4uLgbfe0t03bx7fLvkxjZGba5X/X6Jnz57mM4E8PQFPT+R7euJmq1aambwOCxbwDIi0NG7JVFUZ\n9GDx8PCAh4eHjgUgtYFoVVkJtGwJ35AQdOvWDRs3bkRRUZFt1YxS8Zh6TQLfqqravur1gE4/IDVS\nTMBoFhCgyQQCUGf/P2BdHUBBQYEmK8sUUVFROFlayrNZLl/mPu+pU7kvetUqHsNQI61tbIsFAPBZ\ne0BAAHbv3o2FCxfirbfego+PD3r37o39UpsMPdasWYPy8nLMmzdPZ7tCocDkyZOxc+fO2viMHgUF\nBSguLsbzzz8PX19fLF68GABvyQ4AD+jFjKZOnQpvb28s2byZWz9btnALfvduXrinPQN3deXbtOoB\nlEoliouLERgQgLXBwRiRn49rzz7LLX4LOOPsDABot3UrcMcduO+DDzSZc/fee6/uwXfcAdxzD88G\novoLkWrTrATg66+/RmlZGQI2bQIefBAhISH49NNPsXXrViQkJGD27NkgIly5cgVZWVnyRTc9e/Kb\n96RJteXeEgMGAHfeyd1Apv6BWm0grCEuLg4ZGRlGvxwaunfHz15euLN7d+PHMMbTL595Bhg3Djhx\nwqD6Ur8aWKoCDiop0bSA6Nevn6awzCYBaNeOfwGlFEZ1H6D6QqcfkBqLXEAREXycTk68arWOWOoC\nKi8vR0VFhbyQ6xEVFYUjUmO0c+f4egbp6Tz1UO/5KSkpcHV11XR/rSuenp5YunQp3nrrLXz66aca\nF+CQIUNw/Phx3JLp1//tt9+iV69eskH6KVOmoKqqyugaGBfVLpxevXrhlVdewa+//opdu3Zh+/bt\n6NOnj8GaET4+Pnjsscewfv165Dz2GP/czprFA/lyKeDDh/P37upVALWfjxHHjiHm0CH819kZr6kn\nipZwIjcX+QBYVRXw2GOYO38++vfvj+DgYMRJ/YJ03wB+/QZaU7jZCEBVVRWWLl2K4cOH67Q8AID7\n778fb7zxBtauXYvPPvvMZBsBKBTcT/fdd4b7GONWwOnTPEtEfiB8dlaH1aCkD8xpMw2mlAcPYq41\nM/I+fbg/VG9xbv2OoFlZWQgAEJCZqcnd164f6Gako6pFKBQ8G0PyfzZWAXB25lZAXByP49QRS+sA\npPffEgsgMjISZ6WJx3vvAd9+y5e0vOceg2PPnTuHjh07wlk9Q7WFadOm4aWXXtKJ/wwZMgQ1NTX4\nU92MUCI5ORkJCQl4/PHHZc/Vq1cvREVFYb2RnHhJAMLDwzF//nx06NAB8+fPx/HjxzXLreqzcOFC\nVFZW4qtVq4CPPqqNA8itATJ8OH/ctw8gQsmRI1gKoO+uXcATTyB5+nSsXbfO/CRMTWpaGs5Lfb6m\nT4eTkxN27NiBI0eOyHf/HDeOf8YaqibAUl+RI37sGQNYvXq1YVaPFjU1NfTwww+TQqGgQYMGkaur\nK1VUVFh/oeJiIm9voscfl98v+ZClfGcryMnJIQD00UcfmTzu7NmzBIBWr15t2Ymlathly3Q2Dxo0\niO655x7N36tWraKvAFIpFEQJCURElJqaSgCojT389WPH8jxpIl5bYUtMwQynT58mALRlyxbNtg8/\n/JAAUFFRkeknJyRYlu1lghdffJEUCoXZ45KSkggArTeVb67mzz//JABUKVUE9+lDVFUle2xUVBSN\nHz/e6nFbSmlpKbm4uNA//vEPne0vvPACKRQKgwwebebNm0cBAQGyGWUfqKtnpboVqX4AgCYeIMcD\nDzxAwcHBVF5WxquDO3SQrwGqqSEKDub1Qt26EQFUCdDl++4jqq6mkydPEgBaYmHlf0REBC3t04d3\nIrCUUaOI2rUzWaNkCogYgC5EhI8//hhdu3aVXXQFAJycnLB69Wp06NABhw4dQo8ePQza7lqEjw9f\nrWjDBl69qM/33/Oui/r+PwuQW/REDqtz8iMieDXr33/rbNZ3ATkdP47ZAKrnzeOVi+CreAUFBaG7\nKXeTpXTqxGMTVVX8vXOABcAYM9mZFQB/7Tb6zl1dXVFTU2PQa0kfyQKw1AUEALnBwfxzuG6dznrN\nEpWVlbhw4YLN/n9TeHp6ol+/fvhdq3JepVJh3bp1uPfee01mNUVHR6OgoAC5emtbA9wCCAgI0Fhp\nDz74IIYNG4bo6Gh06SLXhJjz3HPPITc3Fz+sXct7hB05Il8D5OTEK44TEgAfHyTPn49WAK69/jrg\n7Iy4uDj06dMHK/T7islQWVmJzMxM5I4YwRcMspTFi3kXAzs12zNFsxCAffv24cyZM3juuedMpin6\n+vpi27Zt8PX1le26aDFz5vBA3Pff627Py+M94adN42ZeHYiLizMbCE5MTIRCobD8C84YdwMd1e3h\np+MCUioxZONG3GAMrlql9Ywx/PTTT5oe/zbRsSPvzJmQwNPx6tBiwVKMCYCvr6/BAjL1gdQJ1Vwc\nQBJgS1xAISEh8Pb2xpq+fXmbiogI2ePOnz8PlUql6e1fXwwZMgSnT5/WvMdHjhzBpUuXMG3aNJPP\nk8Z1Tqag7eLFi7wNtxrGGLZv347Dhw+b/G4PHjwYsbGx+OSTT0CenqY/W198wVN9jxxBfL9+yIfu\n+z958mQkJSUhMzPT5OvIyMiASqVCx44dTR5nwLBhfJlMO7jnzNEsBODjjz9GaGio2Q8eAHTq1AkX\nLlzAG2/YsGRBjx68b9DXX+sGg3/6id/gjPg/LSEuLk6z6IkxEhMT0aFDB9kVxozSty+vutS6Ieqs\nCfDll2iTk4N3pNmlFsOGDbNPP3MpE0jqx1KPFoC0vKR+FpDJDCA7InVMNRcHsCYGwBhDZGQk/iws\n5MkKRrBXBpA5hgwZApVKpWll/cMPP8DT0xMPP/ywyeeZEoDMzEwdAQC4tWHOQmKMYfHixUhJScGv\nv/5qeuBeXnyBGtQKsPb5JS+CufNINUY67dEbGU1eALZt24bdu3dj0aJFFrt0goKC6rbKlzZz5/Ib\nqnYQ7PvvuftALwhtDXFxcSAikwu8nzlzxvobshTw1loaLyAgAGVlZai4eBF4+WUcDwzEaSOzSrvQ\ngAKgUCjg6+trYAGYDADbEUstAGtcQIC6JYSZ9XpTUlIAwPqZqZX07dsX7u7u2L9/P6qqqrBx40Y8\n9NBDZl1sbdq0gaenp4EAEBEyMzPrnLk0ceJE3HHHHfj4448tfo6cAEdHR6Nt27YWC0B9v8+20KQF\n4ObNm5gzZw5iY2Px3HPPNezFJ00C/PxqfX+pqdzFYsPsH9Bd9ESOvLw8XLx40aL++Tr07s1dQVpu\nIOmmQ4sXA1VVeDUoCKF1yF6ymKAg/qPOk65PAQD0+gHBMQJgzgLIz8/XLGBjCVFRUbio7j1jjJSU\nFLRr1063CrUecHNzQ//+/bF//37s3r0b+fn5FlnhTk5O6NSpk4EAZGVloaKiwsACsBRXV1c8++yz\n+O2333DmzBmLnpOfnw8fHx+dbCnGGEaOHInffvvNpICnpqaiVatW8DWy3khjoEkLwPz581FQUIDV\nq1cbLlJS33h68gZUmzbxlYrWrOEBpqlTbTptq1at0KpVKxw3soi1tN3YwiFG8fPjjc60AsGBgYFo\nB8B92zZg4UIcLygwyLO2Ox07cv8rUO8CoGkJraaxuoD8/PwsjktERUWhqqoKV9V57HIcO3ZMZ/nP\n+mTo0KFISEjAZ599hhYtWli89m10dLSBAGingNaV2bNnw9PTE5988olFxxcUFMhaXyNHjsStW7c0\nRV1ypKamNmr3D9CEBWDDhg3YsGEDXn31VftkqNSFOXN4Rst333EBuO8+wILFzc3Rr18/TRdOfY4d\nOwbGmMZSsAopEKyOWwQGBmIeADg5QTl3LvLy8hBqwbqxNqH9hWkGFoAlQWBL3T+AeqF6wKgb6Nq1\nawuHsQgAABfhSURBVMjIyMA9MrUB9YG0hsG+ffswceJEiydi0dHRyMzMRLnWEqz2EIDAwEBMnDgR\n//vf/0AWVNvm5+fLxl+GDh0KhUKB3SaWnUxLS2vU7h+giQpAdnY2nn76afTq1Qv/+te/HDeQbt14\ndfAbb/DiLxvdPxIDBw7ExYsXce3aNYN9x44dQ5cuXTRBTqvo25dbK+ovWqCHB2YBuNG7N3Ld3UFE\nDScATk7cKqlHHCkA1lgAlgSAJaRUUGMCcPDgQQBoMAG46667NK4mS9w/EtHR0SAivjyoGkkAbK1e\nvuuuuzQLGJnDmAD7+fmhf//+RuMAeXl5yMvLExZAQ0NEmDdvHkpKSrB69Wq7VDraxNy5vCeLjw/w\n0EN2OeXAgQMBwKDbIql7GFnt/pGQAsFqN1DYkSMIAnBm0CBNG4gGE4DAQPk1HeyItguIiFBUVNRg\nLiBrgsDWWACtW7eGm5sbMjIyZPcfOHAAfn5+BtXw9YWLiwuGDh2KqKgog1XnTCGXCXTx4kW0bNkS\nHnXoEKuN9NpNJVJImHr/R44ciVOnTmm+G9rcDhlAQBMUgMLCQqSlpeHNN980WRjSYIwfzwu/pk7l\ncQE7EBsbC09PTwMByMzMxM2bN+suAN268TGq3UCBa9ciEUBKSIhmMfgGiQEA9e7+AXQtgJKSEqhU\nqkYZBLbGAnByckJkZKRJC2DgwIHybQjqiVWrVuHgwYNWtQrv0KEDGGMGAmCL+0dCcgnHx8ebPdbU\n+z9S3Upiz549BvtuFwFw8PTY/gQEBODkyZOOn/lLuLvzHjvmqkutwMXFBX369DHos3JM3X+ozgLg\n7Az06sUtgL//hvOZM/gCQLC6dzzQABZAVBSf+TeAAEgWgDT7l7Y1BPXlAgL4DW7fvn2orq7W8bnf\nuHEDaWlpmDVrlvUDtgFrLBgJDw8PtGvXzkAA+vfvb/N4fHx8EBkZadYCICKTMZjY2FgEBwdj9+7d\nOusaAFwAXFxcbHZX1TdNzgIAePpZQ85wzNKiBRcCOzJw4EDEx8frFIQdO3YMbm5uthVl9e0LxMfz\npll+ftipbgfRYALg5sYrWOv7OuAWgEqlQklJiUYAGpMLiIisdgEBvFI1NzcXe/fu1dkuFWTZVOXe\ngGhnAimVSly5csVuN9TY2FizAlBWVobq6mqj77+TkxNGjBiBPXv2GLT0SEtLQ2RkZOOZiBqhSQpA\nc2DAgAFQqVSazqUAF4C4uDjbUl779OGZS1u2ADNmwC0oCAUFBcjKyoKXl5f5Pjn2YN063s2yntFu\nByG5ghqTC0hascxaC2DUqFEICgrCmjVrdLYfPHgQPj4+JtdKbkxER0cjNTUVKpUKV69eRU1NjV1c\nQACPA6Snp5usqLekDcfIkSNx8+ZNg/Yst0MKKCAE4LalX79+cHJy0riBlEolTp48WXf3j4R2C+z5\n8zUN4bKzs+t/9i9x1121sYB6RLrZFxYWNkoXkDVtILRxdXXFpEmTsG3bNhQXF2u2Hzx4EAMGDGj0\ns1KJ6OholJWV4dq1a3ZJAdVGqoOQW1xeQq4NhD733XcfGGM66aA1NTVIT08XAiCoP3x9fdG9e3dN\nIDg5ORnl5eW2C0Dr1twFM3o0EBWl6QfUoALQQEgWQFFRUaN0AVnbBkKbRx99FBUVFZqFVXJycnD2\n7NkGS/+0B9qZQPYWACkTyFQg2JL3X1rYZcOGDZrjMzMzUVVV1XwEgDE2kjGWyhhLZ4z9W2a/G2Ps\nJ/X+o4yx9va4bnNnwIAB+Ouvv6BUKm0PAGtz8CBfRQq1HUGbsgA0VheQNZ1A9enbty+ioqI0biDJ\n/387C4CTkxPCwsLscu6wsDD4+/ubjANY+v4/99xzSElJQdeuXbFz587bJgMIsIMAMMYUAL4AMApA\nFwBTGGP6+ZczARQQURSATwG8b+t1BTwQXFpaioSEBBw7dgwBAQGaSlCbaNNGs9qVZAFkZWXVfwpo\nA9NUXUAA71fz6KOPYv/+/bh69SoOHjwIT09P63tEOZCQkBD4+/trBCAsLMxuLV0YY4iNjbXZAgD4\nusNHjx5FUFAQRo8ejUWLFgFo3E3gJOxhAfQGkE5EF4ioCsB6APoVTw8BWK3+fROAYcyapGCBLAMG\nDADAC8KOHTuG3r17W5VrbQkBAQEoKChomDYQDYy+C8jFxcXmIiNLscQFZIkP2hTTpk0DEWHdunUa\n/3+D98SyAcaYJhPIXjUA2sTExCAxMdFo4zxrLLC4uDicOHECL774ItLT0xEUFIQW9biehb2whwC0\nBqBdU31VvU32GCJSAigCUP+J3k2csLAwTVvapKQk+7h/9AgMDNQsH9fUBEDbAigsLISfn5/dBdQY\nlriAbLEAAGiqb5ctW4bExMTbyv0jUZ8CEBsbi/LycqNFc/n5+XB2drY4883NzQ1vv/02Tp48iW3b\ntjXYZ8kWGl0QmDE2mzF2gjF2Qm5JOIEuAwcOxK5du6BSqepNACSamgvIzc0N7u7uGhdQQ7l/AMtd\nQAqFwqbU28cee0wTQL1dBeD69eu4ceNGvVgAgPFAsFSDYe2NPDY2VtOupbFjDwG4BkA7MtNGvU32\nGMaYMwA/AHlyJyOi5UTUi4h6BQcH22F4TZsBAwZouhrWhwBozz6bmgUAcDeQ5AJqqAwgwHIXUF1u\nQNpIHTjd3d1x11131fk8jkJ72Up7V9V26dIFzs7ORgPB1rbhuB2xR0LwcQAdGGPh4Df6yQD0m95v\nBzAdwF8AxgP4nSzpxSowizTTaN++vcmFtuuKtgXQVAVA2wXUUFhqAdh6AwoKCsL06dNRXl5u8Yp4\njQltAbC3BeDm5obOnTsbtQCsbcV9O2KzABCRkjH2DIBfASgAfEtEyYyxNwCcIKLtAFYCWMMYSweQ\nDy4SAjvQtWtXBAQEoG/fvvVy/qYuAH5+fhoXUEO+PkvrAOwxA12xYoXN53AUERERcHZ2hlKptLsA\nANxds2/fPtl9BQUFaGWH9TsaM3YpCSSinQB26m17Rev3CgAT7HEtgS4KhQJ79+6tt5uXdANqsDYQ\nDYy/vz8KCgoa3AWkUCjg5ORktg6gPqy62wkXFxfNMpf1cTOOiYnBmjVrkJubC32Xc35+fuPoKFyP\nNLogsMB6evbsiTZt2tTLuSUBaIqzf8BxLiCA39zq2wXUFIiJiUGnTp0sXhbTGqSWEHJxgLo04rvd\nEAIgMIm7uzs8PT2brAD4+fkhLy8PJSUlDS4Arq6uZl1ATf0GZAmff/45duzYUS/nNpYJpFQqUVRU\n1OTf/9ujK5TAoQQGBjZZAfD390deXp7m94bE1dXVqAWgUqlQWFgoLACgXguqWrRogTvuuMPAApBa\ngzT1918IgMAs7733Htq2bevoYdQL2jf9xuQCkhaqaeo3oMaAXEsIWxrx3U4IARCYxZrFvG83tG/6\njckFZGsbCIHlxMbGYs+ePaioqIC7euEmWxrx3U6IGICgWaNtATQmF5CtbSAElhMbGwulUomzZ89q\ntjUXARYCIGjWNFYXkBCAhkPKBNJ2AzUXF5AQAEGzRriABJGRkfDy8tIRAOECEgiaAcIFJHByckJM\nTIwQAIGguSFcQAKgNhNIpVIB4O+/j4/PbbV+Ql0QAiBo1kg3fQ8Pjwb/sptzAbm7uzfYAjXNndjY\nWNy6dQuZmZkAmkcnUEAIgKCZ4+XlBYVC0eDuH8C8C6g53IAaC/qB4OZShS0EQNCsYYzB39+/wd0/\nAHcBGbMAhAA0LN26dYNCodAIQHNoBQ0IARAI4Ofn5xABMGUBNJcbUGPBw8MD0dHROgLQHARYCICg\n2dOyZUuHLHdpLgjcHG5AjQntlhDNxQUkWkEImj1r1qxxSLaHuSCw1KlS0DDExsZi7dq1yMvLazYW\nmBAAQbMnIiLCIdc1FwRuDjegxoQUCD5y5AiqqqqahQUmXEACgYMw5gKqrq5GSUlJs7gBNSYki+v3\n338H0DyqsIUACAQOwpgLqLn0oWlsBAcHo3Xr1hoBaA4CLARAIHAQxlxAogrYccTGxuLMmTMAmocA\nCwEQCByEMReQEADHIcUBgOYhADYFgRljHwIYA6AKQAaAGURUKHNcJoBbAGoAKImoly3XFQiaApIL\niIjAGNNsF51AHYe2ADQHAbbVAtgLoBsRdQeQBuAFE8cOIaJYcfMXCDiurq4A+ALk2ggLwHE0NwvA\nJgEgoj1EJH16/wbQxvYhCQTNA6n2QN8N1FxaETdGIiIi4O3tDWdnZ3h7ezt6OPWOPWMATwLYZWQf\nAdjDGDvJGJttx2sKBLctkgWgnwkkLADHIa0NEBAQoOOWa6qYjQEwxn4DIFcn/xIR/U99zEsAlADW\nGjnNQCK6xhgLAbCXMXaOiA4Zud5sALMBoG3btha8BIHg9kQSAH0LoKCgAN7e3k2+F31jZfr06ZpM\noKaOWQEgouGm9jPGngDwAIBhRERGznFN/ZjDGNsKoDcAWQEgouUAlgNAr169ZM8nEDQFpBu8vgXQ\nXBqRNVaeeuopRw+hwbDJBcQYGwngnwAeJKIyI8d4McZ8pN8B3AcgyZbrCgRNAVMWQHMIQAocj60x\ngM8B+IC7deIZY8sAgDF2B2Nsp/qYUAB/MsYSABwD8AsR7bbxugLBbY+xIHBhYaFDFqgRND9sqgMg\noigj268DuF/9+wUAoq2hQKCHsSBwUVER2rdv74ARCZobohJYIHAQxlxAxcXF8PX1dcSQBM0MIQAC\ngYMw5gISAiBoKIQACAQOQs4FREQoLi52yBKVguaHEACBwEHIuYAqKiqgVCqFBSBoEIQACAQOQs4F\nVFxcDABCAAQNghAAgcBByLmAhAAIGhIhAAKBg5BzAQkBEDQkQgAEAgch5wIqKioCIARA0DAIARAI\nHIRwAQkcjRAAgcBBmHIBiTRQQUMgBEAgcBBy3UCFBSBoSIQACAQOQgSBBY5GCIBA4CCM1QG4urrC\nzc3NUcMSNCOEAAgEDsJYEFjM/gUNhRAAgcBBODvzbuz6FoAQAEFDIQRAIHAQjDG4uLgY1AEIARA0\nFP/f3t3FyFXXYRz/Pt2dsXWFFgSxtAWqEggXssCmQkQjFUhpDFRjFGIMJiT1AhJImhgIidFLjYhc\nEJKK6I0BIoo0teGlSGL0oqVAwYVSqVhDy0u3TRGxoa8/L+Y/ejLsdtueyfmfnfN8ksnOOWc658me\ntk//v5npugDMMmq32x4BWTYuALOM2u32h0ZA/gyAVcUFYJZR7wjIKwCrkgvALCOPgCwnF4BZRpON\ngFwAVpVSBSDpB5J2StqcbsuneNwySVslbZN0e5lzmg2S4gho//79HDhwwAVglRnuw3PcHRE/meqg\npCHgXuAqYAfwrKQ1EfFKH85tNqMVR0D+byCsalWMgJYA2yLi9Yg4ADwEXFfBec1qrzgC8s8CsKr1\nowBukfSSpAcknTLJ8QXAG4XtHWmfWeO1Wq0PrQD8NlCryrQFIGm9pPFJbtcB9wGfBkaBt4C7ygaS\ntFLSJkmbJiYmyj6dWa0VVwAeAVnVpn0NICKuPJYnkvRzYO0kh3YCiwrbC9O+qc63GlgNMDY2Fsdy\nbrOZqt1us2/fPsAFYNUr+y6g+YXNrwLjkzzsWeBcSYsltYHrgTVlzms2KCYbAbkArCpl3wX0Y0mj\nQADbge8CSDoTuD8ilkfEIUm3AE8AQ8ADEfFyyfOaDQSPgCynUgUQEd+eYv+bwPLC9jpgXZlzmQ2i\n4ucAXABWNX8S2Cyj3s8BDA8PM3v27MyprClcAGYZ9Y6A5s6di6TMqawpXABmGRVHQP5hMFY1F4BZ\nRr0jIBeAVckFYJZR7wjIBWBVcgGYZdT7LiAXgFXJBWCWkUdAlpMLwCyjdrvNkSNHOHz4sAvAKucC\nMMuo1WoBcPDgQf9AeKucC8Aso3a7DcD777/PBx984BWAVcoFYJZRtwD27NkD+L+BsGq5AMwy6o6A\ndu/eDbgArFouALOMuisAF4Dl4AIwy8grAMvJBWCWkV8DsJxcAGYZeQRkObkAzDLqHQH5cwBWJReA\nWUYeAVlOLgCzjLoFMDExwdDQEHPmzMmcyJrEBWCWUXEEdPLJJ/ungVmlXABmGRVHQB7/WNWGy/xi\nSQ8D56XNecC7ETE6yeO2A/8GDgOHImKszHnNBkW3APbu3cuCBQsyp7GmKVUAEfHN7n1JdwH/OsrD\nr4iI3WXOZzZouiOgiPAKwCpXqgC61BlcfgNY2o/nM2uK7goA/BZQq16/XgP4AvBORLw2xfEAnpT0\nnKSVfTqn2YxXLACvAKxq064AJK0HPjnJoTsj4rF0/wbgwaM8zeURsVPSJ4CnJL0aEX+a4nwrgZUA\nZ5111nTxzGa07ggIXABWvWkLICKuPNpxScPA14BLjvIcO9PXXZIeBZYAkxZARKwGVgOMjY3FdPnM\nZjKvACynfoyArgRejYgdkx2UNCLppO594GpgvA/nNZvxvAKwnPpRANfTM/6RdKakdWnzDODPkl4E\nNgJ/iIjH+3BesxnPKwDLqfS7gCLiO5PsexNYnu6/DlxY9jxmg8grAMvJnwQ2y2jWrFkMDQ0Bfhuo\nVc8FYJZZdwzkFYBVzQVglpkLwHJxAZhl1n0dwAVgVXMBmGXmFYDl4gIwy8wFYLm4AMwya7VazJo1\ni5GRkdxRrGFcAGaZtdtt/zQwy8IFYJZZtwDMquYCMMus1Wq5ACyLvvxAGDM7ce12+3+fBjarkgvA\nLLNVq1bljmAN5QIwy2zFihW5I1hD+TUAM7OGcgGYmTWUC8DMrKFcAGZmDeUCMDNrKBeAmVlDuQDM\nzBrKBWBm1lCKiNwZpiRpAvjnCf7y04DdfYzTb85XjvOV43zl1Dnf2RFx+rE8sNYFUIakTRExljvH\nVJyvHOcrx/nKqXu+Y+URkJlZQ7kAzMwaapALYHXuANNwvnKcrxznK6fu+Y7JwL4GYGZmRzfIKwAz\nMzuKgSsAScskbZW0TdLtufMASHpA0i5J44V9p0p6StJr6espmbItkvSMpFckvSzp1prlmy1po6QX\nU74fpv2LJW1I1/lhSe0c+Qo5hyS9IGltTfNtl/RXSZslbUr7anGNU5Z5kh6R9KqkLZIuq0s+Seel\n71v39p6k2+qSr4yBKgBJQ8C9wDXABcANki7ImwqAXwHLevbdDjwdEecCT6ftHA4BqyLiAuBS4Ob0\nPatLvv3A0oi4EBgFlkm6FPgRcHdEfAbYC9yUKV/XrcCWwnbd8gFcERGjhbcv1uUaA9wDPB4R5wMX\n0vle1iJfRGxN37dR4BJgH/BoXfKVEhEDcwMuA54obN8B3JE7V8pyDjBe2N4KzE/35wNbc2dMWR4D\nrqpjPuCjwPPA5+h8CGd4suueIddCOn8BLAXWAqpTvpRhO3Baz75aXGNgLvAP0muSdcvXk+lq4C91\nzXe8t4FaAQALgDcK2zvSvjo6IyLeSvffBs7IGQZA0jnARcAGapQvjVc2A7uAp4C/A+9GxKH0kNzX\n+WfA94Ajafvj1CsfQABPSnpO0sq0ry7XeDEwAfwyjdHulzRSo3xF1wMPpvt1zHdcBq0AZqTo/BMi\n69uxJH0M+C1wW0S8VzyWO19EHI7O8nshsAQ4P1eWXpK+AuyKiOdyZ5nG5RFxMZ3x6M2Svlg8mPka\nDwMXA/dFxEXAf+gZp+T+PQiQXse5FvhN77E65DsRg1YAO4FFhe2FaV8dvSNpPkD6uitXEEktOn/5\n/zoifle3fF0R8S7wDJ2RyjxJw+lQzuv8eeBaSduBh+iMge6hPvkAiIid6esuOvPrJdTnGu8AdkTE\nhrT9CJ1CqEu+rmuA5yPinbRdt3zHbdAK4Fng3PQOjDad5dqazJmmsga4Md2/kc7svXKSBPwC2BIR\nPy0cqku+0yXNS/fn0Hl9YgudIvh67nwRcUdELIyIc+j8fvtjRHyrLvkAJI1IOql7n84ce5yaXOOI\neBt4Q9J5adeXgVeoSb6CG/j/+Afql+/45X4Rot83YDnwNzpz4jtz50mZHgTeAg7S+dfOTXTmxE8D\nrwHrgVMzZbucztL1JWBzui2vUb7PAi+kfOPA99P+TwEbgW10luQfqcF1/hKwtm75UpYX0+3l7p+L\nulzjlGUU2JSu8++BU2qWbwTYA8wt7KtNvhO9+ZPAZmYNNWgjIDMzO0YuADOzhnIBmJk1lAvAzKyh\nXABmZg3lAjAzaygXgJlZQ7kAzMwa6r+3gwWQqzCnngAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc9fdc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 2.88979737217 \n",
      "Fixed scheme MAE:  2.04720043766\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 0.6340  Test loss = 1.5766  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 0.6632  Test loss = 1.4422  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 0.6867  Test loss = 0.1478  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 0.6866  Test loss = 1.1957  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 0.4981  Test loss = 2.8128  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 0.6081  Test loss = 0.0752  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 0.6077  Test loss = 1.3357  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 0.6289  Test loss = 0.5108  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 0.5203  Test loss = 2.8789  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 0.6310  Test loss = 2.6396  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 0.7103  Test loss = 2.3282  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 0.7667  Test loss = 1.5815  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 0.6302  Test loss = 3.5570  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 0.7691  Test loss = 3.7470  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 0.8986  Test loss = 3.7070  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 1.0093  Test loss = 5.1636  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 0.6532  Test loss = 0.2457  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 0.6539  Test loss = 0.9141  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 0.6630  Test loss = 0.8536  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 0.6526  Test loss = 0.0256  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 0.5328  Test loss = 2.7302  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 0.6310  Test loss = 2.3829  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 0.6963  Test loss = 1.6848  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 0.7268  Test loss = 1.3812  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 0.6245  Test loss = 0.0853  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 0.6240  Test loss = 1.2110  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 0.6405  Test loss = 1.7765  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 0.6768  Test loss = 1.0802  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 0.5782  Test loss = 0.5062  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 0.5769  Test loss = 0.4135  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 0.5789  Test loss = 3.5643  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 0.7258  Test loss = 0.5760  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 0.6121  Test loss = 1.4089  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 0.6363  Test loss = 0.4358  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 0.6383  Test loss = 1.7090  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 0.6706  Test loss = 3.1419  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 0.7344  Test loss = 0.7314  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 0.7349  Test loss = 0.1340  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 0.7306  Test loss = 0.7624  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 0.7367  Test loss = 3.3115  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 0.7716  Test loss = 1.5766  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 0.7952  Test loss = 2.5974  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 0.8579  Test loss = 3.2030  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 0.9450  Test loss = 13.6829  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 2.0196  Test loss = 7.1969  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.2081  Test loss = 2.2365  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.2250  Test loss = 0.8011  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.2271  Test loss = 0.0213  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 1.3723  Test loss = 3.1521  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 1.4269  Test loss = 4.2075  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 1.5193  Test loss = 2.0995  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 1.5409  Test loss = 0.7454  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 1.2745  Test loss = 0.9562  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 1.2757  Test loss = 2.3560  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 1.3082  Test loss = 1.0340  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 1.3133  Test loss = 1.6577  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 1.1843  Test loss = 1.0954  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 1.1883  Test loss = 3.4167  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 1.2616  Test loss = 0.9318  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 1.2668  Test loss = 0.7342  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.1173  Test loss = 1.9932  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.1437  Test loss = 4.0104  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 1.2385  Test loss = 0.0390  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 1.2384  Test loss = 0.1361  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 1.0772  Test loss = 1.4188  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 1.0880  Test loss = 1.6843  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.1073  Test loss = 1.8213  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 1.1296  Test loss = 3.0036  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 1.0948  Test loss = 3.5327  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 1.1783  Test loss = 0.4967  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 1.1780  Test loss = 0.9750  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 1.1836  Test loss = 0.9553  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 1.0228  Test loss = 2.6045  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 1.0710  Test loss = 1.7888  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 1.0920  Test loss = 0.4880  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 1.0937  Test loss = 0.2371  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 0.9505  Test loss = 0.7068  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns5ZxYTcza5m/ATAcjK2SXj0IAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc653c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.68724182198 \n",
      "Updating scheme MAE:  1.88779140247\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_test_fixed.csv\", yhat_test_fixed)\n",
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_update.csv\", yhat_update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
