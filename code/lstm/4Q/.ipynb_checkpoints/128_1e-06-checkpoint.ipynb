{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"4Q/128_units/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 500\n",
    "learning_rate = 1e-6\n",
    "batch_size = 5\n",
    "early_stop_iters = 15\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 128 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag4',\n",
    "                                       'inflation.lag5',\n",
    "                                       'inflation.lag6',\n",
    "                                       'inflation.lag7']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag4',\n",
    "                                   'unemp.lag5',\n",
    "                                   'unemp.lag6',\n",
    "                                   'unemp.lag7']])\n",
    "train_4lag_oil = np.array(train[['oil.lag4',\n",
    "                                 'oil.lag5',\n",
    "                                 'oil.lag6',\n",
    "                                 'oil.lag7']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag4',\n",
    "                                     'inflation.lag5',\n",
    "                                     'inflation.lag6',\n",
    "                                     'inflation.lag7']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag4',\n",
    "                                 'unemp.lag5',\n",
    "                                 'unemp.lag6',\n",
    "                                 'unemp.lag7']])\n",
    "test_4lag_oil = np.array(test[['oil.lag4',\n",
    "                               'oil.lag5',\n",
    "                               'oil.lag6',\n",
    "                               'oil.lag7']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 128 \n",
      "Learning rate = 1e-05 \n",
      "Epochs = 500 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 15 \n",
      "Learning rate = 1e-05\n",
      "Fold: 1  Epoch: 1  Training loss = 3.3436  Validation loss = 3.5044  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 3.3399  Validation loss = 3.4984  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 3.3359  Validation loss = 3.4920  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 3.3322  Validation loss = 3.4861  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 3.3295  Validation loss = 3.4817  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 3.3260  Validation loss = 3.4759  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 3.3228  Validation loss = 3.4707  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 3.3195  Validation loss = 3.4654  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 3.3155  Validation loss = 3.4589  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 3.3118  Validation loss = 3.4531  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 3.3085  Validation loss = 3.4478  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 3.3051  Validation loss = 3.4422  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 3.3015  Validation loss = 3.4366  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 3.2991  Validation loss = 3.4326  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 3.2961  Validation loss = 3.4278  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 3.2931  Validation loss = 3.4228  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 3.2893  Validation loss = 3.4168  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 3.2862  Validation loss = 3.4117  \n",
      "\n",
      "Fold: 1  Epoch: 19  Training loss = 3.2826  Validation loss = 3.4058  \n",
      "\n",
      "Fold: 1  Epoch: 20  Training loss = 3.2798  Validation loss = 3.4013  \n",
      "\n",
      "Fold: 1  Epoch: 21  Training loss = 3.2762  Validation loss = 3.3956  \n",
      "\n",
      "Fold: 1  Epoch: 22  Training loss = 3.2728  Validation loss = 3.3899  \n",
      "\n",
      "Fold: 1  Epoch: 23  Training loss = 3.2697  Validation loss = 3.3847  \n",
      "\n",
      "Fold: 1  Epoch: 24  Training loss = 3.2653  Validation loss = 3.3776  \n",
      "\n",
      "Fold: 1  Epoch: 25  Training loss = 3.2622  Validation loss = 3.3724  \n",
      "\n",
      "Fold: 1  Epoch: 26  Training loss = 3.2589  Validation loss = 3.3669  \n",
      "\n",
      "Fold: 1  Epoch: 27  Training loss = 3.2568  Validation loss = 3.3634  \n",
      "\n",
      "Fold: 1  Epoch: 28  Training loss = 3.2540  Validation loss = 3.3586  \n",
      "\n",
      "Fold: 1  Epoch: 29  Training loss = 3.2512  Validation loss = 3.3538  \n",
      "\n",
      "Fold: 1  Epoch: 30  Training loss = 3.2469  Validation loss = 3.3468  \n",
      "\n",
      "Fold: 1  Epoch: 31  Training loss = 3.2424  Validation loss = 3.3397  \n",
      "\n",
      "Fold: 1  Epoch: 32  Training loss = 3.2392  Validation loss = 3.3341  \n",
      "\n",
      "Fold: 1  Epoch: 33  Training loss = 3.2358  Validation loss = 3.3285  \n",
      "\n",
      "Fold: 1  Epoch: 34  Training loss = 3.2338  Validation loss = 3.3251  \n",
      "\n",
      "Fold: 1  Epoch: 35  Training loss = 3.2317  Validation loss = 3.3215  \n",
      "\n",
      "Fold: 1  Epoch: 36  Training loss = 3.2290  Validation loss = 3.3171  \n",
      "\n",
      "Fold: 1  Epoch: 37  Training loss = 3.2260  Validation loss = 3.3120  \n",
      "\n",
      "Fold: 1  Epoch: 38  Training loss = 3.2227  Validation loss = 3.3065  \n",
      "\n",
      "Fold: 1  Epoch: 39  Training loss = 3.2198  Validation loss = 3.3017  \n",
      "\n",
      "Fold: 1  Epoch: 40  Training loss = 3.2163  Validation loss = 3.2958  \n",
      "\n",
      "Fold: 1  Epoch: 41  Training loss = 3.2140  Validation loss = 3.2919  \n",
      "\n",
      "Fold: 1  Epoch: 42  Training loss = 3.2104  Validation loss = 3.2857  \n",
      "\n",
      "Fold: 1  Epoch: 43  Training loss = 3.2078  Validation loss = 3.2812  \n",
      "\n",
      "Fold: 1  Epoch: 44  Training loss = 3.2047  Validation loss = 3.2759  \n",
      "\n",
      "Fold: 1  Epoch: 45  Training loss = 3.2011  Validation loss = 3.2698  \n",
      "\n",
      "Fold: 1  Epoch: 46  Training loss = 3.1990  Validation loss = 3.2662  \n",
      "\n",
      "Fold: 1  Epoch: 47  Training loss = 3.1965  Validation loss = 3.2615  \n",
      "\n",
      "Fold: 1  Epoch: 48  Training loss = 3.1937  Validation loss = 3.2567  \n",
      "\n",
      "Fold: 1  Epoch: 49  Training loss = 3.1917  Validation loss = 3.2533  \n",
      "\n",
      "Fold: 1  Epoch: 50  Training loss = 3.1889  Validation loss = 3.2486  \n",
      "\n",
      "Fold: 1  Epoch: 51  Training loss = 3.1861  Validation loss = 3.2438  \n",
      "\n",
      "Fold: 1  Epoch: 52  Training loss = 3.1842  Validation loss = 3.2403  \n",
      "\n",
      "Fold: 1  Epoch: 53  Training loss = 3.1814  Validation loss = 3.2356  \n",
      "\n",
      "Fold: 1  Epoch: 54  Training loss = 3.1778  Validation loss = 3.2296  \n",
      "\n",
      "Fold: 1  Epoch: 55  Training loss = 3.1749  Validation loss = 3.2246  \n",
      "\n",
      "Fold: 1  Epoch: 56  Training loss = 3.1726  Validation loss = 3.2204  \n",
      "\n",
      "Fold: 1  Epoch: 57  Training loss = 3.1702  Validation loss = 3.2163  \n",
      "\n",
      "Fold: 1  Epoch: 58  Training loss = 3.1680  Validation loss = 3.2125  \n",
      "\n",
      "Fold: 1  Epoch: 59  Training loss = 3.1650  Validation loss = 3.2074  \n",
      "\n",
      "Fold: 1  Epoch: 60  Training loss = 3.1619  Validation loss = 3.2020  \n",
      "\n",
      "Fold: 1  Epoch: 61  Training loss = 3.1598  Validation loss = 3.1984  \n",
      "\n",
      "Fold: 1  Epoch: 62  Training loss = 3.1582  Validation loss = 3.1955  \n",
      "\n",
      "Fold: 1  Epoch: 63  Training loss = 3.1557  Validation loss = 3.1910  \n",
      "\n",
      "Fold: 1  Epoch: 64  Training loss = 3.1530  Validation loss = 3.1861  \n",
      "\n",
      "Fold: 1  Epoch: 65  Training loss = 3.1508  Validation loss = 3.1823  \n",
      "\n",
      "Fold: 1  Epoch: 66  Training loss = 3.1491  Validation loss = 3.1793  \n",
      "\n",
      "Fold: 1  Epoch: 67  Training loss = 3.1469  Validation loss = 3.1753  \n",
      "\n",
      "Fold: 1  Epoch: 68  Training loss = 3.1451  Validation loss = 3.1721  \n",
      "\n",
      "Fold: 1  Epoch: 69  Training loss = 3.1428  Validation loss = 3.1680  \n",
      "\n",
      "Fold: 1  Epoch: 70  Training loss = 3.1399  Validation loss = 3.1630  \n",
      "\n",
      "Fold: 1  Epoch: 71  Training loss = 3.1373  Validation loss = 3.1585  \n",
      "\n",
      "Fold: 1  Epoch: 72  Training loss = 3.1339  Validation loss = 3.1526  \n",
      "\n",
      "Fold: 1  Epoch: 73  Training loss = 3.1314  Validation loss = 3.1482  \n",
      "\n",
      "Fold: 1  Epoch: 74  Training loss = 3.1293  Validation loss = 3.1444  \n",
      "\n",
      "Fold: 1  Epoch: 75  Training loss = 3.1270  Validation loss = 3.1403  \n",
      "\n",
      "Fold: 1  Epoch: 76  Training loss = 3.1246  Validation loss = 3.1361  \n",
      "\n",
      "Fold: 1  Epoch: 77  Training loss = 3.1222  Validation loss = 3.1318  \n",
      "\n",
      "Fold: 1  Epoch: 78  Training loss = 3.1206  Validation loss = 3.1288  \n",
      "\n",
      "Fold: 1  Epoch: 79  Training loss = 3.1186  Validation loss = 3.1252  \n",
      "\n",
      "Fold: 1  Epoch: 80  Training loss = 3.1159  Validation loss = 3.1203  \n",
      "\n",
      "Fold: 1  Epoch: 81  Training loss = 3.1141  Validation loss = 3.1168  \n",
      "\n",
      "Fold: 1  Epoch: 82  Training loss = 3.1111  Validation loss = 3.1114  \n",
      "\n",
      "Fold: 1  Epoch: 83  Training loss = 3.1096  Validation loss = 3.1085  \n",
      "\n",
      "Fold: 1  Epoch: 84  Training loss = 3.1071  Validation loss = 3.1041  \n",
      "\n",
      "Fold: 1  Epoch: 85  Training loss = 3.1050  Validation loss = 3.1002  \n",
      "\n",
      "Fold: 1  Epoch: 86  Training loss = 3.1006  Validation loss = 3.0927  \n",
      "\n",
      "Fold: 1  Epoch: 87  Training loss = 3.0981  Validation loss = 3.0882  \n",
      "\n",
      "Fold: 1  Epoch: 88  Training loss = 3.0950  Validation loss = 3.0824  \n",
      "\n",
      "Fold: 1  Epoch: 89  Training loss = 3.0930  Validation loss = 3.0787  \n",
      "\n",
      "Fold: 1  Epoch: 90  Training loss = 3.0912  Validation loss = 3.0754  \n",
      "\n",
      "Fold: 1  Epoch: 91  Training loss = 3.0894  Validation loss = 3.0720  \n",
      "\n",
      "Fold: 1  Epoch: 92  Training loss = 3.0875  Validation loss = 3.0686  \n",
      "\n",
      "Fold: 1  Epoch: 93  Training loss = 3.0853  Validation loss = 3.0644  \n",
      "\n",
      "Fold: 1  Epoch: 94  Training loss = 3.0834  Validation loss = 3.0609  \n",
      "\n",
      "Fold: 1  Epoch: 95  Training loss = 3.0809  Validation loss = 3.0565  \n",
      "\n",
      "Fold: 1  Epoch: 96  Training loss = 3.0788  Validation loss = 3.0523  \n",
      "\n",
      "Fold: 1  Epoch: 97  Training loss = 3.0761  Validation loss = 3.0474  \n",
      "\n",
      "Fold: 1  Epoch: 98  Training loss = 3.0746  Validation loss = 3.0445  \n",
      "\n",
      "Fold: 1  Epoch: 99  Training loss = 3.0723  Validation loss = 3.0403  \n",
      "\n",
      "Fold: 1  Epoch: 100  Training loss = 3.0703  Validation loss = 3.0366  \n",
      "\n",
      "Fold: 1  Epoch: 101  Training loss = 3.0674  Validation loss = 3.0314  \n",
      "\n",
      "Fold: 1  Epoch: 102  Training loss = 3.0655  Validation loss = 3.0278  \n",
      "\n",
      "Fold: 1  Epoch: 103  Training loss = 3.0634  Validation loss = 3.0236  \n",
      "\n",
      "Fold: 1  Epoch: 104  Training loss = 3.0601  Validation loss = 3.0175  \n",
      "\n",
      "Fold: 1  Epoch: 105  Training loss = 3.0578  Validation loss = 3.0132  \n",
      "\n",
      "Fold: 1  Epoch: 106  Training loss = 3.0561  Validation loss = 3.0096  \n",
      "\n",
      "Fold: 1  Epoch: 107  Training loss = 3.0533  Validation loss = 3.0045  \n",
      "\n",
      "Fold: 1  Epoch: 108  Training loss = 3.0512  Validation loss = 3.0003  \n",
      "\n",
      "Fold: 1  Epoch: 109  Training loss = 3.0483  Validation loss = 2.9949  \n",
      "\n",
      "Fold: 1  Epoch: 110  Training loss = 3.0465  Validation loss = 2.9916  \n",
      "\n",
      "Fold: 1  Epoch: 111  Training loss = 3.0447  Validation loss = 2.9878  \n",
      "\n",
      "Fold: 1  Epoch: 112  Training loss = 3.0429  Validation loss = 2.9844  \n",
      "\n",
      "Fold: 1  Epoch: 113  Training loss = 3.0408  Validation loss = 2.9803  \n",
      "\n",
      "Fold: 1  Epoch: 114  Training loss = 3.0384  Validation loss = 2.9758  \n",
      "\n",
      "Fold: 1  Epoch: 115  Training loss = 3.0361  Validation loss = 2.9715  \n",
      "\n",
      "Fold: 1  Epoch: 116  Training loss = 3.0329  Validation loss = 2.9657  \n",
      "\n",
      "Fold: 1  Epoch: 117  Training loss = 3.0307  Validation loss = 2.9613  \n",
      "\n",
      "Fold: 1  Epoch: 118  Training loss = 3.0285  Validation loss = 2.9570  \n",
      "\n",
      "Fold: 1  Epoch: 119  Training loss = 3.0267  Validation loss = 2.9535  \n",
      "\n",
      "Fold: 1  Epoch: 120  Training loss = 3.0249  Validation loss = 2.9501  \n",
      "\n",
      "Fold: 1  Epoch: 121  Training loss = 3.0221  Validation loss = 2.9447  \n",
      "\n",
      "Fold: 1  Epoch: 122  Training loss = 3.0198  Validation loss = 2.9401  \n",
      "\n",
      "Fold: 1  Epoch: 123  Training loss = 3.0180  Validation loss = 2.9366  \n",
      "\n",
      "Fold: 1  Epoch: 124  Training loss = 3.0157  Validation loss = 2.9320  \n",
      "\n",
      "Fold: 1  Epoch: 125  Training loss = 3.0131  Validation loss = 2.9270  \n",
      "\n",
      "Fold: 1  Epoch: 126  Training loss = 3.0114  Validation loss = 2.9236  \n",
      "\n",
      "Fold: 1  Epoch: 127  Training loss = 3.0101  Validation loss = 2.9209  \n",
      "\n",
      "Fold: 1  Epoch: 128  Training loss = 3.0083  Validation loss = 2.9175  \n",
      "\n",
      "Fold: 1  Epoch: 129  Training loss = 3.0068  Validation loss = 2.9146  \n",
      "\n",
      "Fold: 1  Epoch: 130  Training loss = 3.0054  Validation loss = 2.9119  \n",
      "\n",
      "Fold: 1  Epoch: 131  Training loss = 3.0039  Validation loss = 2.9088  \n",
      "\n",
      "Fold: 1  Epoch: 132  Training loss = 3.0015  Validation loss = 2.9042  \n",
      "\n",
      "Fold: 1  Epoch: 133  Training loss = 2.9999  Validation loss = 2.9007  \n",
      "\n",
      "Fold: 1  Epoch: 134  Training loss = 2.9984  Validation loss = 2.8979  \n",
      "\n",
      "Fold: 1  Epoch: 135  Training loss = 2.9959  Validation loss = 2.8931  \n",
      "\n",
      "Fold: 1  Epoch: 136  Training loss = 2.9932  Validation loss = 2.8878  \n",
      "\n",
      "Fold: 1  Epoch: 137  Training loss = 2.9914  Validation loss = 2.8843  \n",
      "\n",
      "Fold: 1  Epoch: 138  Training loss = 2.9900  Validation loss = 2.8814  \n",
      "\n",
      "Fold: 1  Epoch: 139  Training loss = 2.9883  Validation loss = 2.8780  \n",
      "\n",
      "Fold: 1  Epoch: 140  Training loss = 2.9866  Validation loss = 2.8744  \n",
      "\n",
      "Fold: 1  Epoch: 141  Training loss = 2.9850  Validation loss = 2.8712  \n",
      "\n",
      "Fold: 1  Epoch: 142  Training loss = 2.9834  Validation loss = 2.8679  \n",
      "\n",
      "Fold: 1  Epoch: 143  Training loss = 2.9811  Validation loss = 2.8629  \n",
      "\n",
      "Fold: 1  Epoch: 144  Training loss = 2.9789  Validation loss = 2.8586  \n",
      "\n",
      "Fold: 1  Epoch: 145  Training loss = 2.9776  Validation loss = 2.8559  \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-18df72026d66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[1;31m# Backprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_ys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[1;31m# Loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\lim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m     \"\"\"\n\u001b[0;32m-> 1588\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\lim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3830\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3831\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3832\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3833\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\lim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\lim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\lim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mc:\\users\\lim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\lim\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    # Load model with lowest validation error for each fold\n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_test_fixed.csv\", yhat_test_fixed)\n",
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_update.csv\", yhat_update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
