{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"4Q/16_units/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 500\n",
    "learning_rate = 1e-3\n",
    "batch_size = 5\n",
    "early_stop_iters = 10\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 16 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag4',\n",
    "                                       'inflation.lag5',\n",
    "                                       'inflation.lag6',\n",
    "                                       'inflation.lag7']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag4',\n",
    "                                   'unemp.lag5',\n",
    "                                   'unemp.lag6',\n",
    "                                   'unemp.lag7']])\n",
    "train_4lag_oil = np.array(train[['oil.lag4',\n",
    "                                 'oil.lag5',\n",
    "                                 'oil.lag6',\n",
    "                                 'oil.lag7']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag4',\n",
    "                                     'inflation.lag5',\n",
    "                                     'inflation.lag6',\n",
    "                                     'inflation.lag7']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag4',\n",
    "                                 'unemp.lag5',\n",
    "                                 'unemp.lag6',\n",
    "                                 'unemp.lag7']])\n",
    "test_4lag_oil = np.array(test[['oil.lag4',\n",
    "                               'oil.lag5',\n",
    "                               'oil.lag6',\n",
    "                               'oil.lag7']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 16 \n",
      "Learning rate = 0.001 \n",
      "Epochs = 500 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 10 \n",
      "Learning rate = 0.001\n",
      "Fold: 1  Epoch: 1  Training loss = 3.1021  Validation loss = 3.2275  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 2.9935  Validation loss = 3.0190  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 2.9140  Validation loss = 2.8585  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 2.8611  Validation loss = 2.7340  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 2.8109  Validation loss = 2.6028  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 2.7662  Validation loss = 2.4861  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 2.7228  Validation loss = 2.3532  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 2.6901  Validation loss = 2.2507  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 2.6794  Validation loss = 2.2145  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 2.6446  Validation loss = 2.0774  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 2.6198  Validation loss = 1.9671  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 2.6031  Validation loss = 1.8805  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 2.6027  Validation loss = 1.8907  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 2.5888  Validation loss = 1.8073  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 2.5866  Validation loss = 1.8225  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 2.5857  Validation loss = 1.8399  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 2.5823  Validation loss = 1.8349  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 2.5778  Validation loss = 1.8363  \n",
      "\n",
      "Fold: 1  Epoch: 19  Training loss = 2.5640  Validation loss = 1.7451  \n",
      "\n",
      "Fold: 1  Epoch: 20  Training loss = 2.5561  Validation loss = 1.7024  \n",
      "\n",
      "Fold: 1  Epoch: 21  Training loss = 2.5491  Validation loss = 1.6338  \n",
      "\n",
      "Fold: 1  Epoch: 22  Training loss = 2.5466  Validation loss = 1.6212  \n",
      "\n",
      "Fold: 1  Epoch: 23  Training loss = 2.5433  Validation loss = 1.6376  \n",
      "\n",
      "Fold: 1  Epoch: 24  Training loss = 2.5418  Validation loss = 1.6661  \n",
      "\n",
      "Fold: 1  Epoch: 25  Training loss = 2.5363  Validation loss = 1.6357  \n",
      "\n",
      "Fold: 1  Epoch: 26  Training loss = 2.5298  Validation loss = 1.5949  \n",
      "\n",
      "Fold: 1  Epoch: 27  Training loss = 2.5256  Validation loss = 1.6083  \n",
      "\n",
      "Fold: 1  Epoch: 28  Training loss = 2.5237  Validation loss = 1.6635  \n",
      "\n",
      "Fold: 1  Epoch: 29  Training loss = 2.5177  Validation loss = 1.6389  \n",
      "\n",
      "Fold: 1  Epoch: 30  Training loss = 2.5158  Validation loss = 1.6458  \n",
      "\n",
      "Fold: 1  Epoch: 31  Training loss = 2.5075  Validation loss = 1.6727  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 26  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.4188  Validation loss = 2.0300  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.4165  Validation loss = 2.0645  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.4140  Validation loss = 2.0695  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.4103  Validation loss = 2.0591  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.4067  Validation loss = 2.0615  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.4062  Validation loss = 2.0800  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.3998  Validation loss = 2.0520  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.3969  Validation loss = 2.0269  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.3910  Validation loss = 2.0219  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.3848  Validation loss = 1.9949  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.3841  Validation loss = 1.9821  \n",
      "\n",
      "Fold: 2  Epoch: 12  Training loss = 2.3832  Validation loss = 1.9313  \n",
      "\n",
      "Fold: 2  Epoch: 13  Training loss = 2.3778  Validation loss = 1.9451  \n",
      "\n",
      "Fold: 2  Epoch: 14  Training loss = 2.3702  Validation loss = 1.9718  \n",
      "\n",
      "Fold: 2  Epoch: 15  Training loss = 2.3650  Validation loss = 1.9879  \n",
      "\n",
      "Fold: 2  Epoch: 16  Training loss = 2.3606  Validation loss = 2.0013  \n",
      "\n",
      "Fold: 2  Epoch: 17  Training loss = 2.3564  Validation loss = 2.0091  \n",
      "\n",
      "Fold: 2  Epoch: 18  Training loss = 2.3519  Validation loss = 1.9901  \n",
      "\n",
      "Fold: 2  Epoch: 19  Training loss = 2.3476  Validation loss = 1.9797  \n",
      "\n",
      "Fold: 2  Epoch: 20  Training loss = 2.3403  Validation loss = 1.9794  \n",
      "\n",
      "Fold: 2  Epoch: 21  Training loss = 2.3319  Validation loss = 1.9958  \n",
      "\n",
      "Fold: 2  Epoch: 22  Training loss = 2.3293  Validation loss = 2.0275  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 12  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.4585  Validation loss = 3.2913  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.4558  Validation loss = 3.2954  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.4531  Validation loss = 3.2779  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.4507  Validation loss = 3.2489  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.4506  Validation loss = 3.2610  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.4499  Validation loss = 3.2268  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.4483  Validation loss = 3.2054  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.4451  Validation loss = 3.2058  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.4446  Validation loss = 3.1647  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.4404  Validation loss = 3.1678  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.4393  Validation loss = 3.1699  \n",
      "\n",
      "Fold: 3  Epoch: 12  Training loss = 1.4374  Validation loss = 3.1978  \n",
      "\n",
      "Fold: 3  Epoch: 13  Training loss = 1.4365  Validation loss = 3.1855  \n",
      "\n",
      "Fold: 3  Epoch: 14  Training loss = 1.4356  Validation loss = 3.1657  \n",
      "\n",
      "Fold: 3  Epoch: 15  Training loss = 1.4348  Validation loss = 3.1572  \n",
      "\n",
      "Fold: 3  Epoch: 16  Training loss = 1.4327  Validation loss = 3.1836  \n",
      "\n",
      "Fold: 3  Epoch: 17  Training loss = 1.4313  Validation loss = 3.1718  \n",
      "\n",
      "Fold: 3  Epoch: 18  Training loss = 1.4304  Validation loss = 3.2171  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 15  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.5466  Validation loss = 4.4380  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.5379  Validation loss = 4.3821  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.5309  Validation loss = 4.3619  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.5247  Validation loss = 4.3281  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.5184  Validation loss = 4.2387  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.5148  Validation loss = 4.2043  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.5112  Validation loss = 4.2127  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.5075  Validation loss = 4.2092  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.5059  Validation loss = 4.2424  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 1.5035  Validation loss = 4.2271  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.5044  Validation loss = 4.2610  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 1.5034  Validation loss = 4.2699  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.5018  Validation loss = 4.2711  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 1.4962  Validation loss = 4.2225  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.4949  Validation loss = 4.2273  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 1.4916  Validation loss = 4.1931  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 1.4894  Validation loss = 4.1767  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 1.4866  Validation loss = 4.1550  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 1.4836  Validation loss = 4.1239  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 1.4801  Validation loss = 4.0781  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 1.4774  Validation loss = 4.0732  \n",
      "\n",
      "Fold: 4  Epoch: 22  Training loss = 1.4745  Validation loss = 4.0754  \n",
      "\n",
      "Fold: 4  Epoch: 23  Training loss = 1.4730  Validation loss = 4.1071  \n",
      "\n",
      "Fold: 4  Epoch: 24  Training loss = 1.4713  Validation loss = 4.0970  \n",
      "\n",
      "Fold: 4  Epoch: 25  Training loss = 1.4682  Validation loss = 4.0346  \n",
      "\n",
      "Fold: 4  Epoch: 26  Training loss = 1.4666  Validation loss = 4.0293  \n",
      "\n",
      "Fold: 4  Epoch: 27  Training loss = 1.4656  Validation loss = 4.0594  \n",
      "\n",
      "Fold: 4  Epoch: 28  Training loss = 1.4648  Validation loss = 4.0813  \n",
      "\n",
      "Fold: 4  Epoch: 29  Training loss = 1.4642  Validation loss = 4.0968  \n",
      "\n",
      "Fold: 4  Epoch: 30  Training loss = 1.4615  Validation loss = 4.0766  \n",
      "\n",
      "Fold: 4  Epoch: 31  Training loss = 1.4616  Validation loss = 4.0971  \n",
      "\n",
      "Fold: 4  Epoch: 32  Training loss = 1.4580  Validation loss = 4.0627  \n",
      "\n",
      "Fold: 4  Epoch: 33  Training loss = 1.4562  Validation loss = 4.0472  \n",
      "\n",
      "Fold: 4  Epoch: 34  Training loss = 1.4544  Validation loss = 3.9639  \n",
      "\n",
      "Fold: 4  Epoch: 35  Training loss = 1.4533  Validation loss = 3.9659  \n",
      "\n",
      "Fold: 4  Epoch: 36  Training loss = 1.4529  Validation loss = 3.9283  \n",
      "\n",
      "Fold: 4  Epoch: 37  Training loss = 1.4507  Validation loss = 3.9378  \n",
      "\n",
      "Fold: 4  Epoch: 38  Training loss = 1.4494  Validation loss = 3.9572  \n",
      "\n",
      "Fold: 4  Epoch: 39  Training loss = 1.4500  Validation loss = 4.0382  \n",
      "\n",
      "Fold: 4  Epoch: 40  Training loss = 1.4478  Validation loss = 4.0246  \n",
      "\n",
      "Fold: 4  Epoch: 41  Training loss = 1.4453  Validation loss = 3.9787  \n",
      "\n",
      "Fold: 4  Epoch: 42  Training loss = 1.4434  Validation loss = 3.9554  \n",
      "\n",
      "Fold: 4  Epoch: 43  Training loss = 1.4423  Validation loss = 3.9295  \n",
      "\n",
      "Fold: 4  Epoch: 44  Training loss = 1.4401  Validation loss = 3.9392  \n",
      "\n",
      "Fold: 4  Epoch: 45  Training loss = 1.4394  Validation loss = 3.9050  \n",
      "\n",
      "Fold: 4  Epoch: 46  Training loss = 1.4378  Validation loss = 3.9109  \n",
      "\n",
      "Fold: 4  Epoch: 47  Training loss = 1.4361  Validation loss = 3.9302  \n",
      "\n",
      "Fold: 4  Epoch: 48  Training loss = 1.4354  Validation loss = 3.9382  \n",
      "\n",
      "Fold: 4  Epoch: 49  Training loss = 1.4368  Validation loss = 4.0029  \n",
      "\n",
      "Fold: 4  Epoch: 50  Training loss = 1.4343  Validation loss = 3.9858  \n",
      "\n",
      "Fold: 4  Epoch: 51  Training loss = 1.4349  Validation loss = 4.0131  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 45  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.6869  Validation loss = 4.3405  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.6617  Validation loss = 4.2316  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.6519  Validation loss = 4.1803  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.6436  Validation loss = 4.1371  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.6376  Validation loss = 4.1143  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 1.6365  Validation loss = 4.1357  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 1.6318  Validation loss = 4.1393  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.6213  Validation loss = 4.0733  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 1.6128  Validation loss = 4.0095  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.6100  Validation loss = 4.0437  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 1.6047  Validation loss = 4.0177  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 1.5995  Validation loss = 3.9999  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 1.5967  Validation loss = 4.0228  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.5910  Validation loss = 4.0324  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 1.5834  Validation loss = 3.9409  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 1.5811  Validation loss = 3.9705  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 1.5769  Validation loss = 3.9553  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 1.5673  Validation loss = 3.8455  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 1.5664  Validation loss = 3.8815  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 1.5650  Validation loss = 3.8995  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 1.5591  Validation loss = 3.8851  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 1.5522  Validation loss = 3.7930  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 1.5446  Validation loss = 3.7580  \n",
      "\n",
      "Fold: 5  Epoch: 24  Training loss = 1.5405  Validation loss = 3.7889  \n",
      "\n",
      "Fold: 5  Epoch: 25  Training loss = 1.5359  Validation loss = 3.8202  \n",
      "\n",
      "Fold: 5  Epoch: 26  Training loss = 1.5295  Validation loss = 3.8041  \n",
      "\n",
      "Fold: 5  Epoch: 27  Training loss = 1.5231  Validation loss = 3.7576  \n",
      "\n",
      "Fold: 5  Epoch: 28  Training loss = 1.5157  Validation loss = 3.7349  \n",
      "\n",
      "Fold: 5  Epoch: 29  Training loss = 1.5147  Validation loss = 3.8476  \n",
      "\n",
      "Fold: 5  Epoch: 30  Training loss = 1.5135  Validation loss = 3.8776  \n",
      "\n",
      "Fold: 5  Epoch: 31  Training loss = 1.5092  Validation loss = 3.8415  \n",
      "\n",
      "Fold: 5  Epoch: 32  Training loss = 1.5026  Validation loss = 3.8070  \n",
      "\n",
      "Fold: 5  Epoch: 33  Training loss = 1.4946  Validation loss = 3.7236  \n",
      "\n",
      "Fold: 5  Epoch: 34  Training loss = 1.4902  Validation loss = 3.7374  \n",
      "\n",
      "Fold: 5  Epoch: 35  Training loss = 1.4865  Validation loss = 3.7071  \n",
      "\n",
      "Fold: 5  Epoch: 36  Training loss = 1.4803  Validation loss = 3.6623  \n",
      "\n",
      "Fold: 5  Epoch: 37  Training loss = 1.4758  Validation loss = 3.6642  \n",
      "\n",
      "Fold: 5  Epoch: 38  Training loss = 1.4721  Validation loss = 3.6978  \n",
      "\n",
      "Fold: 5  Epoch: 39  Training loss = 1.4664  Validation loss = 3.6272  \n",
      "\n",
      "Fold: 5  Epoch: 40  Training loss = 1.4627  Validation loss = 3.6797  \n",
      "\n",
      "Fold: 5  Epoch: 41  Training loss = 1.4576  Validation loss = 3.6807  \n",
      "\n",
      "Fold: 5  Epoch: 42  Training loss = 1.4525  Validation loss = 3.6129  \n",
      "\n",
      "Fold: 5  Epoch: 43  Training loss = 1.4473  Validation loss = 3.5093  \n",
      "\n",
      "Fold: 5  Epoch: 44  Training loss = 1.4455  Validation loss = 3.4188  \n",
      "\n",
      "Fold: 5  Epoch: 45  Training loss = 1.4408  Validation loss = 3.3173  \n",
      "\n",
      "Fold: 5  Epoch: 46  Training loss = 1.4342  Validation loss = 3.3693  \n",
      "\n",
      "Fold: 5  Epoch: 47  Training loss = 1.4303  Validation loss = 3.4005  \n",
      "\n",
      "Fold: 5  Epoch: 48  Training loss = 1.4274  Validation loss = 3.4117  \n",
      "\n",
      "Fold: 5  Epoch: 49  Training loss = 1.4206  Validation loss = 3.3972  \n",
      "\n",
      "Fold: 5  Epoch: 50  Training loss = 1.4192  Validation loss = 3.2778  \n",
      "\n",
      "Fold: 5  Epoch: 51  Training loss = 1.4147  Validation loss = 3.2802  \n",
      "\n",
      "Fold: 5  Epoch: 52  Training loss = 1.4097  Validation loss = 3.3026  \n",
      "\n",
      "Fold: 5  Epoch: 53  Training loss = 1.4034  Validation loss = 3.3238  \n",
      "\n",
      "Fold: 5  Epoch: 54  Training loss = 1.3995  Validation loss = 3.3401  \n",
      "\n",
      "Fold: 5  Epoch: 55  Training loss = 1.4004  Validation loss = 3.3682  \n",
      "\n",
      "Fold: 5  Epoch: 56  Training loss = 1.3935  Validation loss = 3.3722  \n",
      "\n",
      "Fold: 5  Epoch: 57  Training loss = 1.3864  Validation loss = 3.3544  \n",
      "\n",
      "Fold: 5  Epoch: 58  Training loss = 1.3789  Validation loss = 3.2565  \n",
      "\n",
      "Fold: 5  Epoch: 59  Training loss = 1.3747  Validation loss = 3.2244  \n",
      "\n",
      "Fold: 5  Epoch: 60  Training loss = 1.3707  Validation loss = 3.2698  \n",
      "\n",
      "Fold: 5  Epoch: 61  Training loss = 1.3702  Validation loss = 3.3366  \n",
      "\n",
      "Fold: 5  Epoch: 62  Training loss = 1.3662  Validation loss = 3.3278  \n",
      "\n",
      "Fold: 5  Epoch: 63  Training loss = 1.3521  Validation loss = 3.1948  \n",
      "\n",
      "Fold: 5  Epoch: 64  Training loss = 1.3455  Validation loss = 3.1603  \n",
      "\n",
      "Fold: 5  Epoch: 65  Training loss = 1.3406  Validation loss = 3.1656  \n",
      "\n",
      "Fold: 5  Epoch: 66  Training loss = 1.3235  Validation loss = 3.0695  \n",
      "\n",
      "Fold: 5  Epoch: 67  Training loss = 1.3116  Validation loss = 3.0087  \n",
      "\n",
      "Fold: 5  Epoch: 68  Training loss = 1.3031  Validation loss = 2.9492  \n",
      "\n",
      "Fold: 5  Epoch: 69  Training loss = 1.2941  Validation loss = 2.9104  \n",
      "\n",
      "Fold: 5  Epoch: 70  Training loss = 1.2875  Validation loss = 2.8987  \n",
      "\n",
      "Fold: 5  Epoch: 71  Training loss = 1.2939  Validation loss = 2.8930  \n",
      "\n",
      "Fold: 5  Epoch: 72  Training loss = 1.2947  Validation loss = 2.9205  \n",
      "\n",
      "Fold: 5  Epoch: 73  Training loss = 1.2805  Validation loss = 2.8692  \n",
      "\n",
      "Fold: 5  Epoch: 74  Training loss = 1.2716  Validation loss = 2.8239  \n",
      "\n",
      "Fold: 5  Epoch: 75  Training loss = 1.2686  Validation loss = 2.8147  \n",
      "\n",
      "Fold: 5  Epoch: 76  Training loss = 1.2645  Validation loss = 2.8019  \n",
      "\n",
      "Fold: 5  Epoch: 77  Training loss = 1.2540  Validation loss = 2.7422  \n",
      "\n",
      "Fold: 5  Epoch: 78  Training loss = 1.2518  Validation loss = 2.7413  \n",
      "\n",
      "Fold: 5  Epoch: 79  Training loss = 1.2441  Validation loss = 2.7008  \n",
      "\n",
      "Fold: 5  Epoch: 80  Training loss = 1.2406  Validation loss = 2.6677  \n",
      "\n",
      "Fold: 5  Epoch: 81  Training loss = 1.2368  Validation loss = 2.5616  \n",
      "\n",
      "Fold: 5  Epoch: 82  Training loss = 1.2313  Validation loss = 2.5587  \n",
      "\n",
      "Fold: 5  Epoch: 83  Training loss = 1.2397  Validation loss = 2.7258  \n",
      "\n",
      "Fold: 5  Epoch: 84  Training loss = 1.2231  Validation loss = 2.6576  \n",
      "\n",
      "Fold: 5  Epoch: 85  Training loss = 1.2199  Validation loss = 2.5301  \n",
      "\n",
      "Fold: 5  Epoch: 86  Training loss = 1.2121  Validation loss = 2.5218  \n",
      "\n",
      "Fold: 5  Epoch: 87  Training loss = 1.2080  Validation loss = 2.4617  \n",
      "\n",
      "Fold: 5  Epoch: 88  Training loss = 1.2041  Validation loss = 2.4818  \n",
      "\n",
      "Fold: 5  Epoch: 89  Training loss = 1.2035  Validation loss = 2.5004  \n",
      "\n",
      "Fold: 5  Epoch: 90  Training loss = 1.2047  Validation loss = 2.3678  \n",
      "\n",
      "Fold: 5  Epoch: 91  Training loss = 1.1954  Validation loss = 2.3836  \n",
      "\n",
      "Fold: 5  Epoch: 92  Training loss = 1.1954  Validation loss = 2.3283  \n",
      "\n",
      "Fold: 5  Epoch: 93  Training loss = 1.1897  Validation loss = 2.3969  \n",
      "\n",
      "Fold: 5  Epoch: 94  Training loss = 1.1833  Validation loss = 2.3305  \n",
      "\n",
      "Fold: 5  Epoch: 95  Training loss = 1.1871  Validation loss = 2.3928  \n",
      "\n",
      "Fold: 5  Epoch: 96  Training loss = 1.1750  Validation loss = 2.2977  \n",
      "\n",
      "Fold: 5  Epoch: 97  Training loss = 1.1756  Validation loss = 2.3430  \n",
      "\n",
      "Fold: 5  Epoch: 98  Training loss = 1.1711  Validation loss = 2.3481  \n",
      "\n",
      "Fold: 5  Epoch: 99  Training loss = 1.1709  Validation loss = 2.3438  \n",
      "\n",
      "Fold: 5  Epoch: 100  Training loss = 1.1638  Validation loss = 2.2655  \n",
      "\n",
      "Fold: 5  Epoch: 101  Training loss = 1.1609  Validation loss = 2.2840  \n",
      "\n",
      "Fold: 5  Epoch: 102  Training loss = 1.1589  Validation loss = 2.2363  \n",
      "\n",
      "Fold: 5  Epoch: 103  Training loss = 1.1600  Validation loss = 2.2105  \n",
      "\n",
      "Fold: 5  Epoch: 104  Training loss = 1.1646  Validation loss = 2.1644  \n",
      "\n",
      "Fold: 5  Epoch: 105  Training loss = 1.1526  Validation loss = 2.2375  \n",
      "\n",
      "Fold: 5  Epoch: 106  Training loss = 1.1477  Validation loss = 2.1778  \n",
      "\n",
      "Fold: 5  Epoch: 107  Training loss = 1.1517  Validation loss = 2.2239  \n",
      "\n",
      "Fold: 5  Epoch: 108  Training loss = 1.1452  Validation loss = 2.1464  \n",
      "\n",
      "Fold: 5  Epoch: 109  Training loss = 1.1466  Validation loss = 2.1943  \n",
      "\n",
      "Fold: 5  Epoch: 110  Training loss = 1.1410  Validation loss = 2.1303  \n",
      "\n",
      "Fold: 5  Epoch: 111  Training loss = 1.1416  Validation loss = 2.0838  \n",
      "\n",
      "Fold: 5  Epoch: 112  Training loss = 1.1398  Validation loss = 2.0685  \n",
      "\n",
      "Fold: 5  Epoch: 113  Training loss = 1.1306  Validation loss = 2.0510  \n",
      "\n",
      "Fold: 5  Epoch: 114  Training loss = 1.1277  Validation loss = 2.0604  \n",
      "\n",
      "Fold: 5  Epoch: 115  Training loss = 1.1256  Validation loss = 2.0552  \n",
      "\n",
      "Fold: 5  Epoch: 116  Training loss = 1.1237  Validation loss = 2.0310  \n",
      "\n",
      "Fold: 5  Epoch: 117  Training loss = 1.1242  Validation loss = 2.0074  \n",
      "\n",
      "Fold: 5  Epoch: 118  Training loss = 1.1300  Validation loss = 1.9648  \n",
      "\n",
      "Fold: 5  Epoch: 119  Training loss = 1.1180  Validation loss = 1.9977  \n",
      "\n",
      "Fold: 5  Epoch: 120  Training loss = 1.1257  Validation loss = 1.9498  \n",
      "\n",
      "Fold: 5  Epoch: 121  Training loss = 1.1165  Validation loss = 2.0021  \n",
      "\n",
      "Fold: 5  Epoch: 122  Training loss = 1.1178  Validation loss = 1.9835  \n",
      "\n",
      "Fold: 5  Epoch: 123  Training loss = 1.1144  Validation loss = 1.9813  \n",
      "\n",
      "Fold: 5  Epoch: 124  Training loss = 1.1106  Validation loss = 1.9424  \n",
      "\n",
      "Fold: 5  Epoch: 125  Training loss = 1.1068  Validation loss = 2.0100  \n",
      "\n",
      "Fold: 5  Epoch: 126  Training loss = 1.1019  Validation loss = 1.9572  \n",
      "\n",
      "Fold: 5  Epoch: 127  Training loss = 1.1100  Validation loss = 2.0189  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 124  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 1.1333  Validation loss = 1.1703  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 1.1243  Validation loss = 1.2095  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 1.1159  Validation loss = 1.2756  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 1.1206  Validation loss = 1.2057  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 1.0969  Validation loss = 1.2728  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 1.0988  Validation loss = 1.2874  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 1.0929  Validation loss = 1.2956  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 1.0878  Validation loss = 1.2655  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 1.0873  Validation loss = 1.3251  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 1.0817  Validation loss = 1.3547  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 1.0751  Validation loss = 1.3068  \n",
      "\n",
      "Fold: 6  Epoch: 12  Training loss = 1.0729  Validation loss = 1.2627  \n",
      "\n",
      "Fold: 6  Epoch: 13  Training loss = 1.0613  Validation loss = 1.2915  \n",
      "\n",
      "Fold: 6  Epoch: 14  Training loss = 1.0648  Validation loss = 1.2503  \n",
      "\n",
      "Fold: 6  Epoch: 15  Training loss = 1.0614  Validation loss = 1.3175  \n",
      "\n",
      "Fold: 6  Epoch: 16  Training loss = 1.0607  Validation loss = 1.2844  \n",
      "\n",
      "Fold: 6  Epoch: 17  Training loss = 1.0600  Validation loss = 1.3651  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 1  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 1.0878  Validation loss = 2.3046  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 1.0880  Validation loss = 2.1939  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 1.0842  Validation loss = 2.2452  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.0892  Validation loss = 2.1239  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 1.0830  Validation loss = 2.3375  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 1.0894  Validation loss = 2.1665  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 1.0944  Validation loss = 2.1309  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 1.0854  Validation loss = 2.4691  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 1.0771  Validation loss = 2.4059  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 1.0718  Validation loss = 2.2675  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 1.0710  Validation loss = 2.2431  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 1.0720  Validation loss = 2.3778  \n",
      "\n",
      "Fold: 7  Epoch: 13  Training loss = 1.0714  Validation loss = 2.3879  \n",
      "\n",
      "Fold: 7  Epoch: 14  Training loss = 1.0743  Validation loss = 2.4153  \n",
      "\n",
      "Fold: 7  Epoch: 15  Training loss = 1.0701  Validation loss = 2.3267  \n",
      "\n",
      "Fold: 7  Epoch: 16  Training loss = 1.0648  Validation loss = 2.2290  \n",
      "\n",
      "Fold: 7  Epoch: 17  Training loss = 1.0699  Validation loss = 2.1288  \n",
      "\n",
      "Fold: 7  Epoch: 18  Training loss = 1.0659  Validation loss = 2.1433  \n",
      "\n",
      "Fold: 7  Epoch: 19  Training loss = 1.0634  Validation loss = 2.1613  \n",
      "\n",
      "Fold: 7  Epoch: 20  Training loss = 1.0584  Validation loss = 2.1829  \n",
      "\n",
      "Fold: 7  Epoch: 21  Training loss = 1.0551  Validation loss = 2.2494  \n",
      "\n",
      "Fold: 7  Epoch: 22  Training loss = 1.0604  Validation loss = 2.1754  \n",
      "\n",
      "Fold: 7  Epoch: 23  Training loss = 1.0588  Validation loss = 2.1854  \n",
      "\n",
      "Fold: 7  Epoch: 24  Training loss = 1.0574  Validation loss = 2.3262  \n",
      "\n",
      "Fold: 7  Epoch: 25  Training loss = 1.0574  Validation loss = 2.1242  \n",
      "\n",
      "Fold: 7  Epoch: 26  Training loss = 1.0481  Validation loss = 2.2281  \n",
      "\n",
      "Fold: 7  Epoch: 27  Training loss = 1.0469  Validation loss = 2.2154  \n",
      "\n",
      "Fold: 7  Epoch: 28  Training loss = 1.0498  Validation loss = 2.1855  \n",
      "\n",
      "Fold: 7  Epoch: 29  Training loss = 1.0619  Validation loss = 2.4411  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 4  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 1.1045  Validation loss = 6.0680  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 1.0829  Validation loss = 6.2429  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 1.0783  Validation loss = 6.1974  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 1.0772  Validation loss = 6.3040  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 1.0844  Validation loss = 6.0007  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 1.0709  Validation loss = 6.2731  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 1.0739  Validation loss = 6.3254  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 1.0696  Validation loss = 6.1312  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 1.0591  Validation loss = 6.3157  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 1.0575  Validation loss = 6.4114  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 1.0664  Validation loss = 6.6136  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 5  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 1.9099  Validation loss = 8.2529  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 1.8161  Validation loss = 8.0331  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 1.7576  Validation loss = 7.6365  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 1.7479  Validation loss = 7.6759  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 1.6223  Validation loss = 7.2272  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 1.6219  Validation loss = 7.2463  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 1.7734  Validation loss = 7.7800  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 1.6113  Validation loss = 7.5976  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 1.5689  Validation loss = 7.4516  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 1.5618  Validation loss = 7.4042  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 1.5574  Validation loss = 7.4358  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 1.5465  Validation loss = 7.4251  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 1.5428  Validation loss = 7.4172  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 1.5369  Validation loss = 7.4173  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 1.5502  Validation loss = 7.3267  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 1.5346  Validation loss = 7.4609  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 1.5331  Validation loss = 7.3020  \n",
      "\n",
      "Fold: 9  Epoch: 18  Training loss = 1.5737  Validation loss = 7.2081  \n",
      "\n",
      "Fold: 9  Epoch: 19  Training loss = 1.5172  Validation loss = 7.2399  \n",
      "\n",
      "Fold: 9  Epoch: 20  Training loss = 1.5110  Validation loss = 7.2522  \n",
      "\n",
      "Fold: 9  Epoch: 21  Training loss = 1.5075  Validation loss = 7.3209  \n",
      "\n",
      "Fold: 9  Epoch: 22  Training loss = 1.5106  Validation loss = 7.2596  \n",
      "\n",
      "Fold: 9  Epoch: 23  Training loss = 1.5402  Validation loss = 7.1054  \n",
      "\n",
      "Fold: 9  Epoch: 24  Training loss = 1.5038  Validation loss = 7.2154  \n",
      "\n",
      "Fold: 9  Epoch: 25  Training loss = 1.5176  Validation loss = 7.3636  \n",
      "\n",
      "Fold: 9  Epoch: 26  Training loss = 1.5008  Validation loss = 7.3607  \n",
      "\n",
      "Fold: 9  Epoch: 27  Training loss = 1.4994  Validation loss = 7.3729  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 23  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 2.3229  Validation loss = 5.5413  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 2.2628  Validation loss = 5.3541  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 2.2173  Validation loss = 5.2678  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 2.2044  Validation loss = 5.2146  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 2.1916  Validation loss = 5.2030  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 2.1917  Validation loss = 5.1959  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 2.1834  Validation loss = 5.1710  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 2.1757  Validation loss = 5.1663  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 2.1636  Validation loss = 5.0861  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 2.1517  Validation loss = 4.9552  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 2.1318  Validation loss = 4.8598  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 2.1338  Validation loss = 4.8103  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 2.1249  Validation loss = 4.8606  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 2.1009  Validation loss = 4.7384  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 2.0935  Validation loss = 4.6878  \n",
      "\n",
      "Fold: 10  Epoch: 16  Training loss = 2.0942  Validation loss = 4.6001  \n",
      "\n",
      "Fold: 10  Epoch: 17  Training loss = 2.0815  Validation loss = 4.5172  \n",
      "\n",
      "Fold: 10  Epoch: 18  Training loss = 2.0936  Validation loss = 4.4918  \n",
      "\n",
      "Fold: 10  Epoch: 19  Training loss = 2.0627  Validation loss = 4.8885  \n",
      "\n",
      "Fold: 10  Epoch: 20  Training loss = 2.0570  Validation loss = 4.4599  \n",
      "\n",
      "Fold: 10  Epoch: 21  Training loss = 2.0425  Validation loss = 4.4042  \n",
      "\n",
      "Fold: 10  Epoch: 22  Training loss = 2.0214  Validation loss = 4.3117  \n",
      "\n",
      "Fold: 10  Epoch: 23  Training loss = 2.0139  Validation loss = 4.4353  \n",
      "\n",
      "Fold: 10  Epoch: 24  Training loss = 2.0244  Validation loss = 4.3136  \n",
      "\n",
      "Fold: 10  Epoch: 25  Training loss = 2.0057  Validation loss = 4.3979  \n",
      "\n",
      "Fold: 10  Epoch: 26  Training loss = 1.9884  Validation loss = 4.3846  \n",
      "\n",
      "Fold: 10  Epoch: 27  Training loss = 1.9888  Validation loss = 4.4937  \n",
      "\n",
      "Fold: 10  Epoch: 28  Training loss = 1.9796  Validation loss = 4.5143  \n",
      "\n",
      "Fold: 10  Epoch: 29  Training loss = 2.0274  Validation loss = 4.3725  \n",
      "\n",
      "Fold: 10  Epoch: 30  Training loss = 2.0268  Validation loss = 4.3764  \n",
      "\n",
      "Fold: 10  Epoch: 31  Training loss = 1.9874  Validation loss = 4.4762  \n",
      "\n",
      "Fold: 10  Epoch: 32  Training loss = 1.9761  Validation loss = 4.4668  \n",
      "\n",
      "Fold: 10  Epoch: 33  Training loss = 1.9965  Validation loss = 4.4323  \n",
      "\n",
      "Fold: 10  Epoch: 34  Training loss = 1.9446  Validation loss = 4.4304  \n",
      "\n",
      "Fold: 10  Epoch: 35  Training loss = 1.9566  Validation loss = 4.4241  \n",
      "\n",
      "Fold: 10  Epoch: 36  Training loss = 1.9845  Validation loss = 4.3898  \n",
      "\n",
      "Fold: 10  Epoch: 37  Training loss = 1.9416  Validation loss = 4.4196  \n",
      "\n",
      "Fold: 10  Epoch: 38  Training loss = 1.9506  Validation loss = 4.3803  \n",
      "\n",
      "Fold: 10  Epoch: 39  Training loss = 1.9198  Validation loss = 4.3674  \n",
      "\n",
      "Fold: 10  Epoch: 40  Training loss = 1.9204  Validation loss = 4.3114  \n",
      "\n",
      "Fold: 10  Epoch: 41  Training loss = 1.9673  Validation loss = 4.8532  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 40  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 2.0977  Validation loss = 2.5797  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 2.0745  Validation loss = 2.6495  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 2.0778  Validation loss = 2.7192  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 2.0460  Validation loss = 2.7196  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 2.0713  Validation loss = 2.8650  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 2.0142  Validation loss = 2.8817  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 1.9815  Validation loss = 2.6368  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 1.9322  Validation loss = 2.7013  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 1.9275  Validation loss = 2.6760  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 1.9102  Validation loss = 2.7289  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 1.9084  Validation loss = 2.5247  \n",
      "\n",
      "Fold: 11  Epoch: 12  Training loss = 1.9296  Validation loss = 2.8861  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 11  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 1.9976  Validation loss = 1.8975  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 1.9312  Validation loss = 1.9560  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 1.9593  Validation loss = 1.9532  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 1.9147  Validation loss = 1.9020  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 1.9110  Validation loss = 1.7663  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 1.9018  Validation loss = 2.0123  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 2.0569  Validation loss = 2.5002  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 1.8197  Validation loss = 1.4482  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 1.8226  Validation loss = 1.4899  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 1.8421  Validation loss = 1.6208  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 1.8168  Validation loss = 1.4765  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 1.7966  Validation loss = 1.6012  \n",
      "\n",
      "Fold: 12  Epoch: 13  Training loss = 1.7863  Validation loss = 1.4452  \n",
      "\n",
      "Fold: 12  Epoch: 14  Training loss = 1.7878  Validation loss = 1.4026  \n",
      "\n",
      "Fold: 12  Epoch: 15  Training loss = 1.8349  Validation loss = 1.2659  \n",
      "\n",
      "Fold: 12  Epoch: 16  Training loss = 1.7991  Validation loss = 1.3093  \n",
      "\n",
      "Fold: 12  Epoch: 17  Training loss = 1.7779  Validation loss = 1.3787  \n",
      "\n",
      "Fold: 12  Epoch: 18  Training loss = 1.7590  Validation loss = 1.3314  \n",
      "\n",
      "Fold: 12  Epoch: 19  Training loss = 1.7830  Validation loss = 1.2892  \n",
      "\n",
      "Fold: 12  Epoch: 20  Training loss = 1.7824  Validation loss = 1.2585  \n",
      "\n",
      "Fold: 12  Epoch: 21  Training loss = 1.7476  Validation loss = 1.2951  \n",
      "\n",
      "Fold: 12  Epoch: 22  Training loss = 1.7960  Validation loss = 2.1182  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 20  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 1.7150  Validation loss = 3.8902  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 1.7491  Validation loss = 3.7924  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 1.7400  Validation loss = 3.7939  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 1.7294  Validation loss = 3.8515  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 1.6832  Validation loss = 4.0207  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 1.6955  Validation loss = 4.0971  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 1.6761  Validation loss = 4.0018  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 1.6716  Validation loss = 3.8657  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 1.7517  Validation loss = 3.9359  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 1.7390  Validation loss = 3.8004  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 1.7288  Validation loss = 3.8227  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 1.6521  Validation loss = 4.0510  \n",
      "\n",
      "Fold: 13  Epoch: 13  Training loss = 1.6700  Validation loss = 4.0177  \n",
      "\n",
      "Fold: 13  Epoch: 14  Training loss = 1.7148  Validation loss = 3.2229  \n",
      "\n",
      "Fold: 13  Epoch: 15  Training loss = 1.6342  Validation loss = 4.0122  \n",
      "\n",
      "Fold: 13  Epoch: 16  Training loss = 1.6538  Validation loss = 3.8459  \n",
      "\n",
      "Fold: 13  Epoch: 17  Training loss = 1.6499  Validation loss = 3.7151  \n",
      "\n",
      "Fold: 13  Epoch: 18  Training loss = 1.6212  Validation loss = 3.8750  \n",
      "\n",
      "Fold: 13  Epoch: 19  Training loss = 1.6179  Validation loss = 3.6849  \n",
      "\n",
      "Fold: 13  Epoch: 20  Training loss = 1.6202  Validation loss = 4.0821  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 14  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 1.8251  Validation loss = 6.5728  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 1.8868  Validation loss = 6.3360  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 1.8457  Validation loss = 6.3856  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 1.8144  Validation loss = 6.7148  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 1.8180  Validation loss = 6.7292  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 1.8289  Validation loss = 6.7118  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 1.8006  Validation loss = 6.8496  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 2.0671  Validation loss = 7.0506  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 1.9216  Validation loss = 7.1487  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 1.7310  Validation loss = 7.1687  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 1.7258  Validation loss = 6.5374  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 1.7970  Validation loss = 7.2991  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 2  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 2.3363  Validation loss = 6.1162  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 2.3965  Validation loss = 6.1818  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 2.3342  Validation loss = 6.3880  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 2.2016  Validation loss = 5.9979  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 2.3121  Validation loss = 6.2634  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 2.2935  Validation loss = 5.6630  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 2.2074  Validation loss = 5.9071  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 2.1426  Validation loss = 6.1004  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 2.1642  Validation loss = 5.9829  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 2.2549  Validation loss = 6.8602  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 2.2263  Validation loss = 6.5739  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 2.1307  Validation loss = 5.2474  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 2.0559  Validation loss = 5.4601  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 2.1463  Validation loss = 6.4737  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 2.1049  Validation loss = 5.8765  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 2.1327  Validation loss = 6.3374  \n",
      "\n",
      "Fold: 15  Epoch: 17  Training loss = 2.1136  Validation loss = 5.4215  \n",
      "\n",
      "Fold: 15  Epoch: 18  Training loss = 2.0916  Validation loss = 6.3784  \n",
      "\n",
      "Fold: 15  Epoch: 19  Training loss = 2.2687  Validation loss = 5.8824  \n",
      "\n",
      "Fold: 15  Epoch: 20  Training loss = 1.9803  Validation loss = 5.9031  \n",
      "\n",
      "Fold: 15  Epoch: 21  Training loss = 1.9661  Validation loss = 5.6617  \n",
      "\n",
      "Fold: 15  Epoch: 22  Training loss = 2.0657  Validation loss = 5.6468  \n",
      "\n",
      "Fold: 15  Epoch: 23  Training loss = 1.9891  Validation loss = 5.8107  \n",
      "\n",
      "Fold: 15  Epoch: 24  Training loss = 2.1823  Validation loss = 6.2917  \n",
      "\n",
      "Fold: 15  Epoch: 25  Training loss = 1.9820  Validation loss = 5.4328  \n",
      "\n",
      "Fold: 15  Epoch: 26  Training loss = 1.8562  Validation loss = 5.7079  \n",
      "\n",
      "Fold: 15  Epoch: 27  Training loss = 2.1329  Validation loss = 6.4482  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 12  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 2.4473  Validation loss = 4.9037  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 2.4447  Validation loss = 4.8456  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 2.3652  Validation loss = 4.6469  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 2.4988  Validation loss = 4.6195  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 2.4483  Validation loss = 3.0879  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 2.5306  Validation loss = 3.0222  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 2.4713  Validation loss = 3.0259  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 2.4265  Validation loss = 3.0273  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 2.3316  Validation loss = 3.0065  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 3.5037  Validation loss = 3.0876  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 2.2044  Validation loss = 2.9501  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 2.2229  Validation loss = 2.9647  \n",
      "\n",
      "Fold: 16  Epoch: 13  Training loss = 2.2645  Validation loss = 2.9476  \n",
      "\n",
      "Fold: 16  Epoch: 14  Training loss = 2.2268  Validation loss = 2.9923  \n",
      "\n",
      "Fold: 16  Epoch: 15  Training loss = 2.3339  Validation loss = 2.9838  \n",
      "\n",
      "Fold: 16  Epoch: 16  Training loss = 2.2015  Validation loss = 3.0413  \n",
      "\n",
      "Fold: 16  Epoch: 17  Training loss = 2.1517  Validation loss = 2.9589  \n",
      "\n",
      "Fold: 16  Epoch: 18  Training loss = 2.1038  Validation loss = 2.9960  \n",
      "\n",
      "Fold: 16  Epoch: 19  Training loss = 2.0929  Validation loss = 2.9310  \n",
      "\n",
      "Fold: 16  Epoch: 20  Training loss = 2.0743  Validation loss = 2.9257  \n",
      "\n",
      "Fold: 16  Epoch: 21  Training loss = 2.1768  Validation loss = 2.9605  \n",
      "\n",
      "Fold: 16  Epoch: 22  Training loss = 2.5878  Validation loss = 2.9782  \n",
      "\n",
      "Fold: 16  Epoch: 23  Training loss = 2.1308  Validation loss = 2.9713  \n",
      "\n",
      "Fold: 16  Epoch: 24  Training loss = 2.0372  Validation loss = 2.8949  \n",
      "\n",
      "Fold: 16  Epoch: 25  Training loss = 2.0674  Validation loss = 2.9731  \n",
      "\n",
      "Fold: 16  Epoch: 26  Training loss = 2.1121  Validation loss = 2.9713  \n",
      "\n",
      "Fold: 16  Epoch: 27  Training loss = 2.0167  Validation loss = 2.9405  \n",
      "\n",
      "Fold: 16  Epoch: 28  Training loss = 2.0934  Validation loss = 2.9604  \n",
      "\n",
      "Fold: 16  Epoch: 29  Training loss = 2.6165  Validation loss = 2.9805  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 24  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 2.2098  Validation loss = 5.7724  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 2.3836  Validation loss = 5.7665  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 2.1168  Validation loss = 5.7429  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 2.1154  Validation loss = 5.8111  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 2.1659  Validation loss = 5.5780  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 2.1226  Validation loss = 5.6771  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 2.1564  Validation loss = 5.7066  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 2.0957  Validation loss = 5.7075  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 2.2600  Validation loss = 5.8216  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 2.3543  Validation loss = 5.6586  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 2.3786  Validation loss = 5.6421  \n",
      "\n",
      "Fold: 17  Epoch: 12  Training loss = 2.6161  Validation loss = 5.7591  \n",
      "\n",
      "Fold: 17  Epoch: 13  Training loss = 2.1949  Validation loss = 5.6520  \n",
      "\n",
      "Fold: 17  Epoch: 14  Training loss = 2.1551  Validation loss = 5.5801  \n",
      "\n",
      "Fold: 17  Epoch: 15  Training loss = 2.0473  Validation loss = 5.8175  \n",
      "\n",
      "Fold: 17  Epoch: 16  Training loss = 2.0008  Validation loss = 5.9652  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 5  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 2.3710  Validation loss = 3.8933  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 3.0496  Validation loss = 3.7465  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 2.9837  Validation loss = 3.5075  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 2.6719  Validation loss = 3.0199  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 2.5926  Validation loss = 2.9371  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 2.4104  Validation loss = 2.9369  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 2.3227  Validation loss = 2.8710  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 2.2546  Validation loss = 2.8226  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 2.0997  Validation loss = 2.8266  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 1.9938  Validation loss = 2.8350  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 2.1829  Validation loss = 2.8260  \n",
      "\n",
      "Fold: 18  Epoch: 12  Training loss = 2.1828  Validation loss = 2.8097  \n",
      "\n",
      "Fold: 18  Epoch: 13  Training loss = 2.3106  Validation loss = 2.8012  \n",
      "\n",
      "Fold: 18  Epoch: 14  Training loss = 2.1653  Validation loss = 2.7988  \n",
      "\n",
      "Fold: 18  Epoch: 15  Training loss = 2.2119  Validation loss = 2.8087  \n",
      "\n",
      "Fold: 18  Epoch: 16  Training loss = 2.1386  Validation loss = 2.7945  \n",
      "\n",
      "Fold: 18  Epoch: 17  Training loss = 2.0629  Validation loss = 2.7779  \n",
      "\n",
      "Fold: 18  Epoch: 18  Training loss = 2.0164  Validation loss = 2.7778  \n",
      "\n",
      "Fold: 18  Epoch: 19  Training loss = 2.0020  Validation loss = 2.7815  \n",
      "\n",
      "Fold: 18  Epoch: 20  Training loss = 1.9788  Validation loss = 2.8376  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 18  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 2.2825  Validation loss = 2.1933  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 2.1430  Validation loss = 1.8152  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 1.9355  Validation loss = 2.1819  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 1.8864  Validation loss = 2.2733  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 1.9465  Validation loss = 2.4021  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 1.9201  Validation loss = 2.6764  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 1.8647  Validation loss = 2.4351  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 2.0500  Validation loss = 2.6309  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 1.8576  Validation loss = 2.3061  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 1.8829  Validation loss = 2.1023  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 1.8241  Validation loss = 1.9831  \n",
      "\n",
      "Fold: 19  Epoch: 12  Training loss = 1.8167  Validation loss = 1.8366  \n",
      "\n",
      "Fold: 19  Epoch: 13  Training loss = 1.9660  Validation loss = 2.1967  \n",
      "\n",
      "Fold: 19  Epoch: 14  Training loss = 1.8693  Validation loss = 1.8998  \n",
      "\n",
      "Fold: 19  Epoch: 15  Training loss = 1.8073  Validation loss = 2.0706  \n",
      "\n",
      "Fold: 19  Epoch: 16  Training loss = 1.9162  Validation loss = 1.8896  \n",
      "\n",
      "Fold: 19  Epoch: 17  Training loss = 1.7995  Validation loss = 1.7466  \n",
      "\n",
      "Fold: 19  Epoch: 18  Training loss = 1.7378  Validation loss = 1.7992  \n",
      "\n",
      "Fold: 19  Epoch: 19  Training loss = 1.9104  Validation loss = 1.7705  \n",
      "\n",
      "Fold: 19  Epoch: 20  Training loss = 1.7563  Validation loss = 1.7801  \n",
      "\n",
      "Fold: 19  Epoch: 21  Training loss = 1.7788  Validation loss = 1.8321  \n",
      "\n",
      "Fold: 19  Epoch: 22  Training loss = 2.0838  Validation loss = 2.2807  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 17  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 2.2529  Validation loss = 2.3086  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 1.7940  Validation loss = 2.9506  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 1.7908  Validation loss = 2.3689  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 1.7413  Validation loss = 2.5789  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 1.7946  Validation loss = 2.8164  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 1.8092  Validation loss = 2.6859  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 1.7385  Validation loss = 3.0892  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 1.7164  Validation loss = 2.6002  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 1.8156  Validation loss = 2.9776  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 1.7913  Validation loss = 2.9367  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 1.7222  Validation loss = 3.2207  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 1  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 1.8618  Validation loss = 3.7033  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 1.9712  Validation loss = 3.5497  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 2.0192  Validation loss = 3.4155  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 1.9896  Validation loss = 3.7234  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 1.8517  Validation loss = 3.5644  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 1.9617  Validation loss = 3.4490  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 1.8963  Validation loss = 3.6038  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 1.8047  Validation loss = 3.6356  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 1.8288  Validation loss = 3.4806  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 2.2284  Validation loss = 3.4079  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 1.9153  Validation loss = 3.6010  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 1.8579  Validation loss = 3.7487  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 10  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 1.9266  Validation loss = 1.1673  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 1.8820  Validation loss = 1.0672  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 1.9576  Validation loss = 1.1612  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 1.8534  Validation loss = 1.2436  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 1.9006  Validation loss = 1.2970  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 1.7731  Validation loss = 1.4964  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 1.8403  Validation loss = 1.4844  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 1.8313  Validation loss = 1.6476  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 1.7873  Validation loss = 1.6516  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 1.7592  Validation loss = 1.6848  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 1.7801  Validation loss = 1.8750  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 2  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 1.7147  Validation loss = 2.3491  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 1.6808  Validation loss = 2.3591  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 1.7277  Validation loss = 3.0065  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 1.6870  Validation loss = 3.1488  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 1.6457  Validation loss = 3.3345  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 1.6338  Validation loss = 3.3778  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 1.6501  Validation loss = 2.8147  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 1.6051  Validation loss = 2.8823  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 1.6471  Validation loss = 3.2305  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 1.5510  Validation loss = 3.3800  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 1.5840  Validation loss = 2.9874  \n",
      "\n",
      "Fold: 23  Epoch: 12  Training loss = 1.6911  Validation loss = 2.0672  \n",
      "\n",
      "Fold: 23  Epoch: 13  Training loss = 1.6554  Validation loss = 3.0584  \n",
      "\n",
      "Fold: 23  Epoch: 14  Training loss = 1.6411  Validation loss = 3.0838  \n",
      "\n",
      "Fold: 23  Epoch: 15  Training loss = 1.5796  Validation loss = 3.3991  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 12  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 1.7219  Validation loss = 1.6671  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 1.7317  Validation loss = 1.6566  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 1.6528  Validation loss = 1.6889  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 1.7765  Validation loss = 2.0877  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 1.6617  Validation loss = 1.8438  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 1.7344  Validation loss = 1.6928  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 1.6500  Validation loss = 1.6183  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 1.6730  Validation loss = 1.6352  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 1.7351  Validation loss = 2.0832  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 1.7922  Validation loss = 2.0010  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 1.7561  Validation loss = 1.8042  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 1.7274  Validation loss = 1.8899  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 1.7115  Validation loss = 1.9693  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 1.7338  Validation loss = 1.8075  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 1.6517  Validation loss = 1.6559  \n",
      "\n",
      "Fold: 24  Epoch: 16  Training loss = 1.6469  Validation loss = 1.8305  \n",
      "\n",
      "Fold: 24  Epoch: 17  Training loss = 1.6528  Validation loss = 1.9342  \n",
      "\n",
      "Fold: 24  Epoch: 18  Training loss = 1.6467  Validation loss = 1.8794  \n",
      "\n",
      "Fold: 24  Epoch: 19  Training loss = 1.7129  Validation loss = 1.7107  \n",
      "\n",
      "Fold: 24  Epoch: 20  Training loss = 1.6930  Validation loss = 1.8355  \n",
      "\n",
      "Fold: 24  Epoch: 21  Training loss = 1.7125  Validation loss = 1.7067  \n",
      "\n",
      "Fold: 24  Epoch: 22  Training loss = 1.6505  Validation loss = 1.7980  \n",
      "\n",
      "Fold: 24  Epoch: 23  Training loss = 1.6455  Validation loss = 1.7834  \n",
      "\n",
      "Fold: 24  Epoch: 24  Training loss = 1.6436  Validation loss = 1.6365  \n",
      "\n",
      "Fold: 24  Epoch: 25  Training loss = 1.7228  Validation loss = 1.7469  \n",
      "\n",
      "Fold: 24  Epoch: 26  Training loss = 1.7127  Validation loss = 1.6202  \n",
      "\n",
      "Fold: 24  Epoch: 27  Training loss = 1.5896  Validation loss = 1.7854  \n",
      "\n",
      "Fold: 24  Epoch: 28  Training loss = 1.6837  Validation loss = 1.8048  \n",
      "\n",
      "Fold: 24  Epoch: 29  Training loss = 1.6091  Validation loss = 1.5834  \n",
      "\n",
      "Fold: 24  Epoch: 30  Training loss = 1.5934  Validation loss = 1.5812  \n",
      "\n",
      "Fold: 24  Epoch: 31  Training loss = 1.6172  Validation loss = 1.5335  \n",
      "\n",
      "Fold: 24  Epoch: 32  Training loss = 1.5391  Validation loss = 1.5698  \n",
      "\n",
      "Fold: 24  Epoch: 33  Training loss = 1.5403  Validation loss = 1.5445  \n",
      "\n",
      "Fold: 24  Epoch: 34  Training loss = 1.5829  Validation loss = 1.5231  \n",
      "\n",
      "Fold: 24  Epoch: 35  Training loss = 1.6061  Validation loss = 1.5610  \n",
      "\n",
      "Fold: 24  Epoch: 36  Training loss = 1.5692  Validation loss = 1.5792  \n",
      "\n",
      "Fold: 24  Epoch: 37  Training loss = 1.5684  Validation loss = 1.5832  \n",
      "\n",
      "Fold: 24  Epoch: 38  Training loss = 1.5486  Validation loss = 1.7722  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 34  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 1.5329  Validation loss = 3.5858  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 1.6079  Validation loss = 3.5625  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 1.5181  Validation loss = 3.5975  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 1.5477  Validation loss = 3.5412  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 1.4843  Validation loss = 3.5150  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 1.5361  Validation loss = 3.7725  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 1.5470  Validation loss = 3.6049  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 1.5539  Validation loss = 3.7311  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 1.4834  Validation loss = 3.5969  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 1.5001  Validation loss = 3.6030  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 1.5152  Validation loss = 3.6896  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 1.4979  Validation loss = 3.6321  \n",
      "\n",
      "Fold: 25  Epoch: 13  Training loss = 1.4652  Validation loss = 3.6346  \n",
      "\n",
      "Fold: 25  Epoch: 14  Training loss = 1.4425  Validation loss = 3.6265  \n",
      "\n",
      "Fold: 25  Epoch: 15  Training loss = 1.5105  Validation loss = 3.6584  \n",
      "\n",
      "Fold: 25  Epoch: 16  Training loss = 1.4199  Validation loss = 3.7509  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 5  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 1.6900  Validation loss = 3.1318  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 1.6826  Validation loss = 3.3741  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 1.7142  Validation loss = 3.3791  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 1.6564  Validation loss = 3.1940  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 1.6124  Validation loss = 3.3287  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 1.6130  Validation loss = 3.3353  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 1.6181  Validation loss = 3.0265  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 1.6484  Validation loss = 3.0538  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 1.5899  Validation loss = 3.1319  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 1.5934  Validation loss = 3.4834  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 1.5418  Validation loss = 3.3117  \n",
      "\n",
      "Fold: 26  Epoch: 12  Training loss = 1.5531  Validation loss = 3.2433  \n",
      "\n",
      "Fold: 26  Epoch: 13  Training loss = 1.5172  Validation loss = 3.0104  \n",
      "\n",
      "Fold: 26  Epoch: 14  Training loss = 1.5793  Validation loss = 3.3794  \n",
      "\n",
      "Fold: 26  Epoch: 15  Training loss = 1.5867  Validation loss = 3.5301  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 13  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 1.6197  Validation loss = 2.2054  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 1.6250  Validation loss = 2.2707  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 1.7225  Validation loss = 2.2328  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 1.7096  Validation loss = 2.1942  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 1.6575  Validation loss = 2.2027  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 1.6436  Validation loss = 2.2545  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 1.6747  Validation loss = 2.2786  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 1.6258  Validation loss = 2.3654  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 1.6099  Validation loss = 2.2680  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 1.6012  Validation loss = 2.3661  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 1.6053  Validation loss = 2.2954  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 1.6274  Validation loss = 2.2812  \n",
      "\n",
      "Fold: 27  Epoch: 13  Training loss = 1.6225  Validation loss = 2.1934  \n",
      "\n",
      "Fold: 27  Epoch: 14  Training loss = 1.5743  Validation loss = 2.1983  \n",
      "\n",
      "Fold: 27  Epoch: 15  Training loss = 1.5394  Validation loss = 2.2309  \n",
      "\n",
      "Fold: 27  Epoch: 16  Training loss = 1.5535  Validation loss = 2.2419  \n",
      "\n",
      "Fold: 27  Epoch: 17  Training loss = 1.5687  Validation loss = 2.3307  \n",
      "\n",
      "Fold: 27  Epoch: 18  Training loss = 1.5334  Validation loss = 2.2356  \n",
      "\n",
      "Fold: 27  Epoch: 19  Training loss = 1.5438  Validation loss = 2.3483  \n",
      "\n",
      "Fold: 27  Epoch: 20  Training loss = 1.5027  Validation loss = 2.3172  \n",
      "\n",
      "Fold: 27  Epoch: 21  Training loss = 1.5143  Validation loss = 2.4371  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 13  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 1.5820  Validation loss = 1.1433  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 1.5649  Validation loss = 1.1846  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 1.5567  Validation loss = 1.1811  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 1.5489  Validation loss = 1.1320  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 1.5804  Validation loss = 1.0499  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 1.5786  Validation loss = 1.1171  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 1.5870  Validation loss = 1.1174  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 1.5692  Validation loss = 0.9744  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 1.5396  Validation loss = 0.9926  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 1.5472  Validation loss = 1.0410  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 1.5340  Validation loss = 1.0275  \n",
      "\n",
      "Fold: 28  Epoch: 12  Training loss = 1.5392  Validation loss = 1.0835  \n",
      "\n",
      "Fold: 28  Epoch: 13  Training loss = 1.4932  Validation loss = 1.0192  \n",
      "\n",
      "Fold: 28  Epoch: 14  Training loss = 1.5003  Validation loss = 1.0113  \n",
      "\n",
      "Fold: 28  Epoch: 15  Training loss = 1.5014  Validation loss = 1.0815  \n",
      "\n",
      "Fold: 28  Epoch: 16  Training loss = 1.4854  Validation loss = 1.0806  \n",
      "\n",
      "Fold: 28  Epoch: 17  Training loss = 1.4916  Validation loss = 0.9977  \n",
      "\n",
      "Fold: 28  Epoch: 18  Training loss = 1.4578  Validation loss = 0.9701  \n",
      "\n",
      "Fold: 28  Epoch: 19  Training loss = 1.4569  Validation loss = 0.8074  \n",
      "\n",
      "Fold: 28  Epoch: 20  Training loss = 1.4294  Validation loss = 0.7808  \n",
      "\n",
      "Fold: 28  Epoch: 21  Training loss = 1.4047  Validation loss = 0.7477  \n",
      "\n",
      "Fold: 28  Epoch: 22  Training loss = 1.3979  Validation loss = 0.7139  \n",
      "\n",
      "Fold: 28  Epoch: 23  Training loss = 1.3667  Validation loss = 0.7205  \n",
      "\n",
      "Fold: 28  Epoch: 24  Training loss = 1.4416  Validation loss = 0.6544  \n",
      "\n",
      "Fold: 28  Epoch: 25  Training loss = 1.4180  Validation loss = 0.6799  \n",
      "\n",
      "Fold: 28  Epoch: 26  Training loss = 1.3608  Validation loss = 0.7951  \n",
      "\n",
      "Fold: 28  Epoch: 27  Training loss = 1.3427  Validation loss = 0.7736  \n",
      "\n",
      "Fold: 28  Epoch: 28  Training loss = 1.3258  Validation loss = 0.7530  \n",
      "\n",
      "Fold: 28  Epoch: 29  Training loss = 1.3309  Validation loss = 0.6793  \n",
      "\n",
      "Fold: 28  Epoch: 30  Training loss = 1.3260  Validation loss = 0.6249  \n",
      "\n",
      "Fold: 28  Epoch: 31  Training loss = 1.3489  Validation loss = 0.6347  \n",
      "\n",
      "Fold: 28  Epoch: 32  Training loss = 1.3256  Validation loss = 0.6664  \n",
      "\n",
      "Fold: 28  Epoch: 33  Training loss = 1.3032  Validation loss = 0.7719  \n",
      "\n",
      "Fold: 28  Epoch: 34  Training loss = 1.3152  Validation loss = 0.6470  \n",
      "\n",
      "Fold: 28  Epoch: 35  Training loss = 1.2933  Validation loss = 0.7002  \n",
      "\n",
      "Fold: 28  Epoch: 36  Training loss = 1.3268  Validation loss = 0.4599  \n",
      "\n",
      "Fold: 28  Epoch: 37  Training loss = 1.2969  Validation loss = 0.5362  \n",
      "\n",
      "Fold: 28  Epoch: 38  Training loss = 1.2958  Validation loss = 0.7181  \n",
      "\n",
      "Fold: 28  Epoch: 39  Training loss = 1.2900  Validation loss = 0.7098  \n",
      "\n",
      "Fold: 28  Epoch: 40  Training loss = 1.2850  Validation loss = 0.5236  \n",
      "\n",
      "Fold: 28  Epoch: 41  Training loss = 1.2702  Validation loss = 0.6519  \n",
      "\n",
      "Fold: 28  Epoch: 42  Training loss = 1.2720  Validation loss = 0.7312  \n",
      "\n",
      "Fold: 28  Epoch: 43  Training loss = 1.2994  Validation loss = 0.7589  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 36  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 1.2736  Validation loss = 1.4830  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 1.3181  Validation loss = 1.3099  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 1.2744  Validation loss = 1.6339  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 1.2857  Validation loss = 1.8472  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 1.2982  Validation loss = 1.2791  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 1.2949  Validation loss = 1.3679  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 1.2592  Validation loss = 1.6269  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 1.2404  Validation loss = 1.4371  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 1.2350  Validation loss = 1.6080  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 1.2514  Validation loss = 1.6862  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 1.2545  Validation loss = 1.4101  \n",
      "\n",
      "Fold: 29  Epoch: 12  Training loss = 1.2867  Validation loss = 1.2517  \n",
      "\n",
      "Fold: 29  Epoch: 13  Training loss = 1.2441  Validation loss = 1.3635  \n",
      "\n",
      "Fold: 29  Epoch: 14  Training loss = 1.2116  Validation loss = 1.4184  \n",
      "\n",
      "Fold: 29  Epoch: 15  Training loss = 1.2581  Validation loss = 1.3392  \n",
      "\n",
      "Fold: 29  Epoch: 16  Training loss = 1.2180  Validation loss = 1.6124  \n",
      "\n",
      "Fold: 29  Epoch: 17  Training loss = 1.2191  Validation loss = 1.5008  \n",
      "\n",
      "Fold: 29  Epoch: 18  Training loss = 1.2226  Validation loss = 1.4636  \n",
      "\n",
      "Fold: 29  Epoch: 19  Training loss = 1.1918  Validation loss = 1.4571  \n",
      "\n",
      "Fold: 29  Epoch: 20  Training loss = 1.1986  Validation loss = 1.2694  \n",
      "\n",
      "Fold: 29  Epoch: 21  Training loss = 1.2221  Validation loss = 1.2206  \n",
      "\n",
      "Fold: 29  Epoch: 22  Training loss = 1.1769  Validation loss = 1.3019  \n",
      "\n",
      "Fold: 29  Epoch: 23  Training loss = 1.1958  Validation loss = 1.6201  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 21  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 1.2456  Validation loss = 0.8850  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 1.2134  Validation loss = 0.8822  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 1.2237  Validation loss = 0.7947  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 1.1984  Validation loss = 0.8005  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 1.1929  Validation loss = 0.7479  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 1.2004  Validation loss = 0.6903  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 1.2201  Validation loss = 0.7266  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 1.1935  Validation loss = 0.8572  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 1.1937  Validation loss = 0.7982  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 1.1636  Validation loss = 0.7770  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 1.1910  Validation loss = 0.7687  \n",
      "\n",
      "Fold: 30  Epoch: 12  Training loss = 1.1743  Validation loss = 0.8118  \n",
      "\n",
      "Fold: 30  Epoch: 13  Training loss = 1.1877  Validation loss = 0.8010  \n",
      "\n",
      "Fold: 30  Epoch: 14  Training loss = 1.1734  Validation loss = 0.6980  \n",
      "\n",
      "Fold: 30  Epoch: 15  Training loss = 1.2099  Validation loss = 0.6381  \n",
      "\n",
      "Fold: 30  Epoch: 16  Training loss = 1.2061  Validation loss = 0.7432  \n",
      "\n",
      "Fold: 30  Epoch: 17  Training loss = 1.1699  Validation loss = 0.7453  \n",
      "\n",
      "Fold: 30  Epoch: 18  Training loss = 1.1541  Validation loss = 0.6963  \n",
      "\n",
      "Fold: 30  Epoch: 19  Training loss = 1.1650  Validation loss = 0.7500  \n",
      "\n",
      "Fold: 30  Epoch: 20  Training loss = 1.1485  Validation loss = 0.6692  \n",
      "\n",
      "Fold: 30  Epoch: 21  Training loss = 1.1653  Validation loss = 0.7412  \n",
      "\n",
      "Fold: 30  Epoch: 22  Training loss = 1.1781  Validation loss = 0.8149  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 15  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 1.1552  Validation loss = 1.5098  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 1.1525  Validation loss = 1.6183  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 1.1480  Validation loss = 1.4425  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 1.1414  Validation loss = 1.5943  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 1.1501  Validation loss = 1.3220  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 1.1459  Validation loss = 1.2940  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 1.1350  Validation loss = 1.4136  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 1.1496  Validation loss = 1.3286  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 1.1558  Validation loss = 1.3827  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 1.1243  Validation loss = 1.5969  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 1.1383  Validation loss = 1.6690  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 6  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 1.1218  Validation loss = 3.7861  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 1.0953  Validation loss = 3.9096  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 1.0717  Validation loss = 3.9915  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 1.0831  Validation loss = 3.8752  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 1.0776  Validation loss = 3.9438  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 1.0875  Validation loss = 3.9128  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 1.0763  Validation loss = 4.1145  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 1.0620  Validation loss = 3.9858  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 1.0917  Validation loss = 3.8221  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 1.0808  Validation loss = 4.0602  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 1.0872  Validation loss = 3.8063  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 1.0610  Validation loss = 4.0334  \n",
      "\n",
      "Fold: 32  Epoch: 13  Training loss = 1.0464  Validation loss = 4.0111  \n",
      "\n",
      "Fold: 32  Epoch: 14  Training loss = 1.0328  Validation loss = 3.9643  \n",
      "\n",
      "Fold: 32  Epoch: 15  Training loss = 1.0285  Validation loss = 4.0103  \n",
      "\n",
      "Fold: 32  Epoch: 16  Training loss = 1.0392  Validation loss = 3.8675  \n",
      "\n",
      "Fold: 32  Epoch: 17  Training loss = 1.0377  Validation loss = 3.9799  \n",
      "\n",
      "Fold: 32  Epoch: 18  Training loss = 1.0369  Validation loss = 3.8191  \n",
      "\n",
      "Fold: 32  Epoch: 19  Training loss = 1.0659  Validation loss = 3.7280  \n",
      "\n",
      "Fold: 32  Epoch: 20  Training loss = 1.0700  Validation loss = 4.0331  \n",
      "\n",
      "Fold: 32  Epoch: 21  Training loss = 1.0207  Validation loss = 3.8892  \n",
      "\n",
      "Fold: 32  Epoch: 22  Training loss = 1.0200  Validation loss = 3.9615  \n",
      "\n",
      "Fold: 32  Epoch: 23  Training loss = 1.0334  Validation loss = 3.9711  \n",
      "\n",
      "Fold: 32  Epoch: 24  Training loss = 1.0344  Validation loss = 3.9392  \n",
      "\n",
      "Fold: 32  Epoch: 25  Training loss = 1.0299  Validation loss = 3.9577  \n",
      "\n",
      "Fold: 32  Epoch: 26  Training loss = 1.0590  Validation loss = 4.0908  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 19  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    # Load model with lowest validation error for each fold\n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 18\n",
      "Average validation error: 3.28689\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 1.3431  Test loss = 3.4169  \n",
      "\n",
      "Epoch: 2  Training loss = 1.3215  Test loss = 3.3694  \n",
      "\n",
      "Epoch: 3  Training loss = 1.3050  Test loss = 3.3325  \n",
      "\n",
      "Epoch: 4  Training loss = 1.2910  Test loss = 3.3013  \n",
      "\n",
      "Epoch: 5  Training loss = 1.2788  Test loss = 3.2739  \n",
      "\n",
      "Epoch: 6  Training loss = 1.2678  Test loss = 3.2493  \n",
      "\n",
      "Epoch: 7  Training loss = 1.2575  Test loss = 3.2265  \n",
      "\n",
      "Epoch: 8  Training loss = 1.2476  Test loss = 3.2048  \n",
      "\n",
      "Epoch: 9  Training loss = 1.2377  Test loss = 3.1833  \n",
      "\n",
      "Epoch: 10  Training loss = 1.2276  Test loss = 3.1616  \n",
      "\n",
      "Epoch: 11  Training loss = 1.2169  Test loss = 3.1401  \n",
      "\n",
      "Epoch: 12  Training loss = 1.2051  Test loss = 3.1192  \n",
      "\n",
      "Epoch: 13  Training loss = 1.1905  Test loss = 3.0972  \n",
      "\n",
      "Epoch: 14  Training loss = 1.1748  Test loss = 3.0719  \n",
      "\n",
      "Epoch: 15  Training loss = 1.1648  Test loss = 3.0535  \n",
      "\n",
      "Epoch: 16  Training loss = 1.1580  Test loss = 3.0491  \n",
      "\n",
      "Epoch: 17  Training loss = 1.1522  Test loss = 3.0474  \n",
      "\n",
      "Epoch: 18  Training loss = 1.1474  Test loss = 3.0457  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd587080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWcu4sJuZtYwLu5lZy/wNy5GrANhOPQcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd229da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 3.04566985887 \n",
      "Fixed scheme MAE:  2.10084816441\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 1.1474  Test loss = 3.4861  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 1.2259  Test loss = 1.9020  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 1.2426  Test loss = 0.1380  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 1.2421  Test loss = 1.5001  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 0.9900  Test loss = 2.2171  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 1.0216  Test loss = 0.4585  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 1.0227  Test loss = 1.1043  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 1.0296  Test loss = 1.0848  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 0.9063  Test loss = 0.8388  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 0.9049  Test loss = 1.2863  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 0.9095  Test loss = 0.5828  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 0.9112  Test loss = 2.9256  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 0.9386  Test loss = 1.1358  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 0.9491  Test loss = 3.9801  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 1.0664  Test loss = 4.7311  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 1.2163  Test loss = 5.9896  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 1.1857  Test loss = 3.2421  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 1.2520  Test loss = 0.2464  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 1.2514  Test loss = 1.3517  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 1.2357  Test loss = 0.2643  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 1.1735  Test loss = 1.6465  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 1.1910  Test loss = 2.5527  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 1.2298  Test loss = 0.0461  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 1.2292  Test loss = 0.5462  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 0.9316  Test loss = 1.8532  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 0.9593  Test loss = 3.5152  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 1.0533  Test loss = 0.7888  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 1.0382  Test loss = 1.7953  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 0.9452  Test loss = 0.6384  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 0.9467  Test loss = 0.1219  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 0.9445  Test loss = 4.0545  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 1.0641  Test loss = 0.0909  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 0.9857  Test loss = 0.6851  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 0.9885  Test loss = 0.2818  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 0.9701  Test loss = 0.6296  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 0.9700  Test loss = 2.6370  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 0.9678  Test loss = 3.7051  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 1.0386  Test loss = 3.1792  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 1.1000  Test loss = 0.1559  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 1.0991  Test loss = 4.9542  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 1.0804  Test loss = 1.9962  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 1.1085  Test loss = 2.6551  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 1.1563  Test loss = 3.5726  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 1.2377  Test loss = 14.4007  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 2.1081  Test loss = 8.1255  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.3362  Test loss = 2.3531  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.3544  Test loss = 0.9188  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.3565  Test loss = 0.3296  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 1.8143  Test loss = 1.9065  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 1.8291  Test loss = 4.2431  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 1.9021  Test loss = 1.5906  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 1.9063  Test loss = 0.6257  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 1.7998  Test loss = 1.1014  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 1.8023  Test loss = 2.6005  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 1.8273  Test loss = 1.5879  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 1.8321  Test loss = 1.4218  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 1.7428  Test loss = 0.8658  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 1.7461  Test loss = 2.6880  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 1.7775  Test loss = 0.9821  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 1.7797  Test loss = 0.2004  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.7202  Test loss = 1.0040  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.7235  Test loss = 1.7370  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 1.7355  Test loss = 0.6673  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 1.7367  Test loss = 2.4083  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 1.7298  Test loss = 2.5708  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 1.7564  Test loss = 1.7570  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.7544  Test loss = 1.5587  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 1.7648  Test loss = 3.3354  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 1.7656  Test loss = 5.6391  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 1.8987  Test loss = 0.5511  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 1.8948  Test loss = 0.7713  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 1.8964  Test loss = 1.5240  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 1.8037  Test loss = 0.3232  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 1.8042  Test loss = 1.0691  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 1.8090  Test loss = 1.6213  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 1.8147  Test loss = 0.2197  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 1.7520  Test loss = 4.1236  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd613908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.94149895688 \n",
      "Updating scheme MAE:  2.04402429695\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_test_fixed.csv\", yhat_test_fixed)\n",
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_update.csv\", yhat_update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
