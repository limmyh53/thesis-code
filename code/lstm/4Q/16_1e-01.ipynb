{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"4Q/16_units/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 500\n",
    "learning_rate = 1e-1\n",
    "batch_size = 5\n",
    "early_stop_iters = 10\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 16 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag4',\n",
    "                                       'inflation.lag5',\n",
    "                                       'inflation.lag6',\n",
    "                                       'inflation.lag7']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag4',\n",
    "                                   'unemp.lag5',\n",
    "                                   'unemp.lag6',\n",
    "                                   'unemp.lag7']])\n",
    "train_4lag_oil = np.array(train[['oil.lag4',\n",
    "                                 'oil.lag5',\n",
    "                                 'oil.lag6',\n",
    "                                 'oil.lag7']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag4',\n",
    "                                     'inflation.lag5',\n",
    "                                     'inflation.lag6',\n",
    "                                     'inflation.lag7']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag4',\n",
    "                                 'unemp.lag5',\n",
    "                                 'unemp.lag6',\n",
    "                                 'unemp.lag7']])\n",
    "test_4lag_oil = np.array(test[['oil.lag4',\n",
    "                               'oil.lag5',\n",
    "                               'oil.lag6',\n",
    "                               'oil.lag7']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 16 \n",
      "Learning rate = 0.1 \n",
      "Epochs = 500 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 10 \n",
      "Learning rate = 0.1\n",
      "Fold: 1  Epoch: 1  Training loss = 2.5267  Validation loss = 2.2576  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 2.5395  Validation loss = 2.2275  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 2.3848  Validation loss = 2.2263  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 2.3464  Validation loss = 1.8225  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 2.3223  Validation loss = 1.8427  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 2.3158  Validation loss = 1.8905  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 2.2998  Validation loss = 1.9390  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 2.3260  Validation loss = 2.2452  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 2.3501  Validation loss = 1.3188  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 2.2289  Validation loss = 1.8691  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 2.3434  Validation loss = 2.2553  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 9  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.2284  Validation loss = 2.1151  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.0910  Validation loss = 1.9193  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.1835  Validation loss = 1.9371  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.3399  Validation loss = 2.2004  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.2617  Validation loss = 2.0420  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.2530  Validation loss = 2.2008  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.2599  Validation loss = 2.2081  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.1972  Validation loss = 1.9831  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.2301  Validation loss = 2.1252  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.3499  Validation loss = 2.0564  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.2539  Validation loss = 2.4053  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 2  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.3321  Validation loss = 2.9338  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.3493  Validation loss = 3.5931  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.2615  Validation loss = 3.3367  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.2809  Validation loss = 3.1045  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.2958  Validation loss = 2.9854  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.2888  Validation loss = 2.8482  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.2471  Validation loss = 3.2478  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.2626  Validation loss = 2.9463  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.2218  Validation loss = 3.2829  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.2537  Validation loss = 3.5709  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.2350  Validation loss = 3.5012  \n",
      "\n",
      "Fold: 3  Epoch: 12  Training loss = 1.2352  Validation loss = 3.2593  \n",
      "\n",
      "Fold: 3  Epoch: 13  Training loss = 1.2690  Validation loss = 3.5909  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 6  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.3900  Validation loss = 3.7916  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.2982  Validation loss = 3.5495  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.2505  Validation loss = 3.2235  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.2405  Validation loss = 3.8038  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.1498  Validation loss = 3.2308  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.1511  Validation loss = 3.6907  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.1912  Validation loss = 3.4567  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.1216  Validation loss = 3.7719  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.2018  Validation loss = 4.1287  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 1.1344  Validation loss = 3.4899  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.0838  Validation loss = 3.2961  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 1.0741  Validation loss = 3.4849  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.0592  Validation loss = 3.2365  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 1.0704  Validation loss = 3.0485  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.0379  Validation loss = 2.7628  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 1.0395  Validation loss = 3.1868  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 1.0152  Validation loss = 2.9177  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 1.1123  Validation loss = 3.2746  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 1.1185  Validation loss = 2.5703  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 1.0109  Validation loss = 2.9089  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 1.1006  Validation loss = 2.7084  \n",
      "\n",
      "Fold: 4  Epoch: 22  Training loss = 1.0031  Validation loss = 3.2976  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 19  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.0567  Validation loss = 2.0902  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.0182  Validation loss = 1.5424  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.0081  Validation loss = 2.0872  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.0760  Validation loss = 1.6840  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.0094  Validation loss = 1.5367  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 0.9875  Validation loss = 1.3278  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 0.9958  Validation loss = 1.7087  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.0912  Validation loss = 1.9803  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 1.0113  Validation loss = 1.4072  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.0542  Validation loss = 1.4238  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 1.0191  Validation loss = 0.8536  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 0.9918  Validation loss = 1.1421  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 1.1748  Validation loss = 1.8891  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.0223  Validation loss = 1.4593  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 1.1128  Validation loss = 1.4363  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 0.9061  Validation loss = 1.4709  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 0.9143  Validation loss = 1.1661  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 1.0414  Validation loss = 0.9324  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 0.9198  Validation loss = 1.3883  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 0.9004  Validation loss = 1.3942  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 0.9004  Validation loss = 1.1986  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 0.9866  Validation loss = 0.9073  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 1.0394  Validation loss = 1.6145  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 11  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 1.1761  Validation loss = 1.8907  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 0.9312  Validation loss = 1.5493  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 0.9284  Validation loss = 1.1574  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 0.9549  Validation loss = 1.1367  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 0.8525  Validation loss = 1.3982  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 1.0042  Validation loss = 1.5069  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 0.8511  Validation loss = 1.4076  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 0.9764  Validation loss = 1.1603  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 0.8550  Validation loss = 1.7096  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 0.8369  Validation loss = 1.6369  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 0.8286  Validation loss = 1.7721  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 4  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 0.9388  Validation loss = 2.4661  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 0.8986  Validation loss = 2.4423  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 0.8766  Validation loss = 2.8257  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.0466  Validation loss = 2.1174  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 0.9572  Validation loss = 2.2883  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 0.8913  Validation loss = 2.7251  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 0.9529  Validation loss = 2.8499  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 0.8818  Validation loss = 2.6774  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 0.9073  Validation loss = 2.5996  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 1.0502  Validation loss = 2.8739  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 0.8581  Validation loss = 2.4313  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 0.9585  Validation loss = 2.1773  \n",
      "\n",
      "Fold: 7  Epoch: 13  Training loss = 0.9133  Validation loss = 2.7391  \n",
      "\n",
      "Fold: 7  Epoch: 14  Training loss = 1.0086  Validation loss = 2.6623  \n",
      "\n",
      "Fold: 7  Epoch: 15  Training loss = 0.8584  Validation loss = 2.4829  \n",
      "\n",
      "Fold: 7  Epoch: 16  Training loss = 0.8327  Validation loss = 2.6064  \n",
      "\n",
      "Fold: 7  Epoch: 17  Training loss = 0.8501  Validation loss = 2.5679  \n",
      "\n",
      "Fold: 7  Epoch: 18  Training loss = 0.8064  Validation loss = 2.8309  \n",
      "\n",
      "Fold: 7  Epoch: 19  Training loss = 0.8668  Validation loss = 2.8171  \n",
      "\n",
      "Fold: 7  Epoch: 20  Training loss = 0.8558  Validation loss = 2.5500  \n",
      "\n",
      "Fold: 7  Epoch: 21  Training loss = 0.9819  Validation loss = 2.8823  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 4  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 1.1830  Validation loss = 6.5903  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 0.9675  Validation loss = 5.8702  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 1.3100  Validation loss = 5.3162  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 0.9666  Validation loss = 6.5677  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 0.9371  Validation loss = 6.7090  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 0.9561  Validation loss = 7.0204  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 0.9436  Validation loss = 7.2913  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 0.9032  Validation loss = 6.0902  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 0.9678  Validation loss = 4.7397  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 0.8854  Validation loss = 5.2343  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 0.8815  Validation loss = 5.6882  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 0.9278  Validation loss = 5.3798  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 0.8874  Validation loss = 6.1037  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 0.8769  Validation loss = 6.4484  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 0.9354  Validation loss = 5.7433  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 0.9110  Validation loss = 6.3773  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 0.8823  Validation loss = 6.6553  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 9  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 1.4707  Validation loss = 8.0005  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 1.6772  Validation loss = 6.7994  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 1.4215  Validation loss = 8.0973  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 1.3670  Validation loss = 7.8164  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 1.3617  Validation loss = 7.3674  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 1.3140  Validation loss = 6.7887  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 1.2594  Validation loss = 7.1465  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 1.2342  Validation loss = 7.2058  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 1.2419  Validation loss = 6.9434  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 1.5778  Validation loss = 7.4486  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 1.2892  Validation loss = 7.0631  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 1.2952  Validation loss = 7.2078  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 1.2826  Validation loss = 6.9438  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 1.2840  Validation loss = 7.1111  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 1.2167  Validation loss = 6.6077  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 1.2886  Validation loss = 6.3440  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 1.2249  Validation loss = 6.7646  \n",
      "\n",
      "Fold: 9  Epoch: 18  Training loss = 1.2057  Validation loss = 7.1810  \n",
      "\n",
      "Fold: 9  Epoch: 19  Training loss = 1.2148  Validation loss = 6.5315  \n",
      "\n",
      "Fold: 9  Epoch: 20  Training loss = 1.1817  Validation loss = 6.7078  \n",
      "\n",
      "Fold: 9  Epoch: 21  Training loss = 1.2892  Validation loss = 6.9730  \n",
      "\n",
      "Fold: 9  Epoch: 22  Training loss = 1.2453  Validation loss = 6.4319  \n",
      "\n",
      "Fold: 9  Epoch: 23  Training loss = 1.3341  Validation loss = 7.0581  \n",
      "\n",
      "Fold: 9  Epoch: 24  Training loss = 1.4188  Validation loss = 6.4726  \n",
      "\n",
      "Fold: 9  Epoch: 25  Training loss = 1.2679  Validation loss = 6.3994  \n",
      "\n",
      "Fold: 9  Epoch: 26  Training loss = 1.2495  Validation loss = 6.5644  \n",
      "\n",
      "Fold: 9  Epoch: 27  Training loss = 1.2863  Validation loss = 6.9470  \n",
      "\n",
      "Fold: 9  Epoch: 28  Training loss = 1.2369  Validation loss = 6.1701  \n",
      "\n",
      "Fold: 9  Epoch: 29  Training loss = 1.2682  Validation loss = 6.5749  \n",
      "\n",
      "Fold: 9  Epoch: 30  Training loss = 1.2752  Validation loss = 6.7135  \n",
      "\n",
      "Fold: 9  Epoch: 31  Training loss = 1.2515  Validation loss = 6.6852  \n",
      "\n",
      "Fold: 9  Epoch: 32  Training loss = 1.1107  Validation loss = 5.9069  \n",
      "\n",
      "Fold: 9  Epoch: 33  Training loss = 1.1270  Validation loss = 6.2906  \n",
      "\n",
      "Fold: 9  Epoch: 34  Training loss = 1.1472  Validation loss = 6.2608  \n",
      "\n",
      "Fold: 9  Epoch: 35  Training loss = 1.2424  Validation loss = 6.7234  \n",
      "\n",
      "Fold: 9  Epoch: 36  Training loss = 1.2853  Validation loss = 5.6887  \n",
      "\n",
      "Fold: 9  Epoch: 37  Training loss = 1.0629  Validation loss = 5.8729  \n",
      "\n",
      "Fold: 9  Epoch: 38  Training loss = 1.0749  Validation loss = 5.7847  \n",
      "\n",
      "Fold: 9  Epoch: 39  Training loss = 1.1018  Validation loss = 6.0125  \n",
      "\n",
      "Fold: 9  Epoch: 40  Training loss = 1.1920  Validation loss = 6.5361  \n",
      "\n",
      "Fold: 9  Epoch: 41  Training loss = 1.0923  Validation loss = 6.1291  \n",
      "\n",
      "Fold: 9  Epoch: 42  Training loss = 1.1179  Validation loss = 6.2524  \n",
      "\n",
      "Fold: 9  Epoch: 43  Training loss = 1.4138  Validation loss = 5.4721  \n",
      "\n",
      "Fold: 9  Epoch: 44  Training loss = 1.2368  Validation loss = 6.4441  \n",
      "\n",
      "Fold: 9  Epoch: 45  Training loss = 1.1759  Validation loss = 6.1844  \n",
      "\n",
      "Fold: 9  Epoch: 46  Training loss = 1.0753  Validation loss = 6.2491  \n",
      "\n",
      "Fold: 9  Epoch: 47  Training loss = 1.0423  Validation loss = 5.6363  \n",
      "\n",
      "Fold: 9  Epoch: 48  Training loss = 1.0754  Validation loss = 6.2048  \n",
      "\n",
      "Fold: 9  Epoch: 49  Training loss = 1.0320  Validation loss = 5.9784  \n",
      "\n",
      "Fold: 9  Epoch: 50  Training loss = 1.0150  Validation loss = 5.9758  \n",
      "\n",
      "Fold: 9  Epoch: 51  Training loss = 1.1855  Validation loss = 6.7951  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 43  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 2.1507  Validation loss = 4.1642  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 1.8359  Validation loss = 3.5315  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 1.8177  Validation loss = 3.6334  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 2.0386  Validation loss = 2.6843  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 1.8450  Validation loss = 3.2248  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 1.9871  Validation loss = 2.9659  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 2.1110  Validation loss = 4.2942  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 1.7583  Validation loss = 2.8663  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 2.0518  Validation loss = 2.3025  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 1.7945  Validation loss = 3.2475  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 1.7748  Validation loss = 2.3608  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 1.8336  Validation loss = 3.1911  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 1.7038  Validation loss = 2.8407  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 1.6727  Validation loss = 2.6322  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 1.6638  Validation loss = 3.1219  \n",
      "\n",
      "Fold: 10  Epoch: 16  Training loss = 1.5397  Validation loss = 2.5585  \n",
      "\n",
      "Fold: 10  Epoch: 17  Training loss = 1.9511  Validation loss = 3.2865  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 9  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 2.2708  Validation loss = 3.8049  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 1.8313  Validation loss = 2.3337  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 1.7568  Validation loss = 1.8140  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 1.7020  Validation loss = 2.6285  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 1.6549  Validation loss = 3.2399  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 1.6812  Validation loss = 2.6273  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 1.6581  Validation loss = 2.7343  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 1.8304  Validation loss = 3.2946  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 1.7437  Validation loss = 3.0719  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 1.6715  Validation loss = 2.9327  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 1.6479  Validation loss = 3.4010  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 3  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 1.7773  Validation loss = 0.4660  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 1.7159  Validation loss = 0.2607  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 1.7975  Validation loss = 0.6199  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 1.7341  Validation loss = 0.5920  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 1.7646  Validation loss = 1.3852  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 1.7176  Validation loss = 0.3138  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 1.6624  Validation loss = 1.0945  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 1.6410  Validation loss = 1.1056  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 1.6827  Validation loss = 0.2413  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 1.6944  Validation loss = 1.4466  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 1.7159  Validation loss = 1.4057  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 1.8773  Validation loss = 1.6231  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 9  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 1.6106  Validation loss = 3.4745  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 1.6100  Validation loss = 4.0061  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 1.5910  Validation loss = 3.1919  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 1.7001  Validation loss = 3.6987  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 1.5848  Validation loss = 3.6983  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 1.7512  Validation loss = 4.1018  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 1.6116  Validation loss = 3.3690  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 1.5493  Validation loss = 3.7510  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 1.6093  Validation loss = 3.4417  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 1.6203  Validation loss = 4.0756  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 1.6115  Validation loss = 3.8463  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 1.6427  Validation loss = 2.9138  \n",
      "\n",
      "Fold: 13  Epoch: 13  Training loss = 1.5469  Validation loss = 3.4430  \n",
      "\n",
      "Fold: 13  Epoch: 14  Training loss = 1.6397  Validation loss = 4.3405  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 12  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 2.0054  Validation loss = 7.4189  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 1.7670  Validation loss = 6.4849  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 1.7794  Validation loss = 6.5997  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 1.9115  Validation loss = 6.3165  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 2.5866  Validation loss = 7.1621  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 2.5294  Validation loss = 6.9570  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 3.0897  Validation loss = 6.0691  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 2.5447  Validation loss = 6.2507  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 2.1380  Validation loss = 6.3105  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 2.0690  Validation loss = 6.2085  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 2.3221  Validation loss = 5.6445  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 2.5029  Validation loss = 6.2556  \n",
      "\n",
      "Fold: 14  Epoch: 13  Training loss = 2.4442  Validation loss = 7.6785  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 11  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 2.5480  Validation loss = 6.6940  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 2.7001  Validation loss = 6.2184  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 3.8358  Validation loss = 6.6126  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 2.5344  Validation loss = 5.8433  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 2.6354  Validation loss = 7.1970  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 2.4947  Validation loss = 7.4317  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 2.4379  Validation loss = 6.4267  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 2.7505  Validation loss = 5.0863  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 2.6315  Validation loss = 6.2965  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 3.1697  Validation loss = 5.0566  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 2.3842  Validation loss = 6.3642  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 2.4585  Validation loss = 7.1196  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 2.8523  Validation loss = 6.8784  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 2.4585  Validation loss = 6.1266  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 2.3382  Validation loss = 7.1250  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 3.0865  Validation loss = 7.5656  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 10  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 2.7608  Validation loss = 2.6634  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 3.1856  Validation loss = 2.7997  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 2.8755  Validation loss = 3.5686  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 2.9956  Validation loss = 2.5870  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 2.8748  Validation loss = 3.4125  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 2.8409  Validation loss = 2.6745  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 3.6737  Validation loss = 2.2614  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 3.0293  Validation loss = 2.0348  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 2.8387  Validation loss = 2.3731  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 3.1372  Validation loss = 4.2583  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 2.7913  Validation loss = 3.3986  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 2.9223  Validation loss = 3.1403  \n",
      "\n",
      "Fold: 16  Epoch: 13  Training loss = 2.6489  Validation loss = 3.5573  \n",
      "\n",
      "Fold: 16  Epoch: 14  Training loss = 2.9330  Validation loss = 2.4202  \n",
      "\n",
      "Fold: 16  Epoch: 15  Training loss = 2.8101  Validation loss = 3.3933  \n",
      "\n",
      "Fold: 16  Epoch: 16  Training loss = 2.7123  Validation loss = 2.4399  \n",
      "\n",
      "Fold: 16  Epoch: 17  Training loss = 2.7132  Validation loss = 2.9007  \n",
      "\n",
      "Fold: 16  Epoch: 18  Training loss = 2.6653  Validation loss = 3.3446  \n",
      "\n",
      "Fold: 16  Epoch: 19  Training loss = 2.4879  Validation loss = 2.9203  \n",
      "\n",
      "Fold: 16  Epoch: 20  Training loss = 3.3672  Validation loss = 3.4827  \n",
      "\n",
      "Fold: 16  Epoch: 21  Training loss = 2.8907  Validation loss = 2.9429  \n",
      "\n",
      "Fold: 16  Epoch: 22  Training loss = 3.1737  Validation loss = 5.3052  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 8  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 3.8495  Validation loss = 3.4875  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 3.1895  Validation loss = 2.9763  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 2.9756  Validation loss = 2.8878  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 2.8441  Validation loss = 3.4507  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 3.2420  Validation loss = 2.6829  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 3.8177  Validation loss = 3.0935  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 3.3196  Validation loss = 3.3162  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 3.1193  Validation loss = 3.3157  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 3.0536  Validation loss = 2.3888  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 3.0477  Validation loss = 2.8430  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 3.1010  Validation loss = 3.0447  \n",
      "\n",
      "Fold: 17  Epoch: 12  Training loss = 3.0925  Validation loss = 3.2953  \n",
      "\n",
      "Fold: 17  Epoch: 13  Training loss = 3.0947  Validation loss = 2.7541  \n",
      "\n",
      "Fold: 17  Epoch: 14  Training loss = 3.0321  Validation loss = 2.6360  \n",
      "\n",
      "Fold: 17  Epoch: 15  Training loss = 3.4275  Validation loss = 3.0168  \n",
      "\n",
      "Fold: 17  Epoch: 16  Training loss = 3.9977  Validation loss = 2.6754  \n",
      "\n",
      "Fold: 17  Epoch: 17  Training loss = 3.0222  Validation loss = 2.9351  \n",
      "\n",
      "Fold: 17  Epoch: 18  Training loss = 2.9833  Validation loss = 3.0888  \n",
      "\n",
      "Fold: 17  Epoch: 19  Training loss = 3.2604  Validation loss = 3.1245  \n",
      "\n",
      "Fold: 17  Epoch: 20  Training loss = 3.1014  Validation loss = 3.0395  \n",
      "\n",
      "Fold: 17  Epoch: 21  Training loss = 3.0477  Validation loss = 2.7342  \n",
      "\n",
      "Fold: 17  Epoch: 22  Training loss = 2.9785  Validation loss = 3.7694  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 9  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 2.9669  Validation loss = 1.8188  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 3.2445  Validation loss = 1.7833  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 3.0923  Validation loss = 2.5616  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 3.0821  Validation loss = 1.9534  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 3.0104  Validation loss = 1.7165  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 2.7305  Validation loss = 1.8199  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 2.9732  Validation loss = 2.1206  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 2.9109  Validation loss = 1.9971  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 2.9207  Validation loss = 1.6196  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 3.1668  Validation loss = 2.0193  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 2.9005  Validation loss = 1.6730  \n",
      "\n",
      "Fold: 18  Epoch: 12  Training loss = 3.0325  Validation loss = 2.7053  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 9  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 3.2502  Validation loss = 2.7028  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 3.0782  Validation loss = 2.1163  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 2.9546  Validation loss = 2.7120  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 2.9944  Validation loss = 2.5995  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 2.8894  Validation loss = 2.3255  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 2.8456  Validation loss = 1.8986  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 3.3132  Validation loss = 3.0068  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 2.8875  Validation loss = 2.1779  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 3.1105  Validation loss = 2.4131  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 2.8696  Validation loss = 2.1815  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 2.8633  Validation loss = 1.9263  \n",
      "\n",
      "Fold: 19  Epoch: 12  Training loss = 2.7715  Validation loss = 2.3355  \n",
      "\n",
      "Fold: 19  Epoch: 13  Training loss = 3.1216  Validation loss = 2.3286  \n",
      "\n",
      "Fold: 19  Epoch: 14  Training loss = 2.9717  Validation loss = 2.8889  \n",
      "\n",
      "Fold: 19  Epoch: 15  Training loss = 3.0948  Validation loss = 2.9771  \n",
      "\n",
      "Fold: 19  Epoch: 16  Training loss = 2.9647  Validation loss = 2.3097  \n",
      "\n",
      "Fold: 19  Epoch: 17  Training loss = 2.9391  Validation loss = 2.4513  \n",
      "\n",
      "Fold: 19  Epoch: 18  Training loss = 3.0133  Validation loss = 2.2415  \n",
      "\n",
      "Fold: 19  Epoch: 19  Training loss = 3.0774  Validation loss = 1.6258  \n",
      "\n",
      "Fold: 19  Epoch: 20  Training loss = 2.8879  Validation loss = 2.0115  \n",
      "\n",
      "Fold: 19  Epoch: 21  Training loss = 2.9087  Validation loss = 2.1020  \n",
      "\n",
      "Fold: 19  Epoch: 22  Training loss = 2.9692  Validation loss = 2.0070  \n",
      "\n",
      "Fold: 19  Epoch: 23  Training loss = 2.8949  Validation loss = 1.9922  \n",
      "\n",
      "Fold: 19  Epoch: 24  Training loss = 2.8404  Validation loss = 2.3225  \n",
      "\n",
      "Fold: 19  Epoch: 25  Training loss = 3.3828  Validation loss = 1.6372  \n",
      "\n",
      "Fold: 19  Epoch: 26  Training loss = 3.1418  Validation loss = 2.1634  \n",
      "\n",
      "Fold: 19  Epoch: 27  Training loss = 2.8357  Validation loss = 2.1881  \n",
      "\n",
      "Fold: 19  Epoch: 28  Training loss = 2.8600  Validation loss = 1.8759  \n",
      "\n",
      "Fold: 19  Epoch: 29  Training loss = 2.8309  Validation loss = 2.1065  \n",
      "\n",
      "Fold: 19  Epoch: 30  Training loss = 3.0551  Validation loss = 3.0256  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 19  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 2.8411  Validation loss = 3.0214  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 2.8130  Validation loss = 2.1623  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 2.6705  Validation loss = 2.9759  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 3.1213  Validation loss = 4.6597  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 2.9373  Validation loss = 3.0205  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 2.9129  Validation loss = 3.5027  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 2.8518  Validation loss = 2.6547  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 2.7056  Validation loss = 3.1874  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 3.0402  Validation loss = 3.9825  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 3.3251  Validation loss = 4.0820  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 2.6313  Validation loss = 2.4859  \n",
      "\n",
      "Fold: 20  Epoch: 12  Training loss = 2.7247  Validation loss = 2.8331  \n",
      "\n",
      "Fold: 20  Epoch: 13  Training loss = 2.7593  Validation loss = 2.2792  \n",
      "\n",
      "Fold: 20  Epoch: 14  Training loss = 2.8598  Validation loss = 1.9450  \n",
      "\n",
      "Fold: 20  Epoch: 15  Training loss = 2.6920  Validation loss = 2.7175  \n",
      "\n",
      "Fold: 20  Epoch: 16  Training loss = 2.6934  Validation loss = 2.8438  \n",
      "\n",
      "Fold: 20  Epoch: 17  Training loss = 2.5821  Validation loss = 2.9087  \n",
      "\n",
      "Fold: 20  Epoch: 18  Training loss = 3.0174  Validation loss = 4.5429  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 14  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 2.8100  Validation loss = 2.3595  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 2.8793  Validation loss = 2.3559  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 2.8363  Validation loss = 2.1676  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 2.7987  Validation loss = 2.1945  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 3.0227  Validation loss = 2.6027  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 2.7887  Validation loss = 1.6941  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 3.0659  Validation loss = 2.6003  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 2.9110  Validation loss = 2.8384  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 3.1706  Validation loss = 2.3934  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 2.7952  Validation loss = 2.3757  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 2.9660  Validation loss = 2.5002  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 2.7949  Validation loss = 2.2149  \n",
      "\n",
      "Fold: 21  Epoch: 13  Training loss = 2.9197  Validation loss = 2.6863  \n",
      "\n",
      "Fold: 21  Epoch: 14  Training loss = 3.0306  Validation loss = 2.4484  \n",
      "\n",
      "Fold: 21  Epoch: 15  Training loss = 2.9080  Validation loss = 2.3361  \n",
      "\n",
      "Fold: 21  Epoch: 16  Training loss = 2.8560  Validation loss = 2.5352  \n",
      "\n",
      "Fold: 21  Epoch: 17  Training loss = 2.8472  Validation loss = 2.1157  \n",
      "\n",
      "Fold: 21  Epoch: 18  Training loss = 2.9088  Validation loss = 2.0278  \n",
      "\n",
      "Fold: 21  Epoch: 19  Training loss = 3.0542  Validation loss = 1.8338  \n",
      "\n",
      "Fold: 21  Epoch: 20  Training loss = 2.9589  Validation loss = 1.7102  \n",
      "\n",
      "Fold: 21  Epoch: 21  Training loss = 2.9890  Validation loss = 1.8202  \n",
      "\n",
      "Fold: 21  Epoch: 22  Training loss = 3.0314  Validation loss = 1.9757  \n",
      "\n",
      "Fold: 21  Epoch: 23  Training loss = 2.9148  Validation loss = 1.7574  \n",
      "\n",
      "Fold: 21  Epoch: 24  Training loss = 2.9462  Validation loss = 2.1385  \n",
      "\n",
      "Fold: 21  Epoch: 25  Training loss = 2.7866  Validation loss = 1.7168  \n",
      "\n",
      "Fold: 21  Epoch: 26  Training loss = 2.8250  Validation loss = 1.9059  \n",
      "\n",
      "Fold: 21  Epoch: 27  Training loss = 2.8721  Validation loss = 2.0030  \n",
      "\n",
      "Fold: 21  Epoch: 28  Training loss = 2.7525  Validation loss = 1.8259  \n",
      "\n",
      "Fold: 21  Epoch: 29  Training loss = 2.7773  Validation loss = 1.9808  \n",
      "\n",
      "Fold: 21  Epoch: 30  Training loss = 2.7571  Validation loss = 1.8614  \n",
      "\n",
      "Fold: 21  Epoch: 31  Training loss = 2.9035  Validation loss = 2.1303  \n",
      "\n",
      "Fold: 21  Epoch: 32  Training loss = 2.7697  Validation loss = 1.5135  \n",
      "\n",
      "Fold: 21  Epoch: 33  Training loss = 2.8759  Validation loss = 1.8230  \n",
      "\n",
      "Fold: 21  Epoch: 34  Training loss = 2.8275  Validation loss = 1.6606  \n",
      "\n",
      "Fold: 21  Epoch: 35  Training loss = 2.7623  Validation loss = 1.6060  \n",
      "\n",
      "Fold: 21  Epoch: 36  Training loss = 3.3645  Validation loss = 2.1468  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 32  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 3.2515  Validation loss = 0.9714  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 2.6602  Validation loss = 1.7806  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 2.8243  Validation loss = 1.2921  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 2.6935  Validation loss = 1.9954  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 2.7980  Validation loss = 2.5370  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 2.6899  Validation loss = 1.9260  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 2.7329  Validation loss = 2.1565  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 3.0986  Validation loss = 2.5974  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 2.7360  Validation loss = 2.9806  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 2.9095  Validation loss = 3.7721  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 2.8574  Validation loss = 3.0781  \n",
      "\n",
      "Fold: 22  Epoch: 12  Training loss = 2.7892  Validation loss = 3.6606  \n",
      "\n",
      "Fold: 22  Epoch: 13  Training loss = 2.8939  Validation loss = 3.5602  \n",
      "\n",
      "Fold: 22  Epoch: 14  Training loss = 2.7504  Validation loss = 3.9155  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 1  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 2.7172  Validation loss = 4.2328  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 2.4684  Validation loss = 3.5686  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 2.4259  Validation loss = 4.0380  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 2.8133  Validation loss = 3.9008  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 2.6801  Validation loss = 3.3507  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 2.7668  Validation loss = 2.6871  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 2.6054  Validation loss = 3.3017  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 2.7338  Validation loss = 3.5490  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 2.7021  Validation loss = 3.4004  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 2.8297  Validation loss = 2.2995  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 2.8834  Validation loss = 3.8589  \n",
      "\n",
      "Fold: 23  Epoch: 12  Training loss = 2.5320  Validation loss = 3.0479  \n",
      "\n",
      "Fold: 23  Epoch: 13  Training loss = 2.6116  Validation loss = 2.8627  \n",
      "\n",
      "Fold: 23  Epoch: 14  Training loss = 2.6167  Validation loss = 2.7946  \n",
      "\n",
      "Fold: 23  Epoch: 15  Training loss = 2.5356  Validation loss = 3.0058  \n",
      "\n",
      "Fold: 23  Epoch: 16  Training loss = 2.6324  Validation loss = 2.5186  \n",
      "\n",
      "Fold: 23  Epoch: 17  Training loss = 2.7287  Validation loss = 3.7168  \n",
      "\n",
      "Fold: 23  Epoch: 18  Training loss = 2.6004  Validation loss = 3.1650  \n",
      "\n",
      "Fold: 23  Epoch: 19  Training loss = 2.6454  Validation loss = 2.6440  \n",
      "\n",
      "Fold: 23  Epoch: 20  Training loss = 2.4418  Validation loss = 3.0041  \n",
      "\n",
      "Fold: 23  Epoch: 21  Training loss = 2.4340  Validation loss = 3.1286  \n",
      "\n",
      "Fold: 23  Epoch: 22  Training loss = 2.3659  Validation loss = 2.7936  \n",
      "\n",
      "Fold: 23  Epoch: 23  Training loss = 2.5090  Validation loss = 2.6563  \n",
      "\n",
      "Fold: 23  Epoch: 24  Training loss = 2.5507  Validation loss = 2.6477  \n",
      "\n",
      "Fold: 23  Epoch: 25  Training loss = 2.9982  Validation loss = 4.5416  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 10  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 4.2009  Validation loss = 5.0035  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 3.4045  Validation loss = 1.7768  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 3.0539  Validation loss = 1.0090  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 3.2518  Validation loss = 0.8256  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 3.0560  Validation loss = 0.6643  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 2.8680  Validation loss = 1.0249  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 2.9428  Validation loss = 0.8549  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 2.9713  Validation loss = 1.7565  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 2.9865  Validation loss = 1.9798  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 2.8863  Validation loss = 1.5481  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 2.8243  Validation loss = 1.1106  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 2.8547  Validation loss = 1.6402  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 2.7730  Validation loss = 1.0043  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 2.8419  Validation loss = 1.4131  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 2.8235  Validation loss = 1.2445  \n",
      "\n",
      "Fold: 24  Epoch: 16  Training loss = 2.7773  Validation loss = 0.7970  \n",
      "\n",
      "Fold: 24  Epoch: 17  Training loss = 2.7757  Validation loss = 0.9854  \n",
      "\n",
      "Fold: 24  Epoch: 18  Training loss = 2.9007  Validation loss = 0.8276  \n",
      "\n",
      "Fold: 24  Epoch: 19  Training loss = 3.0321  Validation loss = 1.2573  \n",
      "\n",
      "Fold: 24  Epoch: 20  Training loss = 3.0694  Validation loss = 0.9591  \n",
      "\n",
      "Fold: 24  Epoch: 21  Training loss = 2.9636  Validation loss = 0.7784  \n",
      "\n",
      "Fold: 24  Epoch: 22  Training loss = 2.8025  Validation loss = 1.0903  \n",
      "\n",
      "Fold: 24  Epoch: 23  Training loss = 2.8858  Validation loss = 0.8869  \n",
      "\n",
      "Fold: 24  Epoch: 24  Training loss = 2.7982  Validation loss = 0.8378  \n",
      "\n",
      "Fold: 24  Epoch: 25  Training loss = 3.0418  Validation loss = 1.0219  \n",
      "\n",
      "Fold: 24  Epoch: 26  Training loss = 3.0450  Validation loss = 1.0873  \n",
      "\n",
      "Fold: 24  Epoch: 27  Training loss = 2.7578  Validation loss = 1.1231  \n",
      "\n",
      "Fold: 24  Epoch: 28  Training loss = 2.8518  Validation loss = 0.9734  \n",
      "\n",
      "Fold: 24  Epoch: 29  Training loss = 2.8003  Validation loss = 1.0313  \n",
      "\n",
      "Fold: 24  Epoch: 30  Training loss = 2.8441  Validation loss = 1.0600  \n",
      "\n",
      "Fold: 24  Epoch: 31  Training loss = 2.7884  Validation loss = 1.0893  \n",
      "\n",
      "Fold: 24  Epoch: 32  Training loss = 2.7740  Validation loss = 0.9012  \n",
      "\n",
      "Fold: 24  Epoch: 33  Training loss = 2.7283  Validation loss = 1.1942  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 5  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 2.6334  Validation loss = 1.8845  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 2.9737  Validation loss = 3.2622  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 2.7354  Validation loss = 0.6228  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 2.5798  Validation loss = 0.9439  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 2.6394  Validation loss = 1.7008  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 2.5726  Validation loss = 0.8512  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 2.6704  Validation loss = 1.6478  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 2.5787  Validation loss = 1.2508  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 2.5747  Validation loss = 1.3868  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 2.6116  Validation loss = 1.9326  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 2.5673  Validation loss = 1.7382  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 3.1012  Validation loss = 2.9456  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 3  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 2.5301  Validation loss = 1.5412  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 2.5466  Validation loss = 1.6586  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 2.5917  Validation loss = 1.4700  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 2.7794  Validation loss = 2.6691  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 2.8609  Validation loss = 2.1011  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 2.5646  Validation loss = 1.5815  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 2.5532  Validation loss = 1.1168  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 2.6025  Validation loss = 1.3015  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 2.5777  Validation loss = 1.2017  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 2.6450  Validation loss = 0.6079  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 3.0556  Validation loss = 3.2902  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 10  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 2.4898  Validation loss = 1.0419  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 2.4725  Validation loss = 1.4195  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 2.4587  Validation loss = 1.4921  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 2.9291  Validation loss = 3.0047  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 2.4440  Validation loss = 1.6792  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 2.4296  Validation loss = 1.5847  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 2.4241  Validation loss = 1.5124  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 2.4809  Validation loss = 1.7897  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 2.5192  Validation loss = 2.2693  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 2.4254  Validation loss = 1.2275  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 2.4079  Validation loss = 1.1864  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 2.4577  Validation loss = 1.0139  \n",
      "\n",
      "Fold: 27  Epoch: 13  Training loss = 2.4693  Validation loss = 1.3981  \n",
      "\n",
      "Fold: 27  Epoch: 14  Training loss = 2.4305  Validation loss = 1.4167  \n",
      "\n",
      "Fold: 27  Epoch: 15  Training loss = 2.4583  Validation loss = 1.9302  \n",
      "\n",
      "Fold: 27  Epoch: 16  Training loss = 2.4153  Validation loss = 1.9975  \n",
      "\n",
      "Fold: 27  Epoch: 17  Training loss = 2.4646  Validation loss = 2.1586  \n",
      "\n",
      "Fold: 27  Epoch: 18  Training loss = 2.8722  Validation loss = 3.1464  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 12  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 2.2705  Validation loss = 1.4444  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 2.2554  Validation loss = 1.2685  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 2.3701  Validation loss = 1.0241  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 2.2933  Validation loss = 1.7733  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 2.5750  Validation loss = 2.6997  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 2.4921  Validation loss = 1.9821  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 2.4256  Validation loss = 1.8709  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 2.3749  Validation loss = 1.4291  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 2.3535  Validation loss = 1.9116  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 2.5160  Validation loss = 1.0654  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 2.3196  Validation loss = 1.7119  \n",
      "\n",
      "Fold: 28  Epoch: 12  Training loss = 2.3192  Validation loss = 1.2400  \n",
      "\n",
      "Fold: 28  Epoch: 13  Training loss = 2.4094  Validation loss = 2.0619  \n",
      "\n",
      "Fold: 28  Epoch: 14  Training loss = 2.6321  Validation loss = 2.4257  \n",
      "\n",
      "Fold: 28  Epoch: 15  Training loss = 2.3774  Validation loss = 1.1273  \n",
      "\n",
      "Fold: 28  Epoch: 16  Training loss = 2.4603  Validation loss = 0.7036  \n",
      "\n",
      "Fold: 28  Epoch: 17  Training loss = 2.2926  Validation loss = 1.4267  \n",
      "\n",
      "Fold: 28  Epoch: 18  Training loss = 2.6828  Validation loss = 2.3368  \n",
      "\n",
      "Fold: 28  Epoch: 19  Training loss = 2.3999  Validation loss = 1.3564  \n",
      "\n",
      "Fold: 28  Epoch: 20  Training loss = 2.4602  Validation loss = 1.1485  \n",
      "\n",
      "Fold: 28  Epoch: 21  Training loss = 2.4539  Validation loss = 1.8274  \n",
      "\n",
      "Fold: 28  Epoch: 22  Training loss = 2.6043  Validation loss = 1.3328  \n",
      "\n",
      "Fold: 28  Epoch: 23  Training loss = 2.4370  Validation loss = 1.4768  \n",
      "\n",
      "Fold: 28  Epoch: 24  Training loss = 2.4392  Validation loss = 1.3527  \n",
      "\n",
      "Fold: 28  Epoch: 25  Training loss = 2.3400  Validation loss = 1.2558  \n",
      "\n",
      "Fold: 28  Epoch: 26  Training loss = 2.4611  Validation loss = 0.8902  \n",
      "\n",
      "Fold: 28  Epoch: 27  Training loss = 3.1116  Validation loss = 2.7105  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 16  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 2.6146  Validation loss = 3.3756  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 2.3107  Validation loss = 2.3697  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 2.2588  Validation loss = 2.4267  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 2.1739  Validation loss = 2.0059  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 2.4560  Validation loss = 3.0518  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 2.1248  Validation loss = 2.1729  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 2.3444  Validation loss = 1.1900  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 2.2271  Validation loss = 2.3201  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 2.1810  Validation loss = 1.5073  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 2.1437  Validation loss = 2.0227  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 2.0671  Validation loss = 1.8178  \n",
      "\n",
      "Fold: 29  Epoch: 12  Training loss = 2.0883  Validation loss = 1.8319  \n",
      "\n",
      "Fold: 29  Epoch: 13  Training loss = 2.0747  Validation loss = 1.9758  \n",
      "\n",
      "Fold: 29  Epoch: 14  Training loss = 2.1289  Validation loss = 2.3415  \n",
      "\n",
      "Fold: 29  Epoch: 15  Training loss = 2.1002  Validation loss = 1.2187  \n",
      "\n",
      "Fold: 29  Epoch: 16  Training loss = 2.0366  Validation loss = 1.6711  \n",
      "\n",
      "Fold: 29  Epoch: 17  Training loss = 2.0313  Validation loss = 1.2156  \n",
      "\n",
      "Fold: 29  Epoch: 18  Training loss = 1.9546  Validation loss = 1.7681  \n",
      "\n",
      "Fold: 29  Epoch: 19  Training loss = 1.9758  Validation loss = 1.5098  \n",
      "\n",
      "Fold: 29  Epoch: 20  Training loss = 1.9552  Validation loss = 1.6718  \n",
      "\n",
      "Fold: 29  Epoch: 21  Training loss = 2.0083  Validation loss = 1.4035  \n",
      "\n",
      "Fold: 29  Epoch: 22  Training loss = 1.9305  Validation loss = 1.8730  \n",
      "\n",
      "Fold: 29  Epoch: 23  Training loss = 1.9776  Validation loss = 2.1089  \n",
      "\n",
      "Fold: 29  Epoch: 24  Training loss = 1.9846  Validation loss = 2.2861  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 7  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 2.1230  Validation loss = 0.8054  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 2.0238  Validation loss = 0.7740  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 2.0413  Validation loss = 1.2776  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 2.0013  Validation loss = 0.8851  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 2.0444  Validation loss = 0.9734  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 2.3228  Validation loss = 1.8265  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 2.0385  Validation loss = 1.2036  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 1.8533  Validation loss = 1.0694  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 2.0774  Validation loss = 0.8059  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 1.9482  Validation loss = 0.9379  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 1.8558  Validation loss = 0.9263  \n",
      "\n",
      "Fold: 30  Epoch: 12  Training loss = 1.8200  Validation loss = 1.1159  \n",
      "\n",
      "Fold: 30  Epoch: 13  Training loss = 1.8291  Validation loss = 0.9062  \n",
      "\n",
      "Fold: 30  Epoch: 14  Training loss = 1.8436  Validation loss = 0.7246  \n",
      "\n",
      "Fold: 30  Epoch: 15  Training loss = 2.1747  Validation loss = 1.3334  \n",
      "\n",
      "Fold: 30  Epoch: 16  Training loss = 1.8666  Validation loss = 0.9252  \n",
      "\n",
      "Fold: 30  Epoch: 17  Training loss = 1.9609  Validation loss = 0.8854  \n",
      "\n",
      "Fold: 30  Epoch: 18  Training loss = 1.9439  Validation loss = 0.9334  \n",
      "\n",
      "Fold: 30  Epoch: 19  Training loss = 2.0384  Validation loss = 0.6710  \n",
      "\n",
      "Fold: 30  Epoch: 20  Training loss = 1.9546  Validation loss = 0.9463  \n",
      "\n",
      "Fold: 30  Epoch: 21  Training loss = 1.9508  Validation loss = 0.9344  \n",
      "\n",
      "Fold: 30  Epoch: 22  Training loss = 1.8299  Validation loss = 1.0683  \n",
      "\n",
      "Fold: 30  Epoch: 23  Training loss = 1.8946  Validation loss = 0.7174  \n",
      "\n",
      "Fold: 30  Epoch: 24  Training loss = 2.0248  Validation loss = 0.6757  \n",
      "\n",
      "Fold: 30  Epoch: 25  Training loss = 1.8740  Validation loss = 1.4256  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 19  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 1.7368  Validation loss = 1.0489  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 1.7856  Validation loss = 1.3641  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 1.8585  Validation loss = 0.6828  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 1.8053  Validation loss = 0.4651  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 1.9119  Validation loss = 1.6339  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 2.0007  Validation loss = 1.8497  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 1.7539  Validation loss = 1.0585  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 1.7651  Validation loss = 0.7847  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 1.7243  Validation loss = 0.7424  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 1.8370  Validation loss = 0.6297  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 1.7373  Validation loss = 0.4646  \n",
      "\n",
      "Fold: 31  Epoch: 12  Training loss = 1.7472  Validation loss = 0.5878  \n",
      "\n",
      "Fold: 31  Epoch: 13  Training loss = 1.7383  Validation loss = 0.9438  \n",
      "\n",
      "Fold: 31  Epoch: 14  Training loss = 1.8217  Validation loss = 1.4620  \n",
      "\n",
      "Fold: 31  Epoch: 15  Training loss = 1.7460  Validation loss = 0.5733  \n",
      "\n",
      "Fold: 31  Epoch: 16  Training loss = 1.7233  Validation loss = 1.0670  \n",
      "\n",
      "Fold: 31  Epoch: 17  Training loss = 1.7344  Validation loss = 1.1547  \n",
      "\n",
      "Fold: 31  Epoch: 18  Training loss = 1.7716  Validation loss = 1.0667  \n",
      "\n",
      "Fold: 31  Epoch: 19  Training loss = 1.8615  Validation loss = 1.6699  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 11  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 1.5782  Validation loss = 5.2986  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 1.6620  Validation loss = 6.3150  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 1.5301  Validation loss = 6.0450  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 1.5431  Validation loss = 5.9593  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 1.4987  Validation loss = 6.0247  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 1.5392  Validation loss = 5.5499  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 1.4907  Validation loss = 6.0478  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 1.6233  Validation loss = 5.1476  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 1.4996  Validation loss = 5.8547  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 1.5058  Validation loss = 5.7303  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 1.5646  Validation loss = 6.1604  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 1.6462  Validation loss = 5.0190  \n",
      "\n",
      "Fold: 32  Epoch: 13  Training loss = 1.4981  Validation loss = 5.5257  \n",
      "\n",
      "Fold: 32  Epoch: 14  Training loss = 1.5149  Validation loss = 5.5897  \n",
      "\n",
      "Fold: 32  Epoch: 15  Training loss = 1.5871  Validation loss = 6.2768  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 12  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    # Load model with lowest validation error for each fold\n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 11\n",
      "Average validation error: 3.56441\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 1.9937  Test loss = 4.3242  \n",
      "\n",
      "Epoch: 2  Training loss = 1.9225  Test loss = 4.1646  \n",
      "\n",
      "Epoch: 3  Training loss = 1.8824  Test loss = 4.0631  \n",
      "\n",
      "Epoch: 4  Training loss = 1.8575  Test loss = 3.9946  \n",
      "\n",
      "Epoch: 5  Training loss = 1.8404  Test loss = 3.9455  \n",
      "\n",
      "Epoch: 6  Training loss = 1.8274  Test loss = 3.9085  \n",
      "\n",
      "Epoch: 7  Training loss = 1.8166  Test loss = 3.8789  \n",
      "\n",
      "Epoch: 8  Training loss = 1.8070  Test loss = 3.8543  \n",
      "\n",
      "Epoch: 9  Training loss = 1.7982  Test loss = 3.8331  \n",
      "\n",
      "Epoch: 10  Training loss = 1.7899  Test loss = 3.8142  \n",
      "\n",
      "Epoch: 11  Training loss = 1.7820  Test loss = 3.7969  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nexAEQcOIwh4EQdAw/gUmH0vqnoIBQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc5b8ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\na2hPdcYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd2afeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 3.79688064943 \n",
      "Fixed scheme MAE:  2.78475849672\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 1.7820  Test loss = 2.6332  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 1.7937  Test loss = 2.1315  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 1.8094  Test loss = 1.3154  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 1.8142  Test loss = 0.5221  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 1.4742  Test loss = 1.1370  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 1.4519  Test loss = 1.3428  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 1.4615  Test loss = 4.7303  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 1.5748  Test loss = 0.8468  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 1.3670  Test loss = 1.2264  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 1.3687  Test loss = 0.2152  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 1.3494  Test loss = 0.3685  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 1.3480  Test loss = 2.5215  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 1.2486  Test loss = 0.1207  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 1.2397  Test loss = 1.1810  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 1.2482  Test loss = 2.9314  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 1.3000  Test loss = 4.7644  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 1.3149  Test loss = 2.9379  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 1.3641  Test loss = 0.4006  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 1.3642  Test loss = 2.9007  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 1.3815  Test loss = 0.3328  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 1.2633  Test loss = 1.0798  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 1.2703  Test loss = 3.8793  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 1.3503  Test loss = 1.0590  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 1.3408  Test loss = 1.6372  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 1.2979  Test loss = 0.1519  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 1.2975  Test loss = 0.5292  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 1.2987  Test loss = 0.3157  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 1.2877  Test loss = 0.7538  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 1.2059  Test loss = 1.5761  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 1.2123  Test loss = 0.1656  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 1.2036  Test loss = 3.8435  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 1.2406  Test loss = 1.4368  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 1.1912  Test loss = 0.4055  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 1.1913  Test loss = 1.1492  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 1.1128  Test loss = 1.2824  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 1.1086  Test loss = 4.8301  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 1.2112  Test loss = 0.9317  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 1.1107  Test loss = 1.3897  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 1.1236  Test loss = 1.8155  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 1.1458  Test loss = 2.5257  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 1.1567  Test loss = 2.0574  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 1.1835  Test loss = 3.0203  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 1.2390  Test loss = 4.2058  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 1.3431  Test loss = 14.0136  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 2.1399  Test loss = 5.5566  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.2479  Test loss = 0.6286  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.2492  Test loss = 0.7837  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.2512  Test loss = 0.3440  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 1.9948  Test loss = 2.4370  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 2.0059  Test loss = 1.5366  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 2.0129  Test loss = 1.0071  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 2.0168  Test loss = 1.1042  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 1.8422  Test loss = 1.8717  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 1.8566  Test loss = 1.0378  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 1.8598  Test loss = 1.3232  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 1.8532  Test loss = 1.8357  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 1.7532  Test loss = 2.3229  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 1.7762  Test loss = 4.8521  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 1.8724  Test loss = 0.8886  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 1.8748  Test loss = 0.1165  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.6964  Test loss = 1.2160  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.6963  Test loss = 2.5172  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 1.7239  Test loss = 0.4624  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 1.7218  Test loss = 1.3457  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 1.6773  Test loss = 3.2018  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 1.7229  Test loss = 3.2678  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.7597  Test loss = 0.8585  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 1.7606  Test loss = 3.6519  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 1.6366  Test loss = 4.4641  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 1.7276  Test loss = 0.2358  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 1.7269  Test loss = 0.4043  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 1.7276  Test loss = 1.5482  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 1.5897  Test loss = 2.3312  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 1.6156  Test loss = 0.9675  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 1.6201  Test loss = 0.0487  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 1.6191  Test loss = 1.1853  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 1.6332  Test loss = 1.1197  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAZiZ9aj/AtCxVCH5QKVnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd044ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.70358981044 \n",
      "Updating scheme MAE:  1.88424948838\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_test_fixed.csv\", yhat_test_fixed)\n",
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_update.csv\", yhat_update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
