{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"4Q/64_units/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 500\n",
    "learning_rate = 1e-4\n",
    "batch_size = 5\n",
    "early_stop_iters = 10\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 64 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag4',\n",
    "                                       'inflation.lag5',\n",
    "                                       'inflation.lag6',\n",
    "                                       'inflation.lag7']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag4',\n",
    "                                   'unemp.lag5',\n",
    "                                   'unemp.lag6',\n",
    "                                   'unemp.lag7']])\n",
    "train_4lag_oil = np.array(train[['oil.lag4',\n",
    "                                 'oil.lag5',\n",
    "                                 'oil.lag6',\n",
    "                                 'oil.lag7']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag4',\n",
    "                                     'inflation.lag5',\n",
    "                                     'inflation.lag6',\n",
    "                                     'inflation.lag7']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag4',\n",
    "                                 'unemp.lag5',\n",
    "                                 'unemp.lag6',\n",
    "                                 'unemp.lag7']])\n",
    "test_4lag_oil = np.array(test[['oil.lag4',\n",
    "                               'oil.lag5',\n",
    "                               'oil.lag6',\n",
    "                               'oil.lag7']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 64 \n",
      "Learning rate = 0.0001 \n",
      "Epochs = 500 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 10 \n",
      "Learning rate = 0.0001\n",
      "Fold: 1  Epoch: 1  Training loss = 3.2575  Validation loss = 3.4637  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 3.2410  Validation loss = 3.4338  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 3.2101  Validation loss = 3.3800  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 3.1870  Validation loss = 3.3371  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 3.1620  Validation loss = 3.2918  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 3.1398  Validation loss = 3.2513  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 3.1243  Validation loss = 3.2226  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 3.1065  Validation loss = 3.1883  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 3.0854  Validation loss = 3.1483  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 3.0737  Validation loss = 3.1257  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 3.0565  Validation loss = 3.0914  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 3.0334  Validation loss = 3.0473  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 3.0236  Validation loss = 3.0263  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 3.0104  Validation loss = 2.9982  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 3.0017  Validation loss = 2.9796  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 2.9885  Validation loss = 2.9504  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 2.9765  Validation loss = 2.9252  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 2.9664  Validation loss = 2.9014  \n",
      "\n",
      "Fold: 1  Epoch: 19  Training loss = 2.9513  Validation loss = 2.8688  \n",
      "\n",
      "Fold: 1  Epoch: 20  Training loss = 2.9367  Validation loss = 2.8355  \n",
      "\n",
      "Fold: 1  Epoch: 21  Training loss = 2.9271  Validation loss = 2.8137  \n",
      "\n",
      "Fold: 1  Epoch: 22  Training loss = 2.9141  Validation loss = 2.7836  \n",
      "\n",
      "Fold: 1  Epoch: 23  Training loss = 2.9091  Validation loss = 2.7717  \n",
      "\n",
      "Fold: 1  Epoch: 24  Training loss = 2.8995  Validation loss = 2.7475  \n",
      "\n",
      "Fold: 1  Epoch: 25  Training loss = 2.8898  Validation loss = 2.7249  \n",
      "\n",
      "Fold: 1  Epoch: 26  Training loss = 2.8780  Validation loss = 2.6956  \n",
      "\n",
      "Fold: 1  Epoch: 27  Training loss = 2.8663  Validation loss = 2.6662  \n",
      "\n",
      "Fold: 1  Epoch: 28  Training loss = 2.8612  Validation loss = 2.6518  \n",
      "\n",
      "Fold: 1  Epoch: 29  Training loss = 2.8559  Validation loss = 2.6390  \n",
      "\n",
      "Fold: 1  Epoch: 30  Training loss = 2.8446  Validation loss = 2.6104  \n",
      "\n",
      "Fold: 1  Epoch: 31  Training loss = 2.8344  Validation loss = 2.5836  \n",
      "\n",
      "Fold: 1  Epoch: 32  Training loss = 2.8230  Validation loss = 2.5540  \n",
      "\n",
      "Fold: 1  Epoch: 33  Training loss = 2.8104  Validation loss = 2.5212  \n",
      "\n",
      "Fold: 1  Epoch: 34  Training loss = 2.8040  Validation loss = 2.5026  \n",
      "\n",
      "Fold: 1  Epoch: 35  Training loss = 2.7962  Validation loss = 2.4812  \n",
      "\n",
      "Fold: 1  Epoch: 36  Training loss = 2.7894  Validation loss = 2.4608  \n",
      "\n",
      "Fold: 1  Epoch: 37  Training loss = 2.7864  Validation loss = 2.4510  \n",
      "\n",
      "Fold: 1  Epoch: 38  Training loss = 2.7828  Validation loss = 2.4413  \n",
      "\n",
      "Fold: 1  Epoch: 39  Training loss = 2.7799  Validation loss = 2.4314  \n",
      "\n",
      "Fold: 1  Epoch: 40  Training loss = 2.7722  Validation loss = 2.4079  \n",
      "\n",
      "Fold: 1  Epoch: 41  Training loss = 2.7639  Validation loss = 2.3821  \n",
      "\n",
      "Fold: 1  Epoch: 42  Training loss = 2.7590  Validation loss = 2.3665  \n",
      "\n",
      "Fold: 1  Epoch: 43  Training loss = 2.7480  Validation loss = 2.3334  \n",
      "\n",
      "Fold: 1  Epoch: 44  Training loss = 2.7452  Validation loss = 2.3242  \n",
      "\n",
      "Fold: 1  Epoch: 45  Training loss = 2.7369  Validation loss = 2.2975  \n",
      "\n",
      "Fold: 1  Epoch: 46  Training loss = 2.7304  Validation loss = 2.2739  \n",
      "\n",
      "Fold: 1  Epoch: 47  Training loss = 2.7259  Validation loss = 2.2561  \n",
      "\n",
      "Fold: 1  Epoch: 48  Training loss = 2.7219  Validation loss = 2.2439  \n",
      "\n",
      "Fold: 1  Epoch: 49  Training loss = 2.7146  Validation loss = 2.2187  \n",
      "\n",
      "Fold: 1  Epoch: 50  Training loss = 2.7112  Validation loss = 2.2067  \n",
      "\n",
      "Fold: 1  Epoch: 51  Training loss = 2.7091  Validation loss = 2.2011  \n",
      "\n",
      "Fold: 1  Epoch: 52  Training loss = 2.7039  Validation loss = 2.1805  \n",
      "\n",
      "Fold: 1  Epoch: 53  Training loss = 2.7005  Validation loss = 2.1689  \n",
      "\n",
      "Fold: 1  Epoch: 54  Training loss = 2.6939  Validation loss = 2.1437  \n",
      "\n",
      "Fold: 1  Epoch: 55  Training loss = 2.6910  Validation loss = 2.1311  \n",
      "\n",
      "Fold: 1  Epoch: 56  Training loss = 2.6875  Validation loss = 2.1192  \n",
      "\n",
      "Fold: 1  Epoch: 57  Training loss = 2.6819  Validation loss = 2.0995  \n",
      "\n",
      "Fold: 1  Epoch: 58  Training loss = 2.6805  Validation loss = 2.0972  \n",
      "\n",
      "Fold: 1  Epoch: 59  Training loss = 2.6786  Validation loss = 2.0896  \n",
      "\n",
      "Fold: 1  Epoch: 60  Training loss = 2.6784  Validation loss = 2.0896  \n",
      "\n",
      "Fold: 1  Epoch: 61  Training loss = 2.6783  Validation loss = 2.0928  \n",
      "\n",
      "Fold: 1  Epoch: 62  Training loss = 2.6765  Validation loss = 2.0858  \n",
      "\n",
      "Fold: 1  Epoch: 63  Training loss = 2.6758  Validation loss = 2.0851  \n",
      "\n",
      "Fold: 1  Epoch: 64  Training loss = 2.6747  Validation loss = 2.0838  \n",
      "\n",
      "Fold: 1  Epoch: 65  Training loss = 2.6728  Validation loss = 2.0751  \n",
      "\n",
      "Fold: 1  Epoch: 66  Training loss = 2.6685  Validation loss = 2.0579  \n",
      "\n",
      "Fold: 1  Epoch: 67  Training loss = 2.6643  Validation loss = 2.0397  \n",
      "\n",
      "Fold: 1  Epoch: 68  Training loss = 2.6629  Validation loss = 2.0336  \n",
      "\n",
      "Fold: 1  Epoch: 69  Training loss = 2.6601  Validation loss = 2.0196  \n",
      "\n",
      "Fold: 1  Epoch: 70  Training loss = 2.6573  Validation loss = 2.0099  \n",
      "\n",
      "Fold: 1  Epoch: 71  Training loss = 2.6515  Validation loss = 1.9840  \n",
      "\n",
      "Fold: 1  Epoch: 72  Training loss = 2.6489  Validation loss = 1.9740  \n",
      "\n",
      "Fold: 1  Epoch: 73  Training loss = 2.6483  Validation loss = 1.9725  \n",
      "\n",
      "Fold: 1  Epoch: 74  Training loss = 2.6457  Validation loss = 1.9594  \n",
      "\n",
      "Fold: 1  Epoch: 75  Training loss = 2.6426  Validation loss = 1.9475  \n",
      "\n",
      "Fold: 1  Epoch: 76  Training loss = 2.6398  Validation loss = 1.9341  \n",
      "\n",
      "Fold: 1  Epoch: 77  Training loss = 2.6369  Validation loss = 1.9186  \n",
      "\n",
      "Fold: 1  Epoch: 78  Training loss = 2.6361  Validation loss = 1.9178  \n",
      "\n",
      "Fold: 1  Epoch: 79  Training loss = 2.6360  Validation loss = 1.9196  \n",
      "\n",
      "Fold: 1  Epoch: 80  Training loss = 2.6332  Validation loss = 1.9056  \n",
      "\n",
      "Fold: 1  Epoch: 81  Training loss = 2.6301  Validation loss = 1.8904  \n",
      "\n",
      "Fold: 1  Epoch: 82  Training loss = 2.6294  Validation loss = 1.8828  \n",
      "\n",
      "Fold: 1  Epoch: 83  Training loss = 2.6284  Validation loss = 1.8790  \n",
      "\n",
      "Fold: 1  Epoch: 84  Training loss = 2.6244  Validation loss = 1.8546  \n",
      "\n",
      "Fold: 1  Epoch: 85  Training loss = 2.6245  Validation loss = 1.8597  \n",
      "\n",
      "Fold: 1  Epoch: 86  Training loss = 2.6248  Validation loss = 1.8667  \n",
      "\n",
      "Fold: 1  Epoch: 87  Training loss = 2.6238  Validation loss = 1.8634  \n",
      "\n",
      "Fold: 1  Epoch: 88  Training loss = 2.6222  Validation loss = 1.8577  \n",
      "\n",
      "Fold: 1  Epoch: 89  Training loss = 2.6203  Validation loss = 1.8444  \n",
      "\n",
      "Fold: 1  Epoch: 90  Training loss = 2.6186  Validation loss = 1.8412  \n",
      "\n",
      "Fold: 1  Epoch: 91  Training loss = 2.6163  Validation loss = 1.8265  \n",
      "\n",
      "Fold: 1  Epoch: 92  Training loss = 2.6142  Validation loss = 1.8138  \n",
      "\n",
      "Fold: 1  Epoch: 93  Training loss = 2.6120  Validation loss = 1.7976  \n",
      "\n",
      "Fold: 1  Epoch: 94  Training loss = 2.6115  Validation loss = 1.7950  \n",
      "\n",
      "Fold: 1  Epoch: 95  Training loss = 2.6096  Validation loss = 1.7836  \n",
      "\n",
      "Fold: 1  Epoch: 96  Training loss = 2.6104  Validation loss = 1.7988  \n",
      "\n",
      "Fold: 1  Epoch: 97  Training loss = 2.6094  Validation loss = 1.7937  \n",
      "\n",
      "Fold: 1  Epoch: 98  Training loss = 2.6077  Validation loss = 1.7823  \n",
      "\n",
      "Fold: 1  Epoch: 99  Training loss = 2.6074  Validation loss = 1.7855  \n",
      "\n",
      "Fold: 1  Epoch: 100  Training loss = 2.6059  Validation loss = 1.7781  \n",
      "\n",
      "Fold: 1  Epoch: 101  Training loss = 2.6058  Validation loss = 1.7797  \n",
      "\n",
      "Fold: 1  Epoch: 102  Training loss = 2.6037  Validation loss = 1.7664  \n",
      "\n",
      "Fold: 1  Epoch: 103  Training loss = 2.6011  Validation loss = 1.7525  \n",
      "\n",
      "Fold: 1  Epoch: 104  Training loss = 2.5991  Validation loss = 1.7385  \n",
      "\n",
      "Fold: 1  Epoch: 105  Training loss = 2.5994  Validation loss = 1.7462  \n",
      "\n",
      "Fold: 1  Epoch: 106  Training loss = 2.5982  Validation loss = 1.7411  \n",
      "\n",
      "Fold: 1  Epoch: 107  Training loss = 2.5968  Validation loss = 1.7431  \n",
      "\n",
      "Fold: 1  Epoch: 108  Training loss = 2.5968  Validation loss = 1.7521  \n",
      "\n",
      "Fold: 1  Epoch: 109  Training loss = 2.5938  Validation loss = 1.7279  \n",
      "\n",
      "Fold: 1  Epoch: 110  Training loss = 2.5926  Validation loss = 1.7213  \n",
      "\n",
      "Fold: 1  Epoch: 111  Training loss = 2.5906  Validation loss = 1.7066  \n",
      "\n",
      "Fold: 1  Epoch: 112  Training loss = 2.5902  Validation loss = 1.7055  \n",
      "\n",
      "Fold: 1  Epoch: 113  Training loss = 2.5895  Validation loss = 1.6984  \n",
      "\n",
      "Fold: 1  Epoch: 114  Training loss = 2.5878  Validation loss = 1.6854  \n",
      "\n",
      "Fold: 1  Epoch: 115  Training loss = 2.5872  Validation loss = 1.6838  \n",
      "\n",
      "Fold: 1  Epoch: 116  Training loss = 2.5864  Validation loss = 1.6794  \n",
      "\n",
      "Fold: 1  Epoch: 117  Training loss = 2.5851  Validation loss = 1.6725  \n",
      "\n",
      "Fold: 1  Epoch: 118  Training loss = 2.5848  Validation loss = 1.6750  \n",
      "\n",
      "Fold: 1  Epoch: 119  Training loss = 2.5832  Validation loss = 1.6644  \n",
      "\n",
      "Fold: 1  Epoch: 120  Training loss = 2.5830  Validation loss = 1.6651  \n",
      "\n",
      "Fold: 1  Epoch: 121  Training loss = 2.5818  Validation loss = 1.6559  \n",
      "\n",
      "Fold: 1  Epoch: 122  Training loss = 2.5821  Validation loss = 1.6685  \n",
      "\n",
      "Fold: 1  Epoch: 123  Training loss = 2.5815  Validation loss = 1.6649  \n",
      "\n",
      "Fold: 1  Epoch: 124  Training loss = 2.5811  Validation loss = 1.6648  \n",
      "\n",
      "Fold: 1  Epoch: 125  Training loss = 2.5805  Validation loss = 1.6631  \n",
      "\n",
      "Fold: 1  Epoch: 126  Training loss = 2.5794  Validation loss = 1.6564  \n",
      "\n",
      "Fold: 1  Epoch: 127  Training loss = 2.5796  Validation loss = 1.6593  \n",
      "\n",
      "Fold: 1  Epoch: 128  Training loss = 2.5789  Validation loss = 1.6528  \n",
      "\n",
      "Fold: 1  Epoch: 129  Training loss = 2.5779  Validation loss = 1.6476  \n",
      "\n",
      "Fold: 1  Epoch: 130  Training loss = 2.5769  Validation loss = 1.6447  \n",
      "\n",
      "Fold: 1  Epoch: 131  Training loss = 2.5769  Validation loss = 1.6450  \n",
      "\n",
      "Fold: 1  Epoch: 132  Training loss = 2.5767  Validation loss = 1.6443  \n",
      "\n",
      "Fold: 1  Epoch: 133  Training loss = 2.5764  Validation loss = 1.6506  \n",
      "\n",
      "Fold: 1  Epoch: 134  Training loss = 2.5757  Validation loss = 1.6549  \n",
      "\n",
      "Fold: 1  Epoch: 135  Training loss = 2.5756  Validation loss = 1.6585  \n",
      "\n",
      "Fold: 1  Epoch: 136  Training loss = 2.5752  Validation loss = 1.6582  \n",
      "\n",
      "Fold: 1  Epoch: 137  Training loss = 2.5747  Validation loss = 1.6605  \n",
      "\n",
      "Check model:  Fold: 1  Epoch: 132  Training loss = 2.5747  Validation loss = 1.6605  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.4828  Validation loss = 2.0719  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.4817  Validation loss = 2.0658  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.4801  Validation loss = 2.0524  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.4790  Validation loss = 2.0482  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.4787  Validation loss = 2.0520  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.4787  Validation loss = 2.0502  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.4774  Validation loss = 2.0431  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.4768  Validation loss = 2.0367  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.4763  Validation loss = 2.0353  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.4756  Validation loss = 2.0314  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.4752  Validation loss = 2.0314  \n",
      "\n",
      "Fold: 2  Epoch: 12  Training loss = 2.4752  Validation loss = 2.0359  \n",
      "\n",
      "Fold: 2  Epoch: 13  Training loss = 2.4745  Validation loss = 2.0318  \n",
      "\n",
      "Fold: 2  Epoch: 14  Training loss = 2.4730  Validation loss = 2.0239  \n",
      "\n",
      "Fold: 2  Epoch: 15  Training loss = 2.4730  Validation loss = 2.0270  \n",
      "\n",
      "Fold: 2  Epoch: 16  Training loss = 2.4723  Validation loss = 2.0224  \n",
      "\n",
      "Fold: 2  Epoch: 17  Training loss = 2.4717  Validation loss = 2.0186  \n",
      "\n",
      "Fold: 2  Epoch: 18  Training loss = 2.4710  Validation loss = 2.0158  \n",
      "\n",
      "Fold: 2  Epoch: 19  Training loss = 2.4711  Validation loss = 2.0201  \n",
      "\n",
      "Fold: 2  Epoch: 20  Training loss = 2.4703  Validation loss = 2.0134  \n",
      "\n",
      "Fold: 2  Epoch: 21  Training loss = 2.4703  Validation loss = 2.0158  \n",
      "\n",
      "Fold: 2  Epoch: 22  Training loss = 2.4702  Validation loss = 2.0164  \n",
      "\n",
      "Fold: 2  Epoch: 23  Training loss = 2.4696  Validation loss = 2.0115  \n",
      "\n",
      "Fold: 2  Epoch: 24  Training loss = 2.4691  Validation loss = 2.0123  \n",
      "\n",
      "Fold: 2  Epoch: 25  Training loss = 2.4688  Validation loss = 2.0151  \n",
      "\n",
      "Fold: 2  Epoch: 26  Training loss = 2.4686  Validation loss = 2.0139  \n",
      "\n",
      "Fold: 2  Epoch: 27  Training loss = 2.4684  Validation loss = 2.0191  \n",
      "\n",
      "Fold: 2  Epoch: 28  Training loss = 2.4678  Validation loss = 2.0198  \n",
      "\n",
      "Fold: 2  Epoch: 29  Training loss = 2.4673  Validation loss = 2.0154  \n",
      "\n",
      "Fold: 2  Epoch: 30  Training loss = 2.4672  Validation loss = 2.0195  \n",
      "\n",
      "Fold: 2  Epoch: 31  Training loss = 2.4671  Validation loss = 2.0243  \n",
      "\n",
      "Check model:  Fold: 2  Epoch: 23  Training loss = 2.4671  Validation loss = 2.0243  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.5285  Validation loss = 3.2417  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.5274  Validation loss = 3.2473  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.5263  Validation loss = 3.2518  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.5250  Validation loss = 3.2613  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.5239  Validation loss = 3.2699  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.5230  Validation loss = 3.2723  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.5216  Validation loss = 3.2753  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.5210  Validation loss = 3.2759  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.5205  Validation loss = 3.2757  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.5199  Validation loss = 3.2753  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.5193  Validation loss = 3.2770  \n",
      "\n",
      "Check model:  Fold: 3  Epoch: 1  Training loss = 1.5193  Validation loss = 3.2770  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.6200  Validation loss = 4.5518  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.6193  Validation loss = 4.5438  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.6182  Validation loss = 4.5316  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.6178  Validation loss = 4.5298  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.6174  Validation loss = 4.5262  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.6164  Validation loss = 4.5217  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.6155  Validation loss = 4.5116  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.6147  Validation loss = 4.5114  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.6141  Validation loss = 4.5107  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 1.6131  Validation loss = 4.5152  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.6120  Validation loss = 4.5164  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 1.6114  Validation loss = 4.5057  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.6106  Validation loss = 4.4969  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 1.6098  Validation loss = 4.5001  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.6093  Validation loss = 4.4920  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 1.6085  Validation loss = 4.4875  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 1.6078  Validation loss = 4.4847  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 1.6069  Validation loss = 4.4761  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 1.6059  Validation loss = 4.4736  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 1.6051  Validation loss = 4.4786  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 1.6045  Validation loss = 4.4812  \n",
      "\n",
      "Fold: 4  Epoch: 22  Training loss = 1.6037  Validation loss = 4.4792  \n",
      "\n",
      "Fold: 4  Epoch: 23  Training loss = 1.6030  Validation loss = 4.4733  \n",
      "\n",
      "Fold: 4  Epoch: 24  Training loss = 1.6022  Validation loss = 4.4637  \n",
      "\n",
      "Fold: 4  Epoch: 25  Training loss = 1.6015  Validation loss = 4.4645  \n",
      "\n",
      "Fold: 4  Epoch: 26  Training loss = 1.6005  Validation loss = 4.4617  \n",
      "\n",
      "Fold: 4  Epoch: 27  Training loss = 1.6002  Validation loss = 4.4675  \n",
      "\n",
      "Fold: 4  Epoch: 28  Training loss = 1.5989  Validation loss = 4.4536  \n",
      "\n",
      "Fold: 4  Epoch: 29  Training loss = 1.5983  Validation loss = 4.4517  \n",
      "\n",
      "Fold: 4  Epoch: 30  Training loss = 1.5981  Validation loss = 4.4571  \n",
      "\n",
      "Fold: 4  Epoch: 31  Training loss = 1.5972  Validation loss = 4.4625  \n",
      "\n",
      "Fold: 4  Epoch: 32  Training loss = 1.5965  Validation loss = 4.4566  \n",
      "\n",
      "Fold: 4  Epoch: 33  Training loss = 1.5961  Validation loss = 4.4539  \n",
      "\n",
      "Fold: 4  Epoch: 34  Training loss = 1.5955  Validation loss = 4.4527  \n",
      "\n",
      "Fold: 4  Epoch: 35  Training loss = 1.5950  Validation loss = 4.4513  \n",
      "\n",
      "Fold: 4  Epoch: 36  Training loss = 1.5941  Validation loss = 4.4441  \n",
      "\n",
      "Fold: 4  Epoch: 37  Training loss = 1.5939  Validation loss = 4.4455  \n",
      "\n",
      "Fold: 4  Epoch: 38  Training loss = 1.5930  Validation loss = 4.4411  \n",
      "\n",
      "Fold: 4  Epoch: 39  Training loss = 1.5924  Validation loss = 4.4328  \n",
      "\n",
      "Fold: 4  Epoch: 40  Training loss = 1.5919  Validation loss = 4.4268  \n",
      "\n",
      "Fold: 4  Epoch: 41  Training loss = 1.5910  Validation loss = 4.4202  \n",
      "\n",
      "Fold: 4  Epoch: 42  Training loss = 1.5904  Validation loss = 4.4147  \n",
      "\n",
      "Fold: 4  Epoch: 43  Training loss = 1.5899  Validation loss = 4.4019  \n",
      "\n",
      "Fold: 4  Epoch: 44  Training loss = 1.5895  Validation loss = 4.4079  \n",
      "\n",
      "Fold: 4  Epoch: 45  Training loss = 1.5889  Validation loss = 4.4095  \n",
      "\n",
      "Fold: 4  Epoch: 46  Training loss = 1.5882  Validation loss = 4.4046  \n",
      "\n",
      "Fold: 4  Epoch: 47  Training loss = 1.5879  Validation loss = 4.4073  \n",
      "\n",
      "Fold: 4  Epoch: 48  Training loss = 1.5872  Validation loss = 4.3988  \n",
      "\n",
      "Fold: 4  Epoch: 49  Training loss = 1.5869  Validation loss = 4.4075  \n",
      "\n",
      "Fold: 4  Epoch: 50  Training loss = 1.5864  Validation loss = 4.4015  \n",
      "\n",
      "Fold: 4  Epoch: 51  Training loss = 1.5854  Validation loss = 4.3810  \n",
      "\n",
      "Fold: 4  Epoch: 52  Training loss = 1.5850  Validation loss = 4.3747  \n",
      "\n",
      "Fold: 4  Epoch: 53  Training loss = 1.5848  Validation loss = 4.3815  \n",
      "\n",
      "Fold: 4  Epoch: 54  Training loss = 1.5838  Validation loss = 4.3657  \n",
      "\n",
      "Fold: 4  Epoch: 55  Training loss = 1.5834  Validation loss = 4.3618  \n",
      "\n",
      "Fold: 4  Epoch: 56  Training loss = 1.5829  Validation loss = 4.3724  \n",
      "\n",
      "Fold: 4  Epoch: 57  Training loss = 1.5823  Validation loss = 4.3639  \n",
      "\n",
      "Fold: 4  Epoch: 58  Training loss = 1.5822  Validation loss = 4.3752  \n",
      "\n",
      "Fold: 4  Epoch: 59  Training loss = 1.5816  Validation loss = 4.3712  \n",
      "\n",
      "Fold: 4  Epoch: 60  Training loss = 1.5813  Validation loss = 4.3752  \n",
      "\n",
      "Fold: 4  Epoch: 61  Training loss = 1.5808  Validation loss = 4.3731  \n",
      "\n",
      "Fold: 4  Epoch: 62  Training loss = 1.5801  Validation loss = 4.3645  \n",
      "\n",
      "Fold: 4  Epoch: 63  Training loss = 1.5800  Validation loss = 4.3680  \n",
      "\n",
      "Fold: 4  Epoch: 64  Training loss = 1.5798  Validation loss = 4.3727  \n",
      "\n",
      "Fold: 4  Epoch: 65  Training loss = 1.5796  Validation loss = 4.3677  \n",
      "\n",
      "Fold: 4  Epoch: 66  Training loss = 1.5791  Validation loss = 4.3646  \n",
      "\n",
      "Fold: 4  Epoch: 67  Training loss = 1.5789  Validation loss = 4.3680  \n",
      "\n",
      "Fold: 4  Epoch: 68  Training loss = 1.5788  Validation loss = 4.3728  \n",
      "\n",
      "Fold: 4  Epoch: 69  Training loss = 1.5781  Validation loss = 4.3642  \n",
      "\n",
      "Fold: 4  Epoch: 70  Training loss = 1.5776  Validation loss = 4.3595  \n",
      "\n",
      "Fold: 4  Epoch: 71  Training loss = 1.5772  Validation loss = 4.3595  \n",
      "\n",
      "Fold: 4  Epoch: 72  Training loss = 1.5770  Validation loss = 4.3599  \n",
      "\n",
      "Fold: 4  Epoch: 73  Training loss = 1.5764  Validation loss = 4.3511  \n",
      "\n",
      "Fold: 4  Epoch: 74  Training loss = 1.5761  Validation loss = 4.3431  \n",
      "\n",
      "Fold: 4  Epoch: 75  Training loss = 1.5757  Validation loss = 4.3352  \n",
      "\n",
      "Fold: 4  Epoch: 76  Training loss = 1.5752  Validation loss = 4.3343  \n",
      "\n",
      "Fold: 4  Epoch: 77  Training loss = 1.5748  Validation loss = 4.3246  \n",
      "\n",
      "Fold: 4  Epoch: 78  Training loss = 1.5744  Validation loss = 4.3313  \n",
      "\n",
      "Fold: 4  Epoch: 79  Training loss = 1.5742  Validation loss = 4.3298  \n",
      "\n",
      "Fold: 4  Epoch: 80  Training loss = 1.5739  Validation loss = 4.3217  \n",
      "\n",
      "Fold: 4  Epoch: 81  Training loss = 1.5736  Validation loss = 4.3193  \n",
      "\n",
      "Fold: 4  Epoch: 82  Training loss = 1.5733  Validation loss = 4.3189  \n",
      "\n",
      "Fold: 4  Epoch: 83  Training loss = 1.5730  Validation loss = 4.3235  \n",
      "\n",
      "Fold: 4  Epoch: 84  Training loss = 1.5726  Validation loss = 4.3247  \n",
      "\n",
      "Fold: 4  Epoch: 85  Training loss = 1.5723  Validation loss = 4.3244  \n",
      "\n",
      "Fold: 4  Epoch: 86  Training loss = 1.5719  Validation loss = 4.3302  \n",
      "\n",
      "Fold: 4  Epoch: 87  Training loss = 1.5713  Validation loss = 4.3204  \n",
      "\n",
      "Fold: 4  Epoch: 88  Training loss = 1.5711  Validation loss = 4.3229  \n",
      "\n",
      "Fold: 4  Epoch: 89  Training loss = 1.5707  Validation loss = 4.3133  \n",
      "\n",
      "Fold: 4  Epoch: 90  Training loss = 1.5700  Validation loss = 4.3085  \n",
      "\n",
      "Fold: 4  Epoch: 91  Training loss = 1.5696  Validation loss = 4.3136  \n",
      "\n",
      "Fold: 4  Epoch: 92  Training loss = 1.5690  Validation loss = 4.3000  \n",
      "\n",
      "Fold: 4  Epoch: 93  Training loss = 1.5686  Validation loss = 4.2944  \n",
      "\n",
      "Fold: 4  Epoch: 94  Training loss = 1.5680  Validation loss = 4.2935  \n",
      "\n",
      "Fold: 4  Epoch: 95  Training loss = 1.5678  Validation loss = 4.2970  \n",
      "\n",
      "Fold: 4  Epoch: 96  Training loss = 1.5676  Validation loss = 4.2973  \n",
      "\n",
      "Fold: 4  Epoch: 97  Training loss = 1.5671  Validation loss = 4.2970  \n",
      "\n",
      "Fold: 4  Epoch: 98  Training loss = 1.5666  Validation loss = 4.2943  \n",
      "\n",
      "Fold: 4  Epoch: 99  Training loss = 1.5660  Validation loss = 4.2872  \n",
      "\n",
      "Fold: 4  Epoch: 100  Training loss = 1.5657  Validation loss = 4.2951  \n",
      "\n",
      "Fold: 4  Epoch: 101  Training loss = 1.5654  Validation loss = 4.2816  \n",
      "\n",
      "Fold: 4  Epoch: 102  Training loss = 1.5650  Validation loss = 4.2749  \n",
      "\n",
      "Fold: 4  Epoch: 103  Training loss = 1.5646  Validation loss = 4.2754  \n",
      "\n",
      "Fold: 4  Epoch: 104  Training loss = 1.5644  Validation loss = 4.2747  \n",
      "\n",
      "Fold: 4  Epoch: 105  Training loss = 1.5639  Validation loss = 4.2737  \n",
      "\n",
      "Fold: 4  Epoch: 106  Training loss = 1.5633  Validation loss = 4.2640  \n",
      "\n",
      "Fold: 4  Epoch: 107  Training loss = 1.5631  Validation loss = 4.2546  \n",
      "\n",
      "Fold: 4  Epoch: 108  Training loss = 1.5627  Validation loss = 4.2452  \n",
      "\n",
      "Fold: 4  Epoch: 109  Training loss = 1.5623  Validation loss = 4.2524  \n",
      "\n",
      "Fold: 4  Epoch: 110  Training loss = 1.5618  Validation loss = 4.2512  \n",
      "\n",
      "Fold: 4  Epoch: 111  Training loss = 1.5612  Validation loss = 4.2511  \n",
      "\n",
      "Fold: 4  Epoch: 112  Training loss = 1.5610  Validation loss = 4.2545  \n",
      "\n",
      "Fold: 4  Epoch: 113  Training loss = 1.5607  Validation loss = 4.2476  \n",
      "\n",
      "Fold: 4  Epoch: 114  Training loss = 1.5600  Validation loss = 4.2456  \n",
      "\n",
      "Fold: 4  Epoch: 115  Training loss = 1.5597  Validation loss = 4.2345  \n",
      "\n",
      "Fold: 4  Epoch: 116  Training loss = 1.5592  Validation loss = 4.2378  \n",
      "\n",
      "Fold: 4  Epoch: 117  Training loss = 1.5587  Validation loss = 4.2307  \n",
      "\n",
      "Fold: 4  Epoch: 118  Training loss = 1.5585  Validation loss = 4.2317  \n",
      "\n",
      "Fold: 4  Epoch: 119  Training loss = 1.5582  Validation loss = 4.2245  \n",
      "\n",
      "Fold: 4  Epoch: 120  Training loss = 1.5579  Validation loss = 4.2245  \n",
      "\n",
      "Fold: 4  Epoch: 121  Training loss = 1.5575  Validation loss = 4.2261  \n",
      "\n",
      "Fold: 4  Epoch: 122  Training loss = 1.5570  Validation loss = 4.2152  \n",
      "\n",
      "Fold: 4  Epoch: 123  Training loss = 1.5568  Validation loss = 4.2247  \n",
      "\n",
      "Fold: 4  Epoch: 124  Training loss = 1.5564  Validation loss = 4.2188  \n",
      "\n",
      "Fold: 4  Epoch: 125  Training loss = 1.5561  Validation loss = 4.2222  \n",
      "\n",
      "Fold: 4  Epoch: 126  Training loss = 1.5558  Validation loss = 4.2226  \n",
      "\n",
      "Fold: 4  Epoch: 127  Training loss = 1.5555  Validation loss = 4.2158  \n",
      "\n",
      "Fold: 4  Epoch: 128  Training loss = 1.5552  Validation loss = 4.2057  \n",
      "\n",
      "Fold: 4  Epoch: 129  Training loss = 1.5549  Validation loss = 4.2046  \n",
      "\n",
      "Fold: 4  Epoch: 130  Training loss = 1.5545  Validation loss = 4.1976  \n",
      "\n",
      "Fold: 4  Epoch: 131  Training loss = 1.5541  Validation loss = 4.1953  \n",
      "\n",
      "Fold: 4  Epoch: 132  Training loss = 1.5540  Validation loss = 4.1909  \n",
      "\n",
      "Fold: 4  Epoch: 133  Training loss = 1.5536  Validation loss = 4.1932  \n",
      "\n",
      "Fold: 4  Epoch: 134  Training loss = 1.5534  Validation loss = 4.2001  \n",
      "\n",
      "Fold: 4  Epoch: 135  Training loss = 1.5530  Validation loss = 4.1946  \n",
      "\n",
      "Fold: 4  Epoch: 136  Training loss = 1.5527  Validation loss = 4.1975  \n",
      "\n",
      "Fold: 4  Epoch: 137  Training loss = 1.5524  Validation loss = 4.1966  \n",
      "\n",
      "Fold: 4  Epoch: 138  Training loss = 1.5520  Validation loss = 4.2027  \n",
      "\n",
      "Fold: 4  Epoch: 139  Training loss = 1.5518  Validation loss = 4.2095  \n",
      "\n",
      "Check model:  Fold: 4  Epoch: 132  Training loss = 1.5518  Validation loss = 4.2095  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.8257  Validation loss = 4.3399  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.8218  Validation loss = 4.3231  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.8193  Validation loss = 4.3121  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.8166  Validation loss = 4.2976  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.8145  Validation loss = 4.2874  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 1.8139  Validation loss = 4.2860  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 1.8136  Validation loss = 4.2877  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.8121  Validation loss = 4.2799  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 1.8102  Validation loss = 4.2699  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.8088  Validation loss = 4.2648  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 1.8064  Validation loss = 4.2516  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 1.8059  Validation loss = 4.2508  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 1.8039  Validation loss = 4.2419  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.8013  Validation loss = 4.2281  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 1.7993  Validation loss = 4.2168  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 1.7972  Validation loss = 4.2049  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 1.7953  Validation loss = 4.1936  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 1.7936  Validation loss = 4.1820  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 1.7921  Validation loss = 4.1752  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 1.7903  Validation loss = 4.1686  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 1.7894  Validation loss = 4.1673  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 1.7880  Validation loss = 4.1607  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 1.7863  Validation loss = 4.1500  \n",
      "\n",
      "Fold: 5  Epoch: 24  Training loss = 1.7843  Validation loss = 4.1372  \n",
      "\n",
      "Fold: 5  Epoch: 25  Training loss = 1.7830  Validation loss = 4.1304  \n",
      "\n",
      "Fold: 5  Epoch: 26  Training loss = 1.7809  Validation loss = 4.1165  \n",
      "\n",
      "Fold: 5  Epoch: 27  Training loss = 1.7797  Validation loss = 4.1102  \n",
      "\n",
      "Fold: 5  Epoch: 28  Training loss = 1.7773  Validation loss = 4.0916  \n",
      "\n",
      "Fold: 5  Epoch: 29  Training loss = 1.7761  Validation loss = 4.0850  \n",
      "\n",
      "Fold: 5  Epoch: 30  Training loss = 1.7745  Validation loss = 4.0762  \n",
      "\n",
      "Fold: 5  Epoch: 31  Training loss = 1.7734  Validation loss = 4.0723  \n",
      "\n",
      "Fold: 5  Epoch: 32  Training loss = 1.7724  Validation loss = 4.0674  \n",
      "\n",
      "Fold: 5  Epoch: 33  Training loss = 1.7708  Validation loss = 4.0557  \n",
      "\n",
      "Fold: 5  Epoch: 34  Training loss = 1.7695  Validation loss = 4.0489  \n",
      "\n",
      "Fold: 5  Epoch: 35  Training loss = 1.7684  Validation loss = 4.0458  \n",
      "\n",
      "Fold: 5  Epoch: 36  Training loss = 1.7675  Validation loss = 4.0413  \n",
      "\n",
      "Fold: 5  Epoch: 37  Training loss = 1.7662  Validation loss = 4.0300  \n",
      "\n",
      "Fold: 5  Epoch: 38  Training loss = 1.7643  Validation loss = 4.0144  \n",
      "\n",
      "Fold: 5  Epoch: 39  Training loss = 1.7629  Validation loss = 4.0056  \n",
      "\n",
      "Fold: 5  Epoch: 40  Training loss = 1.7619  Validation loss = 4.0013  \n",
      "\n",
      "Fold: 5  Epoch: 41  Training loss = 1.7614  Validation loss = 4.0084  \n",
      "\n",
      "Fold: 5  Epoch: 42  Training loss = 1.7607  Validation loss = 4.0086  \n",
      "\n",
      "Fold: 5  Epoch: 43  Training loss = 1.7599  Validation loss = 4.0064  \n",
      "\n",
      "Fold: 5  Epoch: 44  Training loss = 1.7588  Validation loss = 4.0047  \n",
      "\n",
      "Fold: 5  Epoch: 45  Training loss = 1.7574  Validation loss = 3.9961  \n",
      "\n",
      "Fold: 5  Epoch: 46  Training loss = 1.7564  Validation loss = 3.9929  \n",
      "\n",
      "Fold: 5  Epoch: 47  Training loss = 1.7557  Validation loss = 3.9893  \n",
      "\n",
      "Fold: 5  Epoch: 48  Training loss = 1.7546  Validation loss = 3.9816  \n",
      "\n",
      "Fold: 5  Epoch: 49  Training loss = 1.7538  Validation loss = 3.9769  \n",
      "\n",
      "Fold: 5  Epoch: 50  Training loss = 1.7529  Validation loss = 3.9736  \n",
      "\n",
      "Fold: 5  Epoch: 51  Training loss = 1.7517  Validation loss = 3.9589  \n",
      "\n",
      "Fold: 5  Epoch: 52  Training loss = 1.7509  Validation loss = 3.9612  \n",
      "\n",
      "Fold: 5  Epoch: 53  Training loss = 1.7499  Validation loss = 3.9563  \n",
      "\n",
      "Fold: 5  Epoch: 54  Training loss = 1.7493  Validation loss = 3.9573  \n",
      "\n",
      "Fold: 5  Epoch: 55  Training loss = 1.7482  Validation loss = 3.9559  \n",
      "\n",
      "Fold: 5  Epoch: 56  Training loss = 1.7476  Validation loss = 3.9544  \n",
      "\n",
      "Fold: 5  Epoch: 57  Training loss = 1.7471  Validation loss = 3.9569  \n",
      "\n",
      "Fold: 5  Epoch: 58  Training loss = 1.7460  Validation loss = 3.9470  \n",
      "\n",
      "Fold: 5  Epoch: 59  Training loss = 1.7449  Validation loss = 3.9405  \n",
      "\n",
      "Fold: 5  Epoch: 60  Training loss = 1.7441  Validation loss = 3.9351  \n",
      "\n",
      "Fold: 5  Epoch: 61  Training loss = 1.7435  Validation loss = 3.9327  \n",
      "\n",
      "Fold: 5  Epoch: 62  Training loss = 1.7425  Validation loss = 3.9253  \n",
      "\n",
      "Fold: 5  Epoch: 63  Training loss = 1.7421  Validation loss = 3.9254  \n",
      "\n",
      "Fold: 5  Epoch: 64  Training loss = 1.7420  Validation loss = 3.9365  \n",
      "\n",
      "Fold: 5  Epoch: 65  Training loss = 1.7416  Validation loss = 3.9425  \n",
      "\n",
      "Fold: 5  Epoch: 66  Training loss = 1.7408  Validation loss = 3.9450  \n",
      "\n",
      "Fold: 5  Epoch: 67  Training loss = 1.7400  Validation loss = 3.9417  \n",
      "\n",
      "Fold: 5  Epoch: 68  Training loss = 1.7388  Validation loss = 3.9317  \n",
      "\n",
      "Fold: 5  Epoch: 69  Training loss = 1.7379  Validation loss = 3.9337  \n",
      "\n",
      "Fold: 5  Epoch: 70  Training loss = 1.7372  Validation loss = 3.9288  \n",
      "\n",
      "Fold: 5  Epoch: 71  Training loss = 1.7362  Validation loss = 3.9225  \n",
      "\n",
      "Fold: 5  Epoch: 72  Training loss = 1.7357  Validation loss = 3.9192  \n",
      "\n",
      "Fold: 5  Epoch: 73  Training loss = 1.7345  Validation loss = 3.9090  \n",
      "\n",
      "Fold: 5  Epoch: 74  Training loss = 1.7333  Validation loss = 3.9040  \n",
      "\n",
      "Fold: 5  Epoch: 75  Training loss = 1.7326  Validation loss = 3.9067  \n",
      "\n",
      "Fold: 5  Epoch: 76  Training loss = 1.7314  Validation loss = 3.8954  \n",
      "\n",
      "Fold: 5  Epoch: 77  Training loss = 1.7308  Validation loss = 3.9013  \n",
      "\n",
      "Fold: 5  Epoch: 78  Training loss = 1.7304  Validation loss = 3.9021  \n",
      "\n",
      "Fold: 5  Epoch: 79  Training loss = 1.7295  Validation loss = 3.9039  \n",
      "\n",
      "Fold: 5  Epoch: 80  Training loss = 1.7284  Validation loss = 3.8976  \n",
      "\n",
      "Fold: 5  Epoch: 81  Training loss = 1.7276  Validation loss = 3.8947  \n",
      "\n",
      "Fold: 5  Epoch: 82  Training loss = 1.7261  Validation loss = 3.8790  \n",
      "\n",
      "Fold: 5  Epoch: 83  Training loss = 1.7255  Validation loss = 3.8794  \n",
      "\n",
      "Fold: 5  Epoch: 84  Training loss = 1.7246  Validation loss = 3.8716  \n",
      "\n",
      "Fold: 5  Epoch: 85  Training loss = 1.7243  Validation loss = 3.8786  \n",
      "\n",
      "Fold: 5  Epoch: 86  Training loss = 1.7238  Validation loss = 3.8847  \n",
      "\n",
      "Fold: 5  Epoch: 87  Training loss = 1.7234  Validation loss = 3.8905  \n",
      "\n",
      "Fold: 5  Epoch: 88  Training loss = 1.7221  Validation loss = 3.8772  \n",
      "\n",
      "Fold: 5  Epoch: 89  Training loss = 1.7209  Validation loss = 3.8699  \n",
      "\n",
      "Fold: 5  Epoch: 90  Training loss = 1.7207  Validation loss = 3.8791  \n",
      "\n",
      "Fold: 5  Epoch: 91  Training loss = 1.7198  Validation loss = 3.8787  \n",
      "\n",
      "Fold: 5  Epoch: 92  Training loss = 1.7196  Validation loss = 3.8860  \n",
      "\n",
      "Fold: 5  Epoch: 93  Training loss = 1.7185  Validation loss = 3.8803  \n",
      "\n",
      "Fold: 5  Epoch: 94  Training loss = 1.7175  Validation loss = 3.8726  \n",
      "\n",
      "Fold: 5  Epoch: 95  Training loss = 1.7161  Validation loss = 3.8612  \n",
      "\n",
      "Fold: 5  Epoch: 96  Training loss = 1.7153  Validation loss = 3.8617  \n",
      "\n",
      "Fold: 5  Epoch: 97  Training loss = 1.7149  Validation loss = 3.8655  \n",
      "\n",
      "Fold: 5  Epoch: 98  Training loss = 1.7139  Validation loss = 3.8574  \n",
      "\n",
      "Fold: 5  Epoch: 99  Training loss = 1.7128  Validation loss = 3.8474  \n",
      "\n",
      "Fold: 5  Epoch: 100  Training loss = 1.7117  Validation loss = 3.8402  \n",
      "\n",
      "Fold: 5  Epoch: 101  Training loss = 1.7109  Validation loss = 3.8381  \n",
      "\n",
      "Fold: 5  Epoch: 102  Training loss = 1.7106  Validation loss = 3.8443  \n",
      "\n",
      "Fold: 5  Epoch: 103  Training loss = 1.7098  Validation loss = 3.8441  \n",
      "\n",
      "Fold: 5  Epoch: 104  Training loss = 1.7088  Validation loss = 3.8371  \n",
      "\n",
      "Fold: 5  Epoch: 105  Training loss = 1.7082  Validation loss = 3.8370  \n",
      "\n",
      "Fold: 5  Epoch: 106  Training loss = 1.7074  Validation loss = 3.8365  \n",
      "\n",
      "Fold: 5  Epoch: 107  Training loss = 1.7058  Validation loss = 3.8193  \n",
      "\n",
      "Fold: 5  Epoch: 108  Training loss = 1.7047  Validation loss = 3.8077  \n",
      "\n",
      "Fold: 5  Epoch: 109  Training loss = 1.7039  Validation loss = 3.8071  \n",
      "\n",
      "Fold: 5  Epoch: 110  Training loss = 1.7035  Validation loss = 3.8148  \n",
      "\n",
      "Fold: 5  Epoch: 111  Training loss = 1.7026  Validation loss = 3.8108  \n",
      "\n",
      "Fold: 5  Epoch: 112  Training loss = 1.7015  Validation loss = 3.7964  \n",
      "\n",
      "Fold: 5  Epoch: 113  Training loss = 1.7007  Validation loss = 3.7953  \n",
      "\n",
      "Fold: 5  Epoch: 114  Training loss = 1.6994  Validation loss = 3.7817  \n",
      "\n",
      "Fold: 5  Epoch: 115  Training loss = 1.6990  Validation loss = 3.7810  \n",
      "\n",
      "Fold: 5  Epoch: 116  Training loss = 1.6981  Validation loss = 3.7796  \n",
      "\n",
      "Fold: 5  Epoch: 117  Training loss = 1.6972  Validation loss = 3.7748  \n",
      "\n",
      "Fold: 5  Epoch: 118  Training loss = 1.6966  Validation loss = 3.7861  \n",
      "\n",
      "Fold: 5  Epoch: 119  Training loss = 1.6956  Validation loss = 3.7818  \n",
      "\n",
      "Fold: 5  Epoch: 120  Training loss = 1.6948  Validation loss = 3.7744  \n",
      "\n",
      "Fold: 5  Epoch: 121  Training loss = 1.6940  Validation loss = 3.7722  \n",
      "\n",
      "Fold: 5  Epoch: 122  Training loss = 1.6926  Validation loss = 3.7557  \n",
      "\n",
      "Fold: 5  Epoch: 123  Training loss = 1.6915  Validation loss = 3.7520  \n",
      "\n",
      "Fold: 5  Epoch: 124  Training loss = 1.6908  Validation loss = 3.7491  \n",
      "\n",
      "Fold: 5  Epoch: 125  Training loss = 1.6902  Validation loss = 3.7438  \n",
      "\n",
      "Fold: 5  Epoch: 126  Training loss = 1.6893  Validation loss = 3.7369  \n",
      "\n",
      "Fold: 5  Epoch: 127  Training loss = 1.6888  Validation loss = 3.7401  \n",
      "\n",
      "Fold: 5  Epoch: 128  Training loss = 1.6879  Validation loss = 3.7392  \n",
      "\n",
      "Fold: 5  Epoch: 129  Training loss = 1.6872  Validation loss = 3.7361  \n",
      "\n",
      "Fold: 5  Epoch: 130  Training loss = 1.6864  Validation loss = 3.7383  \n",
      "\n",
      "Fold: 5  Epoch: 131  Training loss = 1.6856  Validation loss = 3.7351  \n",
      "\n",
      "Fold: 5  Epoch: 132  Training loss = 1.6852  Validation loss = 3.7354  \n",
      "\n",
      "Fold: 5  Epoch: 133  Training loss = 1.6845  Validation loss = 3.7378  \n",
      "\n",
      "Fold: 5  Epoch: 134  Training loss = 1.6835  Validation loss = 3.7356  \n",
      "\n",
      "Fold: 5  Epoch: 135  Training loss = 1.6829  Validation loss = 3.7368  \n",
      "\n",
      "Fold: 5  Epoch: 136  Training loss = 1.6820  Validation loss = 3.7358  \n",
      "\n",
      "Fold: 5  Epoch: 137  Training loss = 1.6813  Validation loss = 3.7327  \n",
      "\n",
      "Fold: 5  Epoch: 138  Training loss = 1.6805  Validation loss = 3.7291  \n",
      "\n",
      "Fold: 5  Epoch: 139  Training loss = 1.6801  Validation loss = 3.7348  \n",
      "\n",
      "Fold: 5  Epoch: 140  Training loss = 1.6794  Validation loss = 3.7325  \n",
      "\n",
      "Fold: 5  Epoch: 141  Training loss = 1.6791  Validation loss = 3.7352  \n",
      "\n",
      "Fold: 5  Epoch: 142  Training loss = 1.6783  Validation loss = 3.7298  \n",
      "\n",
      "Fold: 5  Epoch: 143  Training loss = 1.6772  Validation loss = 3.7175  \n",
      "\n",
      "Fold: 5  Epoch: 144  Training loss = 1.6763  Validation loss = 3.7114  \n",
      "\n",
      "Fold: 5  Epoch: 145  Training loss = 1.6752  Validation loss = 3.6917  \n",
      "\n",
      "Fold: 5  Epoch: 146  Training loss = 1.6744  Validation loss = 3.6820  \n",
      "\n",
      "Fold: 5  Epoch: 147  Training loss = 1.6740  Validation loss = 3.6869  \n",
      "\n",
      "Fold: 5  Epoch: 148  Training loss = 1.6735  Validation loss = 3.6843  \n",
      "\n",
      "Fold: 5  Epoch: 149  Training loss = 1.6728  Validation loss = 3.6884  \n",
      "\n",
      "Fold: 5  Epoch: 150  Training loss = 1.6720  Validation loss = 3.6853  \n",
      "\n",
      "Fold: 5  Epoch: 151  Training loss = 1.6715  Validation loss = 3.6806  \n",
      "\n",
      "Fold: 5  Epoch: 152  Training loss = 1.6710  Validation loss = 3.6864  \n",
      "\n",
      "Fold: 5  Epoch: 153  Training loss = 1.6701  Validation loss = 3.6838  \n",
      "\n",
      "Fold: 5  Epoch: 154  Training loss = 1.6696  Validation loss = 3.6972  \n",
      "\n",
      "Check model:  Fold: 5  Epoch: 151  Training loss = 1.6696  Validation loss = 3.6972  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 1.8602  Validation loss = 1.5080  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 1.8556  Validation loss = 1.4879  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 1.8526  Validation loss = 1.4769  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 1.8518  Validation loss = 1.4749  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 1.8495  Validation loss = 1.4652  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 1.8467  Validation loss = 1.4524  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 1.8457  Validation loss = 1.4502  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 1.8415  Validation loss = 1.4300  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 1.8378  Validation loss = 1.4139  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 1.8326  Validation loss = 1.3886  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 1.8324  Validation loss = 1.3925  \n",
      "\n",
      "Fold: 6  Epoch: 12  Training loss = 1.8289  Validation loss = 1.3749  \n",
      "\n",
      "Fold: 6  Epoch: 13  Training loss = 1.8253  Validation loss = 1.3564  \n",
      "\n",
      "Fold: 6  Epoch: 14  Training loss = 1.8225  Validation loss = 1.3423  \n",
      "\n",
      "Fold: 6  Epoch: 15  Training loss = 1.8204  Validation loss = 1.3322  \n",
      "\n",
      "Fold: 6  Epoch: 16  Training loss = 1.8166  Validation loss = 1.3107  \n",
      "\n",
      "Fold: 6  Epoch: 17  Training loss = 1.8156  Validation loss = 1.3091  \n",
      "\n",
      "Fold: 6  Epoch: 18  Training loss = 1.8133  Validation loss = 1.2960  \n",
      "\n",
      "Fold: 6  Epoch: 19  Training loss = 1.8121  Validation loss = 1.2927  \n",
      "\n",
      "Fold: 6  Epoch: 20  Training loss = 1.8097  Validation loss = 1.2805  \n",
      "\n",
      "Fold: 6  Epoch: 21  Training loss = 1.8084  Validation loss = 1.2747  \n",
      "\n",
      "Fold: 6  Epoch: 22  Training loss = 1.8078  Validation loss = 1.2755  \n",
      "\n",
      "Fold: 6  Epoch: 23  Training loss = 1.8048  Validation loss = 1.2601  \n",
      "\n",
      "Fold: 6  Epoch: 24  Training loss = 1.8040  Validation loss = 1.2609  \n",
      "\n",
      "Fold: 6  Epoch: 25  Training loss = 1.8018  Validation loss = 1.2511  \n",
      "\n",
      "Fold: 6  Epoch: 26  Training loss = 1.8003  Validation loss = 1.2453  \n",
      "\n",
      "Fold: 6  Epoch: 27  Training loss = 1.7994  Validation loss = 1.2467  \n",
      "\n",
      "Fold: 6  Epoch: 28  Training loss = 1.7973  Validation loss = 1.2374  \n",
      "\n",
      "Fold: 6  Epoch: 29  Training loss = 1.7961  Validation loss = 1.2364  \n",
      "\n",
      "Fold: 6  Epoch: 30  Training loss = 1.7927  Validation loss = 1.2141  \n",
      "\n",
      "Fold: 6  Epoch: 31  Training loss = 1.7900  Validation loss = 1.2020  \n",
      "\n",
      "Fold: 6  Epoch: 32  Training loss = 1.7868  Validation loss = 1.1784  \n",
      "\n",
      "Fold: 6  Epoch: 33  Training loss = 1.7856  Validation loss = 1.1753  \n",
      "\n",
      "Fold: 6  Epoch: 34  Training loss = 1.7828  Validation loss = 1.1565  \n",
      "\n",
      "Fold: 6  Epoch: 35  Training loss = 1.7802  Validation loss = 1.1397  \n",
      "\n",
      "Fold: 6  Epoch: 36  Training loss = 1.7795  Validation loss = 1.1426  \n",
      "\n",
      "Fold: 6  Epoch: 37  Training loss = 1.7777  Validation loss = 1.1384  \n",
      "\n",
      "Fold: 6  Epoch: 38  Training loss = 1.7752  Validation loss = 1.1289  \n",
      "\n",
      "Fold: 6  Epoch: 39  Training loss = 1.7739  Validation loss = 1.1260  \n",
      "\n",
      "Fold: 6  Epoch: 40  Training loss = 1.7712  Validation loss = 1.1030  \n",
      "\n",
      "Fold: 6  Epoch: 41  Training loss = 1.7701  Validation loss = 1.1074  \n",
      "\n",
      "Fold: 6  Epoch: 42  Training loss = 1.7676  Validation loss = 1.0954  \n",
      "\n",
      "Fold: 6  Epoch: 43  Training loss = 1.7662  Validation loss = 1.0916  \n",
      "\n",
      "Fold: 6  Epoch: 44  Training loss = 1.7646  Validation loss = 1.0866  \n",
      "\n",
      "Fold: 6  Epoch: 45  Training loss = 1.7630  Validation loss = 1.0796  \n",
      "\n",
      "Fold: 6  Epoch: 46  Training loss = 1.7620  Validation loss = 1.0764  \n",
      "\n",
      "Fold: 6  Epoch: 47  Training loss = 1.7610  Validation loss = 1.0785  \n",
      "\n",
      "Fold: 6  Epoch: 48  Training loss = 1.7597  Validation loss = 1.0772  \n",
      "\n",
      "Fold: 6  Epoch: 49  Training loss = 1.7586  Validation loss = 1.0744  \n",
      "\n",
      "Fold: 6  Epoch: 50  Training loss = 1.7568  Validation loss = 1.0620  \n",
      "\n",
      "Fold: 6  Epoch: 51  Training loss = 1.7547  Validation loss = 1.0510  \n",
      "\n",
      "Fold: 6  Epoch: 52  Training loss = 1.7529  Validation loss = 1.0487  \n",
      "\n",
      "Fold: 6  Epoch: 53  Training loss = 1.7522  Validation loss = 1.0526  \n",
      "\n",
      "Fold: 6  Epoch: 54  Training loss = 1.7507  Validation loss = 1.0503  \n",
      "\n",
      "Fold: 6  Epoch: 55  Training loss = 1.7497  Validation loss = 1.0525  \n",
      "\n",
      "Fold: 6  Epoch: 56  Training loss = 1.7491  Validation loss = 1.0636  \n",
      "\n",
      "Fold: 6  Epoch: 57  Training loss = 1.7458  Validation loss = 1.0377  \n",
      "\n",
      "Fold: 6  Epoch: 58  Training loss = 1.7435  Validation loss = 1.0261  \n",
      "\n",
      "Fold: 6  Epoch: 59  Training loss = 1.7424  Validation loss = 1.0230  \n",
      "\n",
      "Fold: 6  Epoch: 60  Training loss = 1.7405  Validation loss = 1.0101  \n",
      "\n",
      "Fold: 6  Epoch: 61  Training loss = 1.7390  Validation loss = 0.9996  \n",
      "\n",
      "Fold: 6  Epoch: 62  Training loss = 1.7374  Validation loss = 0.9905  \n",
      "\n",
      "Fold: 6  Epoch: 63  Training loss = 1.7363  Validation loss = 0.9857  \n",
      "\n",
      "Fold: 6  Epoch: 64  Training loss = 1.7343  Validation loss = 0.9737  \n",
      "\n",
      "Fold: 6  Epoch: 65  Training loss = 1.7332  Validation loss = 0.9838  \n",
      "\n",
      "Fold: 6  Epoch: 66  Training loss = 1.7320  Validation loss = 0.9853  \n",
      "\n",
      "Fold: 6  Epoch: 67  Training loss = 1.7299  Validation loss = 0.9699  \n",
      "\n",
      "Fold: 6  Epoch: 68  Training loss = 1.7279  Validation loss = 0.9567  \n",
      "\n",
      "Fold: 6  Epoch: 69  Training loss = 1.7269  Validation loss = 0.9567  \n",
      "\n",
      "Fold: 6  Epoch: 70  Training loss = 1.7263  Validation loss = 0.9692  \n",
      "\n",
      "Fold: 6  Epoch: 71  Training loss = 1.7257  Validation loss = 0.9779  \n",
      "\n",
      "Fold: 6  Epoch: 72  Training loss = 1.7247  Validation loss = 0.9769  \n",
      "\n",
      "Fold: 6  Epoch: 73  Training loss = 1.7234  Validation loss = 0.9705  \n",
      "\n",
      "Fold: 6  Epoch: 74  Training loss = 1.7223  Validation loss = 0.9705  \n",
      "\n",
      "Fold: 6  Epoch: 75  Training loss = 1.7212  Validation loss = 0.9736  \n",
      "\n",
      "Fold: 6  Epoch: 76  Training loss = 1.7204  Validation loss = 0.9791  \n",
      "\n",
      "Check model:  Fold: 6  Epoch: 69  Training loss = 1.7204  Validation loss = 0.9791  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 1.6869  Validation loss = 0.9191  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 1.6843  Validation loss = 0.9098  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 1.6813  Validation loss = 0.8946  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.6794  Validation loss = 0.8904  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 1.6737  Validation loss = 0.8590  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 1.6722  Validation loss = 0.8588  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 1.6699  Validation loss = 0.8498  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 1.6675  Validation loss = 0.8380  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 1.6659  Validation loss = 0.8368  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 1.6627  Validation loss = 0.8212  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 1.6618  Validation loss = 0.8235  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 1.6588  Validation loss = 0.8115  \n",
      "\n",
      "Fold: 7  Epoch: 13  Training loss = 1.6579  Validation loss = 0.8112  \n",
      "\n",
      "Fold: 7  Epoch: 14  Training loss = 1.6540  Validation loss = 0.7915  \n",
      "\n",
      "Fold: 7  Epoch: 15  Training loss = 1.6545  Validation loss = 0.8058  \n",
      "\n",
      "Fold: 7  Epoch: 16  Training loss = 1.6520  Validation loss = 0.7999  \n",
      "\n",
      "Fold: 7  Epoch: 17  Training loss = 1.6488  Validation loss = 0.7837  \n",
      "\n",
      "Fold: 7  Epoch: 18  Training loss = 1.6489  Validation loss = 0.7994  \n",
      "\n",
      "Fold: 7  Epoch: 19  Training loss = 1.6468  Validation loss = 0.7928  \n",
      "\n",
      "Fold: 7  Epoch: 20  Training loss = 1.6446  Validation loss = 0.7804  \n",
      "\n",
      "Fold: 7  Epoch: 21  Training loss = 1.6423  Validation loss = 0.7727  \n",
      "\n",
      "Fold: 7  Epoch: 22  Training loss = 1.6404  Validation loss = 0.7644  \n",
      "\n",
      "Fold: 7  Epoch: 23  Training loss = 1.6375  Validation loss = 0.7450  \n",
      "\n",
      "Fold: 7  Epoch: 24  Training loss = 1.6358  Validation loss = 0.7457  \n",
      "\n",
      "Fold: 7  Epoch: 25  Training loss = 1.6346  Validation loss = 0.7487  \n",
      "\n",
      "Fold: 7  Epoch: 26  Training loss = 1.6333  Validation loss = 0.7467  \n",
      "\n",
      "Fold: 7  Epoch: 27  Training loss = 1.6313  Validation loss = 0.7397  \n",
      "\n",
      "Fold: 7  Epoch: 28  Training loss = 1.6300  Validation loss = 0.7449  \n",
      "\n",
      "Fold: 7  Epoch: 29  Training loss = 1.6276  Validation loss = 0.7264  \n",
      "\n",
      "Fold: 7  Epoch: 30  Training loss = 1.6263  Validation loss = 0.7227  \n",
      "\n",
      "Fold: 7  Epoch: 31  Training loss = 1.6242  Validation loss = 0.7073  \n",
      "\n",
      "Fold: 7  Epoch: 32  Training loss = 1.6222  Validation loss = 0.7066  \n",
      "\n",
      "Fold: 7  Epoch: 33  Training loss = 1.6204  Validation loss = 0.7011  \n",
      "\n",
      "Fold: 7  Epoch: 34  Training loss = 1.6193  Validation loss = 0.7120  \n",
      "\n",
      "Fold: 7  Epoch: 35  Training loss = 1.6176  Validation loss = 0.7131  \n",
      "\n",
      "Fold: 7  Epoch: 36  Training loss = 1.6155  Validation loss = 0.7051  \n",
      "\n",
      "Fold: 7  Epoch: 37  Training loss = 1.6145  Validation loss = 0.7082  \n",
      "\n",
      "Fold: 7  Epoch: 38  Training loss = 1.6134  Validation loss = 0.7102  \n",
      "\n",
      "Fold: 7  Epoch: 39  Training loss = 1.6117  Validation loss = 0.7037  \n",
      "\n",
      "Fold: 7  Epoch: 40  Training loss = 1.6109  Validation loss = 0.7081  \n",
      "\n",
      "Fold: 7  Epoch: 41  Training loss = 1.6095  Validation loss = 0.7090  \n",
      "\n",
      "Fold: 7  Epoch: 42  Training loss = 1.6080  Validation loss = 0.7025  \n",
      "\n",
      "Fold: 7  Epoch: 43  Training loss = 1.6060  Validation loss = 0.6866  \n",
      "\n",
      "Fold: 7  Epoch: 44  Training loss = 1.6044  Validation loss = 0.6879  \n",
      "\n",
      "Fold: 7  Epoch: 45  Training loss = 1.6022  Validation loss = 0.6828  \n",
      "\n",
      "Fold: 7  Epoch: 46  Training loss = 1.6007  Validation loss = 0.6750  \n",
      "\n",
      "Fold: 7  Epoch: 47  Training loss = 1.5997  Validation loss = 0.6735  \n",
      "\n",
      "Fold: 7  Epoch: 48  Training loss = 1.5983  Validation loss = 0.6765  \n",
      "\n",
      "Fold: 7  Epoch: 49  Training loss = 1.5962  Validation loss = 0.6716  \n",
      "\n",
      "Fold: 7  Epoch: 50  Training loss = 1.5948  Validation loss = 0.6696  \n",
      "\n",
      "Fold: 7  Epoch: 51  Training loss = 1.5936  Validation loss = 0.6774  \n",
      "\n",
      "Fold: 7  Epoch: 52  Training loss = 1.5926  Validation loss = 0.6860  \n",
      "\n",
      "Fold: 7  Epoch: 53  Training loss = 1.5913  Validation loss = 0.6889  \n",
      "\n",
      "Check model:  Fold: 7  Epoch: 50  Training loss = 1.5913  Validation loss = 0.6889  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 1.5393  Validation loss = 5.7564  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 1.5381  Validation loss = 5.7536  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 1.5365  Validation loss = 5.7494  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 1.5350  Validation loss = 5.7453  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 1.5334  Validation loss = 5.7395  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 1.5324  Validation loss = 5.7366  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 1.5309  Validation loss = 5.7410  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 1.5294  Validation loss = 5.7434  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 1.5279  Validation loss = 5.7338  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 1.5265  Validation loss = 5.7397  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 1.5250  Validation loss = 5.7324  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 1.5241  Validation loss = 5.7211  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 1.5233  Validation loss = 5.7133  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 1.5217  Validation loss = 5.7204  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 1.5206  Validation loss = 5.7273  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 1.5195  Validation loss = 5.7271  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 1.5186  Validation loss = 5.7110  \n",
      "\n",
      "Fold: 8  Epoch: 18  Training loss = 1.5179  Validation loss = 5.7157  \n",
      "\n",
      "Fold: 8  Epoch: 19  Training loss = 1.5166  Validation loss = 5.7177  \n",
      "\n",
      "Fold: 8  Epoch: 20  Training loss = 1.5156  Validation loss = 5.7057  \n",
      "\n",
      "Fold: 8  Epoch: 21  Training loss = 1.5140  Validation loss = 5.7112  \n",
      "\n",
      "Fold: 8  Epoch: 22  Training loss = 1.5129  Validation loss = 5.7022  \n",
      "\n",
      "Fold: 8  Epoch: 23  Training loss = 1.5119  Validation loss = 5.6907  \n",
      "\n",
      "Fold: 8  Epoch: 24  Training loss = 1.5104  Validation loss = 5.6809  \n",
      "\n",
      "Fold: 8  Epoch: 25  Training loss = 1.5088  Validation loss = 5.6827  \n",
      "\n",
      "Fold: 8  Epoch: 26  Training loss = 1.5072  Validation loss = 5.6776  \n",
      "\n",
      "Fold: 8  Epoch: 27  Training loss = 1.5058  Validation loss = 5.6870  \n",
      "\n",
      "Fold: 8  Epoch: 28  Training loss = 1.5050  Validation loss = 5.6666  \n",
      "\n",
      "Fold: 8  Epoch: 29  Training loss = 1.5039  Validation loss = 5.6652  \n",
      "\n",
      "Fold: 8  Epoch: 30  Training loss = 1.5028  Validation loss = 5.6527  \n",
      "\n",
      "Fold: 8  Epoch: 31  Training loss = 1.5020  Validation loss = 5.6413  \n",
      "\n",
      "Fold: 8  Epoch: 32  Training loss = 1.5003  Validation loss = 5.6536  \n",
      "\n",
      "Fold: 8  Epoch: 33  Training loss = 1.4992  Validation loss = 5.6582  \n",
      "\n",
      "Fold: 8  Epoch: 34  Training loss = 1.4983  Validation loss = 5.6509  \n",
      "\n",
      "Fold: 8  Epoch: 35  Training loss = 1.4967  Validation loss = 5.6723  \n",
      "\n",
      "Fold: 8  Epoch: 36  Training loss = 1.4955  Validation loss = 5.6590  \n",
      "\n",
      "Fold: 8  Epoch: 37  Training loss = 1.4943  Validation loss = 5.6671  \n",
      "\n",
      "Fold: 8  Epoch: 38  Training loss = 1.4935  Validation loss = 5.6637  \n",
      "\n",
      "Fold: 8  Epoch: 39  Training loss = 1.4922  Validation loss = 5.6736  \n",
      "\n",
      "Check model:  Fold: 8  Epoch: 31  Training loss = 1.4922  Validation loss = 5.6736  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 2.0142  Validation loss = 8.6636  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 2.0115  Validation loss = 8.6477  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 2.0067  Validation loss = 8.6219  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 2.0025  Validation loss = 8.5962  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 2.0021  Validation loss = 8.6026  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 2.0021  Validation loss = 8.6143  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 1.9999  Validation loss = 8.6030  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 1.9953  Validation loss = 8.5850  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 1.9936  Validation loss = 8.5779  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 1.9932  Validation loss = 8.5887  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 1.9887  Validation loss = 8.5606  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 1.9869  Validation loss = 8.5506  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 1.9854  Validation loss = 8.5531  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 1.9844  Validation loss = 8.5501  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 1.9801  Validation loss = 8.5133  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 1.9775  Validation loss = 8.4851  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 1.9756  Validation loss = 8.4791  \n",
      "\n",
      "Fold: 9  Epoch: 18  Training loss = 1.9737  Validation loss = 8.4726  \n",
      "\n",
      "Fold: 9  Epoch: 19  Training loss = 1.9718  Validation loss = 8.4519  \n",
      "\n",
      "Fold: 9  Epoch: 20  Training loss = 1.9689  Validation loss = 8.4152  \n",
      "\n",
      "Fold: 9  Epoch: 21  Training loss = 1.9671  Validation loss = 8.4052  \n",
      "\n",
      "Fold: 9  Epoch: 22  Training loss = 1.9656  Validation loss = 8.3965  \n",
      "\n",
      "Fold: 9  Epoch: 23  Training loss = 1.9634  Validation loss = 8.4075  \n",
      "\n",
      "Fold: 9  Epoch: 24  Training loss = 1.9618  Validation loss = 8.3872  \n",
      "\n",
      "Fold: 9  Epoch: 25  Training loss = 1.9601  Validation loss = 8.3863  \n",
      "\n",
      "Fold: 9  Epoch: 26  Training loss = 1.9587  Validation loss = 8.3916  \n",
      "\n",
      "Fold: 9  Epoch: 27  Training loss = 1.9567  Validation loss = 8.3957  \n",
      "\n",
      "Fold: 9  Epoch: 28  Training loss = 1.9554  Validation loss = 8.3895  \n",
      "\n",
      "Fold: 9  Epoch: 29  Training loss = 1.9540  Validation loss = 8.3869  \n",
      "\n",
      "Fold: 9  Epoch: 30  Training loss = 1.9526  Validation loss = 8.3754  \n",
      "\n",
      "Fold: 9  Epoch: 31  Training loss = 1.9516  Validation loss = 8.3650  \n",
      "\n",
      "Fold: 9  Epoch: 32  Training loss = 1.9504  Validation loss = 8.3625  \n",
      "\n",
      "Fold: 9  Epoch: 33  Training loss = 1.9494  Validation loss = 8.3845  \n",
      "\n",
      "Fold: 9  Epoch: 34  Training loss = 1.9485  Validation loss = 8.3998  \n",
      "\n",
      "Check model:  Fold: 9  Epoch: 32  Training loss = 1.9485  Validation loss = 8.3998  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 2.8055  Validation loss = 5.6442  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 2.7983  Validation loss = 5.5922  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 2.7903  Validation loss = 5.5789  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 2.7795  Validation loss = 5.5283  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 2.7770  Validation loss = 5.5213  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 2.7704  Validation loss = 5.5227  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 2.7618  Validation loss = 5.4397  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 2.7588  Validation loss = 5.4234  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 2.7549  Validation loss = 5.4244  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 2.7498  Validation loss = 5.4183  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 2.7423  Validation loss = 5.3633  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 2.7377  Validation loss = 5.3370  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 2.7320  Validation loss = 5.3116  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 2.7272  Validation loss = 5.2755  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 2.7239  Validation loss = 5.2537  \n",
      "\n",
      "Fold: 10  Epoch: 16  Training loss = 2.7227  Validation loss = 5.2406  \n",
      "\n",
      "Fold: 10  Epoch: 17  Training loss = 2.7132  Validation loss = 5.1787  \n",
      "\n",
      "Fold: 10  Epoch: 18  Training loss = 2.7086  Validation loss = 5.1589  \n",
      "\n",
      "Fold: 10  Epoch: 19  Training loss = 2.7060  Validation loss = 5.1954  \n",
      "\n",
      "Fold: 10  Epoch: 20  Training loss = 2.7046  Validation loss = 5.2120  \n",
      "\n",
      "Fold: 10  Epoch: 21  Training loss = 2.7005  Validation loss = 5.1672  \n",
      "\n",
      "Fold: 10  Epoch: 22  Training loss = 2.6917  Validation loss = 5.0838  \n",
      "\n",
      "Fold: 10  Epoch: 23  Training loss = 2.6886  Validation loss = 5.0771  \n",
      "\n",
      "Fold: 10  Epoch: 24  Training loss = 2.6869  Validation loss = 5.0876  \n",
      "\n",
      "Fold: 10  Epoch: 25  Training loss = 2.6818  Validation loss = 5.0787  \n",
      "\n",
      "Fold: 10  Epoch: 26  Training loss = 2.6817  Validation loss = 5.1222  \n",
      "\n",
      "Fold: 10  Epoch: 27  Training loss = 2.6772  Validation loss = 5.0853  \n",
      "\n",
      "Fold: 10  Epoch: 28  Training loss = 2.6764  Validation loss = 5.1033  \n",
      "\n",
      "Fold: 10  Epoch: 29  Training loss = 2.6725  Validation loss = 5.1052  \n",
      "\n",
      "Fold: 10  Epoch: 30  Training loss = 2.6680  Validation loss = 5.0614  \n",
      "\n",
      "Fold: 10  Epoch: 31  Training loss = 2.6635  Validation loss = 5.0112  \n",
      "\n",
      "Fold: 10  Epoch: 32  Training loss = 2.6576  Validation loss = 4.9785  \n",
      "\n",
      "Fold: 10  Epoch: 33  Training loss = 2.6553  Validation loss = 4.9980  \n",
      "\n",
      "Fold: 10  Epoch: 34  Training loss = 2.6525  Validation loss = 4.9930  \n",
      "\n",
      "Fold: 10  Epoch: 35  Training loss = 2.6471  Validation loss = 4.9428  \n",
      "\n",
      "Fold: 10  Epoch: 36  Training loss = 2.6414  Validation loss = 4.9033  \n",
      "\n",
      "Fold: 10  Epoch: 37  Training loss = 2.6368  Validation loss = 4.8486  \n",
      "\n",
      "Fold: 10  Epoch: 38  Training loss = 2.6345  Validation loss = 4.8394  \n",
      "\n",
      "Fold: 10  Epoch: 39  Training loss = 2.6326  Validation loss = 4.8578  \n",
      "\n",
      "Fold: 10  Epoch: 40  Training loss = 2.6298  Validation loss = 4.8948  \n",
      "\n",
      "Fold: 10  Epoch: 41  Training loss = 2.6270  Validation loss = 4.8886  \n",
      "\n",
      "Fold: 10  Epoch: 42  Training loss = 2.6244  Validation loss = 4.9156  \n",
      "\n",
      "Fold: 10  Epoch: 43  Training loss = 2.6209  Validation loss = 4.8938  \n",
      "\n",
      "Fold: 10  Epoch: 44  Training loss = 2.6182  Validation loss = 4.8509  \n",
      "\n",
      "Fold: 10  Epoch: 45  Training loss = 2.6144  Validation loss = 4.8633  \n",
      "\n",
      "Fold: 10  Epoch: 46  Training loss = 2.6104  Validation loss = 4.8820  \n",
      "\n",
      "Fold: 10  Epoch: 47  Training loss = 2.6072  Validation loss = 4.8793  \n",
      "\n",
      "Fold: 10  Epoch: 48  Training loss = 2.6065  Validation loss = 4.9173  \n",
      "\n",
      "Check model:  Fold: 10  Epoch: 38  Training loss = 2.6065  Validation loss = 4.9173  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 2.7715  Validation loss = 1.5223  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 2.7574  Validation loss = 1.5180  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 2.7300  Validation loss = 1.5125  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 2.7183  Validation loss = 1.5099  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 2.7052  Validation loss = 1.5087  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 2.6949  Validation loss = 1.5063  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 2.6644  Validation loss = 1.5068  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 2.6484  Validation loss = 1.5089  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 2.6366  Validation loss = 1.5118  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 2.6322  Validation loss = 1.5100  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 2.6257  Validation loss = 1.5132  \n",
      "\n",
      "Fold: 11  Epoch: 12  Training loss = 2.6165  Validation loss = 1.5274  \n",
      "\n",
      "Check model:  Fold: 11  Epoch: 6  Training loss = 2.6165  Validation loss = 1.5274  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 2.6140  Validation loss = 1.3482  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 2.6104  Validation loss = 1.3510  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 2.6082  Validation loss = 1.3599  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 2.5859  Validation loss = 1.3345  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 2.5767  Validation loss = 1.3230  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 2.5719  Validation loss = 1.2954  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 2.5678  Validation loss = 1.2834  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 2.5648  Validation loss = 1.2777  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 2.5600  Validation loss = 1.2561  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 2.5567  Validation loss = 1.2466  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 2.5531  Validation loss = 1.2374  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 2.5524  Validation loss = 1.2552  \n",
      "\n",
      "Fold: 12  Epoch: 13  Training loss = 2.5489  Validation loss = 1.2442  \n",
      "\n",
      "Fold: 12  Epoch: 14  Training loss = 2.5431  Validation loss = 1.2113  \n",
      "\n",
      "Fold: 12  Epoch: 15  Training loss = 2.5397  Validation loss = 1.2005  \n",
      "\n",
      "Fold: 12  Epoch: 16  Training loss = 2.5345  Validation loss = 1.1679  \n",
      "\n",
      "Fold: 12  Epoch: 17  Training loss = 2.5310  Validation loss = 1.1555  \n",
      "\n",
      "Fold: 12  Epoch: 18  Training loss = 2.5257  Validation loss = 1.1537  \n",
      "\n",
      "Fold: 12  Epoch: 19  Training loss = 2.5213  Validation loss = 1.1347  \n",
      "\n",
      "Fold: 12  Epoch: 20  Training loss = 2.5164  Validation loss = 1.0960  \n",
      "\n",
      "Fold: 12  Epoch: 21  Training loss = 2.5111  Validation loss = 1.0528  \n",
      "\n",
      "Fold: 12  Epoch: 22  Training loss = 2.5088  Validation loss = 1.0291  \n",
      "\n",
      "Fold: 12  Epoch: 23  Training loss = 2.5064  Validation loss = 1.0300  \n",
      "\n",
      "Fold: 12  Epoch: 24  Training loss = 2.5024  Validation loss = 1.0291  \n",
      "\n",
      "Fold: 12  Epoch: 25  Training loss = 2.4996  Validation loss = 1.0247  \n",
      "\n",
      "Fold: 12  Epoch: 26  Training loss = 2.4971  Validation loss = 1.0218  \n",
      "\n",
      "Fold: 12  Epoch: 27  Training loss = 2.4943  Validation loss = 1.0164  \n",
      "\n",
      "Fold: 12  Epoch: 28  Training loss = 2.4920  Validation loss = 1.0302  \n",
      "\n",
      "Fold: 12  Epoch: 29  Training loss = 2.4880  Validation loss = 1.0096  \n",
      "\n",
      "Fold: 12  Epoch: 30  Training loss = 2.4846  Validation loss = 1.0067  \n",
      "\n",
      "Fold: 12  Epoch: 31  Training loss = 2.4770  Validation loss = 1.0040  \n",
      "\n",
      "Fold: 12  Epoch: 32  Training loss = 2.4751  Validation loss = 1.0106  \n",
      "\n",
      "Fold: 12  Epoch: 33  Training loss = 2.4714  Validation loss = 0.9828  \n",
      "\n",
      "Fold: 12  Epoch: 34  Training loss = 2.4704  Validation loss = 0.9930  \n",
      "\n",
      "Fold: 12  Epoch: 35  Training loss = 2.4679  Validation loss = 0.9859  \n",
      "\n",
      "Fold: 12  Epoch: 36  Training loss = 2.4650  Validation loss = 0.9701  \n",
      "\n",
      "Fold: 12  Epoch: 37  Training loss = 2.4631  Validation loss = 0.9613  \n",
      "\n",
      "Fold: 12  Epoch: 38  Training loss = 2.4620  Validation loss = 0.9827  \n",
      "\n",
      "Fold: 12  Epoch: 39  Training loss = 2.4586  Validation loss = 0.9599  \n",
      "\n",
      "Fold: 12  Epoch: 40  Training loss = 2.4556  Validation loss = 0.9411  \n",
      "\n",
      "Fold: 12  Epoch: 41  Training loss = 2.4543  Validation loss = 0.9537  \n",
      "\n",
      "Fold: 12  Epoch: 42  Training loss = 2.4515  Validation loss = 0.9434  \n",
      "\n",
      "Fold: 12  Epoch: 43  Training loss = 2.4500  Validation loss = 0.9501  \n",
      "\n",
      "Fold: 12  Epoch: 44  Training loss = 2.4465  Validation loss = 0.9334  \n",
      "\n",
      "Fold: 12  Epoch: 45  Training loss = 2.4449  Validation loss = 0.9312  \n",
      "\n",
      "Fold: 12  Epoch: 46  Training loss = 2.4419  Validation loss = 0.9243  \n",
      "\n",
      "Fold: 12  Epoch: 47  Training loss = 2.4401  Validation loss = 0.9167  \n",
      "\n",
      "Fold: 12  Epoch: 48  Training loss = 2.4378  Validation loss = 0.8893  \n",
      "\n",
      "Fold: 12  Epoch: 49  Training loss = 2.4342  Validation loss = 0.8675  \n",
      "\n",
      "Fold: 12  Epoch: 50  Training loss = 2.4306  Validation loss = 0.8535  \n",
      "\n",
      "Fold: 12  Epoch: 51  Training loss = 2.4275  Validation loss = 0.8648  \n",
      "\n",
      "Fold: 12  Epoch: 52  Training loss = 2.4259  Validation loss = 0.8569  \n",
      "\n",
      "Fold: 12  Epoch: 53  Training loss = 2.4243  Validation loss = 0.8607  \n",
      "\n",
      "Fold: 12  Epoch: 54  Training loss = 2.4219  Validation loss = 0.8279  \n",
      "\n",
      "Fold: 12  Epoch: 55  Training loss = 2.4194  Validation loss = 0.8350  \n",
      "\n",
      "Fold: 12  Epoch: 56  Training loss = 2.4176  Validation loss = 0.8278  \n",
      "\n",
      "Fold: 12  Epoch: 57  Training loss = 2.4160  Validation loss = 0.8337  \n",
      "\n",
      "Fold: 12  Epoch: 58  Training loss = 2.4141  Validation loss = 0.8503  \n",
      "\n",
      "Fold: 12  Epoch: 59  Training loss = 2.4122  Validation loss = 0.8187  \n",
      "\n",
      "Fold: 12  Epoch: 60  Training loss = 2.4099  Validation loss = 0.7996  \n",
      "\n",
      "Fold: 12  Epoch: 61  Training loss = 2.4084  Validation loss = 0.7854  \n",
      "\n",
      "Fold: 12  Epoch: 62  Training loss = 2.4068  Validation loss = 0.7950  \n",
      "\n",
      "Fold: 12  Epoch: 63  Training loss = 2.4044  Validation loss = 0.7799  \n",
      "\n",
      "Fold: 12  Epoch: 64  Training loss = 2.4031  Validation loss = 0.7992  \n",
      "\n",
      "Fold: 12  Epoch: 65  Training loss = 2.4020  Validation loss = 0.8056  \n",
      "\n",
      "Fold: 12  Epoch: 66  Training loss = 2.3996  Validation loss = 0.8026  \n",
      "\n",
      "Fold: 12  Epoch: 67  Training loss = 2.3983  Validation loss = 0.7730  \n",
      "\n",
      "Fold: 12  Epoch: 68  Training loss = 2.3983  Validation loss = 0.7330  \n",
      "\n",
      "Fold: 12  Epoch: 69  Training loss = 2.3966  Validation loss = 0.7368  \n",
      "\n",
      "Fold: 12  Epoch: 70  Training loss = 2.3955  Validation loss = 0.7221  \n",
      "\n",
      "Fold: 12  Epoch: 71  Training loss = 2.3907  Validation loss = 0.7356  \n",
      "\n",
      "Fold: 12  Epoch: 72  Training loss = 2.3885  Validation loss = 0.7522  \n",
      "\n",
      "Fold: 12  Epoch: 73  Training loss = 2.3866  Validation loss = 0.8006  \n",
      "\n",
      "Fold: 12  Epoch: 74  Training loss = 2.3845  Validation loss = 0.7967  \n",
      "\n",
      "Fold: 12  Epoch: 75  Training loss = 2.3834  Validation loss = 0.8188  \n",
      "\n",
      "Check model:  Fold: 12  Epoch: 70  Training loss = 2.3834  Validation loss = 0.8188  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 2.3384  Validation loss = 3.3311  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 2.3357  Validation loss = 3.3269  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 2.3342  Validation loss = 3.3240  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 2.3321  Validation loss = 3.3133  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 2.3312  Validation loss = 3.3239  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 2.3305  Validation loss = 3.3379  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 2.3305  Validation loss = 3.3406  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 2.3269  Validation loss = 3.3153  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 2.3234  Validation loss = 3.2995  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 2.3219  Validation loss = 3.2840  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 2.3210  Validation loss = 3.2880  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 2.3184  Validation loss = 3.2515  \n",
      "\n",
      "Fold: 13  Epoch: 13  Training loss = 2.3180  Validation loss = 3.2489  \n",
      "\n",
      "Fold: 13  Epoch: 14  Training loss = 2.3171  Validation loss = 3.2635  \n",
      "\n",
      "Fold: 13  Epoch: 15  Training loss = 2.3162  Validation loss = 3.2685  \n",
      "\n",
      "Fold: 13  Epoch: 16  Training loss = 2.3147  Validation loss = 3.2655  \n",
      "\n",
      "Fold: 13  Epoch: 17  Training loss = 2.3145  Validation loss = 3.2822  \n",
      "\n",
      "Fold: 13  Epoch: 18  Training loss = 2.3147  Validation loss = 3.2858  \n",
      "\n",
      "Fold: 13  Epoch: 19  Training loss = 2.3125  Validation loss = 3.2570  \n",
      "\n",
      "Fold: 13  Epoch: 20  Training loss = 2.3125  Validation loss = 3.2797  \n",
      "\n",
      "Fold: 13  Epoch: 21  Training loss = 2.3120  Validation loss = 3.2790  \n",
      "\n",
      "Fold: 13  Epoch: 22  Training loss = 2.3148  Validation loss = 3.3077  \n",
      "\n",
      "Check model:  Fold: 13  Epoch: 13  Training loss = 2.3148  Validation loss = 3.3077  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 2.4331  Validation loss = 6.0894  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 2.4281  Validation loss = 6.0643  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 2.4257  Validation loss = 6.0546  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 2.4245  Validation loss = 6.0495  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 2.4193  Validation loss = 6.0150  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 2.4118  Validation loss = 5.9671  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 2.4092  Validation loss = 5.9516  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 2.4060  Validation loss = 5.9297  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 2.4049  Validation loss = 5.9234  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 2.4049  Validation loss = 5.9242  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 2.4009  Validation loss = 5.8919  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 2.3987  Validation loss = 5.8779  \n",
      "\n",
      "Fold: 14  Epoch: 13  Training loss = 2.3964  Validation loss = 5.8562  \n",
      "\n",
      "Fold: 14  Epoch: 14  Training loss = 2.3972  Validation loss = 5.8781  \n",
      "\n",
      "Fold: 14  Epoch: 15  Training loss = 2.3958  Validation loss = 5.8668  \n",
      "\n",
      "Fold: 14  Epoch: 16  Training loss = 2.3961  Validation loss = 5.8785  \n",
      "\n",
      "Fold: 14  Epoch: 17  Training loss = 2.3923  Validation loss = 5.8451  \n",
      "\n",
      "Fold: 14  Epoch: 18  Training loss = 2.3887  Validation loss = 5.8058  \n",
      "\n",
      "Fold: 14  Epoch: 19  Training loss = 2.3866  Validation loss = 5.7837  \n",
      "\n",
      "Fold: 14  Epoch: 20  Training loss = 2.3849  Validation loss = 5.7540  \n",
      "\n",
      "Fold: 14  Epoch: 21  Training loss = 2.3836  Validation loss = 5.7462  \n",
      "\n",
      "Fold: 14  Epoch: 22  Training loss = 2.3826  Validation loss = 5.7410  \n",
      "\n",
      "Fold: 14  Epoch: 23  Training loss = 2.3814  Validation loss = 5.7377  \n",
      "\n",
      "Fold: 14  Epoch: 24  Training loss = 2.3803  Validation loss = 5.7401  \n",
      "\n",
      "Fold: 14  Epoch: 25  Training loss = 2.3783  Validation loss = 5.7112  \n",
      "\n",
      "Fold: 14  Epoch: 26  Training loss = 2.3773  Validation loss = 5.6995  \n",
      "\n",
      "Fold: 14  Epoch: 27  Training loss = 2.3792  Validation loss = 5.7187  \n",
      "\n",
      "Fold: 14  Epoch: 28  Training loss = 2.3821  Validation loss = 5.7608  \n",
      "\n",
      "Fold: 14  Epoch: 29  Training loss = 2.3802  Validation loss = 5.7477  \n",
      "\n",
      "Fold: 14  Epoch: 30  Training loss = 2.3771  Validation loss = 5.7017  \n",
      "\n",
      "Fold: 14  Epoch: 31  Training loss = 2.3752  Validation loss = 5.6816  \n",
      "\n",
      "Fold: 14  Epoch: 32  Training loss = 2.3747  Validation loss = 5.6839  \n",
      "\n",
      "Fold: 14  Epoch: 33  Training loss = 2.3733  Validation loss = 5.6535  \n",
      "\n",
      "Fold: 14  Epoch: 34  Training loss = 2.3735  Validation loss = 5.6870  \n",
      "\n",
      "Fold: 14  Epoch: 35  Training loss = 2.3711  Validation loss = 5.6317  \n",
      "\n",
      "Fold: 14  Epoch: 36  Training loss = 2.3697  Validation loss = 5.6009  \n",
      "\n",
      "Fold: 14  Epoch: 37  Training loss = 2.3693  Validation loss = 5.6123  \n",
      "\n",
      "Fold: 14  Epoch: 38  Training loss = 2.3693  Validation loss = 5.6367  \n",
      "\n",
      "Fold: 14  Epoch: 39  Training loss = 2.3690  Validation loss = 5.6450  \n",
      "\n",
      "Fold: 14  Epoch: 40  Training loss = 2.3682  Validation loss = 5.6388  \n",
      "\n",
      "Fold: 14  Epoch: 41  Training loss = 2.3681  Validation loss = 5.6631  \n",
      "\n",
      "Fold: 14  Epoch: 42  Training loss = 2.3663  Validation loss = 5.6266  \n",
      "\n",
      "Fold: 14  Epoch: 43  Training loss = 2.3658  Validation loss = 5.6190  \n",
      "\n",
      "Fold: 14  Epoch: 44  Training loss = 2.3656  Validation loss = 5.6242  \n",
      "\n",
      "Fold: 14  Epoch: 45  Training loss = 2.3652  Validation loss = 5.6111  \n",
      "\n",
      "Fold: 14  Epoch: 46  Training loss = 2.3637  Validation loss = 5.5907  \n",
      "\n",
      "Fold: 14  Epoch: 47  Training loss = 2.3636  Validation loss = 5.5624  \n",
      "\n",
      "Fold: 14  Epoch: 48  Training loss = 2.3619  Validation loss = 5.5727  \n",
      "\n",
      "Fold: 14  Epoch: 49  Training loss = 2.3614  Validation loss = 5.5866  \n",
      "\n",
      "Fold: 14  Epoch: 50  Training loss = 2.3616  Validation loss = 5.5824  \n",
      "\n",
      "Fold: 14  Epoch: 51  Training loss = 2.3606  Validation loss = 5.6007  \n",
      "\n",
      "Fold: 14  Epoch: 52  Training loss = 2.3593  Validation loss = 5.6159  \n",
      "\n",
      "Fold: 14  Epoch: 53  Training loss = 2.3585  Validation loss = 5.5878  \n",
      "\n",
      "Fold: 14  Epoch: 54  Training loss = 2.3582  Validation loss = 5.5830  \n",
      "\n",
      "Fold: 14  Epoch: 55  Training loss = 2.3581  Validation loss = 5.5453  \n",
      "\n",
      "Fold: 14  Epoch: 56  Training loss = 2.3574  Validation loss = 5.5637  \n",
      "\n",
      "Fold: 14  Epoch: 57  Training loss = 2.3557  Validation loss = 5.5827  \n",
      "\n",
      "Fold: 14  Epoch: 58  Training loss = 2.3551  Validation loss = 5.5766  \n",
      "\n",
      "Fold: 14  Epoch: 59  Training loss = 2.3546  Validation loss = 5.5557  \n",
      "\n",
      "Fold: 14  Epoch: 60  Training loss = 2.3549  Validation loss = 5.5583  \n",
      "\n",
      "Fold: 14  Epoch: 61  Training loss = 2.3534  Validation loss = 5.5948  \n",
      "\n",
      "Fold: 14  Epoch: 62  Training loss = 2.3526  Validation loss = 5.6093  \n",
      "\n",
      "Check model:  Fold: 14  Epoch: 55  Training loss = 2.3526  Validation loss = 5.6093  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 2.7179  Validation loss = 6.3200  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 2.7069  Validation loss = 6.2367  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 2.7028  Validation loss = 6.1944  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 2.6964  Validation loss = 6.1524  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 2.6925  Validation loss = 6.1135  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 2.6928  Validation loss = 6.1133  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 2.6868  Validation loss = 6.0624  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 2.6840  Validation loss = 6.0452  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 2.6813  Validation loss = 6.0509  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 2.6789  Validation loss = 6.0195  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 2.6760  Validation loss = 6.0227  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 2.6729  Validation loss = 5.9998  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 2.6713  Validation loss = 6.0150  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 2.6679  Validation loss = 6.0108  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 2.6679  Validation loss = 6.0412  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 2.6660  Validation loss = 6.0331  \n",
      "\n",
      "Fold: 15  Epoch: 17  Training loss = 2.6641  Validation loss = 6.0054  \n",
      "\n",
      "Fold: 15  Epoch: 18  Training loss = 2.6626  Validation loss = 6.0101  \n",
      "\n",
      "Fold: 15  Epoch: 19  Training loss = 2.6617  Validation loss = 6.0049  \n",
      "\n",
      "Fold: 15  Epoch: 20  Training loss = 2.6599  Validation loss = 5.9579  \n",
      "\n",
      "Fold: 15  Epoch: 21  Training loss = 2.6603  Validation loss = 6.0157  \n",
      "\n",
      "Fold: 15  Epoch: 22  Training loss = 2.6591  Validation loss = 6.0084  \n",
      "\n",
      "Fold: 15  Epoch: 23  Training loss = 2.6584  Validation loss = 6.0061  \n",
      "\n",
      "Fold: 15  Epoch: 24  Training loss = 2.6556  Validation loss = 5.9514  \n",
      "\n",
      "Fold: 15  Epoch: 25  Training loss = 2.6564  Validation loss = 5.9791  \n",
      "\n",
      "Fold: 15  Epoch: 26  Training loss = 2.6542  Validation loss = 5.9576  \n",
      "\n",
      "Fold: 15  Epoch: 27  Training loss = 2.6507  Validation loss = 5.8851  \n",
      "\n",
      "Fold: 15  Epoch: 28  Training loss = 2.6489  Validation loss = 5.9106  \n",
      "\n",
      "Fold: 15  Epoch: 29  Training loss = 2.6481  Validation loss = 5.9219  \n",
      "\n",
      "Fold: 15  Epoch: 30  Training loss = 2.6465  Validation loss = 5.9078  \n",
      "\n",
      "Fold: 15  Epoch: 31  Training loss = 2.6468  Validation loss = 5.9671  \n",
      "\n",
      "Fold: 15  Epoch: 32  Training loss = 2.6450  Validation loss = 5.9167  \n",
      "\n",
      "Fold: 15  Epoch: 33  Training loss = 2.6448  Validation loss = 5.8530  \n",
      "\n",
      "Fold: 15  Epoch: 34  Training loss = 2.6430  Validation loss = 5.8813  \n",
      "\n",
      "Fold: 15  Epoch: 35  Training loss = 2.6421  Validation loss = 5.8957  \n",
      "\n",
      "Fold: 15  Epoch: 36  Training loss = 2.6409  Validation loss = 5.9150  \n",
      "\n",
      "Fold: 15  Epoch: 37  Training loss = 2.6413  Validation loss = 5.9425  \n",
      "\n",
      "Fold: 15  Epoch: 38  Training loss = 2.6393  Validation loss = 5.9348  \n",
      "\n",
      "Fold: 15  Epoch: 39  Training loss = 2.6394  Validation loss = 5.9471  \n",
      "\n",
      "Fold: 15  Epoch: 40  Training loss = 2.6374  Validation loss = 5.8861  \n",
      "\n",
      "Fold: 15  Epoch: 41  Training loss = 2.6358  Validation loss = 5.8926  \n",
      "\n",
      "Fold: 15  Epoch: 42  Training loss = 2.6352  Validation loss = 5.8857  \n",
      "\n",
      "Fold: 15  Epoch: 43  Training loss = 2.6342  Validation loss = 5.8684  \n",
      "\n",
      "Fold: 15  Epoch: 44  Training loss = 2.6327  Validation loss = 5.8519  \n",
      "\n",
      "Fold: 15  Epoch: 45  Training loss = 2.6315  Validation loss = 5.8819  \n",
      "\n",
      "Fold: 15  Epoch: 46  Training loss = 2.6304  Validation loss = 5.8356  \n",
      "\n",
      "Fold: 15  Epoch: 47  Training loss = 2.6293  Validation loss = 5.8244  \n",
      "\n",
      "Fold: 15  Epoch: 48  Training loss = 2.6288  Validation loss = 5.8161  \n",
      "\n",
      "Fold: 15  Epoch: 49  Training loss = 2.6290  Validation loss = 5.9000  \n",
      "\n",
      "Check model:  Fold: 15  Epoch: 48  Training loss = 2.6290  Validation loss = 5.9000  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 2.9673  Validation loss = 4.3574  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 2.9662  Validation loss = 4.3050  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 2.9689  Validation loss = 4.4646  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 2.9624  Validation loss = 4.2168  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 2.9613  Validation loss = 4.1992  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 2.9600  Validation loss = 4.2610  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 2.9516  Validation loss = 4.1039  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 2.9511  Validation loss = 4.0765  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 2.9512  Validation loss = 4.1259  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 2.9563  Validation loss = 4.1744  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 2.9541  Validation loss = 4.2345  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 2.9454  Validation loss = 4.1152  \n",
      "\n",
      "Fold: 16  Epoch: 13  Training loss = 2.9451  Validation loss = 4.1697  \n",
      "\n",
      "Fold: 16  Epoch: 14  Training loss = 2.9434  Validation loss = 4.1397  \n",
      "\n",
      "Fold: 16  Epoch: 15  Training loss = 2.9409  Validation loss = 4.1271  \n",
      "\n",
      "Fold: 16  Epoch: 16  Training loss = 2.9401  Validation loss = 4.0602  \n",
      "\n",
      "Fold: 16  Epoch: 17  Training loss = 2.9376  Validation loss = 4.0670  \n",
      "\n",
      "Fold: 16  Epoch: 18  Training loss = 2.9358  Validation loss = 4.1021  \n",
      "\n",
      "Fold: 16  Epoch: 19  Training loss = 2.9317  Validation loss = 4.0489  \n",
      "\n",
      "Fold: 16  Epoch: 20  Training loss = 2.9292  Validation loss = 4.0870  \n",
      "\n",
      "Fold: 16  Epoch: 21  Training loss = 2.9316  Validation loss = 4.1083  \n",
      "\n",
      "Fold: 16  Epoch: 22  Training loss = 2.9276  Validation loss = 4.1038  \n",
      "\n",
      "Fold: 16  Epoch: 23  Training loss = 2.9256  Validation loss = 4.0609  \n",
      "\n",
      "Fold: 16  Epoch: 24  Training loss = 2.9252  Validation loss = 4.0523  \n",
      "\n",
      "Fold: 16  Epoch: 25  Training loss = 2.9225  Validation loss = 4.0534  \n",
      "\n",
      "Fold: 16  Epoch: 26  Training loss = 2.9210  Validation loss = 4.0758  \n",
      "\n",
      "Fold: 16  Epoch: 27  Training loss = 2.9183  Validation loss = 4.0201  \n",
      "\n",
      "Fold: 16  Epoch: 28  Training loss = 2.9173  Validation loss = 4.0524  \n",
      "\n",
      "Fold: 16  Epoch: 29  Training loss = 2.9167  Validation loss = 4.0461  \n",
      "\n",
      "Fold: 16  Epoch: 30  Training loss = 2.9166  Validation loss = 4.0239  \n",
      "\n",
      "Fold: 16  Epoch: 31  Training loss = 2.9139  Validation loss = 4.0265  \n",
      "\n",
      "Fold: 16  Epoch: 32  Training loss = 2.9117  Validation loss = 4.0567  \n",
      "\n",
      "Fold: 16  Epoch: 33  Training loss = 2.9125  Validation loss = 4.0396  \n",
      "\n",
      "Fold: 16  Epoch: 34  Training loss = 2.9117  Validation loss = 4.0889  \n",
      "\n",
      "Check model:  Fold: 16  Epoch: 27  Training loss = 2.9117  Validation loss = 4.0889  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 3.0694  Validation loss = 3.8808  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 3.0605  Validation loss = 3.9126  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 3.0522  Validation loss = 3.9300  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 3.0468  Validation loss = 3.9569  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 3.0402  Validation loss = 3.9535  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 3.0402  Validation loss = 3.9379  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 3.0387  Validation loss = 3.9438  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 3.0343  Validation loss = 3.9595  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 3.0328  Validation loss = 3.9855  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 3.0301  Validation loss = 3.9853  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 3.0247  Validation loss = 4.0106  \n",
      "\n",
      "Check model:  Fold: 17  Epoch: 1  Training loss = 3.0247  Validation loss = 4.0106  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 3.1649  Validation loss = 2.3666  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 3.1602  Validation loss = 2.3547  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 3.1577  Validation loss = 2.3430  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 3.1513  Validation loss = 2.3633  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 3.1529  Validation loss = 2.3557  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 3.1490  Validation loss = 2.3519  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 3.1454  Validation loss = 2.3368  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 3.1394  Validation loss = 2.3452  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 3.1355  Validation loss = 2.3662  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 3.1353  Validation loss = 2.3689  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 3.1296  Validation loss = 2.3760  \n",
      "\n",
      "Check model:  Fold: 18  Epoch: 7  Training loss = 3.1296  Validation loss = 2.3760  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 3.1174  Validation loss = 1.7567  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 3.1155  Validation loss = 1.7363  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 3.1127  Validation loss = 1.6385  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 3.1098  Validation loss = 1.6489  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 3.1064  Validation loss = 1.6611  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 3.1067  Validation loss = 1.5918  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 3.1092  Validation loss = 1.5254  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 3.0988  Validation loss = 1.5815  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 3.1025  Validation loss = 1.6160  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 3.0942  Validation loss = 1.6504  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 3.0881  Validation loss = 1.6169  \n",
      "\n",
      "Fold: 19  Epoch: 12  Training loss = 3.0857  Validation loss = 1.5877  \n",
      "\n",
      "Fold: 19  Epoch: 13  Training loss = 3.0833  Validation loss = 1.5689  \n",
      "\n",
      "Fold: 19  Epoch: 14  Training loss = 3.0811  Validation loss = 1.5325  \n",
      "\n",
      "Fold: 19  Epoch: 15  Training loss = 3.0761  Validation loss = 1.5754  \n",
      "\n",
      "Fold: 19  Epoch: 16  Training loss = 3.0700  Validation loss = 1.6247  \n",
      "\n",
      "Fold: 19  Epoch: 17  Training loss = 3.0682  Validation loss = 1.6004  \n",
      "\n",
      "Fold: 19  Epoch: 18  Training loss = 3.0648  Validation loss = 1.6118  \n",
      "\n",
      "Fold: 19  Epoch: 19  Training loss = 3.0617  Validation loss = 1.6450  \n",
      "\n",
      "Fold: 19  Epoch: 20  Training loss = 3.0598  Validation loss = 1.6040  \n",
      "\n",
      "Fold: 19  Epoch: 21  Training loss = 3.0606  Validation loss = 1.5512  \n",
      "\n",
      "Fold: 19  Epoch: 22  Training loss = 3.0549  Validation loss = 1.5922  \n",
      "\n",
      "Fold: 19  Epoch: 23  Training loss = 3.0519  Validation loss = 1.6141  \n",
      "\n",
      "Fold: 19  Epoch: 24  Training loss = 3.0447  Validation loss = 1.6987  \n",
      "\n",
      "Check model:  Fold: 19  Epoch: 7  Training loss = 3.0447  Validation loss = 1.6987  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 3.0385  Validation loss = 1.7455  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 3.0331  Validation loss = 1.8062  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 3.0319  Validation loss = 1.7921  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 3.0272  Validation loss = 1.9003  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 3.0213  Validation loss = 2.0534  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 3.0203  Validation loss = 1.9680  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 3.0169  Validation loss = 2.0898  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 3.0142  Validation loss = 2.0912  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 3.0121  Validation loss = 2.1009  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 3.0474  Validation loss = 2.0716  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 3.0418  Validation loss = 2.0988  \n",
      "\n",
      "Fold: 20  Epoch: 12  Training loss = 3.0354  Validation loss = 2.0779  \n",
      "\n",
      "Fold: 20  Epoch: 13  Training loss = 3.0297  Validation loss = 2.1532  \n",
      "\n",
      "Check model:  Fold: 20  Epoch: 1  Training loss = 3.0297  Validation loss = 2.1532  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 3.0487  Validation loss = 2.4528  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 3.0415  Validation loss = 2.4315  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 3.0302  Validation loss = 2.2925  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 3.0280  Validation loss = 2.3465  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 3.0216  Validation loss = 2.2995  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 3.0187  Validation loss = 2.2863  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 3.0153  Validation loss = 2.3278  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 3.0115  Validation loss = 2.3109  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 3.0090  Validation loss = 2.2172  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 3.0059  Validation loss = 2.3064  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 2.9990  Validation loss = 2.2577  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 2.9948  Validation loss = 2.2261  \n",
      "\n",
      "Fold: 21  Epoch: 13  Training loss = 2.9936  Validation loss = 2.2633  \n",
      "\n",
      "Fold: 21  Epoch: 14  Training loss = 2.9949  Validation loss = 2.3212  \n",
      "\n",
      "Fold: 21  Epoch: 15  Training loss = 2.9866  Validation loss = 2.2546  \n",
      "\n",
      "Fold: 21  Epoch: 16  Training loss = 2.9787  Validation loss = 2.1944  \n",
      "\n",
      "Fold: 21  Epoch: 17  Training loss = 2.9747  Validation loss = 2.1707  \n",
      "\n",
      "Fold: 21  Epoch: 18  Training loss = 2.9744  Validation loss = 2.1709  \n",
      "\n",
      "Fold: 21  Epoch: 19  Training loss = 2.9664  Validation loss = 2.0921  \n",
      "\n",
      "Fold: 21  Epoch: 20  Training loss = 2.9621  Validation loss = 2.0692  \n",
      "\n",
      "Fold: 21  Epoch: 21  Training loss = 2.9622  Validation loss = 2.0506  \n",
      "\n",
      "Fold: 21  Epoch: 22  Training loss = 2.9863  Validation loss = 1.9903  \n",
      "\n",
      "Fold: 21  Epoch: 23  Training loss = 2.9550  Validation loss = 2.0557  \n",
      "\n",
      "Fold: 21  Epoch: 24  Training loss = 2.9510  Validation loss = 2.0534  \n",
      "\n",
      "Fold: 21  Epoch: 25  Training loss = 2.9471  Validation loss = 2.0868  \n",
      "\n",
      "Fold: 21  Epoch: 26  Training loss = 2.9403  Validation loss = 2.0616  \n",
      "\n",
      "Fold: 21  Epoch: 27  Training loss = 2.9381  Validation loss = 2.0474  \n",
      "\n",
      "Fold: 21  Epoch: 28  Training loss = 2.9337  Validation loss = 2.1012  \n",
      "\n",
      "Check model:  Fold: 21  Epoch: 22  Training loss = 2.9337  Validation loss = 2.1012  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 2.9396  Validation loss = 1.2693  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 2.9357  Validation loss = 1.2766  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 2.9339  Validation loss = 1.2602  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 2.9275  Validation loss = 1.2781  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 2.9274  Validation loss = 1.2898  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 2.9308  Validation loss = 1.2834  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 2.9190  Validation loss = 1.2694  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 2.9233  Validation loss = 1.2470  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 2.9143  Validation loss = 1.2747  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 2.9171  Validation loss = 1.2206  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 2.9076  Validation loss = 1.2537  \n",
      "\n",
      "Fold: 22  Epoch: 12  Training loss = 2.9015  Validation loss = 1.2494  \n",
      "\n",
      "Fold: 22  Epoch: 13  Training loss = 2.9009  Validation loss = 1.2339  \n",
      "\n",
      "Fold: 22  Epoch: 14  Training loss = 2.8981  Validation loss = 1.2299  \n",
      "\n",
      "Fold: 22  Epoch: 15  Training loss = 2.8945  Validation loss = 1.2954  \n",
      "\n",
      "Check model:  Fold: 22  Epoch: 10  Training loss = 2.8945  Validation loss = 1.2954  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 2.7536  Validation loss = 0.7249  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 2.7505  Validation loss = 0.8286  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 2.7433  Validation loss = 0.8495  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 2.7388  Validation loss = 0.8059  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 2.7351  Validation loss = 0.9021  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 2.7292  Validation loss = 0.7958  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 2.7201  Validation loss = 0.7235  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 2.7131  Validation loss = 0.6669  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 2.7128  Validation loss = 0.7292  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 2.7094  Validation loss = 0.8254  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 2.7144  Validation loss = 1.0180  \n",
      "\n",
      "Check model:  Fold: 23  Epoch: 8  Training loss = 2.7144  Validation loss = 1.0180  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 2.4905  Validation loss = 2.0609  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 2.4929  Validation loss = 2.1530  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 2.4845  Validation loss = 2.1076  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 2.4742  Validation loss = 2.1069  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 2.4762  Validation loss = 2.0288  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 2.4708  Validation loss = 2.0426  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 2.4647  Validation loss = 2.0754  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 2.4624  Validation loss = 2.0900  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 2.4566  Validation loss = 2.1333  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 2.4510  Validation loss = 2.1272  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 2.4593  Validation loss = 2.1350  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 2.4421  Validation loss = 2.0857  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 2.4420  Validation loss = 2.0121  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 2.4412  Validation loss = 2.0132  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 2.4316  Validation loss = 2.0751  \n",
      "\n",
      "Fold: 24  Epoch: 16  Training loss = 2.4311  Validation loss = 2.1016  \n",
      "\n",
      "Fold: 24  Epoch: 17  Training loss = 2.4299  Validation loss = 2.1130  \n",
      "\n",
      "Fold: 24  Epoch: 18  Training loss = 2.4215  Validation loss = 2.0647  \n",
      "\n",
      "Fold: 24  Epoch: 19  Training loss = 2.4263  Validation loss = 2.0802  \n",
      "\n",
      "Fold: 24  Epoch: 20  Training loss = 2.4210  Validation loss = 2.0133  \n",
      "\n",
      "Fold: 24  Epoch: 21  Training loss = 2.4117  Validation loss = 2.0821  \n",
      "\n",
      "Fold: 24  Epoch: 22  Training loss = 2.4074  Validation loss = 2.0479  \n",
      "\n",
      "Fold: 24  Epoch: 23  Training loss = 2.4030  Validation loss = 2.0625  \n",
      "\n",
      "Fold: 24  Epoch: 24  Training loss = 2.4042  Validation loss = 2.0262  \n",
      "\n",
      "Fold: 24  Epoch: 25  Training loss = 2.3962  Validation loss = 2.0616  \n",
      "\n",
      "Fold: 24  Epoch: 26  Training loss = 2.3951  Validation loss = 2.0325  \n",
      "\n",
      "Fold: 24  Epoch: 27  Training loss = 2.3925  Validation loss = 2.0296  \n",
      "\n",
      "Fold: 24  Epoch: 28  Training loss = 2.3909  Validation loss = 2.0291  \n",
      "\n",
      "Fold: 24  Epoch: 29  Training loss = 2.3941  Validation loss = 1.9976  \n",
      "\n",
      "Fold: 24  Epoch: 30  Training loss = 2.3928  Validation loss = 2.0268  \n",
      "\n",
      "Fold: 24  Epoch: 31  Training loss = 2.3879  Validation loss = 2.0930  \n",
      "\n",
      "Check model:  Fold: 24  Epoch: 29  Training loss = 2.3879  Validation loss = 2.0930  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 2.4108  Validation loss = 2.1813  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 2.3940  Validation loss = 2.0903  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 2.3929  Validation loss = 2.1420  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 2.3859  Validation loss = 2.1748  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 2.4062  Validation loss = 2.2962  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 2.3860  Validation loss = 2.1156  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 2.3834  Validation loss = 2.1576  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 2.3774  Validation loss = 2.1427  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 2.3772  Validation loss = 1.9689  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 2.3725  Validation loss = 1.9985  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 2.3682  Validation loss = 1.7294  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 2.3699  Validation loss = 1.8332  \n",
      "\n",
      "Fold: 25  Epoch: 13  Training loss = 2.3613  Validation loss = 1.9866  \n",
      "\n",
      "Fold: 25  Epoch: 14  Training loss = 2.3553  Validation loss = 2.1378  \n",
      "\n",
      "Fold: 25  Epoch: 15  Training loss = 2.3571  Validation loss = 2.0560  \n",
      "\n",
      "Fold: 25  Epoch: 16  Training loss = 2.3562  Validation loss = 2.0304  \n",
      "\n",
      "Fold: 25  Epoch: 17  Training loss = 2.3461  Validation loss = 1.9615  \n",
      "\n",
      "Fold: 25  Epoch: 18  Training loss = 2.3534  Validation loss = 1.9006  \n",
      "\n",
      "Fold: 25  Epoch: 19  Training loss = 2.3535  Validation loss = 2.2165  \n",
      "\n",
      "Check model:  Fold: 25  Epoch: 11  Training loss = 2.3535  Validation loss = 2.2165  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 2.2626  Validation loss = 2.7416  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 2.2431  Validation loss = 2.9868  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 2.2819  Validation loss = 2.5429  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 2.2776  Validation loss = 3.4826  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 2.2278  Validation loss = 3.2850  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 2.2236  Validation loss = 3.1885  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 2.2266  Validation loss = 2.8697  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 2.2121  Validation loss = 3.4247  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 2.2330  Validation loss = 3.5329  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 2.1920  Validation loss = 3.3550  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 2.1925  Validation loss = 3.4230  \n",
      "\n",
      "Fold: 26  Epoch: 12  Training loss = 2.1848  Validation loss = 3.3549  \n",
      "\n",
      "Fold: 26  Epoch: 13  Training loss = 2.1814  Validation loss = 3.4827  \n",
      "\n",
      "Fold: 26  Epoch: 14  Training loss = 2.1856  Validation loss = 3.5315  \n",
      "\n",
      "Fold: 26  Epoch: 15  Training loss = 2.1877  Validation loss = 3.5913  \n",
      "\n",
      "Check model:  Fold: 26  Epoch: 3  Training loss = 2.1877  Validation loss = 3.5913  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 2.2559  Validation loss = 2.6651  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 2.2244  Validation loss = 2.0539  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 2.2292  Validation loss = 2.1103  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 2.2241  Validation loss = 2.2734  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 2.2393  Validation loss = 2.6293  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 2.2320  Validation loss = 1.9163  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 2.2052  Validation loss = 2.3267  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 2.2207  Validation loss = 2.4790  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 2.2227  Validation loss = 2.4823  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 2.2360  Validation loss = 1.7338  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 2.2184  Validation loss = 1.8079  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 2.2154  Validation loss = 1.7690  \n",
      "\n",
      "Fold: 27  Epoch: 13  Training loss = 2.3057  Validation loss = 1.3307  \n",
      "\n",
      "Fold: 27  Epoch: 14  Training loss = 2.2484  Validation loss = 1.5806  \n",
      "\n",
      "Fold: 27  Epoch: 15  Training loss = 2.2263  Validation loss = 2.5298  \n",
      "\n",
      "Check model:  Fold: 27  Epoch: 13  Training loss = 2.2263  Validation loss = 2.5298  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 2.0315  Validation loss = 1.6432  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 2.0242  Validation loss = 1.7166  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 1.9827  Validation loss = 1.7142  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 1.9381  Validation loss = 1.7076  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 1.9526  Validation loss = 1.7388  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 1.9372  Validation loss = 1.7223  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 1.9137  Validation loss = 1.7009  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 1.9763  Validation loss = 1.7598  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 1.9958  Validation loss = 1.6015  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 1.9098  Validation loss = 1.6340  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 1.9043  Validation loss = 1.6912  \n",
      "\n",
      "Fold: 28  Epoch: 12  Training loss = 1.8861  Validation loss = 1.6853  \n",
      "\n",
      "Fold: 28  Epoch: 13  Training loss = 1.8820  Validation loss = 1.7206  \n",
      "\n",
      "Fold: 28  Epoch: 14  Training loss = 1.8931  Validation loss = 1.7291  \n",
      "\n",
      "Fold: 28  Epoch: 15  Training loss = 1.8757  Validation loss = 1.7158  \n",
      "\n",
      "Fold: 28  Epoch: 16  Training loss = 1.8777  Validation loss = 1.6895  \n",
      "\n",
      "Fold: 28  Epoch: 17  Training loss = 1.8725  Validation loss = 1.6573  \n",
      "\n",
      "Fold: 28  Epoch: 18  Training loss = 1.9095  Validation loss = 1.6767  \n",
      "\n",
      "Fold: 28  Epoch: 19  Training loss = 1.8782  Validation loss = 1.6936  \n",
      "\n",
      "Fold: 28  Epoch: 20  Training loss = 1.8730  Validation loss = 1.6446  \n",
      "\n",
      "Fold: 28  Epoch: 21  Training loss = 1.8811  Validation loss = 1.7565  \n",
      "\n",
      "Check model:  Fold: 28  Epoch: 9  Training loss = 1.8811  Validation loss = 1.7565  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 1.8467  Validation loss = 2.3649  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 1.8294  Validation loss = 2.3561  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 1.8528  Validation loss = 2.3353  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 1.8278  Validation loss = 2.3226  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 1.8193  Validation loss = 2.3193  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 1.8349  Validation loss = 2.3261  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 1.7981  Validation loss = 2.3409  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 1.8003  Validation loss = 2.3193  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 1.7939  Validation loss = 2.3589  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 1.7856  Validation loss = 2.3609  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 1.8378  Validation loss = 2.3733  \n",
      "\n",
      "Check model:  Fold: 29  Epoch: 5  Training loss = 1.8378  Validation loss = 2.3733  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 1.8537  Validation loss = 1.7804  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 1.8458  Validation loss = 1.7720  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 1.8627  Validation loss = 1.7737  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 1.8535  Validation loss = 1.7823  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 1.8422  Validation loss = 1.7552  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 1.8335  Validation loss = 1.7445  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 1.8486  Validation loss = 1.7544  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 1.8198  Validation loss = 1.7749  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 1.8217  Validation loss = 1.7605  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 1.8042  Validation loss = 1.7692  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 1.8083  Validation loss = 1.7825  \n",
      "\n",
      "Check model:  Fold: 30  Epoch: 6  Training loss = 1.8083  Validation loss = 1.7825  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 1.8109  Validation loss = 0.8530  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 1.8039  Validation loss = 0.8486  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 1.8295  Validation loss = 0.7919  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 1.7931  Validation loss = 0.8169  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 1.7850  Validation loss = 0.9000  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 1.8236  Validation loss = 0.8137  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 1.7778  Validation loss = 0.8570  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 1.7883  Validation loss = 0.8966  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 1.8141  Validation loss = 0.7746  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 1.7815  Validation loss = 0.8246  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 1.8504  Validation loss = 1.2626  \n",
      "\n",
      "Check model:  Fold: 31  Epoch: 9  Training loss = 1.8504  Validation loss = 1.2626  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 1.6367  Validation loss = 3.3126  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 1.5786  Validation loss = 3.0787  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 1.5839  Validation loss = 2.9306  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 1.6389  Validation loss = 3.1053  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 1.5612  Validation loss = 3.0028  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 1.6254  Validation loss = 3.1317  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 1.5457  Validation loss = 2.7806  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 1.7046  Validation loss = 2.7536  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 1.6879  Validation loss = 2.7145  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 1.6674  Validation loss = 2.8789  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 1.6559  Validation loss = 2.8722  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 1.6749  Validation loss = 3.1631  \n",
      "\n",
      "Check model:  Fold: 32  Epoch: 9  Training loss = 1.6749  Validation loss = 3.1631  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    # Load model with lowest validation error for each fold\n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Epoch: {0:d}\".format(epoch_hat),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 32\n",
      "Average validation error: 2.88128\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 1.5998  Test loss = 3.1029  \n",
      "\n",
      "Epoch: 2  Training loss = 1.5746  Test loss = 3.0411  \n",
      "\n",
      "Epoch: 3  Training loss = 1.5601  Test loss = 2.9902  \n",
      "\n",
      "Epoch: 4  Training loss = 1.5508  Test loss = 2.9551  \n",
      "\n",
      "Epoch: 5  Training loss = 1.5437  Test loss = 2.9283  \n",
      "\n",
      "Epoch: 6  Training loss = 1.5379  Test loss = 2.9068  \n",
      "\n",
      "Epoch: 7  Training loss = 1.5329  Test loss = 2.8889  \n",
      "\n",
      "Epoch: 8  Training loss = 1.5287  Test loss = 2.8736  \n",
      "\n",
      "Epoch: 9  Training loss = 1.5250  Test loss = 2.8604  \n",
      "\n",
      "Epoch: 10  Training loss = 1.5218  Test loss = 2.8490  \n",
      "\n",
      "Epoch: 11  Training loss = 1.5189  Test loss = 2.8390  \n",
      "\n",
      "Epoch: 12  Training loss = 1.5162  Test loss = 2.8303  \n",
      "\n",
      "Epoch: 13  Training loss = 1.5138  Test loss = 2.8226  \n",
      "\n",
      "Epoch: 14  Training loss = 1.5114  Test loss = 2.8158  \n",
      "\n",
      "Epoch: 15  Training loss = 1.5091  Test loss = 2.8097  \n",
      "\n",
      "Epoch: 16  Training loss = 1.5069  Test loss = 2.8042  \n",
      "\n",
      "Epoch: 17  Training loss = 1.5047  Test loss = 2.7992  \n",
      "\n",
      "Epoch: 18  Training loss = 1.5024  Test loss = 2.7946  \n",
      "\n",
      "Epoch: 19  Training loss = 1.5001  Test loss = 2.7902  \n",
      "\n",
      "Epoch: 20  Training loss = 1.4978  Test loss = 2.7861  \n",
      "\n",
      "Epoch: 21  Training loss = 1.4954  Test loss = 2.7822  \n",
      "\n",
      "Epoch: 22  Training loss = 1.4929  Test loss = 2.7784  \n",
      "\n",
      "Epoch: 23  Training loss = 1.4904  Test loss = 2.7746  \n",
      "\n",
      "Epoch: 24  Training loss = 1.4879  Test loss = 2.7710  \n",
      "\n",
      "Epoch: 25  Training loss = 1.4855  Test loss = 2.7675  \n",
      "\n",
      "Epoch: 26  Training loss = 1.4831  Test loss = 2.7642  \n",
      "\n",
      "Epoch: 27  Training loss = 1.4809  Test loss = 2.7611  \n",
      "\n",
      "Epoch: 28  Training loss = 1.4788  Test loss = 2.7581  \n",
      "\n",
      "Epoch: 29  Training loss = 1.4770  Test loss = 2.7555  \n",
      "\n",
      "Epoch: 30  Training loss = 1.4754  Test loss = 2.7532  \n",
      "\n",
      "Epoch: 31  Training loss = 1.4740  Test loss = 2.7511  \n",
      "\n",
      "Epoch: 32  Training loss = 1.4728  Test loss = 2.7494  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc2016d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEARByfgDm+aJ9oUp1HMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc6c3fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 2.7494433475 \n",
      "Fixed scheme MAE:  1.89733070974\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 1.4728  Test loss = 3.2557  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 1.5203  Test loss = 2.7724  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 1.5511  Test loss = 0.5808  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 1.5474  Test loss = 0.4373  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 1.3846  Test loss = 1.4349  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 1.3545  Test loss = 0.7345  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 1.3463  Test loss = 1.2115  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 1.3489  Test loss = 1.0269  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 1.2856  Test loss = 1.8333  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 1.2959  Test loss = 0.6014  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 1.2480  Test loss = 0.8094  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 1.2513  Test loss = 1.1712  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 1.1938  Test loss = 0.6291  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 1.1925  Test loss = 2.5385  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 1.2301  Test loss = 3.5622  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 1.3063  Test loss = 5.2188  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 1.3031  Test loss = 2.6452  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 1.3430  Test loss = 0.0708  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 1.3430  Test loss = 0.3716  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 1.2719  Test loss = 0.2703  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 1.2085  Test loss = 1.8134  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 1.2223  Test loss = 3.5682  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 1.2956  Test loss = 0.4499  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 1.2964  Test loss = 0.8803  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 1.2576  Test loss = 0.8241  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 1.2607  Test loss = 0.9470  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 1.2642  Test loss = 0.7211  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 1.2558  Test loss = 1.6537  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 1.2068  Test loss = 0.1648  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 1.1977  Test loss = 0.3719  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 1.1913  Test loss = 3.6221  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 1.2339  Test loss = 1.4712  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 1.1980  Test loss = 0.2533  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 1.1975  Test loss = 0.9694  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 1.1732  Test loss = 1.3910  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 1.1858  Test loss = 4.2587  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 1.2300  Test loss = 1.5387  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 1.1877  Test loss = 1.3955  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 1.1999  Test loss = 0.0047  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 1.1989  Test loss = 2.7279  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 1.2155  Test loss = 1.7610  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 1.2350  Test loss = 1.7068  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 1.2529  Test loss = 3.5675  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 1.3285  Test loss = 12.0587  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 2.0069  Test loss = 5.4272  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.1163  Test loss = 0.6964  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.1180  Test loss = 0.5340  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.1188  Test loss = 0.6626  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 1.9238  Test loss = 1.8253  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 1.9325  Test loss = 3.5569  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 1.9811  Test loss = 1.8421  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 1.9935  Test loss = 0.7493  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 1.9205  Test loss = 0.3037  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 1.9199  Test loss = 2.0000  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 1.9354  Test loss = 0.4834  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 1.9350  Test loss = 0.4788  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 1.8864  Test loss = 0.7552  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 1.8885  Test loss = 0.4258  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 1.8885  Test loss = 0.9472  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 1.8913  Test loss = 0.1842  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.8548  Test loss = 1.4483  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.8621  Test loss = 2.6510  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 1.8901  Test loss = 0.6163  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 1.8902  Test loss = 1.1285  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 1.8589  Test loss = 1.0673  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 1.8626  Test loss = 1.0213  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.8561  Test loss = 0.7584  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 1.8519  Test loss = 2.9875  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 1.8614  Test loss = 3.9875  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 1.9240  Test loss = 0.8915  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 1.9174  Test loss = 0.4034  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 1.9181  Test loss = 2.7709  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 1.9114  Test loss = 2.8178  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 1.9430  Test loss = 0.8264  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 1.9407  Test loss = 0.6023  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 1.9380  Test loss = 0.8939  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 1.8855  Test loss = 1.3143  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRdRhb2JEmSLiMLe5IkSZfxDxbh5DK5BpDiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd8b6ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.37212063502 \n",
      "Updating scheme MAE:  1.64098051681\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_test_fixed.csv\", yhat_test_fixed)\n",
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_update.csv\", yhat_update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
