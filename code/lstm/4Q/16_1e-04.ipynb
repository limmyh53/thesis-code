{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"4Q/16_units/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 500\n",
    "learning_rate = 1e-4\n",
    "batch_size = 5\n",
    "early_stop_iters = 10\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 16 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag4',\n",
    "                                       'inflation.lag5',\n",
    "                                       'inflation.lag6',\n",
    "                                       'inflation.lag7']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag4',\n",
    "                                   'unemp.lag5',\n",
    "                                   'unemp.lag6',\n",
    "                                   'unemp.lag7']])\n",
    "train_4lag_oil = np.array(train[['oil.lag4',\n",
    "                                 'oil.lag5',\n",
    "                                 'oil.lag6',\n",
    "                                 'oil.lag7']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag4',\n",
    "                                     'inflation.lag5',\n",
    "                                     'inflation.lag6',\n",
    "                                     'inflation.lag7']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag4',\n",
    "                                 'unemp.lag5',\n",
    "                                 'unemp.lag6',\n",
    "                                 'unemp.lag7']])\n",
    "test_4lag_oil = np.array(test[['oil.lag4',\n",
    "                               'oil.lag5',\n",
    "                               'oil.lag6',\n",
    "                               'oil.lag7']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 16 \n",
      "Learning rate = 0.0001 \n",
      "Epochs = 500 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 10 \n",
      "Learning rate = 0.0001\n",
      "Fold: 1  Epoch: 1  Training loss = 3.1958  Validation loss = 3.4055  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 3.1796  Validation loss = 3.3764  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 3.1680  Validation loss = 3.3550  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 3.1576  Validation loss = 3.3354  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 3.1437  Validation loss = 3.3088  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 3.1322  Validation loss = 3.2864  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 3.1229  Validation loss = 3.2687  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 3.1120  Validation loss = 3.2476  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 3.1008  Validation loss = 3.2264  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 3.0863  Validation loss = 3.1999  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 3.0726  Validation loss = 3.1751  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 3.0683  Validation loss = 3.1661  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 3.0558  Validation loss = 3.1429  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 3.0442  Validation loss = 3.1214  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 3.0350  Validation loss = 3.1036  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 3.0287  Validation loss = 3.0902  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 3.0200  Validation loss = 3.0734  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 3.0130  Validation loss = 3.0589  \n",
      "\n",
      "Fold: 1  Epoch: 19  Training loss = 3.0082  Validation loss = 3.0481  \n",
      "\n",
      "Fold: 1  Epoch: 20  Training loss = 2.9983  Validation loss = 3.0282  \n",
      "\n",
      "Fold: 1  Epoch: 21  Training loss = 2.9868  Validation loss = 3.0055  \n",
      "\n",
      "Fold: 1  Epoch: 22  Training loss = 2.9758  Validation loss = 2.9835  \n",
      "\n",
      "Fold: 1  Epoch: 23  Training loss = 2.9681  Validation loss = 2.9674  \n",
      "\n",
      "Fold: 1  Epoch: 24  Training loss = 2.9576  Validation loss = 2.9454  \n",
      "\n",
      "Fold: 1  Epoch: 25  Training loss = 2.9526  Validation loss = 2.9341  \n",
      "\n",
      "Fold: 1  Epoch: 26  Training loss = 2.9450  Validation loss = 2.9178  \n",
      "\n",
      "Fold: 1  Epoch: 27  Training loss = 2.9389  Validation loss = 2.9039  \n",
      "\n",
      "Fold: 1  Epoch: 28  Training loss = 2.9313  Validation loss = 2.8875  \n",
      "\n",
      "Fold: 1  Epoch: 29  Training loss = 2.9262  Validation loss = 2.8755  \n",
      "\n",
      "Fold: 1  Epoch: 30  Training loss = 2.9222  Validation loss = 2.8659  \n",
      "\n",
      "Fold: 1  Epoch: 31  Training loss = 2.9148  Validation loss = 2.8493  \n",
      "\n",
      "Fold: 1  Epoch: 32  Training loss = 2.9069  Validation loss = 2.8315  \n",
      "\n",
      "Fold: 1  Epoch: 33  Training loss = 2.9004  Validation loss = 2.8164  \n",
      "\n",
      "Fold: 1  Epoch: 34  Training loss = 2.8917  Validation loss = 2.7965  \n",
      "\n",
      "Fold: 1  Epoch: 35  Training loss = 2.8870  Validation loss = 2.7860  \n",
      "\n",
      "Fold: 1  Epoch: 36  Training loss = 2.8825  Validation loss = 2.7750  \n",
      "\n",
      "Fold: 1  Epoch: 37  Training loss = 2.8748  Validation loss = 2.7566  \n",
      "\n",
      "Fold: 1  Epoch: 38  Training loss = 2.8695  Validation loss = 2.7441  \n",
      "\n",
      "Fold: 1  Epoch: 39  Training loss = 2.8655  Validation loss = 2.7336  \n",
      "\n",
      "Fold: 1  Epoch: 40  Training loss = 2.8569  Validation loss = 2.7134  \n",
      "\n",
      "Fold: 1  Epoch: 41  Training loss = 2.8528  Validation loss = 2.7024  \n",
      "\n",
      "Fold: 1  Epoch: 42  Training loss = 2.8493  Validation loss = 2.6939  \n",
      "\n",
      "Fold: 1  Epoch: 43  Training loss = 2.8448  Validation loss = 2.6824  \n",
      "\n",
      "Fold: 1  Epoch: 44  Training loss = 2.8381  Validation loss = 2.6656  \n",
      "\n",
      "Fold: 1  Epoch: 45  Training loss = 2.8332  Validation loss = 2.6529  \n",
      "\n",
      "Fold: 1  Epoch: 46  Training loss = 2.8280  Validation loss = 2.6396  \n",
      "\n",
      "Fold: 1  Epoch: 47  Training loss = 2.8233  Validation loss = 2.6274  \n",
      "\n",
      "Fold: 1  Epoch: 48  Training loss = 2.8176  Validation loss = 2.6122  \n",
      "\n",
      "Fold: 1  Epoch: 49  Training loss = 2.8153  Validation loss = 2.6059  \n",
      "\n",
      "Fold: 1  Epoch: 50  Training loss = 2.8100  Validation loss = 2.5916  \n",
      "\n",
      "Fold: 1  Epoch: 51  Training loss = 2.8046  Validation loss = 2.5777  \n",
      "\n",
      "Fold: 1  Epoch: 52  Training loss = 2.7998  Validation loss = 2.5648  \n",
      "\n",
      "Fold: 1  Epoch: 53  Training loss = 2.7956  Validation loss = 2.5537  \n",
      "\n",
      "Fold: 1  Epoch: 54  Training loss = 2.7914  Validation loss = 2.5418  \n",
      "\n",
      "Fold: 1  Epoch: 55  Training loss = 2.7853  Validation loss = 2.5252  \n",
      "\n",
      "Fold: 1  Epoch: 56  Training loss = 2.7823  Validation loss = 2.5163  \n",
      "\n",
      "Fold: 1  Epoch: 57  Training loss = 2.7768  Validation loss = 2.5016  \n",
      "\n",
      "Fold: 1  Epoch: 58  Training loss = 2.7732  Validation loss = 2.4913  \n",
      "\n",
      "Fold: 1  Epoch: 59  Training loss = 2.7715  Validation loss = 2.4859  \n",
      "\n",
      "Fold: 1  Epoch: 60  Training loss = 2.7684  Validation loss = 2.4772  \n",
      "\n",
      "Fold: 1  Epoch: 61  Training loss = 2.7652  Validation loss = 2.4684  \n",
      "\n",
      "Fold: 1  Epoch: 62  Training loss = 2.7630  Validation loss = 2.4613  \n",
      "\n",
      "Fold: 1  Epoch: 63  Training loss = 2.7610  Validation loss = 2.4547  \n",
      "\n",
      "Fold: 1  Epoch: 64  Training loss = 2.7568  Validation loss = 2.4424  \n",
      "\n",
      "Fold: 1  Epoch: 65  Training loss = 2.7548  Validation loss = 2.4360  \n",
      "\n",
      "Fold: 1  Epoch: 66  Training loss = 2.7525  Validation loss = 2.4284  \n",
      "\n",
      "Fold: 1  Epoch: 67  Training loss = 2.7504  Validation loss = 2.4224  \n",
      "\n",
      "Fold: 1  Epoch: 68  Training loss = 2.7457  Validation loss = 2.4079  \n",
      "\n",
      "Fold: 1  Epoch: 69  Training loss = 2.7432  Validation loss = 2.4003  \n",
      "\n",
      "Fold: 1  Epoch: 70  Training loss = 2.7401  Validation loss = 2.3908  \n",
      "\n",
      "Fold: 1  Epoch: 71  Training loss = 2.7369  Validation loss = 2.3815  \n",
      "\n",
      "Fold: 1  Epoch: 72  Training loss = 2.7294  Validation loss = 2.3585  \n",
      "\n",
      "Fold: 1  Epoch: 73  Training loss = 2.7293  Validation loss = 2.3579  \n",
      "\n",
      "Fold: 1  Epoch: 74  Training loss = 2.7284  Validation loss = 2.3549  \n",
      "\n",
      "Fold: 1  Epoch: 75  Training loss = 2.7239  Validation loss = 2.3407  \n",
      "\n",
      "Fold: 1  Epoch: 76  Training loss = 2.7210  Validation loss = 2.3317  \n",
      "\n",
      "Fold: 1  Epoch: 77  Training loss = 2.7169  Validation loss = 2.3183  \n",
      "\n",
      "Fold: 1  Epoch: 78  Training loss = 2.7142  Validation loss = 2.3095  \n",
      "\n",
      "Fold: 1  Epoch: 79  Training loss = 2.7103  Validation loss = 2.2975  \n",
      "\n",
      "Fold: 1  Epoch: 80  Training loss = 2.7060  Validation loss = 2.2831  \n",
      "\n",
      "Fold: 1  Epoch: 81  Training loss = 2.7030  Validation loss = 2.2734  \n",
      "\n",
      "Fold: 1  Epoch: 82  Training loss = 2.7006  Validation loss = 2.2649  \n",
      "\n",
      "Fold: 1  Epoch: 83  Training loss = 2.6959  Validation loss = 2.2489  \n",
      "\n",
      "Fold: 1  Epoch: 84  Training loss = 2.6943  Validation loss = 2.2436  \n",
      "\n",
      "Fold: 1  Epoch: 85  Training loss = 2.6910  Validation loss = 2.2324  \n",
      "\n",
      "Fold: 1  Epoch: 86  Training loss = 2.6865  Validation loss = 2.2171  \n",
      "\n",
      "Fold: 1  Epoch: 87  Training loss = 2.6845  Validation loss = 2.2107  \n",
      "\n",
      "Fold: 1  Epoch: 88  Training loss = 2.6815  Validation loss = 2.2000  \n",
      "\n",
      "Fold: 1  Epoch: 89  Training loss = 2.6792  Validation loss = 2.1919  \n",
      "\n",
      "Fold: 1  Epoch: 90  Training loss = 2.6778  Validation loss = 2.1860  \n",
      "\n",
      "Fold: 1  Epoch: 91  Training loss = 2.6756  Validation loss = 2.1782  \n",
      "\n",
      "Fold: 1  Epoch: 92  Training loss = 2.6743  Validation loss = 2.1726  \n",
      "\n",
      "Fold: 1  Epoch: 93  Training loss = 2.6710  Validation loss = 2.1613  \n",
      "\n",
      "Fold: 1  Epoch: 94  Training loss = 2.6678  Validation loss = 2.1493  \n",
      "\n",
      "Fold: 1  Epoch: 95  Training loss = 2.6668  Validation loss = 2.1455  \n",
      "\n",
      "Fold: 1  Epoch: 96  Training loss = 2.6655  Validation loss = 2.1401  \n",
      "\n",
      "Fold: 1  Epoch: 97  Training loss = 2.6632  Validation loss = 2.1314  \n",
      "\n",
      "Fold: 1  Epoch: 98  Training loss = 2.6615  Validation loss = 2.1247  \n",
      "\n",
      "Fold: 1  Epoch: 99  Training loss = 2.6598  Validation loss = 2.1184  \n",
      "\n",
      "Fold: 1  Epoch: 100  Training loss = 2.6584  Validation loss = 2.1127  \n",
      "\n",
      "Fold: 1  Epoch: 101  Training loss = 2.6560  Validation loss = 2.1026  \n",
      "\n",
      "Fold: 1  Epoch: 102  Training loss = 2.6541  Validation loss = 2.0954  \n",
      "\n",
      "Fold: 1  Epoch: 103  Training loss = 2.6514  Validation loss = 2.0845  \n",
      "\n",
      "Fold: 1  Epoch: 104  Training loss = 2.6506  Validation loss = 2.0810  \n",
      "\n",
      "Fold: 1  Epoch: 105  Training loss = 2.6497  Validation loss = 2.0769  \n",
      "\n",
      "Fold: 1  Epoch: 106  Training loss = 2.6489  Validation loss = 2.0736  \n",
      "\n",
      "Fold: 1  Epoch: 107  Training loss = 2.6464  Validation loss = 2.0640  \n",
      "\n",
      "Fold: 1  Epoch: 108  Training loss = 2.6441  Validation loss = 2.0547  \n",
      "\n",
      "Fold: 1  Epoch: 109  Training loss = 2.6432  Validation loss = 2.0514  \n",
      "\n",
      "Fold: 1  Epoch: 110  Training loss = 2.6422  Validation loss = 2.0471  \n",
      "\n",
      "Fold: 1  Epoch: 111  Training loss = 2.6410  Validation loss = 2.0433  \n",
      "\n",
      "Fold: 1  Epoch: 112  Training loss = 2.6376  Validation loss = 2.0297  \n",
      "\n",
      "Fold: 1  Epoch: 113  Training loss = 2.6356  Validation loss = 2.0223  \n",
      "\n",
      "Fold: 1  Epoch: 114  Training loss = 2.6343  Validation loss = 2.0172  \n",
      "\n",
      "Fold: 1  Epoch: 115  Training loss = 2.6325  Validation loss = 2.0096  \n",
      "\n",
      "Fold: 1  Epoch: 116  Training loss = 2.6298  Validation loss = 1.9978  \n",
      "\n",
      "Fold: 1  Epoch: 117  Training loss = 2.6274  Validation loss = 1.9874  \n",
      "\n",
      "Fold: 1  Epoch: 118  Training loss = 2.6264  Validation loss = 1.9831  \n",
      "\n",
      "Fold: 1  Epoch: 119  Training loss = 2.6255  Validation loss = 1.9795  \n",
      "\n",
      "Fold: 1  Epoch: 120  Training loss = 2.6240  Validation loss = 1.9738  \n",
      "\n",
      "Fold: 1  Epoch: 121  Training loss = 2.6237  Validation loss = 1.9728  \n",
      "\n",
      "Fold: 1  Epoch: 122  Training loss = 2.6233  Validation loss = 1.9708  \n",
      "\n",
      "Fold: 1  Epoch: 123  Training loss = 2.6226  Validation loss = 1.9676  \n",
      "\n",
      "Fold: 1  Epoch: 124  Training loss = 2.6215  Validation loss = 1.9631  \n",
      "\n",
      "Fold: 1  Epoch: 125  Training loss = 2.6192  Validation loss = 1.9525  \n",
      "\n",
      "Fold: 1  Epoch: 126  Training loss = 2.6187  Validation loss = 1.9509  \n",
      "\n",
      "Fold: 1  Epoch: 127  Training loss = 2.6166  Validation loss = 1.9418  \n",
      "\n",
      "Fold: 1  Epoch: 128  Training loss = 2.6158  Validation loss = 1.9389  \n",
      "\n",
      "Fold: 1  Epoch: 129  Training loss = 2.6146  Validation loss = 1.9345  \n",
      "\n",
      "Fold: 1  Epoch: 130  Training loss = 2.6124  Validation loss = 1.9242  \n",
      "\n",
      "Fold: 1  Epoch: 131  Training loss = 2.6113  Validation loss = 1.9195  \n",
      "\n",
      "Fold: 1  Epoch: 132  Training loss = 2.6095  Validation loss = 1.9119  \n",
      "\n",
      "Fold: 1  Epoch: 133  Training loss = 2.6088  Validation loss = 1.9095  \n",
      "\n",
      "Fold: 1  Epoch: 134  Training loss = 2.6080  Validation loss = 1.9070  \n",
      "\n",
      "Fold: 1  Epoch: 135  Training loss = 2.6066  Validation loss = 1.8990  \n",
      "\n",
      "Fold: 1  Epoch: 136  Training loss = 2.6058  Validation loss = 1.8963  \n",
      "\n",
      "Fold: 1  Epoch: 137  Training loss = 2.6054  Validation loss = 1.8953  \n",
      "\n",
      "Fold: 1  Epoch: 138  Training loss = 2.6045  Validation loss = 1.8909  \n",
      "\n",
      "Fold: 1  Epoch: 139  Training loss = 2.6035  Validation loss = 1.8868  \n",
      "\n",
      "Fold: 1  Epoch: 140  Training loss = 2.6033  Validation loss = 1.8891  \n",
      "\n",
      "Fold: 1  Epoch: 141  Training loss = 2.6016  Validation loss = 1.8789  \n",
      "\n",
      "Fold: 1  Epoch: 142  Training loss = 2.5999  Validation loss = 1.8700  \n",
      "\n",
      "Fold: 1  Epoch: 143  Training loss = 2.5986  Validation loss = 1.8636  \n",
      "\n",
      "Fold: 1  Epoch: 144  Training loss = 2.5986  Validation loss = 1.8649  \n",
      "\n",
      "Fold: 1  Epoch: 145  Training loss = 2.5987  Validation loss = 1.8683  \n",
      "\n",
      "Fold: 1  Epoch: 146  Training loss = 2.5984  Validation loss = 1.8688  \n",
      "\n",
      "Fold: 1  Epoch: 147  Training loss = 2.5967  Validation loss = 1.8580  \n",
      "\n",
      "Fold: 1  Epoch: 148  Training loss = 2.5965  Validation loss = 1.8598  \n",
      "\n",
      "Fold: 1  Epoch: 149  Training loss = 2.5950  Validation loss = 1.8544  \n",
      "\n",
      "Fold: 1  Epoch: 150  Training loss = 2.5943  Validation loss = 1.8513  \n",
      "\n",
      "Fold: 1  Epoch: 151  Training loss = 2.5932  Validation loss = 1.8442  \n",
      "\n",
      "Fold: 1  Epoch: 152  Training loss = 2.5912  Validation loss = 1.8318  \n",
      "\n",
      "Fold: 1  Epoch: 153  Training loss = 2.5890  Validation loss = 1.8193  \n",
      "\n",
      "Fold: 1  Epoch: 154  Training loss = 2.5878  Validation loss = 1.8137  \n",
      "\n",
      "Fold: 1  Epoch: 155  Training loss = 2.5875  Validation loss = 1.8145  \n",
      "\n",
      "Fold: 1  Epoch: 156  Training loss = 2.5865  Validation loss = 1.8079  \n",
      "\n",
      "Fold: 1  Epoch: 157  Training loss = 2.5851  Validation loss = 1.8006  \n",
      "\n",
      "Fold: 1  Epoch: 158  Training loss = 2.5841  Validation loss = 1.7966  \n",
      "\n",
      "Fold: 1  Epoch: 159  Training loss = 2.5829  Validation loss = 1.7904  \n",
      "\n",
      "Fold: 1  Epoch: 160  Training loss = 2.5817  Validation loss = 1.7830  \n",
      "\n",
      "Fold: 1  Epoch: 161  Training loss = 2.5798  Validation loss = 1.7716  \n",
      "\n",
      "Fold: 1  Epoch: 162  Training loss = 2.5787  Validation loss = 1.7632  \n",
      "\n",
      "Fold: 1  Epoch: 163  Training loss = 2.5779  Validation loss = 1.7618  \n",
      "\n",
      "Fold: 1  Epoch: 164  Training loss = 2.5761  Validation loss = 1.7516  \n",
      "\n",
      "Fold: 1  Epoch: 165  Training loss = 2.5759  Validation loss = 1.7530  \n",
      "\n",
      "Fold: 1  Epoch: 166  Training loss = 2.5758  Validation loss = 1.7551  \n",
      "\n",
      "Fold: 1  Epoch: 167  Training loss = 2.5755  Validation loss = 1.7558  \n",
      "\n",
      "Fold: 1  Epoch: 168  Training loss = 2.5754  Validation loss = 1.7581  \n",
      "\n",
      "Fold: 1  Epoch: 169  Training loss = 2.5748  Validation loss = 1.7559  \n",
      "\n",
      "Fold: 1  Epoch: 170  Training loss = 2.5740  Validation loss = 1.7494  \n",
      "\n",
      "Fold: 1  Epoch: 171  Training loss = 2.5736  Validation loss = 1.7499  \n",
      "\n",
      "Fold: 1  Epoch: 172  Training loss = 2.5727  Validation loss = 1.7453  \n",
      "\n",
      "Fold: 1  Epoch: 173  Training loss = 2.5724  Validation loss = 1.7432  \n",
      "\n",
      "Fold: 1  Epoch: 174  Training loss = 2.5714  Validation loss = 1.7403  \n",
      "\n",
      "Fold: 1  Epoch: 175  Training loss = 2.5704  Validation loss = 1.7341  \n",
      "\n",
      "Fold: 1  Epoch: 176  Training loss = 2.5686  Validation loss = 1.7242  \n",
      "\n",
      "Fold: 1  Epoch: 177  Training loss = 2.5676  Validation loss = 1.7190  \n",
      "\n",
      "Fold: 1  Epoch: 178  Training loss = 2.5665  Validation loss = 1.7151  \n",
      "\n",
      "Fold: 1  Epoch: 179  Training loss = 2.5660  Validation loss = 1.7165  \n",
      "\n",
      "Fold: 1  Epoch: 180  Training loss = 2.5651  Validation loss = 1.7130  \n",
      "\n",
      "Fold: 1  Epoch: 181  Training loss = 2.5648  Validation loss = 1.7141  \n",
      "\n",
      "Fold: 1  Epoch: 182  Training loss = 2.5648  Validation loss = 1.7195  \n",
      "\n",
      "Fold: 1  Epoch: 183  Training loss = 2.5645  Validation loss = 1.7237  \n",
      "\n",
      "Fold: 1  Epoch: 184  Training loss = 2.5638  Validation loss = 1.7196  \n",
      "\n",
      "Fold: 1  Epoch: 185  Training loss = 2.5631  Validation loss = 1.7157  \n",
      "\n",
      "Fold: 1  Epoch: 186  Training loss = 2.5616  Validation loss = 1.7065  \n",
      "\n",
      "Fold: 1  Epoch: 187  Training loss = 2.5610  Validation loss = 1.7066  \n",
      "\n",
      "Fold: 1  Epoch: 188  Training loss = 2.5608  Validation loss = 1.7093  \n",
      "\n",
      "Fold: 1  Epoch: 189  Training loss = 2.5604  Validation loss = 1.7100  \n",
      "\n",
      "Fold: 1  Epoch: 190  Training loss = 2.5597  Validation loss = 1.7065  \n",
      "\n",
      "Fold: 1  Epoch: 191  Training loss = 2.5588  Validation loss = 1.7024  \n",
      "\n",
      "Fold: 1  Epoch: 192  Training loss = 2.5585  Validation loss = 1.7045  \n",
      "\n",
      "Fold: 1  Epoch: 193  Training loss = 2.5579  Validation loss = 1.7016  \n",
      "\n",
      "Fold: 1  Epoch: 194  Training loss = 2.5567  Validation loss = 1.6950  \n",
      "\n",
      "Fold: 1  Epoch: 195  Training loss = 2.5563  Validation loss = 1.6947  \n",
      "\n",
      "Fold: 1  Epoch: 196  Training loss = 2.5557  Validation loss = 1.6940  \n",
      "\n",
      "Fold: 1  Epoch: 197  Training loss = 2.5549  Validation loss = 1.6918  \n",
      "\n",
      "Fold: 1  Epoch: 198  Training loss = 2.5543  Validation loss = 1.6875  \n",
      "\n",
      "Fold: 1  Epoch: 199  Training loss = 2.5535  Validation loss = 1.6851  \n",
      "\n",
      "Fold: 1  Epoch: 200  Training loss = 2.5524  Validation loss = 1.6786  \n",
      "\n",
      "Fold: 1  Epoch: 201  Training loss = 2.5520  Validation loss = 1.6746  \n",
      "\n",
      "Fold: 1  Epoch: 202  Training loss = 2.5521  Validation loss = 1.6824  \n",
      "\n",
      "Fold: 1  Epoch: 203  Training loss = 2.5516  Validation loss = 1.6828  \n",
      "\n",
      "Fold: 1  Epoch: 204  Training loss = 2.5515  Validation loss = 1.6908  \n",
      "\n",
      "Fold: 1  Epoch: 205  Training loss = 2.5512  Validation loss = 1.6928  \n",
      "\n",
      "Fold: 1  Epoch: 206  Training loss = 2.5510  Validation loss = 1.6924  \n",
      "\n",
      "Fold: 1  Epoch: 207  Training loss = 2.5500  Validation loss = 1.6844  \n",
      "\n",
      "Fold: 1  Epoch: 208  Training loss = 2.5490  Validation loss = 1.6770  \n",
      "\n",
      "Fold: 1  Epoch: 209  Training loss = 2.5485  Validation loss = 1.6763  \n",
      "\n",
      "Fold: 1  Epoch: 210  Training loss = 2.5481  Validation loss = 1.6745  \n",
      "\n",
      "Fold: 1  Epoch: 211  Training loss = 2.5475  Validation loss = 1.6717  \n",
      "\n",
      "Fold: 1  Epoch: 212  Training loss = 2.5470  Validation loss = 1.6684  \n",
      "\n",
      "Fold: 1  Epoch: 213  Training loss = 2.5462  Validation loss = 1.6645  \n",
      "\n",
      "Fold: 1  Epoch: 214  Training loss = 2.5455  Validation loss = 1.6641  \n",
      "\n",
      "Fold: 1  Epoch: 215  Training loss = 2.5450  Validation loss = 1.6600  \n",
      "\n",
      "Fold: 1  Epoch: 216  Training loss = 2.5444  Validation loss = 1.6619  \n",
      "\n",
      "Fold: 1  Epoch: 217  Training loss = 2.5443  Validation loss = 1.6635  \n",
      "\n",
      "Fold: 1  Epoch: 218  Training loss = 2.5441  Validation loss = 1.6622  \n",
      "\n",
      "Fold: 1  Epoch: 219  Training loss = 2.5430  Validation loss = 1.6535  \n",
      "\n",
      "Fold: 1  Epoch: 220  Training loss = 2.5426  Validation loss = 1.6575  \n",
      "\n",
      "Fold: 1  Epoch: 221  Training loss = 2.5417  Validation loss = 1.6557  \n",
      "\n",
      "Fold: 1  Epoch: 222  Training loss = 2.5406  Validation loss = 1.6445  \n",
      "\n",
      "Fold: 1  Epoch: 223  Training loss = 2.5401  Validation loss = 1.6434  \n",
      "\n",
      "Fold: 1  Epoch: 224  Training loss = 2.5398  Validation loss = 1.6439  \n",
      "\n",
      "Fold: 1  Epoch: 225  Training loss = 2.5393  Validation loss = 1.6416  \n",
      "\n",
      "Fold: 1  Epoch: 226  Training loss = 2.5386  Validation loss = 1.6370  \n",
      "\n",
      "Fold: 1  Epoch: 227  Training loss = 2.5379  Validation loss = 1.6376  \n",
      "\n",
      "Fold: 1  Epoch: 228  Training loss = 2.5376  Validation loss = 1.6440  \n",
      "\n",
      "Fold: 1  Epoch: 229  Training loss = 2.5368  Validation loss = 1.6370  \n",
      "\n",
      "Fold: 1  Epoch: 230  Training loss = 2.5360  Validation loss = 1.6389  \n",
      "\n",
      "Fold: 1  Epoch: 231  Training loss = 2.5353  Validation loss = 1.6420  \n",
      "\n",
      "Fold: 1  Epoch: 232  Training loss = 2.5350  Validation loss = 1.6503  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 226  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.4489  Validation loss = 2.0982  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.4485  Validation loss = 2.0980  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.4477  Validation loss = 2.0926  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.4471  Validation loss = 2.0905  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.4469  Validation loss = 2.0914  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.4464  Validation loss = 2.0896  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.4459  Validation loss = 2.0865  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.4455  Validation loss = 2.0885  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.4448  Validation loss = 2.0856  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.4445  Validation loss = 2.0852  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.4443  Validation loss = 2.0839  \n",
      "\n",
      "Fold: 2  Epoch: 12  Training loss = 2.4436  Validation loss = 2.0799  \n",
      "\n",
      "Fold: 2  Epoch: 13  Training loss = 2.4426  Validation loss = 2.0755  \n",
      "\n",
      "Fold: 2  Epoch: 14  Training loss = 2.4424  Validation loss = 2.0741  \n",
      "\n",
      "Fold: 2  Epoch: 15  Training loss = 2.4418  Validation loss = 2.0726  \n",
      "\n",
      "Fold: 2  Epoch: 16  Training loss = 2.4416  Validation loss = 2.0714  \n",
      "\n",
      "Fold: 2  Epoch: 17  Training loss = 2.4410  Validation loss = 2.0719  \n",
      "\n",
      "Fold: 2  Epoch: 18  Training loss = 2.4411  Validation loss = 2.0743  \n",
      "\n",
      "Fold: 2  Epoch: 19  Training loss = 2.4405  Validation loss = 2.0742  \n",
      "\n",
      "Fold: 2  Epoch: 20  Training loss = 2.4395  Validation loss = 2.0694  \n",
      "\n",
      "Fold: 2  Epoch: 21  Training loss = 2.4392  Validation loss = 2.0704  \n",
      "\n",
      "Fold: 2  Epoch: 22  Training loss = 2.4387  Validation loss = 2.0707  \n",
      "\n",
      "Fold: 2  Epoch: 23  Training loss = 2.4381  Validation loss = 2.0679  \n",
      "\n",
      "Fold: 2  Epoch: 24  Training loss = 2.4379  Validation loss = 2.0651  \n",
      "\n",
      "Fold: 2  Epoch: 25  Training loss = 2.4372  Validation loss = 2.0626  \n",
      "\n",
      "Fold: 2  Epoch: 26  Training loss = 2.4368  Validation loss = 2.0604  \n",
      "\n",
      "Fold: 2  Epoch: 27  Training loss = 2.4365  Validation loss = 2.0605  \n",
      "\n",
      "Fold: 2  Epoch: 28  Training loss = 2.4358  Validation loss = 2.0564  \n",
      "\n",
      "Fold: 2  Epoch: 29  Training loss = 2.4357  Validation loss = 2.0601  \n",
      "\n",
      "Fold: 2  Epoch: 30  Training loss = 2.4351  Validation loss = 2.0577  \n",
      "\n",
      "Fold: 2  Epoch: 31  Training loss = 2.4349  Validation loss = 2.0573  \n",
      "\n",
      "Fold: 2  Epoch: 32  Training loss = 2.4347  Validation loss = 2.0579  \n",
      "\n",
      "Fold: 2  Epoch: 33  Training loss = 2.4342  Validation loss = 2.0606  \n",
      "\n",
      "Fold: 2  Epoch: 34  Training loss = 2.4337  Validation loss = 2.0597  \n",
      "\n",
      "Fold: 2  Epoch: 35  Training loss = 2.4331  Validation loss = 2.0557  \n",
      "\n",
      "Fold: 2  Epoch: 36  Training loss = 2.4326  Validation loss = 2.0543  \n",
      "\n",
      "Fold: 2  Epoch: 37  Training loss = 2.4322  Validation loss = 2.0538  \n",
      "\n",
      "Fold: 2  Epoch: 38  Training loss = 2.4320  Validation loss = 2.0563  \n",
      "\n",
      "Fold: 2  Epoch: 39  Training loss = 2.4317  Validation loss = 2.0563  \n",
      "\n",
      "Fold: 2  Epoch: 40  Training loss = 2.4313  Validation loss = 2.0536  \n",
      "\n",
      "Fold: 2  Epoch: 41  Training loss = 2.4306  Validation loss = 2.0540  \n",
      "\n",
      "Fold: 2  Epoch: 42  Training loss = 2.4301  Validation loss = 2.0530  \n",
      "\n",
      "Fold: 2  Epoch: 43  Training loss = 2.4297  Validation loss = 2.0557  \n",
      "\n",
      "Fold: 2  Epoch: 44  Training loss = 2.4294  Validation loss = 2.0530  \n",
      "\n",
      "Fold: 2  Epoch: 45  Training loss = 2.4288  Validation loss = 2.0558  \n",
      "\n",
      "Fold: 2  Epoch: 46  Training loss = 2.4283  Validation loss = 2.0565  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 44  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.5027  Validation loss = 3.4099  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.5027  Validation loss = 3.4077  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.5016  Validation loss = 3.4095  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.5004  Validation loss = 3.4061  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.4996  Validation loss = 3.4067  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.4984  Validation loss = 3.4052  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.4986  Validation loss = 3.4008  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.4981  Validation loss = 3.4021  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.4977  Validation loss = 3.4001  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.4976  Validation loss = 3.3972  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.4968  Validation loss = 3.3988  \n",
      "\n",
      "Fold: 3  Epoch: 12  Training loss = 1.4962  Validation loss = 3.4003  \n",
      "\n",
      "Fold: 3  Epoch: 13  Training loss = 1.4955  Validation loss = 3.4045  \n",
      "\n",
      "Fold: 3  Epoch: 14  Training loss = 1.4943  Validation loss = 3.4045  \n",
      "\n",
      "Fold: 3  Epoch: 15  Training loss = 1.4937  Validation loss = 3.4004  \n",
      "\n",
      "Fold: 3  Epoch: 16  Training loss = 1.4934  Validation loss = 3.3944  \n",
      "\n",
      "Fold: 3  Epoch: 17  Training loss = 1.4927  Validation loss = 3.3928  \n",
      "\n",
      "Fold: 3  Epoch: 18  Training loss = 1.4921  Validation loss = 3.3926  \n",
      "\n",
      "Fold: 3  Epoch: 19  Training loss = 1.4912  Validation loss = 3.3934  \n",
      "\n",
      "Fold: 3  Epoch: 20  Training loss = 1.4900  Validation loss = 3.3953  \n",
      "\n",
      "Fold: 3  Epoch: 21  Training loss = 1.4891  Validation loss = 3.3947  \n",
      "\n",
      "Fold: 3  Epoch: 22  Training loss = 1.4888  Validation loss = 3.3923  \n",
      "\n",
      "Fold: 3  Epoch: 23  Training loss = 1.4881  Validation loss = 3.3927  \n",
      "\n",
      "Fold: 3  Epoch: 24  Training loss = 1.4877  Validation loss = 3.3939  \n",
      "\n",
      "Fold: 3  Epoch: 25  Training loss = 1.4873  Validation loss = 3.3885  \n",
      "\n",
      "Fold: 3  Epoch: 26  Training loss = 1.4867  Validation loss = 3.3940  \n",
      "\n",
      "Fold: 3  Epoch: 27  Training loss = 1.4863  Validation loss = 3.3889  \n",
      "\n",
      "Fold: 3  Epoch: 28  Training loss = 1.4858  Validation loss = 3.3881  \n",
      "\n",
      "Fold: 3  Epoch: 29  Training loss = 1.4853  Validation loss = 3.3940  \n",
      "\n",
      "Fold: 3  Epoch: 30  Training loss = 1.4849  Validation loss = 3.3938  \n",
      "\n",
      "Fold: 3  Epoch: 31  Training loss = 1.4845  Validation loss = 3.3908  \n",
      "\n",
      "Fold: 3  Epoch: 32  Training loss = 1.4841  Validation loss = 3.3903  \n",
      "\n",
      "Fold: 3  Epoch: 33  Training loss = 1.4839  Validation loss = 3.3850  \n",
      "\n",
      "Fold: 3  Epoch: 34  Training loss = 1.4837  Validation loss = 3.3857  \n",
      "\n",
      "Fold: 3  Epoch: 35  Training loss = 1.4833  Validation loss = 3.3874  \n",
      "\n",
      "Fold: 3  Epoch: 36  Training loss = 1.4829  Validation loss = 3.3872  \n",
      "\n",
      "Fold: 3  Epoch: 37  Training loss = 1.4828  Validation loss = 3.3875  \n",
      "\n",
      "Fold: 3  Epoch: 38  Training loss = 1.4826  Validation loss = 3.3897  \n",
      "\n",
      "Fold: 3  Epoch: 39  Training loss = 1.4823  Validation loss = 3.3903  \n",
      "\n",
      "Fold: 3  Epoch: 40  Training loss = 1.4820  Validation loss = 3.3861  \n",
      "\n",
      "Fold: 3  Epoch: 41  Training loss = 1.4816  Validation loss = 3.3875  \n",
      "\n",
      "Fold: 3  Epoch: 42  Training loss = 1.4812  Validation loss = 3.3828  \n",
      "\n",
      "Fold: 3  Epoch: 43  Training loss = 1.4807  Validation loss = 3.3826  \n",
      "\n",
      "Fold: 3  Epoch: 44  Training loss = 1.4804  Validation loss = 3.3812  \n",
      "\n",
      "Fold: 3  Epoch: 45  Training loss = 1.4801  Validation loss = 3.3788  \n",
      "\n",
      "Fold: 3  Epoch: 46  Training loss = 1.4798  Validation loss = 3.3788  \n",
      "\n",
      "Fold: 3  Epoch: 47  Training loss = 1.4794  Validation loss = 3.3776  \n",
      "\n",
      "Fold: 3  Epoch: 48  Training loss = 1.4792  Validation loss = 3.3743  \n",
      "\n",
      "Fold: 3  Epoch: 49  Training loss = 1.4787  Validation loss = 3.3775  \n",
      "\n",
      "Fold: 3  Epoch: 50  Training loss = 1.4782  Validation loss = 3.3776  \n",
      "\n",
      "Fold: 3  Epoch: 51  Training loss = 1.4779  Validation loss = 3.3775  \n",
      "\n",
      "Fold: 3  Epoch: 52  Training loss = 1.4775  Validation loss = 3.3822  \n",
      "\n",
      "Fold: 3  Epoch: 53  Training loss = 1.4774  Validation loss = 3.3809  \n",
      "\n",
      "Fold: 3  Epoch: 54  Training loss = 1.4772  Validation loss = 3.3789  \n",
      "\n",
      "Fold: 3  Epoch: 55  Training loss = 1.4769  Validation loss = 3.3804  \n",
      "\n",
      "Fold: 3  Epoch: 56  Training loss = 1.4765  Validation loss = 3.3846  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 48  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.5954  Validation loss = 4.5547  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.5939  Validation loss = 4.5500  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.5933  Validation loss = 4.5448  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.5913  Validation loss = 4.5371  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.5891  Validation loss = 4.5267  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.5879  Validation loss = 4.5189  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.5859  Validation loss = 4.5113  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.5845  Validation loss = 4.5050  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.5831  Validation loss = 4.4987  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 1.5823  Validation loss = 4.4954  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.5812  Validation loss = 4.4907  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 1.5796  Validation loss = 4.4820  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.5786  Validation loss = 4.4759  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 1.5782  Validation loss = 4.4753  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.5780  Validation loss = 4.4754  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 1.5769  Validation loss = 4.4719  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 1.5759  Validation loss = 4.4683  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 1.5748  Validation loss = 4.4648  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 1.5739  Validation loss = 4.4619  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 1.5731  Validation loss = 4.4600  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 1.5719  Validation loss = 4.4542  \n",
      "\n",
      "Fold: 4  Epoch: 22  Training loss = 1.5709  Validation loss = 4.4486  \n",
      "\n",
      "Fold: 4  Epoch: 23  Training loss = 1.5705  Validation loss = 4.4476  \n",
      "\n",
      "Fold: 4  Epoch: 24  Training loss = 1.5697  Validation loss = 4.4452  \n",
      "\n",
      "Fold: 4  Epoch: 25  Training loss = 1.5684  Validation loss = 4.4361  \n",
      "\n",
      "Fold: 4  Epoch: 26  Training loss = 1.5677  Validation loss = 4.4347  \n",
      "\n",
      "Fold: 4  Epoch: 27  Training loss = 1.5669  Validation loss = 4.4313  \n",
      "\n",
      "Fold: 4  Epoch: 28  Training loss = 1.5666  Validation loss = 4.4304  \n",
      "\n",
      "Fold: 4  Epoch: 29  Training loss = 1.5656  Validation loss = 4.4244  \n",
      "\n",
      "Fold: 4  Epoch: 30  Training loss = 1.5652  Validation loss = 4.4247  \n",
      "\n",
      "Fold: 4  Epoch: 31  Training loss = 1.5641  Validation loss = 4.4182  \n",
      "\n",
      "Fold: 4  Epoch: 32  Training loss = 1.5632  Validation loss = 4.4150  \n",
      "\n",
      "Fold: 4  Epoch: 33  Training loss = 1.5624  Validation loss = 4.4097  \n",
      "\n",
      "Fold: 4  Epoch: 34  Training loss = 1.5618  Validation loss = 4.4059  \n",
      "\n",
      "Fold: 4  Epoch: 35  Training loss = 1.5615  Validation loss = 4.4042  \n",
      "\n",
      "Fold: 4  Epoch: 36  Training loss = 1.5614  Validation loss = 4.4068  \n",
      "\n",
      "Fold: 4  Epoch: 37  Training loss = 1.5609  Validation loss = 4.4034  \n",
      "\n",
      "Fold: 4  Epoch: 38  Training loss = 1.5606  Validation loss = 4.4042  \n",
      "\n",
      "Fold: 4  Epoch: 39  Training loss = 1.5603  Validation loss = 4.4036  \n",
      "\n",
      "Fold: 4  Epoch: 40  Training loss = 1.5597  Validation loss = 4.3995  \n",
      "\n",
      "Fold: 4  Epoch: 41  Training loss = 1.5592  Validation loss = 4.3981  \n",
      "\n",
      "Fold: 4  Epoch: 42  Training loss = 1.5584  Validation loss = 4.3944  \n",
      "\n",
      "Fold: 4  Epoch: 43  Training loss = 1.5572  Validation loss = 4.3842  \n",
      "\n",
      "Fold: 4  Epoch: 44  Training loss = 1.5568  Validation loss = 4.3838  \n",
      "\n",
      "Fold: 4  Epoch: 45  Training loss = 1.5562  Validation loss = 4.3791  \n",
      "\n",
      "Fold: 4  Epoch: 46  Training loss = 1.5559  Validation loss = 4.3770  \n",
      "\n",
      "Fold: 4  Epoch: 47  Training loss = 1.5554  Validation loss = 4.3758  \n",
      "\n",
      "Fold: 4  Epoch: 48  Training loss = 1.5548  Validation loss = 4.3726  \n",
      "\n",
      "Fold: 4  Epoch: 49  Training loss = 1.5544  Validation loss = 4.3702  \n",
      "\n",
      "Fold: 4  Epoch: 50  Training loss = 1.5541  Validation loss = 4.3686  \n",
      "\n",
      "Fold: 4  Epoch: 51  Training loss = 1.5538  Validation loss = 4.3683  \n",
      "\n",
      "Fold: 4  Epoch: 52  Training loss = 1.5535  Validation loss = 4.3657  \n",
      "\n",
      "Fold: 4  Epoch: 53  Training loss = 1.5532  Validation loss = 4.3648  \n",
      "\n",
      "Fold: 4  Epoch: 54  Training loss = 1.5529  Validation loss = 4.3648  \n",
      "\n",
      "Fold: 4  Epoch: 55  Training loss = 1.5524  Validation loss = 4.3604  \n",
      "\n",
      "Fold: 4  Epoch: 56  Training loss = 1.5521  Validation loss = 4.3576  \n",
      "\n",
      "Fold: 4  Epoch: 57  Training loss = 1.5518  Validation loss = 4.3573  \n",
      "\n",
      "Fold: 4  Epoch: 58  Training loss = 1.5517  Validation loss = 4.3574  \n",
      "\n",
      "Fold: 4  Epoch: 59  Training loss = 1.5516  Validation loss = 4.3602  \n",
      "\n",
      "Fold: 4  Epoch: 60  Training loss = 1.5512  Validation loss = 4.3597  \n",
      "\n",
      "Fold: 4  Epoch: 61  Training loss = 1.5510  Validation loss = 4.3588  \n",
      "\n",
      "Fold: 4  Epoch: 62  Training loss = 1.5504  Validation loss = 4.3557  \n",
      "\n",
      "Fold: 4  Epoch: 63  Training loss = 1.5501  Validation loss = 4.3550  \n",
      "\n",
      "Fold: 4  Epoch: 64  Training loss = 1.5495  Validation loss = 4.3498  \n",
      "\n",
      "Fold: 4  Epoch: 65  Training loss = 1.5489  Validation loss = 4.3463  \n",
      "\n",
      "Fold: 4  Epoch: 66  Training loss = 1.5488  Validation loss = 4.3488  \n",
      "\n",
      "Fold: 4  Epoch: 67  Training loss = 1.5483  Validation loss = 4.3419  \n",
      "\n",
      "Fold: 4  Epoch: 68  Training loss = 1.5478  Validation loss = 4.3364  \n",
      "\n",
      "Fold: 4  Epoch: 69  Training loss = 1.5475  Validation loss = 4.3344  \n",
      "\n",
      "Fold: 4  Epoch: 70  Training loss = 1.5469  Validation loss = 4.3258  \n",
      "\n",
      "Fold: 4  Epoch: 71  Training loss = 1.5466  Validation loss = 4.3243  \n",
      "\n",
      "Fold: 4  Epoch: 72  Training loss = 1.5458  Validation loss = 4.3159  \n",
      "\n",
      "Fold: 4  Epoch: 73  Training loss = 1.5454  Validation loss = 4.3127  \n",
      "\n",
      "Fold: 4  Epoch: 74  Training loss = 1.5450  Validation loss = 4.3105  \n",
      "\n",
      "Fold: 4  Epoch: 75  Training loss = 1.5447  Validation loss = 4.3068  \n",
      "\n",
      "Fold: 4  Epoch: 76  Training loss = 1.5444  Validation loss = 4.3048  \n",
      "\n",
      "Fold: 4  Epoch: 77  Training loss = 1.5441  Validation loss = 4.3034  \n",
      "\n",
      "Fold: 4  Epoch: 78  Training loss = 1.5440  Validation loss = 4.3066  \n",
      "\n",
      "Fold: 4  Epoch: 79  Training loss = 1.5437  Validation loss = 4.3065  \n",
      "\n",
      "Fold: 4  Epoch: 80  Training loss = 1.5435  Validation loss = 4.3046  \n",
      "\n",
      "Fold: 4  Epoch: 81  Training loss = 1.5430  Validation loss = 4.3020  \n",
      "\n",
      "Fold: 4  Epoch: 82  Training loss = 1.5428  Validation loss = 4.3006  \n",
      "\n",
      "Fold: 4  Epoch: 83  Training loss = 1.5424  Validation loss = 4.2955  \n",
      "\n",
      "Fold: 4  Epoch: 84  Training loss = 1.5423  Validation loss = 4.2959  \n",
      "\n",
      "Fold: 4  Epoch: 85  Training loss = 1.5423  Validation loss = 4.3015  \n",
      "\n",
      "Fold: 4  Epoch: 86  Training loss = 1.5422  Validation loss = 4.3021  \n",
      "\n",
      "Fold: 4  Epoch: 87  Training loss = 1.5420  Validation loss = 4.3022  \n",
      "\n",
      "Fold: 4  Epoch: 88  Training loss = 1.5418  Validation loss = 4.3007  \n",
      "\n",
      "Fold: 4  Epoch: 89  Training loss = 1.5416  Validation loss = 4.3004  \n",
      "\n",
      "Fold: 4  Epoch: 90  Training loss = 1.5414  Validation loss = 4.2955  \n",
      "\n",
      "Fold: 4  Epoch: 91  Training loss = 1.5410  Validation loss = 4.2898  \n",
      "\n",
      "Fold: 4  Epoch: 92  Training loss = 1.5407  Validation loss = 4.2886  \n",
      "\n",
      "Fold: 4  Epoch: 93  Training loss = 1.5403  Validation loss = 4.2864  \n",
      "\n",
      "Fold: 4  Epoch: 94  Training loss = 1.5400  Validation loss = 4.2853  \n",
      "\n",
      "Fold: 4  Epoch: 95  Training loss = 1.5397  Validation loss = 4.2811  \n",
      "\n",
      "Fold: 4  Epoch: 96  Training loss = 1.5393  Validation loss = 4.2782  \n",
      "\n",
      "Fold: 4  Epoch: 97  Training loss = 1.5390  Validation loss = 4.2758  \n",
      "\n",
      "Fold: 4  Epoch: 98  Training loss = 1.5386  Validation loss = 4.2717  \n",
      "\n",
      "Fold: 4  Epoch: 99  Training loss = 1.5384  Validation loss = 4.2707  \n",
      "\n",
      "Fold: 4  Epoch: 100  Training loss = 1.5383  Validation loss = 4.2745  \n",
      "\n",
      "Fold: 4  Epoch: 101  Training loss = 1.5381  Validation loss = 4.2751  \n",
      "\n",
      "Fold: 4  Epoch: 102  Training loss = 1.5379  Validation loss = 4.2760  \n",
      "\n",
      "Fold: 4  Epoch: 103  Training loss = 1.5377  Validation loss = 4.2699  \n",
      "\n",
      "Fold: 4  Epoch: 104  Training loss = 1.5375  Validation loss = 4.2716  \n",
      "\n",
      "Fold: 4  Epoch: 105  Training loss = 1.5372  Validation loss = 4.2685  \n",
      "\n",
      "Fold: 4  Epoch: 106  Training loss = 1.5370  Validation loss = 4.2674  \n",
      "\n",
      "Fold: 4  Epoch: 107  Training loss = 1.5365  Validation loss = 4.2634  \n",
      "\n",
      "Fold: 4  Epoch: 108  Training loss = 1.5362  Validation loss = 4.2613  \n",
      "\n",
      "Fold: 4  Epoch: 109  Training loss = 1.5359  Validation loss = 4.2602  \n",
      "\n",
      "Fold: 4  Epoch: 110  Training loss = 1.5359  Validation loss = 4.2637  \n",
      "\n",
      "Fold: 4  Epoch: 111  Training loss = 1.5355  Validation loss = 4.2615  \n",
      "\n",
      "Fold: 4  Epoch: 112  Training loss = 1.5353  Validation loss = 4.2621  \n",
      "\n",
      "Fold: 4  Epoch: 113  Training loss = 1.5351  Validation loss = 4.2619  \n",
      "\n",
      "Fold: 4  Epoch: 114  Training loss = 1.5348  Validation loss = 4.2614  \n",
      "\n",
      "Fold: 4  Epoch: 115  Training loss = 1.5344  Validation loss = 4.2591  \n",
      "\n",
      "Fold: 4  Epoch: 116  Training loss = 1.5341  Validation loss = 4.2559  \n",
      "\n",
      "Fold: 4  Epoch: 117  Training loss = 1.5339  Validation loss = 4.2541  \n",
      "\n",
      "Fold: 4  Epoch: 118  Training loss = 1.5339  Validation loss = 4.2578  \n",
      "\n",
      "Fold: 4  Epoch: 119  Training loss = 1.5338  Validation loss = 4.2570  \n",
      "\n",
      "Fold: 4  Epoch: 120  Training loss = 1.5336  Validation loss = 4.2541  \n",
      "\n",
      "Fold: 4  Epoch: 121  Training loss = 1.5333  Validation loss = 4.2489  \n",
      "\n",
      "Fold: 4  Epoch: 122  Training loss = 1.5329  Validation loss = 4.2431  \n",
      "\n",
      "Fold: 4  Epoch: 123  Training loss = 1.5327  Validation loss = 4.2401  \n",
      "\n",
      "Fold: 4  Epoch: 124  Training loss = 1.5324  Validation loss = 4.2379  \n",
      "\n",
      "Fold: 4  Epoch: 125  Training loss = 1.5322  Validation loss = 4.2383  \n",
      "\n",
      "Fold: 4  Epoch: 126  Training loss = 1.5321  Validation loss = 4.2394  \n",
      "\n",
      "Fold: 4  Epoch: 127  Training loss = 1.5320  Validation loss = 4.2396  \n",
      "\n",
      "Fold: 4  Epoch: 128  Training loss = 1.5317  Validation loss = 4.2352  \n",
      "\n",
      "Fold: 4  Epoch: 129  Training loss = 1.5313  Validation loss = 4.2274  \n",
      "\n",
      "Fold: 4  Epoch: 130  Training loss = 1.5311  Validation loss = 4.2284  \n",
      "\n",
      "Fold: 4  Epoch: 131  Training loss = 1.5310  Validation loss = 4.2278  \n",
      "\n",
      "Fold: 4  Epoch: 132  Training loss = 1.5310  Validation loss = 4.2340  \n",
      "\n",
      "Fold: 4  Epoch: 133  Training loss = 1.5308  Validation loss = 4.2315  \n",
      "\n",
      "Fold: 4  Epoch: 134  Training loss = 1.5306  Validation loss = 4.2321  \n",
      "\n",
      "Fold: 4  Epoch: 135  Training loss = 1.5303  Validation loss = 4.2288  \n",
      "\n",
      "Fold: 4  Epoch: 136  Training loss = 1.5300  Validation loss = 4.2253  \n",
      "\n",
      "Fold: 4  Epoch: 137  Training loss = 1.5298  Validation loss = 4.2229  \n",
      "\n",
      "Fold: 4  Epoch: 138  Training loss = 1.5296  Validation loss = 4.2234  \n",
      "\n",
      "Fold: 4  Epoch: 139  Training loss = 1.5292  Validation loss = 4.2216  \n",
      "\n",
      "Fold: 4  Epoch: 140  Training loss = 1.5293  Validation loss = 4.2254  \n",
      "\n",
      "Fold: 4  Epoch: 141  Training loss = 1.5290  Validation loss = 4.2220  \n",
      "\n",
      "Fold: 4  Epoch: 142  Training loss = 1.5290  Validation loss = 4.2240  \n",
      "\n",
      "Fold: 4  Epoch: 143  Training loss = 1.5287  Validation loss = 4.2213  \n",
      "\n",
      "Fold: 4  Epoch: 144  Training loss = 1.5286  Validation loss = 4.2195  \n",
      "\n",
      "Fold: 4  Epoch: 145  Training loss = 1.5284  Validation loss = 4.2177  \n",
      "\n",
      "Fold: 4  Epoch: 146  Training loss = 1.5280  Validation loss = 4.2120  \n",
      "\n",
      "Fold: 4  Epoch: 147  Training loss = 1.5277  Validation loss = 4.2084  \n",
      "\n",
      "Fold: 4  Epoch: 148  Training loss = 1.5274  Validation loss = 4.2082  \n",
      "\n",
      "Fold: 4  Epoch: 149  Training loss = 1.5273  Validation loss = 4.2077  \n",
      "\n",
      "Fold: 4  Epoch: 150  Training loss = 1.5271  Validation loss = 4.2074  \n",
      "\n",
      "Fold: 4  Epoch: 151  Training loss = 1.5268  Validation loss = 4.2033  \n",
      "\n",
      "Fold: 4  Epoch: 152  Training loss = 1.5265  Validation loss = 4.1997  \n",
      "\n",
      "Fold: 4  Epoch: 153  Training loss = 1.5261  Validation loss = 4.1943  \n",
      "\n",
      "Fold: 4  Epoch: 154  Training loss = 1.5259  Validation loss = 4.1919  \n",
      "\n",
      "Fold: 4  Epoch: 155  Training loss = 1.5258  Validation loss = 4.1926  \n",
      "\n",
      "Fold: 4  Epoch: 156  Training loss = 1.5256  Validation loss = 4.1881  \n",
      "\n",
      "Fold: 4  Epoch: 157  Training loss = 1.5255  Validation loss = 4.1896  \n",
      "\n",
      "Fold: 4  Epoch: 158  Training loss = 1.5254  Validation loss = 4.1878  \n",
      "\n",
      "Fold: 4  Epoch: 159  Training loss = 1.5251  Validation loss = 4.1849  \n",
      "\n",
      "Fold: 4  Epoch: 160  Training loss = 1.5250  Validation loss = 4.1865  \n",
      "\n",
      "Fold: 4  Epoch: 161  Training loss = 1.5249  Validation loss = 4.1882  \n",
      "\n",
      "Fold: 4  Epoch: 162  Training loss = 1.5247  Validation loss = 4.1880  \n",
      "\n",
      "Fold: 4  Epoch: 163  Training loss = 1.5245  Validation loss = 4.1877  \n",
      "\n",
      "Fold: 4  Epoch: 164  Training loss = 1.5244  Validation loss = 4.1904  \n",
      "\n",
      "Fold: 4  Epoch: 165  Training loss = 1.5242  Validation loss = 4.1891  \n",
      "\n",
      "Fold: 4  Epoch: 166  Training loss = 1.5240  Validation loss = 4.1883  \n",
      "\n",
      "Fold: 4  Epoch: 167  Training loss = 1.5238  Validation loss = 4.1843  \n",
      "\n",
      "Fold: 4  Epoch: 168  Training loss = 1.5236  Validation loss = 4.1797  \n",
      "\n",
      "Fold: 4  Epoch: 169  Training loss = 1.5234  Validation loss = 4.1733  \n",
      "\n",
      "Fold: 4  Epoch: 170  Training loss = 1.5231  Validation loss = 4.1714  \n",
      "\n",
      "Fold: 4  Epoch: 171  Training loss = 1.5231  Validation loss = 4.1713  \n",
      "\n",
      "Fold: 4  Epoch: 172  Training loss = 1.5227  Validation loss = 4.1678  \n",
      "\n",
      "Fold: 4  Epoch: 173  Training loss = 1.5226  Validation loss = 4.1664  \n",
      "\n",
      "Fold: 4  Epoch: 174  Training loss = 1.5224  Validation loss = 4.1651  \n",
      "\n",
      "Fold: 4  Epoch: 175  Training loss = 1.5222  Validation loss = 4.1638  \n",
      "\n",
      "Fold: 4  Epoch: 176  Training loss = 1.5220  Validation loss = 4.1603  \n",
      "\n",
      "Fold: 4  Epoch: 177  Training loss = 1.5218  Validation loss = 4.1619  \n",
      "\n",
      "Fold: 4  Epoch: 178  Training loss = 1.5217  Validation loss = 4.1617  \n",
      "\n",
      "Fold: 4  Epoch: 179  Training loss = 1.5215  Validation loss = 4.1605  \n",
      "\n",
      "Fold: 4  Epoch: 180  Training loss = 1.5214  Validation loss = 4.1620  \n",
      "\n",
      "Fold: 4  Epoch: 181  Training loss = 1.5212  Validation loss = 4.1622  \n",
      "\n",
      "Fold: 4  Epoch: 182  Training loss = 1.5210  Validation loss = 4.1629  \n",
      "\n",
      "Fold: 4  Epoch: 183  Training loss = 1.5207  Validation loss = 4.1561  \n",
      "\n",
      "Fold: 4  Epoch: 184  Training loss = 1.5205  Validation loss = 4.1555  \n",
      "\n",
      "Fold: 4  Epoch: 185  Training loss = 1.5204  Validation loss = 4.1586  \n",
      "\n",
      "Fold: 4  Epoch: 186  Training loss = 1.5203  Validation loss = 4.1590  \n",
      "\n",
      "Fold: 4  Epoch: 187  Training loss = 1.5202  Validation loss = 4.1568  \n",
      "\n",
      "Fold: 4  Epoch: 188  Training loss = 1.5200  Validation loss = 4.1507  \n",
      "\n",
      "Fold: 4  Epoch: 189  Training loss = 1.5197  Validation loss = 4.1452  \n",
      "\n",
      "Fold: 4  Epoch: 190  Training loss = 1.5196  Validation loss = 4.1434  \n",
      "\n",
      "Fold: 4  Epoch: 191  Training loss = 1.5194  Validation loss = 4.1431  \n",
      "\n",
      "Fold: 4  Epoch: 192  Training loss = 1.5192  Validation loss = 4.1379  \n",
      "\n",
      "Fold: 4  Epoch: 193  Training loss = 1.5190  Validation loss = 4.1399  \n",
      "\n",
      "Fold: 4  Epoch: 194  Training loss = 1.5188  Validation loss = 4.1373  \n",
      "\n",
      "Fold: 4  Epoch: 195  Training loss = 1.5185  Validation loss = 4.1342  \n",
      "\n",
      "Fold: 4  Epoch: 196  Training loss = 1.5184  Validation loss = 4.1321  \n",
      "\n",
      "Fold: 4  Epoch: 197  Training loss = 1.5181  Validation loss = 4.1341  \n",
      "\n",
      "Fold: 4  Epoch: 198  Training loss = 1.5179  Validation loss = 4.1352  \n",
      "\n",
      "Fold: 4  Epoch: 199  Training loss = 1.5176  Validation loss = 4.1260  \n",
      "\n",
      "Fold: 4  Epoch: 200  Training loss = 1.5175  Validation loss = 4.1238  \n",
      "\n",
      "Fold: 4  Epoch: 201  Training loss = 1.5173  Validation loss = 4.1218  \n",
      "\n",
      "Fold: 4  Epoch: 202  Training loss = 1.5171  Validation loss = 4.1215  \n",
      "\n",
      "Fold: 4  Epoch: 203  Training loss = 1.5170  Validation loss = 4.1226  \n",
      "\n",
      "Fold: 4  Epoch: 204  Training loss = 1.5168  Validation loss = 4.1192  \n",
      "\n",
      "Fold: 4  Epoch: 205  Training loss = 1.5165  Validation loss = 4.1224  \n",
      "\n",
      "Fold: 4  Epoch: 206  Training loss = 1.5164  Validation loss = 4.1208  \n",
      "\n",
      "Fold: 4  Epoch: 207  Training loss = 1.5163  Validation loss = 4.1226  \n",
      "\n",
      "Fold: 4  Epoch: 208  Training loss = 1.5161  Validation loss = 4.1197  \n",
      "\n",
      "Fold: 4  Epoch: 209  Training loss = 1.5159  Validation loss = 4.1172  \n",
      "\n",
      "Fold: 4  Epoch: 210  Training loss = 1.5157  Validation loss = 4.1130  \n",
      "\n",
      "Fold: 4  Epoch: 211  Training loss = 1.5156  Validation loss = 4.1115  \n",
      "\n",
      "Fold: 4  Epoch: 212  Training loss = 1.5154  Validation loss = 4.1135  \n",
      "\n",
      "Fold: 4  Epoch: 213  Training loss = 1.5153  Validation loss = 4.1094  \n",
      "\n",
      "Fold: 4  Epoch: 214  Training loss = 1.5151  Validation loss = 4.1058  \n",
      "\n",
      "Fold: 4  Epoch: 215  Training loss = 1.5150  Validation loss = 4.1060  \n",
      "\n",
      "Fold: 4  Epoch: 216  Training loss = 1.5148  Validation loss = 4.1028  \n",
      "\n",
      "Fold: 4  Epoch: 217  Training loss = 1.5147  Validation loss = 4.1006  \n",
      "\n",
      "Fold: 4  Epoch: 218  Training loss = 1.5145  Validation loss = 4.1027  \n",
      "\n",
      "Fold: 4  Epoch: 219  Training loss = 1.5143  Validation loss = 4.1051  \n",
      "\n",
      "Fold: 4  Epoch: 220  Training loss = 1.5142  Validation loss = 4.1022  \n",
      "\n",
      "Fold: 4  Epoch: 221  Training loss = 1.5140  Validation loss = 4.1039  \n",
      "\n",
      "Fold: 4  Epoch: 222  Training loss = 1.5138  Validation loss = 4.1040  \n",
      "\n",
      "Fold: 4  Epoch: 223  Training loss = 1.5137  Validation loss = 4.1041  \n",
      "\n",
      "Fold: 4  Epoch: 224  Training loss = 1.5135  Validation loss = 4.0977  \n",
      "\n",
      "Fold: 4  Epoch: 225  Training loss = 1.5134  Validation loss = 4.0995  \n",
      "\n",
      "Fold: 4  Epoch: 226  Training loss = 1.5133  Validation loss = 4.0960  \n",
      "\n",
      "Fold: 4  Epoch: 227  Training loss = 1.5132  Validation loss = 4.0919  \n",
      "\n",
      "Fold: 4  Epoch: 228  Training loss = 1.5131  Validation loss = 4.0878  \n",
      "\n",
      "Fold: 4  Epoch: 229  Training loss = 1.5129  Validation loss = 4.0884  \n",
      "\n",
      "Fold: 4  Epoch: 230  Training loss = 1.5127  Validation loss = 4.0854  \n",
      "\n",
      "Fold: 4  Epoch: 231  Training loss = 1.5127  Validation loss = 4.0845  \n",
      "\n",
      "Fold: 4  Epoch: 232  Training loss = 1.5126  Validation loss = 4.0810  \n",
      "\n",
      "Fold: 4  Epoch: 233  Training loss = 1.5124  Validation loss = 4.0833  \n",
      "\n",
      "Fold: 4  Epoch: 234  Training loss = 1.5121  Validation loss = 4.0883  \n",
      "\n",
      "Fold: 4  Epoch: 235  Training loss = 1.5121  Validation loss = 4.0855  \n",
      "\n",
      "Fold: 4  Epoch: 236  Training loss = 1.5120  Validation loss = 4.0872  \n",
      "\n",
      "Fold: 4  Epoch: 237  Training loss = 1.5118  Validation loss = 4.0850  \n",
      "\n",
      "Fold: 4  Epoch: 238  Training loss = 1.5116  Validation loss = 4.0816  \n",
      "\n",
      "Fold: 4  Epoch: 239  Training loss = 1.5115  Validation loss = 4.0789  \n",
      "\n",
      "Fold: 4  Epoch: 240  Training loss = 1.5114  Validation loss = 4.0761  \n",
      "\n",
      "Fold: 4  Epoch: 241  Training loss = 1.5112  Validation loss = 4.0747  \n",
      "\n",
      "Fold: 4  Epoch: 242  Training loss = 1.5111  Validation loss = 4.0743  \n",
      "\n",
      "Fold: 4  Epoch: 243  Training loss = 1.5110  Validation loss = 4.0777  \n",
      "\n",
      "Fold: 4  Epoch: 244  Training loss = 1.5107  Validation loss = 4.0800  \n",
      "\n",
      "Fold: 4  Epoch: 245  Training loss = 1.5107  Validation loss = 4.0847  \n",
      "\n",
      "Fold: 4  Epoch: 246  Training loss = 1.5105  Validation loss = 4.0865  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 242  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.7677  Validation loss = 4.3425  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.7675  Validation loss = 4.3418  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.7661  Validation loss = 4.3354  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.7654  Validation loss = 4.3349  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.7641  Validation loss = 4.3276  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 1.7630  Validation loss = 4.3235  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 1.7613  Validation loss = 4.3142  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.7607  Validation loss = 4.3119  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 1.7588  Validation loss = 4.3012  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.7575  Validation loss = 4.2964  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 1.7571  Validation loss = 4.2973  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 1.7561  Validation loss = 4.2915  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 1.7556  Validation loss = 4.2890  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.7544  Validation loss = 4.2828  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 1.7532  Validation loss = 4.2794  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 1.7524  Validation loss = 4.2740  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 1.7522  Validation loss = 4.2749  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 1.7511  Validation loss = 4.2703  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 1.7498  Validation loss = 4.2629  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 1.7486  Validation loss = 4.2561  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 1.7469  Validation loss = 4.2480  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 1.7462  Validation loss = 4.2442  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 1.7454  Validation loss = 4.2418  \n",
      "\n",
      "Fold: 5  Epoch: 24  Training loss = 1.7444  Validation loss = 4.2382  \n",
      "\n",
      "Fold: 5  Epoch: 25  Training loss = 1.7438  Validation loss = 4.2369  \n",
      "\n",
      "Fold: 5  Epoch: 26  Training loss = 1.7432  Validation loss = 4.2369  \n",
      "\n",
      "Fold: 5  Epoch: 27  Training loss = 1.7427  Validation loss = 4.2354  \n",
      "\n",
      "Fold: 5  Epoch: 28  Training loss = 1.7419  Validation loss = 4.2325  \n",
      "\n",
      "Fold: 5  Epoch: 29  Training loss = 1.7405  Validation loss = 4.2243  \n",
      "\n",
      "Fold: 5  Epoch: 30  Training loss = 1.7401  Validation loss = 4.2227  \n",
      "\n",
      "Fold: 5  Epoch: 31  Training loss = 1.7393  Validation loss = 4.2213  \n",
      "\n",
      "Fold: 5  Epoch: 32  Training loss = 1.7391  Validation loss = 4.2216  \n",
      "\n",
      "Fold: 5  Epoch: 33  Training loss = 1.7378  Validation loss = 4.2165  \n",
      "\n",
      "Fold: 5  Epoch: 34  Training loss = 1.7372  Validation loss = 4.2178  \n",
      "\n",
      "Fold: 5  Epoch: 35  Training loss = 1.7367  Validation loss = 4.2159  \n",
      "\n",
      "Fold: 5  Epoch: 36  Training loss = 1.7355  Validation loss = 4.2086  \n",
      "\n",
      "Fold: 5  Epoch: 37  Training loss = 1.7349  Validation loss = 4.2052  \n",
      "\n",
      "Fold: 5  Epoch: 38  Training loss = 1.7343  Validation loss = 4.2046  \n",
      "\n",
      "Fold: 5  Epoch: 39  Training loss = 1.7338  Validation loss = 4.2023  \n",
      "\n",
      "Fold: 5  Epoch: 40  Training loss = 1.7329  Validation loss = 4.1995  \n",
      "\n",
      "Fold: 5  Epoch: 41  Training loss = 1.7321  Validation loss = 4.1949  \n",
      "\n",
      "Fold: 5  Epoch: 42  Training loss = 1.7311  Validation loss = 4.1918  \n",
      "\n",
      "Fold: 5  Epoch: 43  Training loss = 1.7304  Validation loss = 4.1912  \n",
      "\n",
      "Fold: 5  Epoch: 44  Training loss = 1.7296  Validation loss = 4.1888  \n",
      "\n",
      "Fold: 5  Epoch: 45  Training loss = 1.7291  Validation loss = 4.1886  \n",
      "\n",
      "Fold: 5  Epoch: 46  Training loss = 1.7286  Validation loss = 4.1879  \n",
      "\n",
      "Fold: 5  Epoch: 47  Training loss = 1.7280  Validation loss = 4.1853  \n",
      "\n",
      "Fold: 5  Epoch: 48  Training loss = 1.7273  Validation loss = 4.1807  \n",
      "\n",
      "Fold: 5  Epoch: 49  Training loss = 1.7262  Validation loss = 4.1730  \n",
      "\n",
      "Fold: 5  Epoch: 50  Training loss = 1.7250  Validation loss = 4.1643  \n",
      "\n",
      "Fold: 5  Epoch: 51  Training loss = 1.7247  Validation loss = 4.1630  \n",
      "\n",
      "Fold: 5  Epoch: 52  Training loss = 1.7234  Validation loss = 4.1551  \n",
      "\n",
      "Fold: 5  Epoch: 53  Training loss = 1.7228  Validation loss = 4.1545  \n",
      "\n",
      "Fold: 5  Epoch: 54  Training loss = 1.7223  Validation loss = 4.1535  \n",
      "\n",
      "Fold: 5  Epoch: 55  Training loss = 1.7220  Validation loss = 4.1572  \n",
      "\n",
      "Fold: 5  Epoch: 56  Training loss = 1.7216  Validation loss = 4.1592  \n",
      "\n",
      "Fold: 5  Epoch: 57  Training loss = 1.7207  Validation loss = 4.1517  \n",
      "\n",
      "Fold: 5  Epoch: 58  Training loss = 1.7202  Validation loss = 4.1492  \n",
      "\n",
      "Fold: 5  Epoch: 59  Training loss = 1.7192  Validation loss = 4.1434  \n",
      "\n",
      "Fold: 5  Epoch: 60  Training loss = 1.7177  Validation loss = 4.1312  \n",
      "\n",
      "Fold: 5  Epoch: 61  Training loss = 1.7168  Validation loss = 4.1245  \n",
      "\n",
      "Fold: 5  Epoch: 62  Training loss = 1.7165  Validation loss = 4.1232  \n",
      "\n",
      "Fold: 5  Epoch: 63  Training loss = 1.7154  Validation loss = 4.1182  \n",
      "\n",
      "Fold: 5  Epoch: 64  Training loss = 1.7151  Validation loss = 4.1165  \n",
      "\n",
      "Fold: 5  Epoch: 65  Training loss = 1.7146  Validation loss = 4.1160  \n",
      "\n",
      "Fold: 5  Epoch: 66  Training loss = 1.7140  Validation loss = 4.1149  \n",
      "\n",
      "Fold: 5  Epoch: 67  Training loss = 1.7134  Validation loss = 4.1133  \n",
      "\n",
      "Fold: 5  Epoch: 68  Training loss = 1.7124  Validation loss = 4.1072  \n",
      "\n",
      "Fold: 5  Epoch: 69  Training loss = 1.7124  Validation loss = 4.1117  \n",
      "\n",
      "Fold: 5  Epoch: 70  Training loss = 1.7122  Validation loss = 4.1155  \n",
      "\n",
      "Fold: 5  Epoch: 71  Training loss = 1.7120  Validation loss = 4.1173  \n",
      "\n",
      "Fold: 5  Epoch: 72  Training loss = 1.7114  Validation loss = 4.1164  \n",
      "\n",
      "Fold: 5  Epoch: 73  Training loss = 1.7105  Validation loss = 4.1124  \n",
      "\n",
      "Fold: 5  Epoch: 74  Training loss = 1.7097  Validation loss = 4.1065  \n",
      "\n",
      "Fold: 5  Epoch: 75  Training loss = 1.7092  Validation loss = 4.1051  \n",
      "\n",
      "Fold: 5  Epoch: 76  Training loss = 1.7085  Validation loss = 4.1032  \n",
      "\n",
      "Fold: 5  Epoch: 77  Training loss = 1.7078  Validation loss = 4.0983  \n",
      "\n",
      "Fold: 5  Epoch: 78  Training loss = 1.7067  Validation loss = 4.0870  \n",
      "\n",
      "Fold: 5  Epoch: 79  Training loss = 1.7059  Validation loss = 4.0823  \n",
      "\n",
      "Fold: 5  Epoch: 80  Training loss = 1.7056  Validation loss = 4.0882  \n",
      "\n",
      "Fold: 5  Epoch: 81  Training loss = 1.7053  Validation loss = 4.0899  \n",
      "\n",
      "Fold: 5  Epoch: 82  Training loss = 1.7048  Validation loss = 4.0867  \n",
      "\n",
      "Fold: 5  Epoch: 83  Training loss = 1.7042  Validation loss = 4.0860  \n",
      "\n",
      "Fold: 5  Epoch: 84  Training loss = 1.7036  Validation loss = 4.0871  \n",
      "\n",
      "Fold: 5  Epoch: 85  Training loss = 1.7027  Validation loss = 4.0819  \n",
      "\n",
      "Fold: 5  Epoch: 86  Training loss = 1.7025  Validation loss = 4.0839  \n",
      "\n",
      "Fold: 5  Epoch: 87  Training loss = 1.7017  Validation loss = 4.0797  \n",
      "\n",
      "Fold: 5  Epoch: 88  Training loss = 1.7014  Validation loss = 4.0802  \n",
      "\n",
      "Fold: 5  Epoch: 89  Training loss = 1.7009  Validation loss = 4.0796  \n",
      "\n",
      "Fold: 5  Epoch: 90  Training loss = 1.7002  Validation loss = 4.0732  \n",
      "\n",
      "Fold: 5  Epoch: 91  Training loss = 1.6996  Validation loss = 4.0705  \n",
      "\n",
      "Fold: 5  Epoch: 92  Training loss = 1.6992  Validation loss = 4.0712  \n",
      "\n",
      "Fold: 5  Epoch: 93  Training loss = 1.6986  Validation loss = 4.0694  \n",
      "\n",
      "Fold: 5  Epoch: 94  Training loss = 1.6981  Validation loss = 4.0679  \n",
      "\n",
      "Fold: 5  Epoch: 95  Training loss = 1.6980  Validation loss = 4.0682  \n",
      "\n",
      "Fold: 5  Epoch: 96  Training loss = 1.6979  Validation loss = 4.0705  \n",
      "\n",
      "Fold: 5  Epoch: 97  Training loss = 1.6974  Validation loss = 4.0667  \n",
      "\n",
      "Fold: 5  Epoch: 98  Training loss = 1.6968  Validation loss = 4.0653  \n",
      "\n",
      "Fold: 5  Epoch: 99  Training loss = 1.6966  Validation loss = 4.0642  \n",
      "\n",
      "Fold: 5  Epoch: 100  Training loss = 1.6959  Validation loss = 4.0603  \n",
      "\n",
      "Fold: 5  Epoch: 101  Training loss = 1.6953  Validation loss = 4.0572  \n",
      "\n",
      "Fold: 5  Epoch: 102  Training loss = 1.6947  Validation loss = 4.0512  \n",
      "\n",
      "Fold: 5  Epoch: 103  Training loss = 1.6943  Validation loss = 4.0518  \n",
      "\n",
      "Fold: 5  Epoch: 104  Training loss = 1.6939  Validation loss = 4.0527  \n",
      "\n",
      "Fold: 5  Epoch: 105  Training loss = 1.6933  Validation loss = 4.0506  \n",
      "\n",
      "Fold: 5  Epoch: 106  Training loss = 1.6930  Validation loss = 4.0511  \n",
      "\n",
      "Fold: 5  Epoch: 107  Training loss = 1.6922  Validation loss = 4.0450  \n",
      "\n",
      "Fold: 5  Epoch: 108  Training loss = 1.6913  Validation loss = 4.0371  \n",
      "\n",
      "Fold: 5  Epoch: 109  Training loss = 1.6909  Validation loss = 4.0380  \n",
      "\n",
      "Fold: 5  Epoch: 110  Training loss = 1.6904  Validation loss = 4.0369  \n",
      "\n",
      "Fold: 5  Epoch: 111  Training loss = 1.6904  Validation loss = 4.0447  \n",
      "\n",
      "Fold: 5  Epoch: 112  Training loss = 1.6902  Validation loss = 4.0429  \n",
      "\n",
      "Fold: 5  Epoch: 113  Training loss = 1.6899  Validation loss = 4.0447  \n",
      "\n",
      "Fold: 5  Epoch: 114  Training loss = 1.6889  Validation loss = 4.0386  \n",
      "\n",
      "Fold: 5  Epoch: 115  Training loss = 1.6880  Validation loss = 4.0289  \n",
      "\n",
      "Fold: 5  Epoch: 116  Training loss = 1.6871  Validation loss = 4.0220  \n",
      "\n",
      "Fold: 5  Epoch: 117  Training loss = 1.6863  Validation loss = 4.0124  \n",
      "\n",
      "Fold: 5  Epoch: 118  Training loss = 1.6860  Validation loss = 4.0105  \n",
      "\n",
      "Fold: 5  Epoch: 119  Training loss = 1.6855  Validation loss = 4.0088  \n",
      "\n",
      "Fold: 5  Epoch: 120  Training loss = 1.6849  Validation loss = 4.0073  \n",
      "\n",
      "Fold: 5  Epoch: 121  Training loss = 1.6846  Validation loss = 4.0085  \n",
      "\n",
      "Fold: 5  Epoch: 122  Training loss = 1.6842  Validation loss = 4.0060  \n",
      "\n",
      "Fold: 5  Epoch: 123  Training loss = 1.6837  Validation loss = 4.0050  \n",
      "\n",
      "Fold: 5  Epoch: 124  Training loss = 1.6830  Validation loss = 3.9987  \n",
      "\n",
      "Fold: 5  Epoch: 125  Training loss = 1.6827  Validation loss = 3.9996  \n",
      "\n",
      "Fold: 5  Epoch: 126  Training loss = 1.6824  Validation loss = 3.9996  \n",
      "\n",
      "Fold: 5  Epoch: 127  Training loss = 1.6822  Validation loss = 4.0035  \n",
      "\n",
      "Fold: 5  Epoch: 128  Training loss = 1.6815  Validation loss = 3.9974  \n",
      "\n",
      "Fold: 5  Epoch: 129  Training loss = 1.6807  Validation loss = 3.9942  \n",
      "\n",
      "Fold: 5  Epoch: 130  Training loss = 1.6799  Validation loss = 3.9874  \n",
      "\n",
      "Fold: 5  Epoch: 131  Training loss = 1.6794  Validation loss = 3.9853  \n",
      "\n",
      "Fold: 5  Epoch: 132  Training loss = 1.6788  Validation loss = 3.9814  \n",
      "\n",
      "Fold: 5  Epoch: 133  Training loss = 1.6785  Validation loss = 3.9789  \n",
      "\n",
      "Fold: 5  Epoch: 134  Training loss = 1.6779  Validation loss = 3.9728  \n",
      "\n",
      "Fold: 5  Epoch: 135  Training loss = 1.6775  Validation loss = 3.9731  \n",
      "\n",
      "Fold: 5  Epoch: 136  Training loss = 1.6771  Validation loss = 3.9681  \n",
      "\n",
      "Fold: 5  Epoch: 137  Training loss = 1.6765  Validation loss = 3.9621  \n",
      "\n",
      "Fold: 5  Epoch: 138  Training loss = 1.6760  Validation loss = 3.9588  \n",
      "\n",
      "Fold: 5  Epoch: 139  Training loss = 1.6759  Validation loss = 3.9635  \n",
      "\n",
      "Fold: 5  Epoch: 140  Training loss = 1.6752  Validation loss = 3.9550  \n",
      "\n",
      "Fold: 5  Epoch: 141  Training loss = 1.6750  Validation loss = 3.9612  \n",
      "\n",
      "Fold: 5  Epoch: 142  Training loss = 1.6745  Validation loss = 3.9612  \n",
      "\n",
      "Fold: 5  Epoch: 143  Training loss = 1.6742  Validation loss = 3.9597  \n",
      "\n",
      "Fold: 5  Epoch: 144  Training loss = 1.6740  Validation loss = 3.9590  \n",
      "\n",
      "Fold: 5  Epoch: 145  Training loss = 1.6735  Validation loss = 3.9531  \n",
      "\n",
      "Fold: 5  Epoch: 146  Training loss = 1.6730  Validation loss = 3.9497  \n",
      "\n",
      "Fold: 5  Epoch: 147  Training loss = 1.6724  Validation loss = 3.9462  \n",
      "\n",
      "Fold: 5  Epoch: 148  Training loss = 1.6718  Validation loss = 3.9430  \n",
      "\n",
      "Fold: 5  Epoch: 149  Training loss = 1.6716  Validation loss = 3.9446  \n",
      "\n",
      "Fold: 5  Epoch: 150  Training loss = 1.6711  Validation loss = 3.9405  \n",
      "\n",
      "Fold: 5  Epoch: 151  Training loss = 1.6709  Validation loss = 3.9479  \n",
      "\n",
      "Fold: 5  Epoch: 152  Training loss = 1.6705  Validation loss = 3.9420  \n",
      "\n",
      "Fold: 5  Epoch: 153  Training loss = 1.6700  Validation loss = 3.9422  \n",
      "\n",
      "Fold: 5  Epoch: 154  Training loss = 1.6697  Validation loss = 3.9440  \n",
      "\n",
      "Fold: 5  Epoch: 155  Training loss = 1.6693  Validation loss = 3.9440  \n",
      "\n",
      "Fold: 5  Epoch: 156  Training loss = 1.6689  Validation loss = 3.9418  \n",
      "\n",
      "Fold: 5  Epoch: 157  Training loss = 1.6684  Validation loss = 3.9339  \n",
      "\n",
      "Fold: 5  Epoch: 158  Training loss = 1.6683  Validation loss = 3.9429  \n",
      "\n",
      "Fold: 5  Epoch: 159  Training loss = 1.6681  Validation loss = 3.9471  \n",
      "\n",
      "Fold: 5  Epoch: 160  Training loss = 1.6677  Validation loss = 3.9475  \n",
      "\n",
      "Fold: 5  Epoch: 161  Training loss = 1.6675  Validation loss = 3.9553  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 157  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 1.8825  Validation loss = 1.6833  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 1.8791  Validation loss = 1.6751  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 1.8782  Validation loss = 1.6741  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 1.8762  Validation loss = 1.6724  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 1.8716  Validation loss = 1.6626  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 1.8669  Validation loss = 1.6499  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 1.8621  Validation loss = 1.6403  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 1.8595  Validation loss = 1.6314  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 1.8578  Validation loss = 1.6289  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 1.8550  Validation loss = 1.6210  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 1.8533  Validation loss = 1.6153  \n",
      "\n",
      "Fold: 6  Epoch: 12  Training loss = 1.8490  Validation loss = 1.6055  \n",
      "\n",
      "Fold: 6  Epoch: 13  Training loss = 1.8479  Validation loss = 1.6024  \n",
      "\n",
      "Fold: 6  Epoch: 14  Training loss = 1.8460  Validation loss = 1.5982  \n",
      "\n",
      "Fold: 6  Epoch: 15  Training loss = 1.8427  Validation loss = 1.5893  \n",
      "\n",
      "Fold: 6  Epoch: 16  Training loss = 1.8405  Validation loss = 1.5832  \n",
      "\n",
      "Fold: 6  Epoch: 17  Training loss = 1.8390  Validation loss = 1.5792  \n",
      "\n",
      "Fold: 6  Epoch: 18  Training loss = 1.8392  Validation loss = 1.5812  \n",
      "\n",
      "Fold: 6  Epoch: 19  Training loss = 1.8380  Validation loss = 1.5777  \n",
      "\n",
      "Fold: 6  Epoch: 20  Training loss = 1.8373  Validation loss = 1.5764  \n",
      "\n",
      "Fold: 6  Epoch: 21  Training loss = 1.8359  Validation loss = 1.5729  \n",
      "\n",
      "Fold: 6  Epoch: 22  Training loss = 1.8334  Validation loss = 1.5641  \n",
      "\n",
      "Fold: 6  Epoch: 23  Training loss = 1.8308  Validation loss = 1.5566  \n",
      "\n",
      "Fold: 6  Epoch: 24  Training loss = 1.8283  Validation loss = 1.5489  \n",
      "\n",
      "Fold: 6  Epoch: 25  Training loss = 1.8267  Validation loss = 1.5446  \n",
      "\n",
      "Fold: 6  Epoch: 26  Training loss = 1.8256  Validation loss = 1.5437  \n",
      "\n",
      "Fold: 6  Epoch: 27  Training loss = 1.8235  Validation loss = 1.5387  \n",
      "\n",
      "Fold: 6  Epoch: 28  Training loss = 1.8224  Validation loss = 1.5356  \n",
      "\n",
      "Fold: 6  Epoch: 29  Training loss = 1.8206  Validation loss = 1.5284  \n",
      "\n",
      "Fold: 6  Epoch: 30  Training loss = 1.8187  Validation loss = 1.5216  \n",
      "\n",
      "Fold: 6  Epoch: 31  Training loss = 1.8165  Validation loss = 1.5146  \n",
      "\n",
      "Fold: 6  Epoch: 32  Training loss = 1.8145  Validation loss = 1.5074  \n",
      "\n",
      "Fold: 6  Epoch: 33  Training loss = 1.8136  Validation loss = 1.5077  \n",
      "\n",
      "Fold: 6  Epoch: 34  Training loss = 1.8126  Validation loss = 1.5062  \n",
      "\n",
      "Fold: 6  Epoch: 35  Training loss = 1.8108  Validation loss = 1.4995  \n",
      "\n",
      "Fold: 6  Epoch: 36  Training loss = 1.8095  Validation loss = 1.4946  \n",
      "\n",
      "Fold: 6  Epoch: 37  Training loss = 1.8071  Validation loss = 1.4827  \n",
      "\n",
      "Fold: 6  Epoch: 38  Training loss = 1.8060  Validation loss = 1.4793  \n",
      "\n",
      "Fold: 6  Epoch: 39  Training loss = 1.8045  Validation loss = 1.4741  \n",
      "\n",
      "Fold: 6  Epoch: 40  Training loss = 1.8032  Validation loss = 1.4706  \n",
      "\n",
      "Fold: 6  Epoch: 41  Training loss = 1.8029  Validation loss = 1.4729  \n",
      "\n",
      "Fold: 6  Epoch: 42  Training loss = 1.8021  Validation loss = 1.4703  \n",
      "\n",
      "Fold: 6  Epoch: 43  Training loss = 1.8010  Validation loss = 1.4656  \n",
      "\n",
      "Fold: 6  Epoch: 44  Training loss = 1.7999  Validation loss = 1.4621  \n",
      "\n",
      "Fold: 6  Epoch: 45  Training loss = 1.7985  Validation loss = 1.4569  \n",
      "\n",
      "Fold: 6  Epoch: 46  Training loss = 1.7973  Validation loss = 1.4515  \n",
      "\n",
      "Fold: 6  Epoch: 47  Training loss = 1.7967  Validation loss = 1.4518  \n",
      "\n",
      "Fold: 6  Epoch: 48  Training loss = 1.7963  Validation loss = 1.4529  \n",
      "\n",
      "Fold: 6  Epoch: 49  Training loss = 1.7948  Validation loss = 1.4466  \n",
      "\n",
      "Fold: 6  Epoch: 50  Training loss = 1.7931  Validation loss = 1.4403  \n",
      "\n",
      "Fold: 6  Epoch: 51  Training loss = 1.7923  Validation loss = 1.4388  \n",
      "\n",
      "Fold: 6  Epoch: 52  Training loss = 1.7913  Validation loss = 1.4340  \n",
      "\n",
      "Fold: 6  Epoch: 53  Training loss = 1.7898  Validation loss = 1.4268  \n",
      "\n",
      "Fold: 6  Epoch: 54  Training loss = 1.7891  Validation loss = 1.4252  \n",
      "\n",
      "Fold: 6  Epoch: 55  Training loss = 1.7886  Validation loss = 1.4241  \n",
      "\n",
      "Fold: 6  Epoch: 56  Training loss = 1.7875  Validation loss = 1.4195  \n",
      "\n",
      "Fold: 6  Epoch: 57  Training loss = 1.7864  Validation loss = 1.4143  \n",
      "\n",
      "Fold: 6  Epoch: 58  Training loss = 1.7855  Validation loss = 1.4089  \n",
      "\n",
      "Fold: 6  Epoch: 59  Training loss = 1.7845  Validation loss = 1.4068  \n",
      "\n",
      "Fold: 6  Epoch: 60  Training loss = 1.7839  Validation loss = 1.4067  \n",
      "\n",
      "Fold: 6  Epoch: 61  Training loss = 1.7819  Validation loss = 1.3956  \n",
      "\n",
      "Fold: 6  Epoch: 62  Training loss = 1.7814  Validation loss = 1.3954  \n",
      "\n",
      "Fold: 6  Epoch: 63  Training loss = 1.7807  Validation loss = 1.3917  \n",
      "\n",
      "Fold: 6  Epoch: 64  Training loss = 1.7792  Validation loss = 1.3808  \n",
      "\n",
      "Fold: 6  Epoch: 65  Training loss = 1.7787  Validation loss = 1.3824  \n",
      "\n",
      "Fold: 6  Epoch: 66  Training loss = 1.7779  Validation loss = 1.3793  \n",
      "\n",
      "Fold: 6  Epoch: 67  Training loss = 1.7772  Validation loss = 1.3771  \n",
      "\n",
      "Fold: 6  Epoch: 68  Training loss = 1.7764  Validation loss = 1.3747  \n",
      "\n",
      "Fold: 6  Epoch: 69  Training loss = 1.7750  Validation loss = 1.3692  \n",
      "\n",
      "Fold: 6  Epoch: 70  Training loss = 1.7737  Validation loss = 1.3629  \n",
      "\n",
      "Fold: 6  Epoch: 71  Training loss = 1.7723  Validation loss = 1.3552  \n",
      "\n",
      "Fold: 6  Epoch: 72  Training loss = 1.7710  Validation loss = 1.3476  \n",
      "\n",
      "Fold: 6  Epoch: 73  Training loss = 1.7700  Validation loss = 1.3435  \n",
      "\n",
      "Fold: 6  Epoch: 74  Training loss = 1.7690  Validation loss = 1.3414  \n",
      "\n",
      "Fold: 6  Epoch: 75  Training loss = 1.7681  Validation loss = 1.3366  \n",
      "\n",
      "Fold: 6  Epoch: 76  Training loss = 1.7674  Validation loss = 1.3357  \n",
      "\n",
      "Fold: 6  Epoch: 77  Training loss = 1.7668  Validation loss = 1.3380  \n",
      "\n",
      "Fold: 6  Epoch: 78  Training loss = 1.7657  Validation loss = 1.3311  \n",
      "\n",
      "Fold: 6  Epoch: 79  Training loss = 1.7650  Validation loss = 1.3311  \n",
      "\n",
      "Fold: 6  Epoch: 80  Training loss = 1.7642  Validation loss = 1.3310  \n",
      "\n",
      "Fold: 6  Epoch: 81  Training loss = 1.7632  Validation loss = 1.3280  \n",
      "\n",
      "Fold: 6  Epoch: 82  Training loss = 1.7624  Validation loss = 1.3256  \n",
      "\n",
      "Fold: 6  Epoch: 83  Training loss = 1.7620  Validation loss = 1.3293  \n",
      "\n",
      "Fold: 6  Epoch: 84  Training loss = 1.7614  Validation loss = 1.3311  \n",
      "\n",
      "Fold: 6  Epoch: 85  Training loss = 1.7608  Validation loss = 1.3284  \n",
      "\n",
      "Fold: 6  Epoch: 86  Training loss = 1.7600  Validation loss = 1.3264  \n",
      "\n",
      "Fold: 6  Epoch: 87  Training loss = 1.7592  Validation loss = 1.3254  \n",
      "\n",
      "Fold: 6  Epoch: 88  Training loss = 1.7581  Validation loss = 1.3201  \n",
      "\n",
      "Fold: 6  Epoch: 89  Training loss = 1.7578  Validation loss = 1.3230  \n",
      "\n",
      "Fold: 6  Epoch: 90  Training loss = 1.7570  Validation loss = 1.3202  \n",
      "\n",
      "Fold: 6  Epoch: 91  Training loss = 1.7561  Validation loss = 1.3198  \n",
      "\n",
      "Fold: 6  Epoch: 92  Training loss = 1.7554  Validation loss = 1.3170  \n",
      "\n",
      "Fold: 6  Epoch: 93  Training loss = 1.7545  Validation loss = 1.3151  \n",
      "\n",
      "Fold: 6  Epoch: 94  Training loss = 1.7532  Validation loss = 1.3081  \n",
      "\n",
      "Fold: 6  Epoch: 95  Training loss = 1.7527  Validation loss = 1.3082  \n",
      "\n",
      "Fold: 6  Epoch: 96  Training loss = 1.7520  Validation loss = 1.3105  \n",
      "\n",
      "Fold: 6  Epoch: 97  Training loss = 1.7512  Validation loss = 1.3074  \n",
      "\n",
      "Fold: 6  Epoch: 98  Training loss = 1.7502  Validation loss = 1.3017  \n",
      "\n",
      "Fold: 6  Epoch: 99  Training loss = 1.7499  Validation loss = 1.3031  \n",
      "\n",
      "Fold: 6  Epoch: 100  Training loss = 1.7489  Validation loss = 1.2994  \n",
      "\n",
      "Fold: 6  Epoch: 101  Training loss = 1.7478  Validation loss = 1.2947  \n",
      "\n",
      "Fold: 6  Epoch: 102  Training loss = 1.7468  Validation loss = 1.2911  \n",
      "\n",
      "Fold: 6  Epoch: 103  Training loss = 1.7460  Validation loss = 1.2891  \n",
      "\n",
      "Fold: 6  Epoch: 104  Training loss = 1.7454  Validation loss = 1.2918  \n",
      "\n",
      "Fold: 6  Epoch: 105  Training loss = 1.7445  Validation loss = 1.2889  \n",
      "\n",
      "Fold: 6  Epoch: 106  Training loss = 1.7438  Validation loss = 1.2877  \n",
      "\n",
      "Fold: 6  Epoch: 107  Training loss = 1.7431  Validation loss = 1.2860  \n",
      "\n",
      "Fold: 6  Epoch: 108  Training loss = 1.7426  Validation loss = 1.2891  \n",
      "\n",
      "Fold: 6  Epoch: 109  Training loss = 1.7417  Validation loss = 1.2853  \n",
      "\n",
      "Fold: 6  Epoch: 110  Training loss = 1.7412  Validation loss = 1.2884  \n",
      "\n",
      "Fold: 6  Epoch: 111  Training loss = 1.7404  Validation loss = 1.2859  \n",
      "\n",
      "Fold: 6  Epoch: 112  Training loss = 1.7402  Validation loss = 1.2900  \n",
      "\n",
      "Fold: 6  Epoch: 113  Training loss = 1.7390  Validation loss = 1.2853  \n",
      "\n",
      "Fold: 6  Epoch: 114  Training loss = 1.7385  Validation loss = 1.2849  \n",
      "\n",
      "Fold: 6  Epoch: 115  Training loss = 1.7377  Validation loss = 1.2819  \n",
      "\n",
      "Fold: 6  Epoch: 116  Training loss = 1.7369  Validation loss = 1.2785  \n",
      "\n",
      "Fold: 6  Epoch: 117  Training loss = 1.7363  Validation loss = 1.2796  \n",
      "\n",
      "Fold: 6  Epoch: 118  Training loss = 1.7355  Validation loss = 1.2769  \n",
      "\n",
      "Fold: 6  Epoch: 119  Training loss = 1.7347  Validation loss = 1.2747  \n",
      "\n",
      "Fold: 6  Epoch: 120  Training loss = 1.7338  Validation loss = 1.2738  \n",
      "\n",
      "Fold: 6  Epoch: 121  Training loss = 1.7332  Validation loss = 1.2741  \n",
      "\n",
      "Fold: 6  Epoch: 122  Training loss = 1.7327  Validation loss = 1.2743  \n",
      "\n",
      "Fold: 6  Epoch: 123  Training loss = 1.7318  Validation loss = 1.2698  \n",
      "\n",
      "Fold: 6  Epoch: 124  Training loss = 1.7310  Validation loss = 1.2662  \n",
      "\n",
      "Fold: 6  Epoch: 125  Training loss = 1.7303  Validation loss = 1.2636  \n",
      "\n",
      "Fold: 6  Epoch: 126  Training loss = 1.7298  Validation loss = 1.2645  \n",
      "\n",
      "Fold: 6  Epoch: 127  Training loss = 1.7293  Validation loss = 1.2653  \n",
      "\n",
      "Fold: 6  Epoch: 128  Training loss = 1.7283  Validation loss = 1.2600  \n",
      "\n",
      "Fold: 6  Epoch: 129  Training loss = 1.7276  Validation loss = 1.2600  \n",
      "\n",
      "Fold: 6  Epoch: 130  Training loss = 1.7261  Validation loss = 1.2504  \n",
      "\n",
      "Fold: 6  Epoch: 131  Training loss = 1.7249  Validation loss = 1.2437  \n",
      "\n",
      "Fold: 6  Epoch: 132  Training loss = 1.7241  Validation loss = 1.2406  \n",
      "\n",
      "Fold: 6  Epoch: 133  Training loss = 1.7238  Validation loss = 1.2429  \n",
      "\n",
      "Fold: 6  Epoch: 134  Training loss = 1.7230  Validation loss = 1.2386  \n",
      "\n",
      "Fold: 6  Epoch: 135  Training loss = 1.7217  Validation loss = 1.2317  \n",
      "\n",
      "Fold: 6  Epoch: 136  Training loss = 1.7210  Validation loss = 1.2295  \n",
      "\n",
      "Fold: 6  Epoch: 137  Training loss = 1.7203  Validation loss = 1.2264  \n",
      "\n",
      "Fold: 6  Epoch: 138  Training loss = 1.7196  Validation loss = 1.2260  \n",
      "\n",
      "Fold: 6  Epoch: 139  Training loss = 1.7187  Validation loss = 1.2218  \n",
      "\n",
      "Fold: 6  Epoch: 140  Training loss = 1.7177  Validation loss = 1.2157  \n",
      "\n",
      "Fold: 6  Epoch: 141  Training loss = 1.7170  Validation loss = 1.2156  \n",
      "\n",
      "Fold: 6  Epoch: 142  Training loss = 1.7166  Validation loss = 1.2155  \n",
      "\n",
      "Fold: 6  Epoch: 143  Training loss = 1.7155  Validation loss = 1.2056  \n",
      "\n",
      "Fold: 6  Epoch: 144  Training loss = 1.7150  Validation loss = 1.2093  \n",
      "\n",
      "Fold: 6  Epoch: 145  Training loss = 1.7146  Validation loss = 1.2111  \n",
      "\n",
      "Fold: 6  Epoch: 146  Training loss = 1.7139  Validation loss = 1.2125  \n",
      "\n",
      "Fold: 6  Epoch: 147  Training loss = 1.7129  Validation loss = 1.2089  \n",
      "\n",
      "Fold: 6  Epoch: 148  Training loss = 1.7122  Validation loss = 1.2070  \n",
      "\n",
      "Fold: 6  Epoch: 149  Training loss = 1.7113  Validation loss = 1.2040  \n",
      "\n",
      "Fold: 6  Epoch: 150  Training loss = 1.7104  Validation loss = 1.1977  \n",
      "\n",
      "Fold: 6  Epoch: 151  Training loss = 1.7092  Validation loss = 1.1892  \n",
      "\n",
      "Fold: 6  Epoch: 152  Training loss = 1.7086  Validation loss = 1.1881  \n",
      "\n",
      "Fold: 6  Epoch: 153  Training loss = 1.7080  Validation loss = 1.1895  \n",
      "\n",
      "Fold: 6  Epoch: 154  Training loss = 1.7071  Validation loss = 1.1856  \n",
      "\n",
      "Fold: 6  Epoch: 155  Training loss = 1.7061  Validation loss = 1.1790  \n",
      "\n",
      "Fold: 6  Epoch: 156  Training loss = 1.7053  Validation loss = 1.1753  \n",
      "\n",
      "Fold: 6  Epoch: 157  Training loss = 1.7046  Validation loss = 1.1745  \n",
      "\n",
      "Fold: 6  Epoch: 158  Training loss = 1.7038  Validation loss = 1.1684  \n",
      "\n",
      "Fold: 6  Epoch: 159  Training loss = 1.7030  Validation loss = 1.1680  \n",
      "\n",
      "Fold: 6  Epoch: 160  Training loss = 1.7022  Validation loss = 1.1626  \n",
      "\n",
      "Fold: 6  Epoch: 161  Training loss = 1.7012  Validation loss = 1.1579  \n",
      "\n",
      "Fold: 6  Epoch: 162  Training loss = 1.7002  Validation loss = 1.1544  \n",
      "\n",
      "Fold: 6  Epoch: 163  Training loss = 1.6994  Validation loss = 1.1500  \n",
      "\n",
      "Fold: 6  Epoch: 164  Training loss = 1.6987  Validation loss = 1.1493  \n",
      "\n",
      "Fold: 6  Epoch: 165  Training loss = 1.6981  Validation loss = 1.1508  \n",
      "\n",
      "Fold: 6  Epoch: 166  Training loss = 1.6972  Validation loss = 1.1450  \n",
      "\n",
      "Fold: 6  Epoch: 167  Training loss = 1.6963  Validation loss = 1.1438  \n",
      "\n",
      "Fold: 6  Epoch: 168  Training loss = 1.6954  Validation loss = 1.1427  \n",
      "\n",
      "Fold: 6  Epoch: 169  Training loss = 1.6944  Validation loss = 1.1436  \n",
      "\n",
      "Fold: 6  Epoch: 170  Training loss = 1.6938  Validation loss = 1.1435  \n",
      "\n",
      "Fold: 6  Epoch: 171  Training loss = 1.6931  Validation loss = 1.1434  \n",
      "\n",
      "Fold: 6  Epoch: 172  Training loss = 1.6923  Validation loss = 1.1409  \n",
      "\n",
      "Fold: 6  Epoch: 173  Training loss = 1.6918  Validation loss = 1.1443  \n",
      "\n",
      "Fold: 6  Epoch: 174  Training loss = 1.6915  Validation loss = 1.1476  \n",
      "\n",
      "Fold: 6  Epoch: 175  Training loss = 1.6906  Validation loss = 1.1416  \n",
      "\n",
      "Fold: 6  Epoch: 176  Training loss = 1.6900  Validation loss = 1.1412  \n",
      "\n",
      "Fold: 6  Epoch: 177  Training loss = 1.6888  Validation loss = 1.1340  \n",
      "\n",
      "Fold: 6  Epoch: 178  Training loss = 1.6882  Validation loss = 1.1363  \n",
      "\n",
      "Fold: 6  Epoch: 179  Training loss = 1.6872  Validation loss = 1.1300  \n",
      "\n",
      "Fold: 6  Epoch: 180  Training loss = 1.6865  Validation loss = 1.1325  \n",
      "\n",
      "Fold: 6  Epoch: 181  Training loss = 1.6860  Validation loss = 1.1366  \n",
      "\n",
      "Fold: 6  Epoch: 182  Training loss = 1.6856  Validation loss = 1.1417  \n",
      "\n",
      "Fold: 6  Epoch: 183  Training loss = 1.6849  Validation loss = 1.1426  \n",
      "\n",
      "Fold: 6  Epoch: 184  Training loss = 1.6838  Validation loss = 1.1373  \n",
      "\n",
      "Fold: 6  Epoch: 185  Training loss = 1.6830  Validation loss = 1.1311  \n",
      "\n",
      "Fold: 6  Epoch: 186  Training loss = 1.6823  Validation loss = 1.1300  \n",
      "\n",
      "Fold: 6  Epoch: 187  Training loss = 1.6818  Validation loss = 1.1298  \n",
      "\n",
      "Fold: 6  Epoch: 188  Training loss = 1.6806  Validation loss = 1.1233  \n",
      "\n",
      "Fold: 6  Epoch: 189  Training loss = 1.6796  Validation loss = 1.1202  \n",
      "\n",
      "Fold: 6  Epoch: 190  Training loss = 1.6787  Validation loss = 1.1215  \n",
      "\n",
      "Fold: 6  Epoch: 191  Training loss = 1.6777  Validation loss = 1.1129  \n",
      "\n",
      "Fold: 6  Epoch: 192  Training loss = 1.6767  Validation loss = 1.1076  \n",
      "\n",
      "Fold: 6  Epoch: 193  Training loss = 1.6763  Validation loss = 1.1122  \n",
      "\n",
      "Fold: 6  Epoch: 194  Training loss = 1.6758  Validation loss = 1.1160  \n",
      "\n",
      "Fold: 6  Epoch: 195  Training loss = 1.6748  Validation loss = 1.1155  \n",
      "\n",
      "Fold: 6  Epoch: 196  Training loss = 1.6741  Validation loss = 1.1152  \n",
      "\n",
      "Fold: 6  Epoch: 197  Training loss = 1.6732  Validation loss = 1.1139  \n",
      "\n",
      "Fold: 6  Epoch: 198  Training loss = 1.6725  Validation loss = 1.1141  \n",
      "\n",
      "Fold: 6  Epoch: 199  Training loss = 1.6717  Validation loss = 1.1104  \n",
      "\n",
      "Fold: 6  Epoch: 200  Training loss = 1.6708  Validation loss = 1.1083  \n",
      "\n",
      "Fold: 6  Epoch: 201  Training loss = 1.6700  Validation loss = 1.1069  \n",
      "\n",
      "Fold: 6  Epoch: 202  Training loss = 1.6692  Validation loss = 1.1044  \n",
      "\n",
      "Fold: 6  Epoch: 203  Training loss = 1.6685  Validation loss = 1.1022  \n",
      "\n",
      "Fold: 6  Epoch: 204  Training loss = 1.6679  Validation loss = 1.1014  \n",
      "\n",
      "Fold: 6  Epoch: 205  Training loss = 1.6671  Validation loss = 1.1010  \n",
      "\n",
      "Fold: 6  Epoch: 206  Training loss = 1.6665  Validation loss = 1.1037  \n",
      "\n",
      "Fold: 6  Epoch: 207  Training loss = 1.6660  Validation loss = 1.1066  \n",
      "\n",
      "Fold: 6  Epoch: 208  Training loss = 1.6655  Validation loss = 1.1083  \n",
      "\n",
      "Fold: 6  Epoch: 209  Training loss = 1.6647  Validation loss = 1.1060  \n",
      "\n",
      "Fold: 6  Epoch: 210  Training loss = 1.6639  Validation loss = 1.1045  \n",
      "\n",
      "Fold: 6  Epoch: 211  Training loss = 1.6625  Validation loss = 1.0969  \n",
      "\n",
      "Fold: 6  Epoch: 212  Training loss = 1.6621  Validation loss = 1.1012  \n",
      "\n",
      "Fold: 6  Epoch: 213  Training loss = 1.6617  Validation loss = 1.1032  \n",
      "\n",
      "Fold: 6  Epoch: 214  Training loss = 1.6612  Validation loss = 1.1044  \n",
      "\n",
      "Fold: 6  Epoch: 215  Training loss = 1.6604  Validation loss = 1.1047  \n",
      "\n",
      "Fold: 6  Epoch: 216  Training loss = 1.6599  Validation loss = 1.1063  \n",
      "\n",
      "Fold: 6  Epoch: 217  Training loss = 1.6591  Validation loss = 1.1037  \n",
      "\n",
      "Fold: 6  Epoch: 218  Training loss = 1.6581  Validation loss = 1.1006  \n",
      "\n",
      "Fold: 6  Epoch: 219  Training loss = 1.6575  Validation loss = 1.1022  \n",
      "\n",
      "Fold: 6  Epoch: 220  Training loss = 1.6570  Validation loss = 1.1046  \n",
      "\n",
      "Fold: 6  Epoch: 221  Training loss = 1.6563  Validation loss = 1.1014  \n",
      "\n",
      "Fold: 6  Epoch: 222  Training loss = 1.6554  Validation loss = 1.0983  \n",
      "\n",
      "Fold: 6  Epoch: 223  Training loss = 1.6545  Validation loss = 1.0941  \n",
      "\n",
      "Fold: 6  Epoch: 224  Training loss = 1.6539  Validation loss = 1.0950  \n",
      "\n",
      "Fold: 6  Epoch: 225  Training loss = 1.6529  Validation loss = 1.0917  \n",
      "\n",
      "Fold: 6  Epoch: 226  Training loss = 1.6523  Validation loss = 1.0891  \n",
      "\n",
      "Fold: 6  Epoch: 227  Training loss = 1.6513  Validation loss = 1.0846  \n",
      "\n",
      "Fold: 6  Epoch: 228  Training loss = 1.6505  Validation loss = 1.0818  \n",
      "\n",
      "Fold: 6  Epoch: 229  Training loss = 1.6493  Validation loss = 1.0758  \n",
      "\n",
      "Fold: 6  Epoch: 230  Training loss = 1.6485  Validation loss = 1.0710  \n",
      "\n",
      "Fold: 6  Epoch: 231  Training loss = 1.6476  Validation loss = 1.0682  \n",
      "\n",
      "Fold: 6  Epoch: 232  Training loss = 1.6467  Validation loss = 1.0672  \n",
      "\n",
      "Fold: 6  Epoch: 233  Training loss = 1.6461  Validation loss = 1.0674  \n",
      "\n",
      "Fold: 6  Epoch: 234  Training loss = 1.6447  Validation loss = 1.0603  \n",
      "\n",
      "Fold: 6  Epoch: 235  Training loss = 1.6441  Validation loss = 1.0593  \n",
      "\n",
      "Fold: 6  Epoch: 236  Training loss = 1.6434  Validation loss = 1.0606  \n",
      "\n",
      "Fold: 6  Epoch: 237  Training loss = 1.6426  Validation loss = 1.0555  \n",
      "\n",
      "Fold: 6  Epoch: 238  Training loss = 1.6417  Validation loss = 1.0531  \n",
      "\n",
      "Fold: 6  Epoch: 239  Training loss = 1.6406  Validation loss = 1.0479  \n",
      "\n",
      "Fold: 6  Epoch: 240  Training loss = 1.6394  Validation loss = 1.0386  \n",
      "\n",
      "Fold: 6  Epoch: 241  Training loss = 1.6388  Validation loss = 1.0450  \n",
      "\n",
      "Fold: 6  Epoch: 242  Training loss = 1.6380  Validation loss = 1.0459  \n",
      "\n",
      "Fold: 6  Epoch: 243  Training loss = 1.6372  Validation loss = 1.0412  \n",
      "\n",
      "Fold: 6  Epoch: 244  Training loss = 1.6364  Validation loss = 1.0393  \n",
      "\n",
      "Fold: 6  Epoch: 245  Training loss = 1.6356  Validation loss = 1.0356  \n",
      "\n",
      "Fold: 6  Epoch: 246  Training loss = 1.6348  Validation loss = 1.0341  \n",
      "\n",
      "Fold: 6  Epoch: 247  Training loss = 1.6337  Validation loss = 1.0352  \n",
      "\n",
      "Fold: 6  Epoch: 248  Training loss = 1.6327  Validation loss = 1.0307  \n",
      "\n",
      "Fold: 6  Epoch: 249  Training loss = 1.6319  Validation loss = 1.0317  \n",
      "\n",
      "Fold: 6  Epoch: 250  Training loss = 1.6311  Validation loss = 1.0283  \n",
      "\n",
      "Fold: 6  Epoch: 251  Training loss = 1.6303  Validation loss = 1.0275  \n",
      "\n",
      "Fold: 6  Epoch: 252  Training loss = 1.6294  Validation loss = 1.0219  \n",
      "\n",
      "Fold: 6  Epoch: 253  Training loss = 1.6287  Validation loss = 1.0236  \n",
      "\n",
      "Fold: 6  Epoch: 254  Training loss = 1.6281  Validation loss = 1.0253  \n",
      "\n",
      "Fold: 6  Epoch: 255  Training loss = 1.6277  Validation loss = 1.0269  \n",
      "\n",
      "Fold: 6  Epoch: 256  Training loss = 1.6269  Validation loss = 1.0243  \n",
      "\n",
      "Fold: 6  Epoch: 257  Training loss = 1.6260  Validation loss = 1.0221  \n",
      "\n",
      "Fold: 6  Epoch: 258  Training loss = 1.6253  Validation loss = 1.0205  \n",
      "\n",
      "Fold: 6  Epoch: 259  Training loss = 1.6243  Validation loss = 1.0185  \n",
      "\n",
      "Fold: 6  Epoch: 260  Training loss = 1.6238  Validation loss = 1.0243  \n",
      "\n",
      "Fold: 6  Epoch: 261  Training loss = 1.6227  Validation loss = 1.0210  \n",
      "\n",
      "Fold: 6  Epoch: 262  Training loss = 1.6217  Validation loss = 1.0211  \n",
      "\n",
      "Fold: 6  Epoch: 263  Training loss = 1.6210  Validation loss = 1.0187  \n",
      "\n",
      "Fold: 6  Epoch: 264  Training loss = 1.6201  Validation loss = 1.0176  \n",
      "\n",
      "Fold: 6  Epoch: 265  Training loss = 1.6195  Validation loss = 1.0168  \n",
      "\n",
      "Fold: 6  Epoch: 266  Training loss = 1.6186  Validation loss = 1.0173  \n",
      "\n",
      "Fold: 6  Epoch: 267  Training loss = 1.6179  Validation loss = 1.0122  \n",
      "\n",
      "Fold: 6  Epoch: 268  Training loss = 1.6173  Validation loss = 1.0097  \n",
      "\n",
      "Fold: 6  Epoch: 269  Training loss = 1.6168  Validation loss = 1.0090  \n",
      "\n",
      "Fold: 6  Epoch: 270  Training loss = 1.6162  Validation loss = 1.0084  \n",
      "\n",
      "Fold: 6  Epoch: 271  Training loss = 1.6149  Validation loss = 1.0067  \n",
      "\n",
      "Fold: 6  Epoch: 272  Training loss = 1.6139  Validation loss = 1.0070  \n",
      "\n",
      "Fold: 6  Epoch: 273  Training loss = 1.6132  Validation loss = 1.0100  \n",
      "\n",
      "Fold: 6  Epoch: 274  Training loss = 1.6125  Validation loss = 1.0096  \n",
      "\n",
      "Fold: 6  Epoch: 275  Training loss = 1.6116  Validation loss = 1.0098  \n",
      "\n",
      "Fold: 6  Epoch: 276  Training loss = 1.6107  Validation loss = 1.0064  \n",
      "\n",
      "Fold: 6  Epoch: 277  Training loss = 1.6098  Validation loss = 1.0061  \n",
      "\n",
      "Fold: 6  Epoch: 278  Training loss = 1.6094  Validation loss = 1.0087  \n",
      "\n",
      "Fold: 6  Epoch: 279  Training loss = 1.6084  Validation loss = 1.0093  \n",
      "\n",
      "Fold: 6  Epoch: 280  Training loss = 1.6070  Validation loss = 1.0015  \n",
      "\n",
      "Fold: 6  Epoch: 281  Training loss = 1.6056  Validation loss = 0.9967  \n",
      "\n",
      "Fold: 6  Epoch: 282  Training loss = 1.6046  Validation loss = 0.9909  \n",
      "\n",
      "Fold: 6  Epoch: 283  Training loss = 1.6038  Validation loss = 0.9969  \n",
      "\n",
      "Fold: 6  Epoch: 284  Training loss = 1.6031  Validation loss = 0.9959  \n",
      "\n",
      "Fold: 6  Epoch: 285  Training loss = 1.6023  Validation loss = 0.9983  \n",
      "\n",
      "Fold: 6  Epoch: 286  Training loss = 1.6020  Validation loss = 1.0002  \n",
      "\n",
      "Fold: 6  Epoch: 287  Training loss = 1.6013  Validation loss = 1.0002  \n",
      "\n",
      "Fold: 6  Epoch: 288  Training loss = 1.6003  Validation loss = 0.9961  \n",
      "\n",
      "Fold: 6  Epoch: 289  Training loss = 1.5989  Validation loss = 0.9889  \n",
      "\n",
      "Fold: 6  Epoch: 290  Training loss = 1.5980  Validation loss = 0.9895  \n",
      "\n",
      "Fold: 6  Epoch: 291  Training loss = 1.5971  Validation loss = 0.9900  \n",
      "\n",
      "Fold: 6  Epoch: 292  Training loss = 1.5967  Validation loss = 0.9965  \n",
      "\n",
      "Fold: 6  Epoch: 293  Training loss = 1.5957  Validation loss = 0.9932  \n",
      "\n",
      "Fold: 6  Epoch: 294  Training loss = 1.5946  Validation loss = 0.9885  \n",
      "\n",
      "Fold: 6  Epoch: 295  Training loss = 1.5938  Validation loss = 0.9865  \n",
      "\n",
      "Fold: 6  Epoch: 296  Training loss = 1.5928  Validation loss = 0.9840  \n",
      "\n",
      "Fold: 6  Epoch: 297  Training loss = 1.5914  Validation loss = 0.9746  \n",
      "\n",
      "Fold: 6  Epoch: 298  Training loss = 1.5907  Validation loss = 0.9743  \n",
      "\n",
      "Fold: 6  Epoch: 299  Training loss = 1.5900  Validation loss = 0.9758  \n",
      "\n",
      "Fold: 6  Epoch: 300  Training loss = 1.5891  Validation loss = 0.9736  \n",
      "\n",
      "Fold: 6  Epoch: 301  Training loss = 1.5884  Validation loss = 0.9732  \n",
      "\n",
      "Fold: 6  Epoch: 302  Training loss = 1.5874  Validation loss = 0.9722  \n",
      "\n",
      "Fold: 6  Epoch: 303  Training loss = 1.5866  Validation loss = 0.9716  \n",
      "\n",
      "Fold: 6  Epoch: 304  Training loss = 1.5854  Validation loss = 0.9674  \n",
      "\n",
      "Fold: 6  Epoch: 305  Training loss = 1.5845  Validation loss = 0.9659  \n",
      "\n",
      "Fold: 6  Epoch: 306  Training loss = 1.5836  Validation loss = 0.9661  \n",
      "\n",
      "Fold: 6  Epoch: 307  Training loss = 1.5830  Validation loss = 0.9676  \n",
      "\n",
      "Fold: 6  Epoch: 308  Training loss = 1.5824  Validation loss = 0.9732  \n",
      "\n",
      "Fold: 6  Epoch: 309  Training loss = 1.5817  Validation loss = 0.9725  \n",
      "\n",
      "Fold: 6  Epoch: 310  Training loss = 1.5808  Validation loss = 0.9702  \n",
      "\n",
      "Fold: 6  Epoch: 311  Training loss = 1.5798  Validation loss = 0.9694  \n",
      "\n",
      "Fold: 6  Epoch: 312  Training loss = 1.5788  Validation loss = 0.9661  \n",
      "\n",
      "Fold: 6  Epoch: 313  Training loss = 1.5778  Validation loss = 0.9646  \n",
      "\n",
      "Fold: 6  Epoch: 314  Training loss = 1.5766  Validation loss = 0.9643  \n",
      "\n",
      "Fold: 6  Epoch: 315  Training loss = 1.5755  Validation loss = 0.9621  \n",
      "\n",
      "Fold: 6  Epoch: 316  Training loss = 1.5747  Validation loss = 0.9597  \n",
      "\n",
      "Fold: 6  Epoch: 317  Training loss = 1.5740  Validation loss = 0.9591  \n",
      "\n",
      "Fold: 6  Epoch: 318  Training loss = 1.5727  Validation loss = 0.9526  \n",
      "\n",
      "Fold: 6  Epoch: 319  Training loss = 1.5716  Validation loss = 0.9510  \n",
      "\n",
      "Fold: 6  Epoch: 320  Training loss = 1.5708  Validation loss = 0.9465  \n",
      "\n",
      "Fold: 6  Epoch: 321  Training loss = 1.5699  Validation loss = 0.9486  \n",
      "\n",
      "Fold: 6  Epoch: 322  Training loss = 1.5693  Validation loss = 0.9475  \n",
      "\n",
      "Fold: 6  Epoch: 323  Training loss = 1.5681  Validation loss = 0.9466  \n",
      "\n",
      "Fold: 6  Epoch: 324  Training loss = 1.5671  Validation loss = 0.9449  \n",
      "\n",
      "Fold: 6  Epoch: 325  Training loss = 1.5657  Validation loss = 0.9396  \n",
      "\n",
      "Fold: 6  Epoch: 326  Training loss = 1.5648  Validation loss = 0.9365  \n",
      "\n",
      "Fold: 6  Epoch: 327  Training loss = 1.5638  Validation loss = 0.9362  \n",
      "\n",
      "Fold: 6  Epoch: 328  Training loss = 1.5628  Validation loss = 0.9374  \n",
      "\n",
      "Fold: 6  Epoch: 329  Training loss = 1.5617  Validation loss = 0.9360  \n",
      "\n",
      "Fold: 6  Epoch: 330  Training loss = 1.5607  Validation loss = 0.9273  \n",
      "\n",
      "Fold: 6  Epoch: 331  Training loss = 1.5597  Validation loss = 0.9300  \n",
      "\n",
      "Fold: 6  Epoch: 332  Training loss = 1.5590  Validation loss = 0.9337  \n",
      "\n",
      "Fold: 6  Epoch: 333  Training loss = 1.5580  Validation loss = 0.9311  \n",
      "\n",
      "Fold: 6  Epoch: 334  Training loss = 1.5573  Validation loss = 0.9323  \n",
      "\n",
      "Fold: 6  Epoch: 335  Training loss = 1.5563  Validation loss = 0.9316  \n",
      "\n",
      "Fold: 6  Epoch: 336  Training loss = 1.5555  Validation loss = 0.9300  \n",
      "\n",
      "Fold: 6  Epoch: 337  Training loss = 1.5547  Validation loss = 0.9291  \n",
      "\n",
      "Fold: 6  Epoch: 338  Training loss = 1.5537  Validation loss = 0.9295  \n",
      "\n",
      "Fold: 6  Epoch: 339  Training loss = 1.5529  Validation loss = 0.9353  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 330  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 1.5218  Validation loss = 1.2898  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 1.5210  Validation loss = 1.2982  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 1.5192  Validation loss = 1.2801  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.5176  Validation loss = 1.2797  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 1.5164  Validation loss = 1.2724  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 1.5149  Validation loss = 1.2567  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 1.5138  Validation loss = 1.2476  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 1.5119  Validation loss = 1.2226  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 1.5110  Validation loss = 1.2344  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 1.5099  Validation loss = 1.2321  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 1.5086  Validation loss = 1.2312  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 1.5077  Validation loss = 1.2325  \n",
      "\n",
      "Fold: 7  Epoch: 13  Training loss = 1.5070  Validation loss = 1.2513  \n",
      "\n",
      "Fold: 7  Epoch: 14  Training loss = 1.5055  Validation loss = 1.2285  \n",
      "\n",
      "Fold: 7  Epoch: 15  Training loss = 1.5045  Validation loss = 1.2436  \n",
      "\n",
      "Fold: 7  Epoch: 16  Training loss = 1.5036  Validation loss = 1.2452  \n",
      "\n",
      "Fold: 7  Epoch: 17  Training loss = 1.5022  Validation loss = 1.2195  \n",
      "\n",
      "Fold: 7  Epoch: 18  Training loss = 1.5010  Validation loss = 1.2199  \n",
      "\n",
      "Fold: 7  Epoch: 19  Training loss = 1.5000  Validation loss = 1.2234  \n",
      "\n",
      "Fold: 7  Epoch: 20  Training loss = 1.4988  Validation loss = 1.2184  \n",
      "\n",
      "Fold: 7  Epoch: 21  Training loss = 1.4981  Validation loss = 1.2273  \n",
      "\n",
      "Fold: 7  Epoch: 22  Training loss = 1.4974  Validation loss = 1.2574  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 20  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 1.4677  Validation loss = 5.8587  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 1.4669  Validation loss = 5.8607  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 1.4643  Validation loss = 5.8568  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 1.4612  Validation loss = 5.8539  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 1.4605  Validation loss = 5.8597  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 1.4595  Validation loss = 5.8624  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 1.4580  Validation loss = 5.8602  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 1.4563  Validation loss = 5.8563  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 1.4554  Validation loss = 5.8637  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 1.4538  Validation loss = 5.8550  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 1.4518  Validation loss = 5.8498  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 1.4504  Validation loss = 5.8390  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 1.4500  Validation loss = 5.8477  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 1.4470  Validation loss = 5.8435  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 1.4455  Validation loss = 5.8370  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 1.4452  Validation loss = 5.8441  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 1.4449  Validation loss = 5.8535  \n",
      "\n",
      "Fold: 8  Epoch: 18  Training loss = 1.4419  Validation loss = 5.8330  \n",
      "\n",
      "Fold: 8  Epoch: 19  Training loss = 1.4411  Validation loss = 5.8364  \n",
      "\n",
      "Fold: 8  Epoch: 20  Training loss = 1.4398  Validation loss = 5.8319  \n",
      "\n",
      "Fold: 8  Epoch: 21  Training loss = 1.4376  Validation loss = 5.8348  \n",
      "\n",
      "Fold: 8  Epoch: 22  Training loss = 1.4368  Validation loss = 5.8293  \n",
      "\n",
      "Fold: 8  Epoch: 23  Training loss = 1.4352  Validation loss = 5.8248  \n",
      "\n",
      "Fold: 8  Epoch: 24  Training loss = 1.4345  Validation loss = 5.8306  \n",
      "\n",
      "Fold: 8  Epoch: 25  Training loss = 1.4330  Validation loss = 5.8242  \n",
      "\n",
      "Fold: 8  Epoch: 26  Training loss = 1.4321  Validation loss = 5.8152  \n",
      "\n",
      "Fold: 8  Epoch: 27  Training loss = 1.4307  Validation loss = 5.8070  \n",
      "\n",
      "Fold: 8  Epoch: 28  Training loss = 1.4291  Validation loss = 5.8017  \n",
      "\n",
      "Fold: 8  Epoch: 29  Training loss = 1.4283  Validation loss = 5.8111  \n",
      "\n",
      "Fold: 8  Epoch: 30  Training loss = 1.4271  Validation loss = 5.8093  \n",
      "\n",
      "Fold: 8  Epoch: 31  Training loss = 1.4263  Validation loss = 5.7994  \n",
      "\n",
      "Fold: 8  Epoch: 32  Training loss = 1.4256  Validation loss = 5.8079  \n",
      "\n",
      "Fold: 8  Epoch: 33  Training loss = 1.4247  Validation loss = 5.8161  \n",
      "\n",
      "Fold: 8  Epoch: 34  Training loss = 1.4238  Validation loss = 5.8205  \n",
      "\n",
      "Fold: 8  Epoch: 35  Training loss = 1.4227  Validation loss = 5.8177  \n",
      "\n",
      "Fold: 8  Epoch: 36  Training loss = 1.4216  Validation loss = 5.8223  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 31  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 1.9948  Validation loss = 8.9364  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 1.9924  Validation loss = 8.9251  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 1.9866  Validation loss = 8.8947  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 1.9867  Validation loss = 8.9020  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 1.9861  Validation loss = 8.9028  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 1.9841  Validation loss = 8.8943  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 1.9800  Validation loss = 8.8731  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 1.9780  Validation loss = 8.8670  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 1.9774  Validation loss = 8.8673  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 1.9760  Validation loss = 8.8635  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 1.9750  Validation loss = 8.8566  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 1.9743  Validation loss = 8.8560  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 1.9722  Validation loss = 8.8427  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 1.9701  Validation loss = 8.8311  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 1.9687  Validation loss = 8.8240  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 1.9668  Validation loss = 8.8175  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 1.9653  Validation loss = 8.8167  \n",
      "\n",
      "Fold: 9  Epoch: 18  Training loss = 1.9638  Validation loss = 8.8157  \n",
      "\n",
      "Fold: 9  Epoch: 19  Training loss = 1.9613  Validation loss = 8.7944  \n",
      "\n",
      "Fold: 9  Epoch: 20  Training loss = 1.9598  Validation loss = 8.7880  \n",
      "\n",
      "Fold: 9  Epoch: 21  Training loss = 1.9589  Validation loss = 8.7865  \n",
      "\n",
      "Fold: 9  Epoch: 22  Training loss = 1.9558  Validation loss = 8.7705  \n",
      "\n",
      "Fold: 9  Epoch: 23  Training loss = 1.9538  Validation loss = 8.7614  \n",
      "\n",
      "Fold: 9  Epoch: 24  Training loss = 1.9519  Validation loss = 8.7476  \n",
      "\n",
      "Fold: 9  Epoch: 25  Training loss = 1.9510  Validation loss = 8.7548  \n",
      "\n",
      "Fold: 9  Epoch: 26  Training loss = 1.9503  Validation loss = 8.7557  \n",
      "\n",
      "Fold: 9  Epoch: 27  Training loss = 1.9492  Validation loss = 8.7495  \n",
      "\n",
      "Fold: 9  Epoch: 28  Training loss = 1.9490  Validation loss = 8.7520  \n",
      "\n",
      "Fold: 9  Epoch: 29  Training loss = 1.9479  Validation loss = 8.7479  \n",
      "\n",
      "Fold: 9  Epoch: 30  Training loss = 1.9464  Validation loss = 8.7361  \n",
      "\n",
      "Fold: 9  Epoch: 31  Training loss = 1.9432  Validation loss = 8.7113  \n",
      "\n",
      "Fold: 9  Epoch: 32  Training loss = 1.9426  Validation loss = 8.7081  \n",
      "\n",
      "Fold: 9  Epoch: 33  Training loss = 1.9412  Validation loss = 8.6998  \n",
      "\n",
      "Fold: 9  Epoch: 34  Training loss = 1.9394  Validation loss = 8.6922  \n",
      "\n",
      "Fold: 9  Epoch: 35  Training loss = 1.9388  Validation loss = 8.6876  \n",
      "\n",
      "Fold: 9  Epoch: 36  Training loss = 1.9381  Validation loss = 8.6899  \n",
      "\n",
      "Fold: 9  Epoch: 37  Training loss = 1.9375  Validation loss = 8.6967  \n",
      "\n",
      "Fold: 9  Epoch: 38  Training loss = 1.9364  Validation loss = 8.6889  \n",
      "\n",
      "Fold: 9  Epoch: 39  Training loss = 1.9341  Validation loss = 8.6656  \n",
      "\n",
      "Fold: 9  Epoch: 40  Training loss = 1.9327  Validation loss = 8.6596  \n",
      "\n",
      "Fold: 9  Epoch: 41  Training loss = 1.9316  Validation loss = 8.6528  \n",
      "\n",
      "Fold: 9  Epoch: 42  Training loss = 1.9305  Validation loss = 8.6431  \n",
      "\n",
      "Fold: 9  Epoch: 43  Training loss = 1.9290  Validation loss = 8.6337  \n",
      "\n",
      "Fold: 9  Epoch: 44  Training loss = 1.9279  Validation loss = 8.6200  \n",
      "\n",
      "Fold: 9  Epoch: 45  Training loss = 1.9268  Validation loss = 8.6139  \n",
      "\n",
      "Fold: 9  Epoch: 46  Training loss = 1.9257  Validation loss = 8.6107  \n",
      "\n",
      "Fold: 9  Epoch: 47  Training loss = 1.9252  Validation loss = 8.6116  \n",
      "\n",
      "Fold: 9  Epoch: 48  Training loss = 1.9238  Validation loss = 8.6149  \n",
      "\n",
      "Fold: 9  Epoch: 49  Training loss = 1.9232  Validation loss = 8.6191  \n",
      "\n",
      "Fold: 9  Epoch: 50  Training loss = 1.9227  Validation loss = 8.6252  \n",
      "\n",
      "Fold: 9  Epoch: 51  Training loss = 1.9215  Validation loss = 8.6197  \n",
      "\n",
      "Fold: 9  Epoch: 52  Training loss = 1.9206  Validation loss = 8.6130  \n",
      "\n",
      "Fold: 9  Epoch: 53  Training loss = 1.9195  Validation loss = 8.6091  \n",
      "\n",
      "Fold: 9  Epoch: 54  Training loss = 1.9182  Validation loss = 8.6007  \n",
      "\n",
      "Fold: 9  Epoch: 55  Training loss = 1.9168  Validation loss = 8.5831  \n",
      "\n",
      "Fold: 9  Epoch: 56  Training loss = 1.9158  Validation loss = 8.5799  \n",
      "\n",
      "Fold: 9  Epoch: 57  Training loss = 1.9147  Validation loss = 8.5810  \n",
      "\n",
      "Fold: 9  Epoch: 58  Training loss = 1.9136  Validation loss = 8.5785  \n",
      "\n",
      "Fold: 9  Epoch: 59  Training loss = 1.9126  Validation loss = 8.5738  \n",
      "\n",
      "Fold: 9  Epoch: 60  Training loss = 1.9115  Validation loss = 8.5673  \n",
      "\n",
      "Fold: 9  Epoch: 61  Training loss = 1.9108  Validation loss = 8.5625  \n",
      "\n",
      "Fold: 9  Epoch: 62  Training loss = 1.9102  Validation loss = 8.5625  \n",
      "\n",
      "Fold: 9  Epoch: 63  Training loss = 1.9092  Validation loss = 8.5581  \n",
      "\n",
      "Fold: 9  Epoch: 64  Training loss = 1.9079  Validation loss = 8.5391  \n",
      "\n",
      "Fold: 9  Epoch: 65  Training loss = 1.9069  Validation loss = 8.5323  \n",
      "\n",
      "Fold: 9  Epoch: 66  Training loss = 1.9061  Validation loss = 8.5277  \n",
      "\n",
      "Fold: 9  Epoch: 67  Training loss = 1.9052  Validation loss = 8.5272  \n",
      "\n",
      "Fold: 9  Epoch: 68  Training loss = 1.9046  Validation loss = 8.5327  \n",
      "\n",
      "Fold: 9  Epoch: 69  Training loss = 1.9037  Validation loss = 8.5336  \n",
      "\n",
      "Fold: 9  Epoch: 70  Training loss = 1.9027  Validation loss = 8.5326  \n",
      "\n",
      "Fold: 9  Epoch: 71  Training loss = 1.9017  Validation loss = 8.5199  \n",
      "\n",
      "Fold: 9  Epoch: 72  Training loss = 1.9013  Validation loss = 8.5347  \n",
      "\n",
      "Fold: 9  Epoch: 73  Training loss = 1.9005  Validation loss = 8.5337  \n",
      "\n",
      "Fold: 9  Epoch: 74  Training loss = 1.8993  Validation loss = 8.5309  \n",
      "\n",
      "Fold: 9  Epoch: 75  Training loss = 1.8989  Validation loss = 8.5342  \n",
      "\n",
      "Fold: 9  Epoch: 76  Training loss = 1.8983  Validation loss = 8.5330  \n",
      "\n",
      "Fold: 9  Epoch: 77  Training loss = 1.8968  Validation loss = 8.5171  \n",
      "\n",
      "Fold: 9  Epoch: 78  Training loss = 1.8960  Validation loss = 8.5115  \n",
      "\n",
      "Fold: 9  Epoch: 79  Training loss = 1.8954  Validation loss = 8.5137  \n",
      "\n",
      "Fold: 9  Epoch: 80  Training loss = 1.8946  Validation loss = 8.5192  \n",
      "\n",
      "Fold: 9  Epoch: 81  Training loss = 1.8944  Validation loss = 8.5248  \n",
      "\n",
      "Fold: 9  Epoch: 82  Training loss = 1.8939  Validation loss = 8.5284  \n",
      "\n",
      "Fold: 9  Epoch: 83  Training loss = 1.8933  Validation loss = 8.5303  \n",
      "\n",
      "Fold: 9  Epoch: 84  Training loss = 1.8926  Validation loss = 8.5300  \n",
      "\n",
      "Fold: 9  Epoch: 85  Training loss = 1.8915  Validation loss = 8.5212  \n",
      "\n",
      "Fold: 9  Epoch: 86  Training loss = 1.8900  Validation loss = 8.5081  \n",
      "\n",
      "Fold: 9  Epoch: 87  Training loss = 1.8895  Validation loss = 8.5097  \n",
      "\n",
      "Fold: 9  Epoch: 88  Training loss = 1.8883  Validation loss = 8.4994  \n",
      "\n",
      "Fold: 9  Epoch: 89  Training loss = 1.8883  Validation loss = 8.5059  \n",
      "\n",
      "Fold: 9  Epoch: 90  Training loss = 1.8868  Validation loss = 8.4910  \n",
      "\n",
      "Fold: 9  Epoch: 91  Training loss = 1.8855  Validation loss = 8.4747  \n",
      "\n",
      "Fold: 9  Epoch: 92  Training loss = 1.8843  Validation loss = 8.4643  \n",
      "\n",
      "Fold: 9  Epoch: 93  Training loss = 1.8839  Validation loss = 8.4602  \n",
      "\n",
      "Fold: 9  Epoch: 94  Training loss = 1.8833  Validation loss = 8.4603  \n",
      "\n",
      "Fold: 9  Epoch: 95  Training loss = 1.8822  Validation loss = 8.4518  \n",
      "\n",
      "Fold: 9  Epoch: 96  Training loss = 1.8815  Validation loss = 8.4556  \n",
      "\n",
      "Fold: 9  Epoch: 97  Training loss = 1.8808  Validation loss = 8.4552  \n",
      "\n",
      "Fold: 9  Epoch: 98  Training loss = 1.8799  Validation loss = 8.4395  \n",
      "\n",
      "Fold: 9  Epoch: 99  Training loss = 1.8794  Validation loss = 8.4527  \n",
      "\n",
      "Fold: 9  Epoch: 100  Training loss = 1.8788  Validation loss = 8.4509  \n",
      "\n",
      "Fold: 9  Epoch: 101  Training loss = 1.8781  Validation loss = 8.4490  \n",
      "\n",
      "Fold: 9  Epoch: 102  Training loss = 1.8776  Validation loss = 8.4366  \n",
      "\n",
      "Fold: 9  Epoch: 103  Training loss = 1.8765  Validation loss = 8.4219  \n",
      "\n",
      "Fold: 9  Epoch: 104  Training loss = 1.8756  Validation loss = 8.4272  \n",
      "\n",
      "Fold: 9  Epoch: 105  Training loss = 1.8749  Validation loss = 8.4209  \n",
      "\n",
      "Fold: 9  Epoch: 106  Training loss = 1.8742  Validation loss = 8.4059  \n",
      "\n",
      "Fold: 9  Epoch: 107  Training loss = 1.8738  Validation loss = 8.3931  \n",
      "\n",
      "Fold: 9  Epoch: 108  Training loss = 1.8739  Validation loss = 8.3714  \n",
      "\n",
      "Fold: 9  Epoch: 109  Training loss = 1.8725  Validation loss = 8.3693  \n",
      "\n",
      "Fold: 9  Epoch: 110  Training loss = 1.8709  Validation loss = 8.3725  \n",
      "\n",
      "Fold: 9  Epoch: 111  Training loss = 1.8688  Validation loss = 8.3876  \n",
      "\n",
      "Fold: 9  Epoch: 112  Training loss = 1.8681  Validation loss = 8.3971  \n",
      "\n",
      "Fold: 9  Epoch: 113  Training loss = 1.8671  Validation loss = 8.4046  \n",
      "\n",
      "Fold: 9  Epoch: 114  Training loss = 1.8667  Validation loss = 8.4125  \n",
      "\n",
      "Fold: 9  Epoch: 115  Training loss = 1.8658  Validation loss = 8.4111  \n",
      "\n",
      "Fold: 9  Epoch: 116  Training loss = 1.8651  Validation loss = 8.4108  \n",
      "\n",
      "Fold: 9  Epoch: 117  Training loss = 1.8639  Validation loss = 8.4001  \n",
      "\n",
      "Fold: 9  Epoch: 118  Training loss = 1.8633  Validation loss = 8.4060  \n",
      "\n",
      "Fold: 9  Epoch: 119  Training loss = 1.8629  Validation loss = 8.4109  \n",
      "\n",
      "Fold: 9  Epoch: 120  Training loss = 1.8627  Validation loss = 8.4173  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 109  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 2.7688  Validation loss = 5.7275  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 2.7566  Validation loss = 5.6748  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 2.7502  Validation loss = 5.6548  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 2.7510  Validation loss = 5.6535  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 2.7453  Validation loss = 5.6175  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 2.7388  Validation loss = 5.5984  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 2.7339  Validation loss = 5.5866  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 2.7258  Validation loss = 5.5340  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 2.7196  Validation loss = 5.5484  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 2.7121  Validation loss = 5.2025  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 2.7071  Validation loss = 5.1504  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 2.7020  Validation loss = 5.1334  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 2.6970  Validation loss = 5.1297  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 2.6916  Validation loss = 5.1014  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 2.6860  Validation loss = 5.0705  \n",
      "\n",
      "Fold: 10  Epoch: 16  Training loss = 2.6834  Validation loss = 5.0884  \n",
      "\n",
      "Fold: 10  Epoch: 17  Training loss = 2.6798  Validation loss = 5.0632  \n",
      "\n",
      "Fold: 10  Epoch: 18  Training loss = 2.6727  Validation loss = 5.0046  \n",
      "\n",
      "Fold: 10  Epoch: 19  Training loss = 2.6684  Validation loss = 4.9841  \n",
      "\n",
      "Fold: 10  Epoch: 20  Training loss = 2.6648  Validation loss = 4.9755  \n",
      "\n",
      "Fold: 10  Epoch: 21  Training loss = 2.6635  Validation loss = 4.9828  \n",
      "\n",
      "Fold: 10  Epoch: 22  Training loss = 2.6576  Validation loss = 4.9337  \n",
      "\n",
      "Fold: 10  Epoch: 23  Training loss = 2.6552  Validation loss = 4.9651  \n",
      "\n",
      "Fold: 10  Epoch: 24  Training loss = 2.6537  Validation loss = 4.9833  \n",
      "\n",
      "Fold: 10  Epoch: 25  Training loss = 2.6507  Validation loss = 4.9817  \n",
      "\n",
      "Fold: 10  Epoch: 26  Training loss = 2.6427  Validation loss = 4.8877  \n",
      "\n",
      "Fold: 10  Epoch: 27  Training loss = 2.6397  Validation loss = 4.8593  \n",
      "\n",
      "Fold: 10  Epoch: 28  Training loss = 2.6387  Validation loss = 4.8663  \n",
      "\n",
      "Fold: 10  Epoch: 29  Training loss = 2.6342  Validation loss = 4.8248  \n",
      "\n",
      "Fold: 10  Epoch: 30  Training loss = 2.6293  Validation loss = 4.7701  \n",
      "\n",
      "Fold: 10  Epoch: 31  Training loss = 2.6259  Validation loss = 4.7775  \n",
      "\n",
      "Fold: 10  Epoch: 32  Training loss = 2.6222  Validation loss = 4.8018  \n",
      "\n",
      "Fold: 10  Epoch: 33  Training loss = 2.6200  Validation loss = 4.8173  \n",
      "\n",
      "Fold: 10  Epoch: 34  Training loss = 2.6174  Validation loss = 4.8014  \n",
      "\n",
      "Fold: 10  Epoch: 35  Training loss = 2.6151  Validation loss = 4.8204  \n",
      "\n",
      "Fold: 10  Epoch: 36  Training loss = 2.6128  Validation loss = 4.8112  \n",
      "\n",
      "Fold: 10  Epoch: 37  Training loss = 2.6090  Validation loss = 4.7769  \n",
      "\n",
      "Fold: 10  Epoch: 38  Training loss = 2.6053  Validation loss = 4.7582  \n",
      "\n",
      "Fold: 10  Epoch: 39  Training loss = 2.6018  Validation loss = 4.7678  \n",
      "\n",
      "Fold: 10  Epoch: 40  Training loss = 2.5980  Validation loss = 4.7441  \n",
      "\n",
      "Fold: 10  Epoch: 41  Training loss = 2.5964  Validation loss = 4.7463  \n",
      "\n",
      "Fold: 10  Epoch: 42  Training loss = 2.5911  Validation loss = 4.6930  \n",
      "\n",
      "Fold: 10  Epoch: 43  Training loss = 2.5880  Validation loss = 4.7124  \n",
      "\n",
      "Fold: 10  Epoch: 44  Training loss = 2.5873  Validation loss = 4.7222  \n",
      "\n",
      "Fold: 10  Epoch: 45  Training loss = 2.5844  Validation loss = 4.7092  \n",
      "\n",
      "Fold: 10  Epoch: 46  Training loss = 2.5818  Validation loss = 4.7014  \n",
      "\n",
      "Fold: 10  Epoch: 47  Training loss = 2.5799  Validation loss = 4.7062  \n",
      "\n",
      "Fold: 10  Epoch: 48  Training loss = 2.5775  Validation loss = 4.7189  \n",
      "\n",
      "Fold: 10  Epoch: 49  Training loss = 2.5729  Validation loss = 4.6822  \n",
      "\n",
      "Fold: 10  Epoch: 50  Training loss = 2.5708  Validation loss = 4.7261  \n",
      "\n",
      "Fold: 10  Epoch: 51  Training loss = 2.5672  Validation loss = 4.7140  \n",
      "\n",
      "Fold: 10  Epoch: 52  Training loss = 2.5649  Validation loss = 4.7290  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 49  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 2.7413  Validation loss = 1.7514  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 2.7253  Validation loss = 1.7471  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 2.7071  Validation loss = 1.7220  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 2.6978  Validation loss = 1.6893  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 2.6681  Validation loss = 1.6811  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 2.6576  Validation loss = 1.6299  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 2.6447  Validation loss = 1.6110  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 2.6418  Validation loss = 1.5994  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 2.6354  Validation loss = 1.5938  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 2.6331  Validation loss = 1.5879  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 2.6236  Validation loss = 1.5710  \n",
      "\n",
      "Fold: 11  Epoch: 12  Training loss = 2.6214  Validation loss = 1.5702  \n",
      "\n",
      "Fold: 11  Epoch: 13  Training loss = 2.6149  Validation loss = 1.5680  \n",
      "\n",
      "Fold: 11  Epoch: 14  Training loss = 2.6124  Validation loss = 1.5668  \n",
      "\n",
      "Fold: 11  Epoch: 15  Training loss = 2.6082  Validation loss = 1.5596  \n",
      "\n",
      "Fold: 11  Epoch: 16  Training loss = 2.6041  Validation loss = 1.5578  \n",
      "\n",
      "Fold: 11  Epoch: 17  Training loss = 2.6002  Validation loss = 1.5590  \n",
      "\n",
      "Fold: 11  Epoch: 18  Training loss = 2.5992  Validation loss = 1.5669  \n",
      "\n",
      "Fold: 11  Epoch: 19  Training loss = 2.5941  Validation loss = 1.5622  \n",
      "\n",
      "Fold: 11  Epoch: 20  Training loss = 2.5911  Validation loss = 1.5607  \n",
      "\n",
      "Fold: 11  Epoch: 21  Training loss = 2.5881  Validation loss = 1.5484  \n",
      "\n",
      "Fold: 11  Epoch: 22  Training loss = 2.5853  Validation loss = 1.5473  \n",
      "\n",
      "Fold: 11  Epoch: 23  Training loss = 2.5823  Validation loss = 1.5583  \n",
      "\n",
      "Fold: 11  Epoch: 24  Training loss = 2.5805  Validation loss = 1.5617  \n",
      "\n",
      "Fold: 11  Epoch: 25  Training loss = 2.5732  Validation loss = 1.5677  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 22  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 2.5762  Validation loss = 1.8170  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 2.5720  Validation loss = 1.8409  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 2.5686  Validation loss = 1.8322  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 2.5658  Validation loss = 1.7829  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 2.5620  Validation loss = 1.8233  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 2.5595  Validation loss = 1.8185  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 2.5580  Validation loss = 1.8178  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 2.5575  Validation loss = 1.8558  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 2.5534  Validation loss = 1.8760  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 2.5518  Validation loss = 1.8880  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 2.5501  Validation loss = 1.9034  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 4  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 2.5278  Validation loss = 4.1106  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 2.5105  Validation loss = 4.0633  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 2.5057  Validation loss = 4.0406  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 2.4996  Validation loss = 4.0188  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 2.4976  Validation loss = 4.0185  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 2.4949  Validation loss = 4.0091  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 2.4912  Validation loss = 3.9990  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 2.4875  Validation loss = 3.9856  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 2.4833  Validation loss = 3.9799  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 2.4832  Validation loss = 3.9771  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 2.4780  Validation loss = 3.9583  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 2.4749  Validation loss = 3.9456  \n",
      "\n",
      "Fold: 13  Epoch: 13  Training loss = 2.4733  Validation loss = 3.9385  \n",
      "\n",
      "Fold: 13  Epoch: 14  Training loss = 2.4699  Validation loss = 3.9325  \n",
      "\n",
      "Fold: 13  Epoch: 15  Training loss = 2.4656  Validation loss = 3.9164  \n",
      "\n",
      "Fold: 13  Epoch: 16  Training loss = 2.4603  Validation loss = 3.8875  \n",
      "\n",
      "Fold: 13  Epoch: 17  Training loss = 2.4571  Validation loss = 3.8670  \n",
      "\n",
      "Fold: 13  Epoch: 18  Training loss = 2.4526  Validation loss = 3.8536  \n",
      "\n",
      "Fold: 13  Epoch: 19  Training loss = 2.4503  Validation loss = 3.8580  \n",
      "\n",
      "Fold: 13  Epoch: 20  Training loss = 2.4477  Validation loss = 3.8416  \n",
      "\n",
      "Fold: 13  Epoch: 21  Training loss = 2.4438  Validation loss = 3.8310  \n",
      "\n",
      "Fold: 13  Epoch: 22  Training loss = 2.4397  Validation loss = 3.8176  \n",
      "\n",
      "Fold: 13  Epoch: 23  Training loss = 2.4373  Validation loss = 3.8118  \n",
      "\n",
      "Fold: 13  Epoch: 24  Training loss = 2.4344  Validation loss = 3.8057  \n",
      "\n",
      "Fold: 13  Epoch: 25  Training loss = 2.4312  Validation loss = 3.7861  \n",
      "\n",
      "Fold: 13  Epoch: 26  Training loss = 2.4282  Validation loss = 3.7730  \n",
      "\n",
      "Fold: 13  Epoch: 27  Training loss = 2.4241  Validation loss = 3.7657  \n",
      "\n",
      "Fold: 13  Epoch: 28  Training loss = 2.4207  Validation loss = 3.7726  \n",
      "\n",
      "Fold: 13  Epoch: 29  Training loss = 2.4192  Validation loss = 3.7745  \n",
      "\n",
      "Fold: 13  Epoch: 30  Training loss = 2.4164  Validation loss = 3.7669  \n",
      "\n",
      "Fold: 13  Epoch: 31  Training loss = 2.4144  Validation loss = 3.7458  \n",
      "\n",
      "Fold: 13  Epoch: 32  Training loss = 2.4113  Validation loss = 3.7271  \n",
      "\n",
      "Fold: 13  Epoch: 33  Training loss = 2.4085  Validation loss = 3.7198  \n",
      "\n",
      "Fold: 13  Epoch: 34  Training loss = 2.4064  Validation loss = 3.7099  \n",
      "\n",
      "Fold: 13  Epoch: 35  Training loss = 2.4026  Validation loss = 3.6908  \n",
      "\n",
      "Fold: 13  Epoch: 36  Training loss = 2.3988  Validation loss = 3.6891  \n",
      "\n",
      "Fold: 13  Epoch: 37  Training loss = 2.3966  Validation loss = 3.6901  \n",
      "\n",
      "Fold: 13  Epoch: 38  Training loss = 2.3950  Validation loss = 3.6945  \n",
      "\n",
      "Fold: 13  Epoch: 39  Training loss = 2.3926  Validation loss = 3.6977  \n",
      "\n",
      "Fold: 13  Epoch: 40  Training loss = 2.3909  Validation loss = 3.6920  \n",
      "\n",
      "Fold: 13  Epoch: 41  Training loss = 2.3873  Validation loss = 3.6518  \n",
      "\n",
      "Fold: 13  Epoch: 42  Training loss = 2.3863  Validation loss = 3.6585  \n",
      "\n",
      "Fold: 13  Epoch: 43  Training loss = 2.3840  Validation loss = 3.6503  \n",
      "\n",
      "Fold: 13  Epoch: 44  Training loss = 2.3826  Validation loss = 3.6424  \n",
      "\n",
      "Fold: 13  Epoch: 45  Training loss = 2.3808  Validation loss = 3.6375  \n",
      "\n",
      "Fold: 13  Epoch: 46  Training loss = 2.3790  Validation loss = 3.6379  \n",
      "\n",
      "Fold: 13  Epoch: 47  Training loss = 2.3782  Validation loss = 3.6594  \n",
      "\n",
      "Fold: 13  Epoch: 48  Training loss = 2.3775  Validation loss = 3.7029  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 45  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 2.5203  Validation loss = 6.7379  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 2.5134  Validation loss = 6.7224  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 2.5106  Validation loss = 6.7143  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 2.5085  Validation loss = 6.7076  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 2.5086  Validation loss = 6.7069  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 2.5027  Validation loss = 6.6937  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 2.4949  Validation loss = 6.6584  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 2.4913  Validation loss = 6.6474  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 2.4856  Validation loss = 6.6266  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 2.4824  Validation loss = 6.6178  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 2.4809  Validation loss = 6.6169  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 2.4780  Validation loss = 6.6093  \n",
      "\n",
      "Fold: 14  Epoch: 13  Training loss = 2.4755  Validation loss = 6.5998  \n",
      "\n",
      "Fold: 14  Epoch: 14  Training loss = 2.4707  Validation loss = 6.5767  \n",
      "\n",
      "Fold: 14  Epoch: 15  Training loss = 2.4693  Validation loss = 6.5745  \n",
      "\n",
      "Fold: 14  Epoch: 16  Training loss = 2.4648  Validation loss = 6.5508  \n",
      "\n",
      "Fold: 14  Epoch: 17  Training loss = 2.4617  Validation loss = 6.5365  \n",
      "\n",
      "Fold: 14  Epoch: 18  Training loss = 2.4592  Validation loss = 6.5278  \n",
      "\n",
      "Fold: 14  Epoch: 19  Training loss = 2.4553  Validation loss = 6.5027  \n",
      "\n",
      "Fold: 14  Epoch: 20  Training loss = 2.4528  Validation loss = 6.4942  \n",
      "\n",
      "Fold: 14  Epoch: 21  Training loss = 2.4501  Validation loss = 6.4758  \n",
      "\n",
      "Fold: 14  Epoch: 22  Training loss = 2.4487  Validation loss = 6.4741  \n",
      "\n",
      "Fold: 14  Epoch: 23  Training loss = 2.4467  Validation loss = 6.4716  \n",
      "\n",
      "Fold: 14  Epoch: 24  Training loss = 2.4430  Validation loss = 6.4499  \n",
      "\n",
      "Fold: 14  Epoch: 25  Training loss = 2.4414  Validation loss = 6.4459  \n",
      "\n",
      "Fold: 14  Epoch: 26  Training loss = 2.4401  Validation loss = 6.4428  \n",
      "\n",
      "Fold: 14  Epoch: 27  Training loss = 2.4382  Validation loss = 6.4314  \n",
      "\n",
      "Fold: 14  Epoch: 28  Training loss = 2.4360  Validation loss = 6.4204  \n",
      "\n",
      "Fold: 14  Epoch: 29  Training loss = 2.4337  Validation loss = 6.4079  \n",
      "\n",
      "Fold: 14  Epoch: 30  Training loss = 2.4328  Validation loss = 6.4124  \n",
      "\n",
      "Fold: 14  Epoch: 31  Training loss = 2.4304  Validation loss = 6.3921  \n",
      "\n",
      "Fold: 14  Epoch: 32  Training loss = 2.4294  Validation loss = 6.3960  \n",
      "\n",
      "Fold: 14  Epoch: 33  Training loss = 2.4283  Validation loss = 6.3988  \n",
      "\n",
      "Fold: 14  Epoch: 34  Training loss = 2.4267  Validation loss = 6.3918  \n",
      "\n",
      "Fold: 14  Epoch: 35  Training loss = 2.4256  Validation loss = 6.3924  \n",
      "\n",
      "Fold: 14  Epoch: 36  Training loss = 2.4242  Validation loss = 6.3893  \n",
      "\n",
      "Fold: 14  Epoch: 37  Training loss = 2.4215  Validation loss = 6.3701  \n",
      "\n",
      "Fold: 14  Epoch: 38  Training loss = 2.4205  Validation loss = 6.3713  \n",
      "\n",
      "Fold: 14  Epoch: 39  Training loss = 2.4195  Validation loss = 6.3747  \n",
      "\n",
      "Fold: 14  Epoch: 40  Training loss = 2.4174  Validation loss = 6.3629  \n",
      "\n",
      "Fold: 14  Epoch: 41  Training loss = 2.4153  Validation loss = 6.3417  \n",
      "\n",
      "Fold: 14  Epoch: 42  Training loss = 2.4140  Validation loss = 6.3407  \n",
      "\n",
      "Fold: 14  Epoch: 43  Training loss = 2.4120  Validation loss = 6.3283  \n",
      "\n",
      "Fold: 14  Epoch: 44  Training loss = 2.4103  Validation loss = 6.3222  \n",
      "\n",
      "Fold: 14  Epoch: 45  Training loss = 2.4085  Validation loss = 6.3135  \n",
      "\n",
      "Fold: 14  Epoch: 46  Training loss = 2.4069  Validation loss = 6.3005  \n",
      "\n",
      "Fold: 14  Epoch: 47  Training loss = 2.4055  Validation loss = 6.2879  \n",
      "\n",
      "Fold: 14  Epoch: 48  Training loss = 2.4042  Validation loss = 6.2819  \n",
      "\n",
      "Fold: 14  Epoch: 49  Training loss = 2.4029  Validation loss = 6.2835  \n",
      "\n",
      "Fold: 14  Epoch: 50  Training loss = 2.4012  Validation loss = 6.2772  \n",
      "\n",
      "Fold: 14  Epoch: 51  Training loss = 2.4005  Validation loss = 6.2641  \n",
      "\n",
      "Fold: 14  Epoch: 52  Training loss = 2.3992  Validation loss = 6.2576  \n",
      "\n",
      "Fold: 14  Epoch: 53  Training loss = 2.3984  Validation loss = 6.2648  \n",
      "\n",
      "Fold: 14  Epoch: 54  Training loss = 2.3971  Validation loss = 6.2528  \n",
      "\n",
      "Fold: 14  Epoch: 55  Training loss = 2.3965  Validation loss = 6.2438  \n",
      "\n",
      "Fold: 14  Epoch: 56  Training loss = 2.3938  Validation loss = 6.2357  \n",
      "\n",
      "Fold: 14  Epoch: 57  Training loss = 2.3925  Validation loss = 6.2270  \n",
      "\n",
      "Fold: 14  Epoch: 58  Training loss = 2.3914  Validation loss = 6.2250  \n",
      "\n",
      "Fold: 14  Epoch: 59  Training loss = 2.3904  Validation loss = 6.2352  \n",
      "\n",
      "Fold: 14  Epoch: 60  Training loss = 2.3893  Validation loss = 6.2300  \n",
      "\n",
      "Fold: 14  Epoch: 61  Training loss = 2.3881  Validation loss = 6.2317  \n",
      "\n",
      "Fold: 14  Epoch: 62  Training loss = 2.3865  Validation loss = 6.2134  \n",
      "\n",
      "Fold: 14  Epoch: 63  Training loss = 2.3848  Validation loss = 6.2019  \n",
      "\n",
      "Fold: 14  Epoch: 64  Training loss = 2.3841  Validation loss = 6.1971  \n",
      "\n",
      "Fold: 14  Epoch: 65  Training loss = 2.3830  Validation loss = 6.1919  \n",
      "\n",
      "Fold: 14  Epoch: 66  Training loss = 2.3821  Validation loss = 6.1942  \n",
      "\n",
      "Fold: 14  Epoch: 67  Training loss = 2.3815  Validation loss = 6.1975  \n",
      "\n",
      "Fold: 14  Epoch: 68  Training loss = 2.3813  Validation loss = 6.1679  \n",
      "\n",
      "Fold: 14  Epoch: 69  Training loss = 2.3806  Validation loss = 6.1775  \n",
      "\n",
      "Fold: 14  Epoch: 70  Training loss = 2.3792  Validation loss = 6.1684  \n",
      "\n",
      "Fold: 14  Epoch: 71  Training loss = 2.3774  Validation loss = 6.1604  \n",
      "\n",
      "Fold: 14  Epoch: 72  Training loss = 2.3753  Validation loss = 6.1631  \n",
      "\n",
      "Fold: 14  Epoch: 73  Training loss = 2.3741  Validation loss = 6.1617  \n",
      "\n",
      "Fold: 14  Epoch: 74  Training loss = 2.3737  Validation loss = 6.1561  \n",
      "\n",
      "Fold: 14  Epoch: 75  Training loss = 2.3728  Validation loss = 6.1464  \n",
      "\n",
      "Fold: 14  Epoch: 76  Training loss = 2.3721  Validation loss = 6.1297  \n",
      "\n",
      "Fold: 14  Epoch: 77  Training loss = 2.3712  Validation loss = 6.1240  \n",
      "\n",
      "Fold: 14  Epoch: 78  Training loss = 2.3709  Validation loss = 6.1090  \n",
      "\n",
      "Fold: 14  Epoch: 79  Training loss = 2.3703  Validation loss = 6.1052  \n",
      "\n",
      "Fold: 14  Epoch: 80  Training loss = 2.3710  Validation loss = 6.1121  \n",
      "\n",
      "Fold: 14  Epoch: 81  Training loss = 2.3720  Validation loss = 6.0991  \n",
      "\n",
      "Fold: 14  Epoch: 82  Training loss = 2.3662  Validation loss = 6.0960  \n",
      "\n",
      "Fold: 14  Epoch: 83  Training loss = 2.3642  Validation loss = 6.0928  \n",
      "\n",
      "Fold: 14  Epoch: 84  Training loss = 2.3636  Validation loss = 6.1047  \n",
      "\n",
      "Fold: 14  Epoch: 85  Training loss = 2.3623  Validation loss = 6.1006  \n",
      "\n",
      "Fold: 14  Epoch: 86  Training loss = 2.3615  Validation loss = 6.0921  \n",
      "\n",
      "Fold: 14  Epoch: 87  Training loss = 2.3607  Validation loss = 6.0781  \n",
      "\n",
      "Fold: 14  Epoch: 88  Training loss = 2.3600  Validation loss = 6.0752  \n",
      "\n",
      "Fold: 14  Epoch: 89  Training loss = 2.3588  Validation loss = 6.0757  \n",
      "\n",
      "Fold: 14  Epoch: 90  Training loss = 2.3581  Validation loss = 6.0702  \n",
      "\n",
      "Fold: 14  Epoch: 91  Training loss = 2.3566  Validation loss = 6.0667  \n",
      "\n",
      "Fold: 14  Epoch: 92  Training loss = 2.3575  Validation loss = 6.0495  \n",
      "\n",
      "Fold: 14  Epoch: 93  Training loss = 2.3561  Validation loss = 6.0537  \n",
      "\n",
      "Fold: 14  Epoch: 94  Training loss = 2.3540  Validation loss = 6.0602  \n",
      "\n",
      "Fold: 14  Epoch: 95  Training loss = 2.3529  Validation loss = 6.0489  \n",
      "\n",
      "Fold: 14  Epoch: 96  Training loss = 2.3522  Validation loss = 6.0551  \n",
      "\n",
      "Fold: 14  Epoch: 97  Training loss = 2.3511  Validation loss = 6.0495  \n",
      "\n",
      "Fold: 14  Epoch: 98  Training loss = 2.3505  Validation loss = 6.0481  \n",
      "\n",
      "Fold: 14  Epoch: 99  Training loss = 2.3497  Validation loss = 6.0445  \n",
      "\n",
      "Fold: 14  Epoch: 100  Training loss = 2.3482  Validation loss = 6.0518  \n",
      "\n",
      "Fold: 14  Epoch: 101  Training loss = 2.3473  Validation loss = 6.0419  \n",
      "\n",
      "Fold: 14  Epoch: 102  Training loss = 2.3470  Validation loss = 6.0439  \n",
      "\n",
      "Fold: 14  Epoch: 103  Training loss = 2.3462  Validation loss = 6.0365  \n",
      "\n",
      "Fold: 14  Epoch: 104  Training loss = 2.3455  Validation loss = 6.0340  \n",
      "\n",
      "Fold: 14  Epoch: 105  Training loss = 2.3455  Validation loss = 6.0459  \n",
      "\n",
      "Fold: 14  Epoch: 106  Training loss = 2.3434  Validation loss = 6.0333  \n",
      "\n",
      "Fold: 14  Epoch: 107  Training loss = 2.3435  Validation loss = 6.0376  \n",
      "\n",
      "Fold: 14  Epoch: 108  Training loss = 2.3430  Validation loss = 6.0431  \n",
      "\n",
      "Fold: 14  Epoch: 109  Training loss = 2.3434  Validation loss = 6.0525  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 106  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 2.7686  Validation loss = 6.4050  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 2.7654  Validation loss = 6.3940  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 2.7620  Validation loss = 6.3809  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 2.7588  Validation loss = 6.3667  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 2.7553  Validation loss = 6.3499  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 2.7544  Validation loss = 6.3491  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 2.7541  Validation loss = 6.3492  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 2.7527  Validation loss = 6.3505  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 2.7480  Validation loss = 6.3229  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 2.7456  Validation loss = 6.3103  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 2.7424  Validation loss = 6.2880  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 2.7401  Validation loss = 6.2796  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 2.7365  Validation loss = 6.2526  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 2.7371  Validation loss = 6.2560  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 2.7353  Validation loss = 6.2526  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 2.7202  Validation loss = 6.2523  \n",
      "\n",
      "Fold: 15  Epoch: 17  Training loss = 2.7198  Validation loss = 6.2624  \n",
      "\n",
      "Fold: 15  Epoch: 18  Training loss = 2.7177  Validation loss = 6.2581  \n",
      "\n",
      "Fold: 15  Epoch: 19  Training loss = 2.7190  Validation loss = 6.2667  \n",
      "\n",
      "Fold: 15  Epoch: 20  Training loss = 2.7180  Validation loss = 6.2564  \n",
      "\n",
      "Fold: 15  Epoch: 21  Training loss = 2.7128  Validation loss = 6.2405  \n",
      "\n",
      "Fold: 15  Epoch: 22  Training loss = 2.7086  Validation loss = 6.2288  \n",
      "\n",
      "Fold: 15  Epoch: 23  Training loss = 2.7055  Validation loss = 6.2156  \n",
      "\n",
      "Fold: 15  Epoch: 24  Training loss = 2.7038  Validation loss = 6.2057  \n",
      "\n",
      "Fold: 15  Epoch: 25  Training loss = 2.7033  Validation loss = 6.2024  \n",
      "\n",
      "Fold: 15  Epoch: 26  Training loss = 2.6996  Validation loss = 6.1850  \n",
      "\n",
      "Fold: 15  Epoch: 27  Training loss = 2.6976  Validation loss = 6.1700  \n",
      "\n",
      "Fold: 15  Epoch: 28  Training loss = 2.6963  Validation loss = 6.1655  \n",
      "\n",
      "Fold: 15  Epoch: 29  Training loss = 2.6944  Validation loss = 6.1541  \n",
      "\n",
      "Fold: 15  Epoch: 30  Training loss = 2.6938  Validation loss = 6.1484  \n",
      "\n",
      "Fold: 15  Epoch: 31  Training loss = 2.6920  Validation loss = 6.1313  \n",
      "\n",
      "Fold: 15  Epoch: 32  Training loss = 2.6901  Validation loss = 6.1168  \n",
      "\n",
      "Fold: 15  Epoch: 33  Training loss = 2.6891  Validation loss = 6.1082  \n",
      "\n",
      "Fold: 15  Epoch: 34  Training loss = 2.6875  Validation loss = 6.1068  \n",
      "\n",
      "Fold: 15  Epoch: 35  Training loss = 2.6854  Validation loss = 6.0948  \n",
      "\n",
      "Fold: 15  Epoch: 36  Training loss = 2.6834  Validation loss = 6.0811  \n",
      "\n",
      "Fold: 15  Epoch: 37  Training loss = 2.6820  Validation loss = 6.0776  \n",
      "\n",
      "Fold: 15  Epoch: 38  Training loss = 2.6811  Validation loss = 6.0626  \n",
      "\n",
      "Fold: 15  Epoch: 39  Training loss = 2.6800  Validation loss = 6.0661  \n",
      "\n",
      "Fold: 15  Epoch: 40  Training loss = 2.6788  Validation loss = 6.0695  \n",
      "\n",
      "Fold: 15  Epoch: 41  Training loss = 2.6773  Validation loss = 6.0526  \n",
      "\n",
      "Fold: 15  Epoch: 42  Training loss = 2.6758  Validation loss = 6.0462  \n",
      "\n",
      "Fold: 15  Epoch: 43  Training loss = 2.6739  Validation loss = 6.0252  \n",
      "\n",
      "Fold: 15  Epoch: 44  Training loss = 2.6721  Validation loss = 6.0114  \n",
      "\n",
      "Fold: 15  Epoch: 45  Training loss = 2.6694  Validation loss = 5.9962  \n",
      "\n",
      "Fold: 15  Epoch: 46  Training loss = 2.6688  Validation loss = 5.9839  \n",
      "\n",
      "Fold: 15  Epoch: 47  Training loss = 2.6681  Validation loss = 5.9843  \n",
      "\n",
      "Fold: 15  Epoch: 48  Training loss = 2.6673  Validation loss = 5.9944  \n",
      "\n",
      "Fold: 15  Epoch: 49  Training loss = 2.6683  Validation loss = 5.9757  \n",
      "\n",
      "Fold: 15  Epoch: 50  Training loss = 2.6641  Validation loss = 5.9865  \n",
      "\n",
      "Fold: 15  Epoch: 51  Training loss = 2.6651  Validation loss = 5.9958  \n",
      "\n",
      "Fold: 15  Epoch: 52  Training loss = 2.6643  Validation loss = 5.9935  \n",
      "\n",
      "Fold: 15  Epoch: 53  Training loss = 2.6614  Validation loss = 5.9793  \n",
      "\n",
      "Fold: 15  Epoch: 54  Training loss = 2.6621  Validation loss = 5.9845  \n",
      "\n",
      "Fold: 15  Epoch: 55  Training loss = 2.6628  Validation loss = 5.9845  \n",
      "\n",
      "Fold: 15  Epoch: 56  Training loss = 2.6625  Validation loss = 5.9993  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 49  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 3.0277  Validation loss = 4.7978  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 3.0216  Validation loss = 4.7550  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 3.0178  Validation loss = 4.7316  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 3.0146  Validation loss = 4.7076  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 3.0160  Validation loss = 4.7272  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 3.0137  Validation loss = 4.6852  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 3.0221  Validation loss = 4.7125  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 3.0080  Validation loss = 4.6746  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 3.0036  Validation loss = 4.6581  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 2.9997  Validation loss = 4.6386  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 2.9951  Validation loss = 4.6324  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 2.9932  Validation loss = 4.6268  \n",
      "\n",
      "Fold: 16  Epoch: 13  Training loss = 2.9925  Validation loss = 4.6531  \n",
      "\n",
      "Fold: 16  Epoch: 14  Training loss = 2.9869  Validation loss = 4.5918  \n",
      "\n",
      "Fold: 16  Epoch: 15  Training loss = 2.9862  Validation loss = 4.5755  \n",
      "\n",
      "Fold: 16  Epoch: 16  Training loss = 2.9812  Validation loss = 4.5560  \n",
      "\n",
      "Fold: 16  Epoch: 17  Training loss = 2.9815  Validation loss = 4.5432  \n",
      "\n",
      "Fold: 16  Epoch: 18  Training loss = 2.9787  Validation loss = 4.5746  \n",
      "\n",
      "Fold: 16  Epoch: 19  Training loss = 2.9769  Validation loss = 4.5727  \n",
      "\n",
      "Fold: 16  Epoch: 20  Training loss = 2.9815  Validation loss = 4.5196  \n",
      "\n",
      "Fold: 16  Epoch: 21  Training loss = 2.9774  Validation loss = 4.5191  \n",
      "\n",
      "Fold: 16  Epoch: 22  Training loss = 2.9729  Validation loss = 4.5345  \n",
      "\n",
      "Fold: 16  Epoch: 23  Training loss = 2.9716  Validation loss = 4.5234  \n",
      "\n",
      "Fold: 16  Epoch: 24  Training loss = 2.9827  Validation loss = 4.6847  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 21  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 3.1679  Validation loss = 3.1087  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 3.1654  Validation loss = 3.1066  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 3.1584  Validation loss = 3.1157  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 3.1522  Validation loss = 3.1065  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 3.1445  Validation loss = 3.1077  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 3.1370  Validation loss = 3.1004  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 3.1276  Validation loss = 3.1011  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 3.1252  Validation loss = 3.1185  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 3.1201  Validation loss = 3.1144  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 3.1163  Validation loss = 3.1181  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 3.1119  Validation loss = 3.1563  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 6  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 3.1995  Validation loss = 2.3982  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 3.1916  Validation loss = 2.4236  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 3.1824  Validation loss = 2.5310  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 3.1840  Validation loss = 2.5453  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 3.1771  Validation loss = 2.5888  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 3.1747  Validation loss = 2.5838  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 3.1676  Validation loss = 2.5389  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 3.1620  Validation loss = 2.5730  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 3.1574  Validation loss = 2.5532  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 3.1527  Validation loss = 2.5574  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 3.1496  Validation loss = 2.5932  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 1  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 3.1516  Validation loss = 2.0688  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 3.1333  Validation loss = 1.9742  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 3.0993  Validation loss = 1.7526  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 3.0889  Validation loss = 1.6157  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 3.0835  Validation loss = 1.5856  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 3.0816  Validation loss = 1.5409  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 3.0699  Validation loss = 1.5633  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 3.0692  Validation loss = 1.5440  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 3.0622  Validation loss = 1.5310  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 3.0613  Validation loss = 1.4983  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 3.0568  Validation loss = 1.4894  \n",
      "\n",
      "Fold: 19  Epoch: 12  Training loss = 3.0526  Validation loss = 1.4946  \n",
      "\n",
      "Fold: 19  Epoch: 13  Training loss = 3.0488  Validation loss = 1.5122  \n",
      "\n",
      "Fold: 19  Epoch: 14  Training loss = 3.0491  Validation loss = 1.5105  \n",
      "\n",
      "Fold: 19  Epoch: 15  Training loss = 3.0442  Validation loss = 1.5103  \n",
      "\n",
      "Fold: 19  Epoch: 16  Training loss = 3.0455  Validation loss = 1.4778  \n",
      "\n",
      "Fold: 19  Epoch: 17  Training loss = 3.0520  Validation loss = 1.4306  \n",
      "\n",
      "Fold: 19  Epoch: 18  Training loss = 3.0428  Validation loss = 1.4667  \n",
      "\n",
      "Fold: 19  Epoch: 19  Training loss = 3.0449  Validation loss = 1.4908  \n",
      "\n",
      "Fold: 19  Epoch: 20  Training loss = 3.0316  Validation loss = 1.4870  \n",
      "\n",
      "Fold: 19  Epoch: 21  Training loss = 3.0291  Validation loss = 1.4737  \n",
      "\n",
      "Fold: 19  Epoch: 22  Training loss = 3.0261  Validation loss = 1.4703  \n",
      "\n",
      "Fold: 19  Epoch: 23  Training loss = 3.0255  Validation loss = 1.4796  \n",
      "\n",
      "Fold: 19  Epoch: 24  Training loss = 3.0213  Validation loss = 1.4518  \n",
      "\n",
      "Fold: 19  Epoch: 25  Training loss = 3.0189  Validation loss = 1.4442  \n",
      "\n",
      "Fold: 19  Epoch: 26  Training loss = 3.0154  Validation loss = 1.4680  \n",
      "\n",
      "Fold: 19  Epoch: 27  Training loss = 3.0168  Validation loss = 1.4379  \n",
      "\n",
      "Fold: 19  Epoch: 28  Training loss = 3.0095  Validation loss = 1.4385  \n",
      "\n",
      "Fold: 19  Epoch: 29  Training loss = 3.0073  Validation loss = 1.4484  \n",
      "\n",
      "Fold: 19  Epoch: 30  Training loss = 3.0099  Validation loss = 1.4630  \n",
      "\n",
      "Fold: 19  Epoch: 31  Training loss = 3.0080  Validation loss = 1.4485  \n",
      "\n",
      "Fold: 19  Epoch: 32  Training loss = 3.0021  Validation loss = 1.4528  \n",
      "\n",
      "Fold: 19  Epoch: 33  Training loss = 3.0004  Validation loss = 1.4468  \n",
      "\n",
      "Fold: 19  Epoch: 34  Training loss = 3.0016  Validation loss = 1.4211  \n",
      "\n",
      "Fold: 19  Epoch: 35  Training loss = 2.9956  Validation loss = 1.4290  \n",
      "\n",
      "Fold: 19  Epoch: 36  Training loss = 2.9897  Validation loss = 1.4494  \n",
      "\n",
      "Fold: 19  Epoch: 37  Training loss = 2.9885  Validation loss = 1.4526  \n",
      "\n",
      "Fold: 19  Epoch: 38  Training loss = 2.9868  Validation loss = 1.4404  \n",
      "\n",
      "Fold: 19  Epoch: 39  Training loss = 2.9831  Validation loss = 1.4149  \n",
      "\n",
      "Fold: 19  Epoch: 40  Training loss = 2.9801  Validation loss = 1.3881  \n",
      "\n",
      "Fold: 19  Epoch: 41  Training loss = 2.9767  Validation loss = 1.3993  \n",
      "\n",
      "Fold: 19  Epoch: 42  Training loss = 2.9733  Validation loss = 1.4020  \n",
      "\n",
      "Fold: 19  Epoch: 43  Training loss = 2.9732  Validation loss = 1.4033  \n",
      "\n",
      "Fold: 19  Epoch: 44  Training loss = 2.9688  Validation loss = 1.4022  \n",
      "\n",
      "Fold: 19  Epoch: 45  Training loss = 2.9660  Validation loss = 1.4149  \n",
      "\n",
      "Fold: 19  Epoch: 46  Training loss = 2.9636  Validation loss = 1.4212  \n",
      "\n",
      "Fold: 19  Epoch: 47  Training loss = 2.9590  Validation loss = 1.3992  \n",
      "\n",
      "Fold: 19  Epoch: 48  Training loss = 2.9570  Validation loss = 1.4057  \n",
      "\n",
      "Fold: 19  Epoch: 49  Training loss = 2.9484  Validation loss = 1.3826  \n",
      "\n",
      "Fold: 19  Epoch: 50  Training loss = 2.9498  Validation loss = 1.3695  \n",
      "\n",
      "Fold: 19  Epoch: 51  Training loss = 2.9477  Validation loss = 1.3811  \n",
      "\n",
      "Fold: 19  Epoch: 52  Training loss = 2.9453  Validation loss = 1.3255  \n",
      "\n",
      "Fold: 19  Epoch: 53  Training loss = 2.9454  Validation loss = 1.2897  \n",
      "\n",
      "Fold: 19  Epoch: 54  Training loss = 2.9357  Validation loss = 1.3074  \n",
      "\n",
      "Fold: 19  Epoch: 55  Training loss = 2.9247  Validation loss = 1.3477  \n",
      "\n",
      "Fold: 19  Epoch: 56  Training loss = 2.9202  Validation loss = 1.2990  \n",
      "\n",
      "Fold: 19  Epoch: 57  Training loss = 2.9233  Validation loss = 1.2757  \n",
      "\n",
      "Fold: 19  Epoch: 58  Training loss = 2.9129  Validation loss = 1.3125  \n",
      "\n",
      "Fold: 19  Epoch: 59  Training loss = 2.9295  Validation loss = 1.3832  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 57  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 2.9098  Validation loss = 1.2340  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 2.9060  Validation loss = 1.2414  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 2.9013  Validation loss = 1.2619  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 2.8947  Validation loss = 1.2706  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 2.8884  Validation loss = 1.3069  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 2.8838  Validation loss = 1.3229  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 2.8881  Validation loss = 1.3593  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 2.8750  Validation loss = 1.3558  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 2.8878  Validation loss = 1.3423  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 2.9322  Validation loss = 1.3248  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 2.8777  Validation loss = 1.3378  \n",
      "\n",
      "Fold: 20  Epoch: 12  Training loss = 2.8720  Validation loss = 1.3626  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 1  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 2.8733  Validation loss = 2.8086  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 2.8633  Validation loss = 2.8050  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 2.8620  Validation loss = 2.8077  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 2.8571  Validation loss = 2.8188  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 2.8575  Validation loss = 2.8324  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 2.8568  Validation loss = 2.8424  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 2.8471  Validation loss = 2.8557  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 2.8459  Validation loss = 2.8441  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 2.8459  Validation loss = 2.8667  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 2.8443  Validation loss = 2.8779  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 2.8497  Validation loss = 2.8981  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 2  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 2.8820  Validation loss = 0.9480  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 2.8660  Validation loss = 0.9201  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 2.8905  Validation loss = 0.9594  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 2.8704  Validation loss = 0.9368  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 2.8575  Validation loss = 0.8903  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 2.8529  Validation loss = 0.9232  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 2.8552  Validation loss = 0.9383  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 2.8606  Validation loss = 0.8571  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 2.8421  Validation loss = 0.8852  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 2.8376  Validation loss = 0.8786  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 2.8368  Validation loss = 0.8810  \n",
      "\n",
      "Fold: 22  Epoch: 12  Training loss = 2.8315  Validation loss = 0.8680  \n",
      "\n",
      "Fold: 22  Epoch: 13  Training loss = 2.8265  Validation loss = 0.8860  \n",
      "\n",
      "Fold: 22  Epoch: 14  Training loss = 2.8270  Validation loss = 0.9290  \n",
      "\n",
      "Fold: 22  Epoch: 15  Training loss = 2.8172  Validation loss = 0.8959  \n",
      "\n",
      "Fold: 22  Epoch: 16  Training loss = 2.8202  Validation loss = 0.9450  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 8  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 2.6821  Validation loss = 1.8093  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 2.6844  Validation loss = 1.5549  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 2.7128  Validation loss = 0.9703  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 2.6764  Validation loss = 1.7449  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 2.6617  Validation loss = 1.8308  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 2.6589  Validation loss = 1.8163  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 2.6545  Validation loss = 1.7804  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 2.6516  Validation loss = 1.7489  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 2.6465  Validation loss = 1.8722  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 2.6437  Validation loss = 1.6735  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 2.6411  Validation loss = 1.8301  \n",
      "\n",
      "Fold: 23  Epoch: 12  Training loss = 2.6425  Validation loss = 1.2993  \n",
      "\n",
      "Fold: 23  Epoch: 13  Training loss = 2.6341  Validation loss = 1.7818  \n",
      "\n",
      "Fold: 23  Epoch: 14  Training loss = 2.6521  Validation loss = 1.9092  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 3  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 2.4932  Validation loss = 1.6981  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 2.4710  Validation loss = 1.8317  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 2.4798  Validation loss = 1.7103  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 2.4762  Validation loss = 1.6923  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 2.4651  Validation loss = 1.8500  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 2.4609  Validation loss = 1.6627  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 2.4430  Validation loss = 1.7164  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 2.4447  Validation loss = 1.7555  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 2.4379  Validation loss = 1.7247  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 2.4597  Validation loss = 1.8225  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 2.4410  Validation loss = 1.6450  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 2.4353  Validation loss = 1.6742  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 2.4215  Validation loss = 1.7215  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 2.4182  Validation loss = 1.7403  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 2.4087  Validation loss = 1.7440  \n",
      "\n",
      "Fold: 24  Epoch: 16  Training loss = 2.4050  Validation loss = 1.7467  \n",
      "\n",
      "Fold: 24  Epoch: 17  Training loss = 2.4002  Validation loss = 1.7609  \n",
      "\n",
      "Fold: 24  Epoch: 18  Training loss = 2.3998  Validation loss = 1.8053  \n",
      "\n",
      "Fold: 24  Epoch: 19  Training loss = 2.3970  Validation loss = 1.7925  \n",
      "\n",
      "Fold: 24  Epoch: 20  Training loss = 2.3979  Validation loss = 1.7876  \n",
      "\n",
      "Fold: 24  Epoch: 21  Training loss = 2.4017  Validation loss = 1.7772  \n",
      "\n",
      "Fold: 24  Epoch: 22  Training loss = 2.3981  Validation loss = 1.8451  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 11  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 2.4125  Validation loss = 1.8933  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 2.3980  Validation loss = 1.9324  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 2.3896  Validation loss = 1.8207  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 2.3901  Validation loss = 2.2062  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 2.3912  Validation loss = 2.1906  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 2.3765  Validation loss = 2.0398  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 2.3745  Validation loss = 2.1489  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 2.3522  Validation loss = 1.7046  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 2.3712  Validation loss = 1.6509  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 2.3449  Validation loss = 1.7142  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 2.3374  Validation loss = 1.8015  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 2.3521  Validation loss = 1.9721  \n",
      "\n",
      "Fold: 25  Epoch: 13  Training loss = 2.3383  Validation loss = 1.8226  \n",
      "\n",
      "Fold: 25  Epoch: 14  Training loss = 2.3206  Validation loss = 1.7566  \n",
      "\n",
      "Fold: 25  Epoch: 15  Training loss = 2.3300  Validation loss = 1.6751  \n",
      "\n",
      "Fold: 25  Epoch: 16  Training loss = 2.3199  Validation loss = 1.7854  \n",
      "\n",
      "Fold: 25  Epoch: 17  Training loss = 2.3078  Validation loss = 1.8043  \n",
      "\n",
      "Fold: 25  Epoch: 18  Training loss = 2.3075  Validation loss = 1.7533  \n",
      "\n",
      "Fold: 25  Epoch: 19  Training loss = 2.3433  Validation loss = 1.6585  \n",
      "\n",
      "Fold: 25  Epoch: 20  Training loss = 2.3055  Validation loss = 1.7195  \n",
      "\n",
      "Fold: 25  Epoch: 21  Training loss = 2.3001  Validation loss = 1.7393  \n",
      "\n",
      "Fold: 25  Epoch: 22  Training loss = 2.3065  Validation loss = 2.2140  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 9  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 2.1918  Validation loss = 3.0697  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 2.1784  Validation loss = 3.0000  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 2.1740  Validation loss = 2.9014  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 2.1672  Validation loss = 2.8806  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 2.1553  Validation loss = 3.0203  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 2.1740  Validation loss = 3.1017  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 2.1474  Validation loss = 3.0973  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 2.1541  Validation loss = 3.2227  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 2.1749  Validation loss = 3.3282  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 2.1494  Validation loss = 2.8955  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 2.1309  Validation loss = 3.1647  \n",
      "\n",
      "Fold: 26  Epoch: 12  Training loss = 2.1337  Validation loss = 3.3197  \n",
      "\n",
      "Fold: 26  Epoch: 13  Training loss = 2.1852  Validation loss = 2.5843  \n",
      "\n",
      "Fold: 26  Epoch: 14  Training loss = 2.1165  Validation loss = 3.1700  \n",
      "\n",
      "Fold: 26  Epoch: 15  Training loss = 2.1219  Validation loss = 3.1100  \n",
      "\n",
      "Fold: 26  Epoch: 16  Training loss = 2.1311  Validation loss = 3.3281  \n",
      "\n",
      "Fold: 26  Epoch: 17  Training loss = 2.1227  Validation loss = 3.3246  \n",
      "\n",
      "Fold: 26  Epoch: 18  Training loss = 2.1168  Validation loss = 3.1196  \n",
      "\n",
      "Fold: 26  Epoch: 19  Training loss = 2.1050  Validation loss = 3.2981  \n",
      "\n",
      "Fold: 26  Epoch: 20  Training loss = 2.1306  Validation loss = 3.4695  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 13  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 2.1878  Validation loss = 2.3918  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 2.1831  Validation loss = 2.3155  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 2.1958  Validation loss = 1.5662  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 2.1854  Validation loss = 2.3902  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 2.1830  Validation loss = 2.5486  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 2.1844  Validation loss = 1.8058  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 2.2698  Validation loss = 2.7555  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 2.2008  Validation loss = 1.8674  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 2.1693  Validation loss = 2.6583  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 2.2166  Validation loss = 2.6358  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 2.1806  Validation loss = 2.6278  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 2.1946  Validation loss = 1.3586  \n",
      "\n",
      "Fold: 27  Epoch: 13  Training loss = 2.1722  Validation loss = 1.8231  \n",
      "\n",
      "Fold: 27  Epoch: 14  Training loss = 2.1810  Validation loss = 1.3243  \n",
      "\n",
      "Fold: 27  Epoch: 15  Training loss = 2.1474  Validation loss = 2.4098  \n",
      "\n",
      "Fold: 27  Epoch: 16  Training loss = 2.1415  Validation loss = 2.1583  \n",
      "\n",
      "Fold: 27  Epoch: 17  Training loss = 2.1436  Validation loss = 2.2276  \n",
      "\n",
      "Fold: 27  Epoch: 18  Training loss = 2.1627  Validation loss = 2.1206  \n",
      "\n",
      "Fold: 27  Epoch: 19  Training loss = 2.1594  Validation loss = 1.0454  \n",
      "\n",
      "Fold: 27  Epoch: 20  Training loss = 2.1300  Validation loss = 1.8212  \n",
      "\n",
      "Fold: 27  Epoch: 21  Training loss = 2.2020  Validation loss = 0.7632  \n",
      "\n",
      "Fold: 27  Epoch: 22  Training loss = 2.1449  Validation loss = 1.6546  \n",
      "\n",
      "Fold: 27  Epoch: 23  Training loss = 2.1214  Validation loss = 2.0395  \n",
      "\n",
      "Fold: 27  Epoch: 24  Training loss = 2.1279  Validation loss = 1.9879  \n",
      "\n",
      "Fold: 27  Epoch: 25  Training loss = 2.1353  Validation loss = 2.1044  \n",
      "\n",
      "Fold: 27  Epoch: 26  Training loss = 2.1372  Validation loss = 2.0925  \n",
      "\n",
      "Fold: 27  Epoch: 27  Training loss = 2.1171  Validation loss = 2.2002  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 21  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 1.9255  Validation loss = 1.6488  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 1.9110  Validation loss = 1.6608  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 1.9143  Validation loss = 1.6548  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 1.9366  Validation loss = 1.6859  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 1.8922  Validation loss = 1.6535  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 1.8835  Validation loss = 1.7157  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 1.9399  Validation loss = 1.6677  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 1.9057  Validation loss = 1.6189  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 1.8771  Validation loss = 1.7033  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 1.8779  Validation loss = 1.7263  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 1.9012  Validation loss = 1.7532  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 8  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 1.8659  Validation loss = 1.5046  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 1.9321  Validation loss = 1.4345  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 1.8689  Validation loss = 1.4986  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 1.8986  Validation loss = 1.5790  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 1.8889  Validation loss = 1.5424  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 1.8554  Validation loss = 1.5496  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 1.9476  Validation loss = 1.6001  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 1.8832  Validation loss = 1.5401  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 1.8722  Validation loss = 1.5826  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 1.8468  Validation loss = 1.5851  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 1.8458  Validation loss = 1.6127  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 2  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 1.9527  Validation loss = 1.0685  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 1.8463  Validation loss = 1.0668  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 1.8527  Validation loss = 1.0886  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 1.8474  Validation loss = 1.0899  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 1.8480  Validation loss = 1.1200  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 1.8362  Validation loss = 1.1139  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 1.8940  Validation loss = 1.0975  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 1.8291  Validation loss = 1.1048  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 1.9578  Validation loss = 1.0945  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 1.9197  Validation loss = 1.1243  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 1.8747  Validation loss = 1.1323  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 2  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 1.7812  Validation loss = 1.0646  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 1.7541  Validation loss = 1.2440  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 1.7898  Validation loss = 1.3842  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 1.7394  Validation loss = 1.1387  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 1.7390  Validation loss = 1.1811  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 1.7247  Validation loss = 1.2269  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 1.7276  Validation loss = 1.2969  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 1.7886  Validation loss = 1.4502  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 1.7218  Validation loss = 1.2003  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 1.7196  Validation loss = 1.2577  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 1.7142  Validation loss = 1.3065  \n",
      "\n",
      "Fold: 31  Epoch: 12  Training loss = 1.8092  Validation loss = 1.4926  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 1  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 1.5170  Validation loss = 3.0535  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 1.8129  Validation loss = 3.0297  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 1.6244  Validation loss = 3.7649  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 1.5262  Validation loss = 2.9681  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 1.5263  Validation loss = 2.9532  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 1.5023  Validation loss = 3.0084  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 1.6506  Validation loss = 3.0087  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 1.6518  Validation loss = 3.8865  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 1.5227  Validation loss = 2.9642  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 1.5021  Validation loss = 2.9856  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 1.5202  Validation loss = 2.9081  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 1.5069  Validation loss = 2.9239  \n",
      "\n",
      "Fold: 32  Epoch: 13  Training loss = 1.5116  Validation loss = 2.9011  \n",
      "\n",
      "Fold: 32  Epoch: 14  Training loss = 1.4899  Validation loss = 2.9363  \n",
      "\n",
      "Fold: 32  Epoch: 15  Training loss = 1.4929  Validation loss = 2.9419  \n",
      "\n",
      "Fold: 32  Epoch: 16  Training loss = 1.4894  Validation loss = 2.9729  \n",
      "\n",
      "Fold: 32  Epoch: 17  Training loss = 1.4893  Validation loss = 2.9621  \n",
      "\n",
      "Fold: 32  Epoch: 18  Training loss = 1.4905  Validation loss = 2.9626  \n",
      "\n",
      "Fold: 32  Epoch: 19  Training loss = 1.4837  Validation loss = 2.9718  \n",
      "\n",
      "Fold: 32  Epoch: 20  Training loss = 1.4868  Validation loss = 2.9956  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 13  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    # Load model with lowest validation error for each fold\n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 51\n",
      "Average validation error: 2.91149\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 1.5314  Test loss = 2.8466  \n",
      "\n",
      "Epoch: 2  Training loss = 1.5292  Test loss = 2.8402  \n",
      "\n",
      "Epoch: 3  Training loss = 1.5271  Test loss = 2.8343  \n",
      "\n",
      "Epoch: 4  Training loss = 1.5250  Test loss = 2.8288  \n",
      "\n",
      "Epoch: 5  Training loss = 1.5231  Test loss = 2.8236  \n",
      "\n",
      "Epoch: 6  Training loss = 1.5213  Test loss = 2.8188  \n",
      "\n",
      "Epoch: 7  Training loss = 1.5196  Test loss = 2.8142  \n",
      "\n",
      "Epoch: 8  Training loss = 1.5179  Test loss = 2.8099  \n",
      "\n",
      "Epoch: 9  Training loss = 1.5164  Test loss = 2.8058  \n",
      "\n",
      "Epoch: 10  Training loss = 1.5149  Test loss = 2.8019  \n",
      "\n",
      "Epoch: 11  Training loss = 1.5135  Test loss = 2.7982  \n",
      "\n",
      "Epoch: 12  Training loss = 1.5122  Test loss = 2.7946  \n",
      "\n",
      "Epoch: 13  Training loss = 1.5110  Test loss = 2.7912  \n",
      "\n",
      "Epoch: 14  Training loss = 1.5098  Test loss = 2.7879  \n",
      "\n",
      "Epoch: 15  Training loss = 1.5087  Test loss = 2.7847  \n",
      "\n",
      "Epoch: 16  Training loss = 1.5076  Test loss = 2.7817  \n",
      "\n",
      "Epoch: 17  Training loss = 1.5066  Test loss = 2.7787  \n",
      "\n",
      "Epoch: 18  Training loss = 1.5056  Test loss = 2.7759  \n",
      "\n",
      "Epoch: 19  Training loss = 1.5047  Test loss = 2.7732  \n",
      "\n",
      "Epoch: 20  Training loss = 1.5038  Test loss = 2.7705  \n",
      "\n",
      "Epoch: 21  Training loss = 1.5029  Test loss = 2.7679  \n",
      "\n",
      "Epoch: 22  Training loss = 1.5021  Test loss = 2.7655  \n",
      "\n",
      "Epoch: 23  Training loss = 1.5013  Test loss = 2.7631  \n",
      "\n",
      "Epoch: 24  Training loss = 1.5006  Test loss = 2.7608  \n",
      "\n",
      "Epoch: 25  Training loss = 1.4999  Test loss = 2.7585  \n",
      "\n",
      "Epoch: 26  Training loss = 1.4992  Test loss = 2.7564  \n",
      "\n",
      "Epoch: 27  Training loss = 1.4985  Test loss = 2.7543  \n",
      "\n",
      "Epoch: 28  Training loss = 1.4978  Test loss = 2.7523  \n",
      "\n",
      "Epoch: 29  Training loss = 1.4972  Test loss = 2.7503  \n",
      "\n",
      "Epoch: 30  Training loss = 1.4966  Test loss = 2.7484  \n",
      "\n",
      "Epoch: 31  Training loss = 1.4960  Test loss = 2.7466  \n",
      "\n",
      "Epoch: 32  Training loss = 1.4954  Test loss = 2.7448  \n",
      "\n",
      "Epoch: 33  Training loss = 1.4949  Test loss = 2.7431  \n",
      "\n",
      "Epoch: 34  Training loss = 1.4943  Test loss = 2.7415  \n",
      "\n",
      "Epoch: 35  Training loss = 1.4938  Test loss = 2.7399  \n",
      "\n",
      "Epoch: 36  Training loss = 1.4933  Test loss = 2.7384  \n",
      "\n",
      "Epoch: 37  Training loss = 1.4928  Test loss = 2.7369  \n",
      "\n",
      "Epoch: 38  Training loss = 1.4923  Test loss = 2.7355  \n",
      "\n",
      "Epoch: 39  Training loss = 1.4919  Test loss = 2.7342  \n",
      "\n",
      "Epoch: 40  Training loss = 1.4914  Test loss = 2.7329  \n",
      "\n",
      "Epoch: 41  Training loss = 1.4910  Test loss = 2.7316  \n",
      "\n",
      "Epoch: 42  Training loss = 1.4906  Test loss = 2.7304  \n",
      "\n",
      "Epoch: 43  Training loss = 1.4901  Test loss = 2.7292  \n",
      "\n",
      "Epoch: 44  Training loss = 1.4897  Test loss = 2.7281  \n",
      "\n",
      "Epoch: 45  Training loss = 1.4894  Test loss = 2.7271  \n",
      "\n",
      "Epoch: 46  Training loss = 1.4890  Test loss = 2.7260  \n",
      "\n",
      "Epoch: 47  Training loss = 1.4886  Test loss = 2.7251  \n",
      "\n",
      "Epoch: 48  Training loss = 1.4883  Test loss = 2.7241  \n",
      "\n",
      "Epoch: 49  Training loss = 1.4879  Test loss = 2.7232  \n",
      "\n",
      "Epoch: 50  Training loss = 1.4876  Test loss = 2.7224  \n",
      "\n",
      "Epoch: 51  Training loss = 1.4873  Test loss = 2.7216  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXd4VGX+9j9n0hMyCUnoCQkCEkJJUGBF2bWhIsWCioDs\nrr239XXtfddd689du1jWxQaKooKoiLvu6gYVEBIIhARIgBBCSUjvyXn/eOaZzEzOtGTSZp7PdXEF\nZk7mnBnO3Od77udbNF3XUSgUCoX/YOrpA1AoFAqFb1HCrlAoFH6GEnaFQqHwM5SwKxQKhZ+hhF2h\nUCj8DCXsCoVC4WcoYVcoFAo/Qwm7QqFQ+BlK2BUKhcLPCO6JnSYkJOgpKSk9sWuFQqHos2zatOmo\nrusD3G3XI8KekpLCxo0be2LXCoVC0WfRNG2vJ9spK0ahUCj8DCXsCoVC4WcoYVcoFAo/Qwm7QqFQ\n+BlK2BUKhcLPUMKuUCgUfoYSdoVCofAzAkLY6+vreeutt1BjABUKRSAQEMK+fPlyrrrqKrKzs3v6\nUBR9DF3X3QYEra2tPProo/z73//upqNSKFwTEMKelZUFQHV1dQ8fiaKvcc8993DGGWe43GbHjh08\n8sgjnHHGGVx44YXs2rWrm45OoTAmIIR969atANTV1fXwkSj6Glu3bmXbtm0utykrKwPg4osvZt26\ndaSlpXHHHXdw7Nix7jhEhaIdASHs0oKpra3t4SNR9DVKS0spKyujtbXV6TZS2O+55x7y8/P53e9+\nx9/+9jcmTJhATU1Ndx2qQmHF74X90KFDHD58GFARu8J7SktLaW1tpbKy0uk2Utjj4uIYPHgwb7zx\nBq+++ioHDhxg37593XWoCoUVvxd2acOAitgV3lNaWgq0ibcR8rn4+HjrY0lJSQAuLwgKRVfh98Ju\nmwmjInaFNzQ3N1NeXg64F/agoCCio6Otj5nNZkAJu6Jn8Hth37p1q/ULpyJ2hTfYLn7KyN2IsrIy\n4uLi0DTN+pgSdkVP4vfCnp2dzeTJkwEVsSu8w1bMXUXspaWlxMXF2T0mhb2ioqJrDk6hcEGfEvas\nrCw+//xzj7dvbm5m+/btnHDCCYSEhKiIXeEVngq7jNhtiYmJAVTErugZ+pSwL1myhCuuuMLj7Xft\n2kV9fT0TJkwgMjJSRewBzL///W8KCgq8+h1bYffEirFF2n9K2BU9QZ8S9vj4eI4dO+Yyp9gWmREz\nceJEIiIiVMQewMybN4+nnnrKq9/xJmK3zYgBCAoKIioqSgm7okfoU8IeFxeHruse+5bZ2dkEBQUx\nduxYFbEHMJWVlZSXl7uMuo2Q28fFxXltxYDw2ZWwK3qCPifs4Dp6smXr1q0cf/zxhIeHq4g9gDlw\n4ACANXXRU0pLSwkJCSE5OdnpRaGpqYmqqiol7IpeRZ8Udk8jr+zsbCZOnAigIvYApqioCOiYsMfH\nxxMfH+80mJApkc6EXWXFKHqCPiXs0sf0JGKvqqqioKCACRMmAKiIPYCRwu5tUy4p7K6sGFu7xpGY\nmBgVsSt6hD4l7N5YMbIjn4rYFZ2N2F0Ju22fGEeUFaPoKfxW2GUrAduIXQl7YGLrsXszRcvRijHK\nxjLqEyNRwq7oKfqWsGdm8iSeeeyylUBycjKgrJhARkbszc3NXp0DthG7sw6PKmJX9Eb6lLAHbdzI\nXcCALVvcbisXTmX/DmXFBC5S2MFzO0bXdTthB+M7RU+EXc3aVXQ3fUrYuf9+CoKDuWjdOnAh0rqu\ns3XrVqsNAypiD2SKioqsVomnwl5dXU1jY6PVigHjO8WysjJMJpO1N4wtZrOZ1tZWNWxD0e34RNg1\nTYvVNG2Fpmm5mqbt0DRtmi9etx3h4Tw5YgSDqqvhT39yullRURHl5eXWhVNQEXugUldXR2lpKePH\njwc8F3Yp4u4i9tLSUvr374/J1P6rpDo8KnoKX0Xsfwe+0nU9FUgHdvjodduxJyWF1QkJ8PTTYDNE\nwxbZSsAxYm9oaKClpaWrDk3RC5ELp1LYPU159FTYnVWdgmoEpug5Oi3smqbFAL8B3gTQdb1R13Xv\n8sq8IC4ujsfMZoiNhWuuAQOhdsyIARGxA9TX13fVoSl6IdJf70zE7s6KcSbsKmJX9BS+iNhHAEeA\nf2iatlnTtDc0TYvywesaEh8fz56KCnjuOfjpJ3j11XbbbN26leTkZGvEBCJiBzVsI9CQwi4v8h0R\n9v79+wPOI3ajVEdQwq7oOXwh7MHACcAruq5PAmqAexw30jTtWk3TNmqatvHIkSMd3llcXJzo8Lhw\nIZx1Ftx7L1hutyXZ2dl20Tq0RezKZw8spBUzbtw4oGPCHhISQnR0tNdWjBJ2RU/hC2EvAop0Xf/J\n8u8VCKG3Q9f1JbquT9Z1ffKAAQM6vDOZU1xRWQmvvAJNTfDww9bnGxsbyc3NtVs4BRWxBypFRUXE\nxsYSGxtLVFSU18IuRdtZ9akSdkVvpNPCrut6CbBf07QxlofOBLZ39nWdYbeQNXIkzJ4N//qX9fni\n4mKam5sZOXKk3e+piD0wKSoqIjExEYDY2FivhD0mJobg4GBARO6OHntzczMVFRVuhV01AgtQGhqg\nh2oYfJUVcwvwnqZp2UAG8BcfvW472jUCO+kkKCiAQ4cAKCkpAWDIkCF2v9fdEfvevXutNoCi5+iM\nsNt650YRu6vOjqCmKAU0ug6jR8Ndd/XI7n0i7Lqub7HYLBN1Xb9A13Xv2uh5QbvUs2mWlPkffwTg\n4MGDQHth7+6IfdGiRVx77bXdsi+FcxyF3Zt0R3fC7qrqFCAkJITIyEgl7IFIZSXs3w/PPgsbNnT7\n7vtW5SkGPdlPPBFCQmD9eqAtYh88eLDd73V3xJ6bm8vu3bu7ZV8KYxobGzl06BDDhg0DOhexG1kx\nrhqASVS/mADFEmCi63DttdDc3K2777PCbo2ewsNh0iQ7YTeZTNgt0NbVMWTtWstfuz5ir6yspKys\njKKiItUnpAc5ePAguq77zIpxnLfrLmIHJewBiyXA5MYbYcsWeP75bt193xd2ED77hg3Q1ERJSQkD\nBgwgKCio7fk332ToffdxAt0Tse/duxeAmpoar3uAK3yHzGGXwt6/f/9OCbtjh0cl7AqnSGG/6SaY\nMwcefBAsutAd9DlhDw4Oxmw22wv7tGmiKVh2NgcPHmxnw7BqFQCD6J6IvbCw0Pp3286Ciu7FUdhl\nxO7uLqqpqYnKysp2VgzYV596KuwqKyYAkVbMkCHw4ovi7zff3G1ZMn1O2MFgIctmAbWkpMR+4bSq\nCr77DoCBKGEPJGRWkq2wt7a2Ul1d7fL3jLxzozvFsrIyNE2zq3B2REXsAUpJCYSFidYnycnw2GOw\nejWsXNktu++zwm63kDV8uLgyrl9PSUmJfcT+zTfQ2AjAALrHirEV9v3793f5/hTGFBUVERUVZRXe\n2NhYwH31qW3VqcRI2EtLS4mNjbW3/RxQc08DlIMHYfBgsMyD4LbbID0dbrlFZMx0MX1S2NtNjdc0\nmDYN3UjYV68WV82wMAabTN0WsR9//PGYTCYVsfcgMtVRDluRwu4u5dFI2I0GqbvqEyNREXuAUlIi\nhF0SHAxLlsDRo1YHoSvpk8JuWN590kloe/YQ29TUJuytrfDFF3DuuTBgAIOCgrolYi8oKGDUqFEM\nGTJECbsv2bdPfFn+/W+PNrfNYQffROyOHrsrfx3UFKWApaREuAi2TJ0qzuHzzuvy3fuPsFt89pOw\nKU7asAEOHxar0gMGdGvEnpKSQmJiorJifMlnn4kK48ce82hzXwq7UYdHT4W9paVFtbIINKQV48ig\nQd2y+z4t7HZT4088kdagIKZhU5y0ahUEBcHMmTBwIAN0vcsj9oqKCo4dO2YVdhWx+5AvvxQ/v/sO\nNm50uWlLSwvFxcXW4iRoE+eOCHtISEi7bCxPhR1Uv5iAoqlJWC5Gwt5N9Flhbzc1PiKCsuRke2Ff\nvRpOOQXi4mDAAOJ1vcsjJ5nDnpKSQlJSEvv371e34b6grk4I+uWXg9ksJmi54NChQ7S0tHQ4Yg8N\nDSUqyn6sgOOivTfCrnz2AOLwYfHT0YrpRvqksBstZAHsGzKEKcDghAThZWVlwdy54skBA4hraeny\niF1mxIwYMYLExERqampUtOYLvv9eiPv8+XD99bBiBezZ43Rzxxx2aBNZT4Q9Pj7euugqsbUAW1pa\nKC8vV8KuaI/MYVcRu3c4m0G5PSaGKCC6sFAsmoLw1wEGDCCytZVWNznMnUUKu7RiQOWy+4QvvxR5\nwaeeKlLHgoLEFC0nOOawgyhui46O9ljYHbHNxpKFTu6yYtTc0wBEVp2qiN07jDIUADaGhACg/fij\n8NdHjYIxljbxAwcCENbFX7DCwkKioqKIj48nKSkJULnsPuGrr+C00yAyEoYOhcsug7feAoM5pGAc\nsYNnHR6dCbutFeNJ1SmoiD0gURF7x3AWsW+rquJoSAisWyeGb8yZ01YgYGkKFtHFEXtBQQEpKSlo\nmqYidl9RWAi5uWIRXHLnnVBbCy+/bPgrRUVFhIaGkpCQYPe4J43AXAm7POeUsCucIiP2bsqAMaJP\nCrszj/1gSQm7BgyATz8V00ukvw5WYY+sqenSY5OpjiDSLlWRkg/46ivx89xz2x4bNw5mzYIXXhDe\nuwOOxUmSzgq77PCohF3hlJISiI+H0NAeO4Q+KezOpsaXlJRwMDlZNNoxm2H69LYnLcIeXV/fpcdm\nK+whISEMHjxYWTGd5csvISUFffRosrOz27KM/vhHOHIEli5t9yuOOewSd8Ku67pLj721tZWKigqP\nhV1OUVIL6AGEsxz2bqRPCrucGm/rsTc0NFBWVkZlWpp4YOZM+yumxWM3d6Gwl5eXU15ebhV2wK9z\n2SsrKz2eSNRhGhvh229h5kxeefVV0tPTee2118Rzp54KkyeLKTW2NQ04F3Z3rXurqqpobm52GrGD\nCCg8FfbQ0FDCw8NVxB5IOLYT6AH6pLBD++rTw5bc0dYTTxS5645j6aKjaQ4KIqapqcuOSeawjxgx\nwvqYzGX3R6666iomTZrUtT3n//c/qKmhcOxY7rjjDgBefPFFEbVrmmiFmp8vhhlY0HWdoqIiu+Ik\nibuI3ag4SWIk7PLu0RWqX0yAYdROoJvps8Lu2AhMzjodkJQEP/wAZ55p/wuaRm1kJHEtLfYVqz7E\nNtVRItsK+GORUn5uLvv27uWGG27ouvf35ZfoISFc/NJL9O/fn7/+9a/k5OTw/fffi+fPOUf8XLfO\n+itHjx6lsbHRqRVTUVHh9BxwJey2azuedHaUqA6PAYSuKyumMzhG7M5mndpSFx3NALquJ7szYa+p\nqfHLL/b9eXl8ERLCsmXLeO+997pmJ199Re6AAfySn88777zDrbfeSv/+/XnppZfE84MHw/jxdsLu\nLNURhLDruu70/8OTiL20tNSjqlOJitgDiMpKqK9XEXtHcSzvlsI+xMUH2mA2d+mwjYKCAvr162f3\nhffXXPb6Y8eY3djIacHBTD/lFG688UYKCgp8u5OiIti6lbeKi7n77ruZMWMGkZGRXHHFFXzyySfW\nuzRmzBCVqZb1E6PiJIm7tgLeWDFdIuwtLXDffcJeUvQ9ekEOO/RxYTeK2AdaFkmNaIyJ8WrYhq7r\n/PnPfyYvL8+j7WVGjG2KnU9y2V95BWSE2ks49umnRAIRdXW899xzaJrG4sWLafbhNPajlruAgxMn\n8phNR8frr7+e5uZm3njjDfHAmWcKUc/MBNxH7ND9wu5xVsyWLfDXv4rWrl1cc6HoAmQOuxL2jiE9\ndumVlpSUkJCQQIil+tSI5rg4r6yYY8eO8eCDD/LMM894tL1tqqNERuydEvbXXoO77+6WySue0ipb\nNgDDa2p45ZVXyMzM5C9/+YvP9pHzzDMc0DT+tHKl3f/r6NGjOfvss3nttdfEheTUU0WLAYsdU1RU\nRFBQEIMMCkQ8FXbrouiPP4q+NLTN2+1SK0YuAufmwjXXdNuMTIWP6AXtBKAPC7vs8FhVVQVgPMTa\ngda4OKKBOg9T9ORrr1mzxqPFQSNhHzJkCJqmdc6KOXQIamqgq3xsb9F1Yr7/Hmseyo4dLFq0iMsu\nu4zHHnuM9evXd3oXdbW1jD96lEMTJzLiuOPaPX/TTTdx4MABPv/8c4iOhpNOgnXraG5uZsOGDQwd\nOtRwYdNd6165KBocHCweePJJuPpqq8DKO8UuFfaoKPjzn2HZsl53p6Zwg7JiOodjv5h2I/EM0C1F\nSs3yw3eDFPYDBw6QnZ3tctvy8nIqKirsUh2hrUipwxF7S0tbG9BXX+0dEVxeHv0OH2YJoPfrBzt2\nAPDSSy+RlJTEokWLOp0CWbB5M/GANm6c4fOzZ89m+PDhvCxbCsyYgb5xIwvOOYe1a9dy1VVXGf6e\nJxG7nQ1TUgIVFaKtAeJO8ejRoxw7dsxtAzCJzIrxKHMoK0vMxrz3XtES44474KefPNqPohdgO8S6\nB+nzwi599pKSEpcLp4C1+rTFQ2G3nWa/Zs0al9saZcRIkpKSOi7sR4+K4puJEyE7u3d8yS2fxbdh\nYZCaahX2mJgYPvjgA4qKirj66qs7lQJZYon6o9PTDZ8PCgriuuuu49tvvyU3N5f9Y8ag6Trad9/x\n2muv8fDDDxv+XoeEHWDzZkCcd3v27KG1tdWriL25uZl6d8Vxra0iYs/IAJMJ/vlPGDYMLrlEnAeK\n3o8sTnJoZdHd9Flht80p1nXdo4g9yPK8LiNgN8iIPTQ0lC9sPGUjXAl7p0bkHTokfv7hD9Cvn/Db\nPWXbNrEQ52u++IL9MTG0JiWhjR1rFXaAk046iccff5yPP/6YJUuWdHgXlVlZAAz61a+cbnP11VcT\nEhLCLbfcwgk33EA18Le5c7nWsTjNBrPZjKZpTitm7YRd1w2FXWb/eCPs4EG/mMJCqKoSwi52AB99\nJM6BxYvbVdd2KYsXi3UdhXf0ghx26MPCbhuxV1RUUF9f71bYg2VEf+SIR/uQEfvMmTNZv359+zmr\nNsgvuzNh73DELoVl1CjRqnbZMvC0jP+tt0TqnC8XXauq4L//5fvoaFHZOXYsHDhgt48777yTc845\nh9tvv52tW7d2aDeNlkyk6IkTnW4zcOBALrnkEtatW8egxERMp5/OsO3bXb6uyWTCbDZ7FrHLnGSw\nCnt8fDxNluplb4XdbWaMXDi1vUuZPBmeeAK+/tp6DN3CV1+JzpndMPzdr+gFVafgB8JeWlrqUXES\nQIilxNzkQqBtkRH7pZdeSmtrK19//bXTbQsLC4mOjjYsMU9KSqKqqqpjjaBs06euu04IzTvvePa7\ncsJQcbH3+3XGt99CUxOrW1oYOnSoEHYQWRwWTCYTS5cuJTY2lksvvZSaDnTUDCkqojooyK1X+fjj\nj/Pwww+TmZlJ5Hnnifzvfftc/o6rtgJ2wi4/+4gIu4hd4vOIfcsWYcGMH2//+IUXip8//+zR/jpN\nVZXoc19dLcZLKjxHReydw7bDoyfFSQDhAwfSCAQ7Gc7giBT2008/nYSEBJc+u1EOu6RTuezSihk8\nGCZNgilThB3jiX/dFcL+xRfoZjOrysraInaws2NARNPvvvsuubm53HbbbV7vxlxaSpnZ7NarTElJ\n4ZFHHhHiOWOGePDbb13+jjNhb2xspKqqqr2wn3aa+AwPH+5aYc/KEoNhIiPtH09OFutD3SXslp5H\nQO/JxOoL9IIh1pI+K+yhoaH069fPTtjdReyRUVEcAUI8jJylFRMTE8PMmTP58ssvaWlpMdzWKNVR\n0qlc9pIS8UXv10/8+/rrYft20Q/HFboOshLUV8Ku67BmDU2nnUZ1Q4MQ9pEjISSknbADnHnmmdx3\n3328+eabrF271uPd1NTUMKShgTpvvyDjxonhBjbtBYxwJuzSamsn7LIP/ObNdgurnmbFeBWxS3/d\nFk2DqVO7T9gt60WcfLJomezhHW7A0wuGWEv6rLBDW5GSLC13J+wREREcAcI8FPaqqipMJhMRERHM\nnj2b0tJSfjb4cum6TmFhYbtUR4mM2Du0gOrYAvTSS0WveXeLqEePtlUu+krYs7KguJhDU6YACCsm\nOBhGjzYUdoCHHnqIQYMG8cILL3i8m135+aQAGOSvu0TTRNS+bp3LOxpnrXvbVZ1KYZeTmzZvtovS\nPensCB7OPS0rExaSkbCDuFPbsUPYJF2NjNjvv19EoZYCLYUbekkOO/RxYZf9YkpKSggNDbWmsjkj\nODiYI5pGuIdfjurqavr164emaZx99tmYTCZDO6a8vJzKykqnEfvQoUPRNK3jVoztiRIVBb/9rfiy\nubKUpA0DvhN2y3vPtwiutS2uQ2aMLaGhoVxzzTV88cUX1swhd+zftIlIIEr21veGM88UkdO2bU43\ncRaxGwp7SIi4K0lOthN2s9ncVsTkBo8idksWEE7SO5k6VVysNm3yaJ+dorAQwsPFBW3MGHj//a7f\nZ1eTnw8PPSQWhFevFp93WZlnlqan39te0k4A/EDYpRUzePBgQ3/bkWNBQR6Px6uqqrJOwImLi+Pk\nk082THt0leoInSxSKilpPzvxuuvE6D9XXziLDaOHhPhW2E88kUJLpoidsO/eLY7JgGuvvRZN09oG\nZLihzCJecZMne3+Msl2zCzvG2UBrQ2EfNEgsaE6aZCfsnvrr0DZFySNhdxWxA2zY4PF+O0xhobiQ\nmUwiE+s//4G+3sTuT38Sf266SYzMzMgQ4+ucFLJZ+flnSEoSn4E7ekk7AfChsGuaFqRp2mZN07pt\nGd1W2N0WJ1koDw0lysMUrurqauuXEmDWrFls3ryZYgehdJXqKOlwLrvRNJYJE8TJ4yp6s0TsG1ta\naPXFBKfSUli/HmbPtnZPtH7mY8eKHGsnHQmTkpI477zzeOONN2hwIv621Fmi/8iOROzDh8Pxx8M3\n3zjdJDY21jopyRY5rMVO2OVnP2kS5OeTEBYGeCDsZWWiLcBttxF2zTWsNplY+PLLcMstxttv2SL2\n5WwAckICjBjRPT67FHaAhQvFzw8+6Pr9dhUtLSIoWbRIpOb++KOoD/j1r9vm6TpDtsf47DP3+5FW\njItGhN2FLyP22wDj+/EuwtZjd+evSyrDwohoanIaXdpSVVVFP7loiShjB/jyyy/ttpPtBlwJ+3FD\nhjA9O1ucWJ7S1CQE1ei9jR0rFlGdsWcP5eHh7GxtpcUXwr52rRDvWbM4cOAACQkJhFlEzllmjC03\n3ngjR48eZYUnfq20bFx8ni454wyxuOxkoVtado4R9KZNm4iNjW3rCuko7ECsxX92K+zPPgsPPghv\nvw3//S9JJhNhtbXw4ouQk9N+e2cLp7Z01wJqYWHbZz9qFPzqV33bjlm/XnyPzj8fhg4V7+fii+GC\nC4QYu6prkXdS7i4AIM6XuDjRUqCH8Ymwa5qWCMwG3vDF63mKjNi9Efaq8HDxFw+KlGytGIAJEyaQ\nmJjImjVraGlpYeXKlZxyyik8+uijjB8/3rnHn5XFcz/8wIMlJfC3v3l0nEDbKrtRFJeWJnLHnXmE\nBQXsDw6mGAg6dKjzPWY2bBD53JMnU1xcbD92bswYsXDpQtjPPPNMRo8e3dbbxQVRhw6J/yebi6pX\nTJ8uFhmdFEc5ayuQmZnJtGnTMJksXwsDYQ/euhWz2ew6I6a1VdQazJxp7TMzLzmZR08/XczhffVV\n++0bG8VF2hNh37evLQW2K6ipEQvvthfVRYuEwBldkPoCq1aJRX45bUsi1zOkeBshn9uxw219BAcP\n9gobBnwXsf8NuAvoxppnIewtLS0cPXrUY2GvkTnCHgi7XDyVaJrGrFmz+OqrrxgzZgzz5s3j4MGD\n/P3vf2f9+vXtPf7WVhG5TZ1KdFMT+4FmbzxSV4sxY8cK8XJ2B7BnD3lNTRQDpsZGz6tVnZGbKwQ8\nKIgDBw7YC3tkpLh1dyHsJpOJG264gczMTLbYzCd1pKqqikF1dVQlJHT8WH/9a/FTjs9zwEjYy8vL\nycnJ4eSTTxYPyOZr8rMfNkzYIZs3M3/+fM4++2zn+//uO+FJ//731ofMZjPFjY2i78vSpUJAJdu3\ni7szZwunku7w2WVGjK2wX3qp8Nv7atS+apVo7WzJTrLiTtibm8XFTKa7uihQBHrFEGtJp4Vd07Q5\nwGFd110u12uadq2maRs1Tdt4xMOSfnfY3g576rHXRkWJv3jQL8YxYge4+OKLqa2tJSEhgQ8//JC8\nvDxuvfVWuwsAIFbSzzoL7rwTZs1i7TPP8DWIk8jT6Nm2OMkR6T8b2TFNTej79rG9oQHrakBnF1Bz\nc0XDL0S3y6FDh9o/7yIzRnL55ZcTERHBK6+84nSbXbt2kQK0DB/e8WMdPlwseDnJ9Tdq3fvjjz8C\ntAm7bL4mP3tNsy6gvv7661x55ZXO9790qUhJPf9860PWuac33CBaFdh61u4WTiUnnCAEtivtGCMb\nbNAgkUb6/vu9o7uoN+zeLc7LuXPbP5eQIKwZZ8Kelycs2wULxPnkzo7pJe0EwDcR+ynAeZqmFQLL\ngDM0TXvXcSNd15fouj5Z1/XJAyxdFjuL7e2wpxF7gyX1zNOI3VHYzzrrLEpKSli/fj2XXHKJccpb\nTQ2cfrpYpHn9dfjkEwampbEZCC4v99xnlxG7kRXjytfevx+ttZU9gHVPnRH2ujrxhU9NpampicOH\nD9tH7PJ4du506muDENSFCxfy7rvvOm2vsGvnTpKBsDFjOn68IOyYH34wFCKjiH39+vWYTCamTp0q\nHjC6W5o0SURwjY3O91tTI1JR588X1pUFa0/2k08WLQNs7ZgtW8S2o0e7fk9RUeJ3uzJid7a+cdll\n4jnLlKo+w6pV4qeRsIOI2p0Ju20K6jnniEwrS5+gdvSSIdaSTgu7ruv36rqeqOt6CrAA+Jeu64s7\nfWQeYBuxeyrs9V4Iu+PiqWTQoEGuUyvvvltECmvWiCENmkZiYmLbYApPmzm5EvaBA6F/f2Nht2TE\n7AGsDYo7I+x5eeLEHTuWkpISdF03Fvb6evtydANuvPFGamtrWbp0qeHzBzdtIhSIcWdLuGP6dPGe\nDXLnpbCNUE9CAAAgAElEQVTbpjxmZmaSnp7e9v/tTNibmlwvWn/yiRD33/3O7mHreDxNE1H7pk1t\nAr1li2jLbDAYpB1TpoiIvasi58JCsfjneM5dcIFY83BcH+jtrFol7m6dFbulp4v/T6Nkiqws4c2P\nHSvWSyornbfNlg3j/EXYe5KOCDsxMTSCW2FvaWmhtra2XcTulm+/FVNvbrtN+HoWhg0bxl6zmVag\n5ZdfPHutQ4eEL2gT+VnRNHHCGomMRdgLgHjZUMqbbBxHZIOv1FRrqqOhFQNu7ZgTTzyRqVOn8vLL\nLxv2a6+2LHiGWWyfDjN9uvhpYMc4RuwtLS38+OOPbTYMOBd2cH1hXrpUpCXK/Vuwm6K0eLGIvuXg\nFE8yYiRTp4pUStsCNF9SWCisLJODNJjNIkhZtqzv5LRXVMB//+s8Wgch7M3NxudtdrY4r0NDRX1E\nUJBzn12mOvqRFWNF1/XvdF2f48vXdEVHhD0iMpIyk8mtxy47EhpF7E6prIQrrxSLjA6zP0NCQnji\npZfYBeQtW+bZ61kKZOrq6igsLLTOd7XizNcuKKDZZKLabGZMejrHTKbORey5ueJCMnq0NYffMGIH\nt8IOcN1115Gbm2vYnqFl927xFyftGTxm/HhxUTQQ9n79+mEymazCvm3bNqqrq5k2bVrbRkZ3S6NH\nC0F2Juz794sL++9+1655mRR2XdeFSF52mfDZs7OhvNz9wqlEWkVdZcfYpjo6cvvt4kLkTWaXAU1N\nTZxyyint0oZ9zldfCdF2Jezygmpkx8hpViC6jJ50knOfvRdVnYKfROz9+/dvy6l2Q2RkJEc1zW3E\nLjs7ehWx33GHWDT95z8No+zFixdzLDmZsNxcfnDXxAugpIT6/v0ZO3YsI0aMICoqioyMDBYsWMBj\njz1G5bBhYpHP8b3s2cOh8HBSRo4kMTGRA7qO3tmIPSUFIiKsEXs7YY+LE/aQB8I+b948QkND+cCg\n6CVMXoBkgUxHMZnglFMMM2NMJhMxMTFWYc+0+MbtIvZ+/exTLk0m8UV3JuzvvSeEz8GGASHsTU1N\nbQVa118v1i7+3/8T//Y0Yh83TpT7d9UCqithT04WawdLloiLkRG67raH++7du8nMzHQ7lazTrFol\nFkhPOsn5NqNHi++qo7AfPSqCIdsL7syZwkIz0g4l7L5Ddnj02IZBNAI71NrqVthlZ0ePhX3NGnjz\nTbjrLlEA4YSMyy/nOOD6BQucTvGRNB84wL9ycigvL+e5557jxhtvZNiwYWzYsIGHH36YD2U/FEcx\n3bOHAuC4445j2LBhHNB1mt3l4Lpixw67jJiQkBDjPG4PMmNAWCGzZs1i+fLldt0yKyoqGFhbS5XZ\n7Jsij+nTxfEYjJWz7ReTmZnJ4MGD7QvMnKWuTZokrBPHuyddFxf06dMN/dx2jcAmTRLnybffiuh+\nwgTP3lNIiMiO6Qphr60V3wtXhWF//KNoLmc0HUvXRQbJ2LEuF5jzLENUdu7c2ckDtvDDD+3vwJub\nxXdy1izXaxdBQeLuzlHY5YxjW2E/5xzxHo2qmv3ZiukJ4uLivBL2yMhIDuk6uocRu0dWTFmZ8B/H\nj4dHHnG5aZglehhYUsI111zjdC5oWVkZtQUF7K2vZ82aNdx+++08++yzfPHFF+zevZsTTzyRf8so\nwUFM9YICttfXM3LkSIYNG0YxoHfUimltFdkuFqtFpjqaHD1YaBN2Dxb2Fi5cSElJCf/973+tj+Vb\nujo2Ovr3HUX63AaZHI7CfvLJJ9sviLsS9upqsThuy8aN4s7GJnfdFsNGYDfcIH6OHu1dMdbUqfDL\nL0K8fIlRDrsjkyYJv/nvf28v3k89BR9+KAp5XOR8S2HPtRnO0mE2bxZ1CxMn2vdzycwUtRsWGyY3\nN5fHH38cw1RrmRlje95Kobed4HXiieIOwMiO6SVDrCV9Xtjnzp3LrFmzPN5etu71mRXT0CD6aRw5\nIiI2d5Gm5Zb7/tmz+fjjj3njjfbFupWVlZx39tmYW1s563e/s7cILKSnp7Nu5070yEj7BdSKCrTS\nUvJbWznuuONITEykGAg+cqRjMzP37ROr/ZaIvV3VqS1jx4pbdA8qI+fMmUNUVJSdHZOfn88IIGjU\nKO+P04gpU8TCl4HtJVv3Hjp0iD179rT/jF0JO7S3Y/75T2GRXHKJ4aEYjsebP19YWN42O5syRdg4\nHagEra+vZ+XKlcYBhaetHO68U9gUtlbaunViDOMll4jmWi56y0hh379/f4ema9nx8MNCTGNixAXn\n6aeFQK9aJe5uLIVkL730Eg888AAjR47kr3/9K3V1dW2vkZ4uWg7YBj9ZWWJ9xXaNxWQStSmyvYYt\nvWSItaTPC/uLL77InXfe6fH2kZGRHAG0igqX/WKkFeMyYm9qEreea9eKW9MTTnB/AJZGT2fExnLW\nWWdxyy23MHPmTG6++Wb+9re/sWrVKubOnctBS3XmqFNOMXyZ9PR0Dh89StOoUfYRu6Uh2R7arJhi\nwOSB/WSITUYMOClOknixgBoZGcn555/PihUraLREfrtzc0kEohxHw3WU8HAhgk4yY44dO8Z6S5Mn\nj4V93DiRAvfAAzBnjug5sngxvPuuSAl0rG60YBixR0SIPibPPefd+5ILqB2wY5544gnmzZtnPPjE\nU2E/5xxhHT3zjBDRvXvbLJi33hKfyWef2VfX2iCF3fHvXvPzz0LA77xTLCZfeKGwQufNg08/FZOv\nLJ97bm4uo0eP5vTTT+e+++5jzJgxLF26VCQkGFWgZmdDejpHjhzhoosu4pAMVmbOFIGL3FbXxV3K\nmjUim6iX0OeF3VsiIiKwunEG3qvEbcTe0gKXXy5OoOefhyuu8PwgJk1Cy8rinXfeYcGCBRw9epR3\n3nmHP/zhD5x33nl8//33vCItHSfd/jIskf+R+HhDYZce+6BBgyiRUURH7BgDYXcZsYNHwg7Cjjl2\n7JhVZI5lZREEhLgr1PGG6dOFTWIbodFmxWRmZhIaGsoJthfl+npx52Ek7GFhwmdOSBC+6o4dohAt\nLs5550Zc9GQ//njvuwGOHClqGLzMjKmurub5558H4N1329UQCmEPDXW/AKhpQky3bRPn/0UXiSBn\n5UphKS1cKPx6WRzkQF5eHlMs7RE6Zcc8/LC4O7j1ViHgH34oLpKrV8OuXXbZMLm5ufzqV7/is88+\n47vvvmPQoEH8/ve/54YbbmizW6RYNzWJu6H0dNauXcsnn3zS1q5btpL46ithx517rmi5MHy4SHPu\nJQSksFvjVhcRrIzYBy9dKvKNbTsk6rrwR99/H/76V5dfaEMyMiAnh0Gxsbz99tts3LiR8vJyjhw5\nwvr168nOzuZsuZjm5Es20XIy7goJEccmBcOS37zXZGL48OEEBwfTIBc6OyrscXGQkEBVVRXV1dXO\nhX3YMLGtbHXqhrPPPpv+/fuzzJL+2SAX0zqb6mjL9Onii+oQ3doK++TJk+2zqly1cgCRypqZKTIk\ncnKEiOzZI6pKneDxeDxP0DRxJ/Lll/D5586rIR1YsmQJx44dY8qUKaxcubK9DeIsh92IBQvE//eC\nBeJzeOedtsrZ6dNFqb6BHVNVVcXBgweZNWsWmqZ1fAE1M1OI6113gQy+NE2kZH73nbi4WFoOV1dX\nU1RURKolODn11FP56aefuPDCC1m9erW4y0pJEYviINaUGhth4kSyLGK/QV5EBw8W398XXhB3b5mZ\nIrD7+WfPF8C7gYATdmnFAC6FvaqqiqFAzF/+IkQ8KUksnjzyCNx4o2gVcP/9cM893h/EpEli4cvG\nG9c0jYSEBE466STGjx/vVlxiY2NJSUlhg/xyyshnzx6qQ0OJSU4mJCREvLYU4o4I+44dIhLXNOep\njm1vQtgRn30mol43hIaGcvHFF/Ppp59SW1tLkMzc8aWwS7F1sGNiY2Opqalhw4YNxjYM+DR1rTPC\n3traym7Hxdrbbxfic/75QmD/8Ic2YTKgoaGBZ599llNPPZVnnnmGmpoaPnPsMe4q1dGR0FD0W28V\nx3D//XDeeW3PBQWJKPbLL9s1n5PWy8SJExkxYkTHI/aHHhJ3Ojfd1P65U04RQZelkZzcZ6pN0ZvJ\nZGL69OkUFxeLPvy2rQVsWgnIltwbN25se/25c8Xd2nnnie/dLbd4VjXcjQScsNtF7C6KlKqqqrAu\n4b30EjzxhPBEH3tMRPC33SYmsnQEmbPsqoJRiouL2/T09HS+lWIt7Y+CAoqCgznOJuUuXH5ZOxqx\n29gwYFB1asuCBaLrpIfFJwsXLqSmpoalS5cysLaWVpMJZD90XxAXJyIrA2EHaGxs7BZh92juqRNu\nuukmRo8ezWbb8+Xcc8Wd2uefw29+I0a+TZokrIn0dJg9W0zaevJJqKzk3Xffpbi4mHvvvZfp06cz\nfPjw9nbM3r1e9cD/V3o6ZwO5chiHLQsXijuJTz6xe1iK7PHHH09qaqrXwl5ZWUndV1+JNNF77hEF\nY26Q+0h1qGaWdmZWVpb4TubnCwspO1tYUqmp1og9KyurrQbhvvuEDfXhh+LOpBcScMIeGRnZ5rG7\nsWLSQkPFP2bOFP1ffvhBRNL/+hf83/91fAV81ChxQrqIsCgpEV9SS9RtRHp6Ot8WForxdzL637OH\nvOZmO2EfnJTEYU3zXtjLysTFzyYjBlxE7CCanw0YIErPnbF5s7jrqa/nN7/5DUOGDOGJJ54gBahL\nSBCLk75k+nRxy2yTM2/bO9+u4hS6RNjDwsIIDQ31WtjfeustXn31VXRdby/EISEielyxQkSQr7wi\nIuXkZPHvlSvhnntovecennzySSZNmmSd3XvZZZexdu3atkXBujpxbnsh7JuysvgG2GR0Hk+eLNYC\nHOyYvLw8NE1j5MiRjBkzhry8vPYV1U6oq6tj6pQp7Fm8WOSLX3+9R7+Xm5uLyWRilEO2Vbpl0XTL\nli3iYtjaKgQ7KwvS0jh87BglJSVMmzaNpqYmtsr+/uHhIljoxQScsEdERFAOtAYFubViUoODhcjY\nrnYPGCDEyxMf0hmygtGVsDsOsTYgIyODJl2nLilJROytregFBexobGTkyJHW7RITEynqSJGS9D+9\nidiDg0XK26pVIt/biNtuEyL05JMEBQUxf/589u7dywjwrQ0j+fWvxRqEzYBr2bp3xIgR7esgPLhb\n6gjWRmAesnHjRm688UbOPPNM5syZw7Jly+wKuuyIixNC9/LLIor/5RdxUb7ySlrffJPS/Hzuvfde\na67+4sWLaWlpYfny5eL3PclhdyDfMgpxu1G/Ik0TUfu//932eSKEffjw4URERJCamkpdXZ3HIyP/\n9Kc/kZiXx7jSUhE1G/VQMmDnzp2MGDGiXXV6fHw8SUlJ4k7INjMmKwsmTrTaMFdffTVg47P3AQJO\n2CMtgzYaoqPdRuyjNE2c6L6OIEHc+hlVMEo8aNovI46S2Fgh7AcPojU2WjNiJDLl0WthN8iIiYmJ\nIcrd7e+CBSICNMqKyMwUZf6DBomF5127WGi5lR8BhHe2+ZcRslDJpr2AjNiNagQoKRH+rIu7pY5g\n1wjMDUeOHGHevHkMGjSIZcuW8dvf/pbi4mK+dzI8xBn6H/5AcGMjD8bHM2/ePOvjaWlpZGRk8N57\n74kHOjCOUNoqhsIOQthbW4VlYfM7xx9/PNBmjXhix2zZsoWnnnqKB4OD2Q+UXXSRx8eZm5vbzoaR\nTJo0SUTsKSliEXbdOvH/b+Ovz5kzhwEDBihh781EWK7ydf36ufXYR7S2itvJrmDSJOFFW9IT22Fp\nAOaKlJQUoqOj2WkyiawMyxdM5rBLpLBrsuzZU3Jzhddo+bK7LE6y5ZRTxIKekR3z5JPCYvrhB/Ha\nN93E1ClTGJuSwhB8WJxky/DhImPjpZesi7qyJYJTYe+Cnh+eCntzczMLFy7k8OHDfPLJJyQkJDBn\nzhz69evH+15OMVp74ABrgOsaGwlyyJ5ZvHgxP//8sxBoKexe9OhxGbGD6D46caLVjtF13U7Yx1h6\n7rvLjGlububqq6/m+P79+XVLC28BObt2eXSMra2t5OXlWfflSEZGBjt37qS2vl4cq1xQTk8nKyuL\nwYMHM3DgQCZPnmy/gNrLCThhlxF7bVSUayumspKkxkbhh3cF7hZQPRAXk8lEeno6P1ZWisjIUurs\nTNhDy8s9To0DxF3A8cdb71hc5rDbH5hxVkROjrAJbr5ZfK6PPw5r16KtWMGS++8X23R0gLUrNE0M\nkc7NtbZ8SE1N5Z133uEKo/qDHhb2+++/n2+//ZZXX32VE088ERDn7QUXXGBX0GWLrutcdtllTJgw\ngXPPPZdrrrmGRx99lPvuu4+3ExKIqKoSRVQ2LFy4EE3TRNReWCjuUDzsdVJdXc3BgweJiopi165d\nbQuLjixcKPL8Cwo4fPgwlZWVVpEdOHAgsbGxbiP2v//972zatIl/XnQRJl1nJZDjYdXtvn37qK+v\ndxqxZ2Rk0NrayrZt24QdI9+HxYqRd8VTpkwhJyen85Wy3UTACbuM2KsjIlwKe1BFBdEtLV0XsY8f\nL1KkjHz26mqxOu+BuKSnp/ONzLH/4gtaNY2q2Firhww2EbuuezcI2SYjBtxUnTqyYIG4iHz6adtj\nTz8t5qPefLP49403imrd229nusxF7gqPHURhyVVXiWPYsAFN01i8eLH1fLCjB4U9Ly+Pp556iuuu\nu47LL7/c7rlFixZx7Ngxvjbow/LRRx/x/vvvExsby9GjR1m1ahWPPPIIv/zyC7956CFxh/jss3bW\n39ChQznzzDN599130WUOu4dpezJanzlzpjUqNmTBAvFz+XK7jBgQKb6pqakuI/Y9e/bw4IMPMnfu\nXCYXFaGnpFAQHS2E2AOcZcRIZGaMdQEVYMgQmmJj2b59u1XYJ0+eTGtrq8t5vb2JgBX2yvBwkTng\npGFV/7Iy8ZeuEvbwcJEfbhSxu5qc5EBGRgaba2vRNQ127uRIWBhJDsccFRVFpfTFPc2MaWgQ9o7l\nC9HS0sLBgwc9i9ihLStC2jH79om2tldfbc0vJihIpI6WlIg8bOg6YQchbEOGiCphZxGmrneZsMfE\nxFBaWupym08s6YEPPPBAu+dmzJhBQkJCOzumtraWO++8k4yMDL777js2bNhASUkJDQ0NHDhwgJtu\nvllUiubmitJ3GxYvXsyePXuozsnp0MLp+Za5rjucVRunpIj6j6+/bifsIOwYZxG7rutcd911BAcH\n88pTT6F98w3ahRcybvx4nwl7SkoKZrPZXtjT09m5cyeNjY3WQkBZKdtXfPaAE/aQkBCCg4MpjokR\nkbGTBcVBlpYCXWbFQFsLWEe8SLdLT0+nHqixXAQKNM3OhpG0yIuEp8K+e7dID7R8IY4cOUJLS4vn\nwq5pIlr79luxlvHcc0I077jDfrspU0QB2MGDoly/K/tZx8SInj45Oc5rELpwxNnUqVPZt2+fc08a\nWLlyJVOmTCHRIJc/JCSE+fPn89lnn1krowGeeuop9u/fz/PPP0+QTcQdGhrK0KFDRSbMJZeIIrtn\nn7V7zQsvvJDw8HBadu+m2Yv6ASnSs2fPxmQyuXxPTJwIO3aQl5dHaGgow22yzFJTUykuLja8k/ng\ngw9Yt24dTz75JMOys0Ux1AUXMN4i7M46o9qyc+dO+vfvT4IMJhzQNI2MjAwh7BMmiHNw8mRr/rqM\n2AcPHkxiYqIS9t5MZGQkhbJZk+y77MBgOSygKyPIjAwhtI6LuO5K2m0YP348JpOJAxYrY0d9vaGw\nB8svk6fCbpARA25y2B1ZsEBcHJYsEX8WLTJenHv8cXF3MmJE59JIPWHWLNFa94knREqgI104MGH+\n/PmYTCbDASMgPuOff/6ZCy+80OlrLFy4kLq6OmvV6N69e3nyySe59NJL+fWvf+185yEhbeX2NouA\nZrOZS+bMIbaujkf++U9iYmJITU3l9NNPZ8WKFU5fLj8/n2HDhhEXF8fIkSNdC3taGhw6xIFt2xg1\napTdxUf67UZWzssvv8zYsWO57rrrRE7+gAFwyimMGzeO0tJSUTHqBpkR42pGcUZGBtnZ2bSEhYn+\nO3fdRXZ2NqGhoXaLrn1pATUghT0iIoI9lkVUo5FYuq6T1NBARXS0x7myHUJ26XNs3O+FFRMREcGY\nMWPIsXinu3XdLoddEpWSQjN4L+yWE9ujHHZHxo8XhRyPPCLWDO66y3i72Fix0Pr6656/dmd47jmR\no37FFe17inehsA8ePJjTTz+dDz74wDDa/NSyHuFK2E8++WSGDx9utWP++Mc/omkaTz31lPsDuPpq\n0SzLIWr/m+Uuauoll/D73/+e8ePHk5OTw9NPP+30pfLz862WSlpammthtzSH07dvt7NhwHnK4969\ne/nf//7H4sWLMTU1wRdfiBL+oCDRcgM8smNcpTpKMjIyqKmpEW0bJkyA6GiysrJIS0uztuUAYcfk\n5eVZ+/j3ZgJS2CMjIymXC6MGwl5XV8dIoMLJ7ZvPOPlkEaU69mQvKRGRq4f7T09PJ9NysjlmxEiG\nJiVRArTaNjNzxY4d4tbd0ra4QxE7tEXts2cLoXfGpEntBkB3Gf37C28/O1u0mbWli0ecLVy4kN27\ndxtGfp988gmpqakuhchkMrFw4ULWrl3Lxx9/zEcffcTdd99tZ284xWyGa6+Fjz4SldSWi3ycxQY5\n75ZbeP7551mxYgW///3v2bJli2EGDogIe7Sl6dfYsWPJy8ujyVnGVVoaANH797cT9pEjRxIUFNRu\nAVU2hlu4cKGo9K6qEm15wSrs7jJjKioqKCkpcZrqKLFbQLWQnZ1t9dclky1983/xdBh9DxKQwh4R\nEUFtba194x8bqqqqGAnUdPX8QpNJRFHffSf6VEgOHRIRpYcZCunp6XxcWkrZgAGsx1jYZWZMo8xX\ndodNRkxzczOvv/46iYmJDPLgLsKO3/5WvI6byVLdzty5IiPnxRftF9C7WNjnzZtHSEhIOzumtLSU\n//znPy6jdcmiRYtobm5m0aJFDB8+nD/+8Y+eH8B994l+6c88IxY2r7pKFOWA3eLplClTaGxsbCuj\nt+HYsWOUlpZahT0tLY2mpqb2jcokycm0hoczuqWlnbCHhoZy3HHHtYvY33//faZNm8aIESPa2gGf\neSYg0iTj4+PdRuzyYuEuYk9LSyM4ONgq7EeOHOHgwYNWf10ihb0v+OwBK+x1dXVC2HftajcQoObQ\nIYYAdd5Gpx3hiiuEgNtG7R4UJ9mSkZFBIXDe8cdTFBxMUlJSu23kJCWPInZdtxP2l19+mc2bN/Pc\nc8/Z+aMekZwson9vpwR1NZomOgPm5IDNeD5KSoQfbZMu6kv69+/Pueeey/Lly+16pKxevZqWlha7\n6lBnTJgwgbS0NBobG3nmmWestRkeHoDIVMrPF9H7Bx8IkQ8Otmto5UrEZEaMrRUDLgqVTCaqhw0j\nDdoJO9CuGdi2bdvIzs4W0XpLiygamjVLZJIhFjzHe5AZ4y4jRhIWFkZaWppV2GXFqWPELtcTlLD3\nUiIjI9sidl0Hh6ik0XJCNHtRhddhhgwRk3jefrvN7/Uy3U5GFpmZmSQnJxNs0AJBRuxBHiw4UVws\nMoZSUzl48CAPPPAA55xzDhd5UcbdJ1iwQAjdyy+3PSYvql24iLtw4cJ27QFWrlxJUlKStSDJFZqm\n8eijj3LLLbdw8cUXd+wgjjtO3K3s2yfuph5+2O4OccSIEcTHxxuKmFzolBG7FE5XPntxbCxjcS7s\n+fn51j44H3zwASaTifnz54vipsOHRTtoG8aNG0dOTo7LzJidO3cS7NDp1BnWzBholxFjS19ZQA1I\nYbeL2KGdHaNbypVbuzIjxpZrrhEnr+yt4kEDMFtk2bOu605PYinsYVVV7nuly5zk1FTuuOMOGhsb\nefHFF11mFvRJIiPFHdMnn7RNme+iHHZb5s6dS2RkpNWOqamp4euvv+aCCy7w+DO++OKLef755zv/\nf5KQIETdIW9e0zSnIpafn4/JZLKea1FRUaSkpLgU9vzgYFKAgQZ3F2PGjKGhoYG9e/ei6zoffPAB\nM2bMELbfypXiDsphrvH48eOprKykyMUdaG5uLiNHjrRbAHVGRkYGBw8e5NChQ2RnZzN48GAGDBjQ\nbrspU6awd+9e46HYvYiAFHZrxJ6cLBaUHIRds3iFQQbRRZcwc6boQf76620FMl5YMZqmWaMLZ8Ke\nkJDAYRmRuesZY1mU+m9pKcuWLePee+9t1/LUb7jhBjH0ZMkS8e9uEPaoqCjOO+88VqxYQVNTE19/\n/TX19fUe+evdiSyjr5Wpvxby8vJITk6265boLjPmF8toQs0grdE2M+ann36ioKCARYsWie/CypXC\nW3eYJevJAqonGTES297sWVlZhtE6tFlUvT1qD0hht0bsmiaKJxyEPWTfPo4Ckd3VRD8oCK68UgzF\nzsoSloyX4uJO2DVNo0Fm2bhLedy+HT0ujmvuv59Ro0Zx9913e3UsfYpRo8SF9bXXRAuEbhB2EHZM\naWkp33zzDStXriQ+Pt51HnoPMGXKFFpaWuwHfCAi9tEOc2nT0tLIzc112lb4v3K+sEGFqm0zsPff\nf5+wsDBxkdu6VVQ/G1zwxln6oTvz2Zubm9m1a5fHwi6/Pxs2bLBrJeDICSecgKZpvd5nD0hhj4yM\nFMIOwo7JzrbroRFeXMxuoJ8l1a9buPJK8fMvfxE/vRQXGXEY5bBbkRcqD4R9f3Q0efn5vPTSS4Rb\nFq38lptuEncxH38sLLFuEPZzzjmH2NhYli5dyqpVq5g7d67h2khPYrSAquu6XQ67JC0tjYaGBgoN\nsq7q6ur4vriYFpPJbhykJCEhgfj4eHJycli+fDlz5swRowRXrBDBl+3YPQtxcXEMGTLEqbAXFhbS\n2NjosbDHxcUxfPhwli1bZtdKwJHo6GjGjh2rhL03Yk13BCHs1dVtbUuBfiUl7EL8J3Ybyclwzjni\nZGyhrawAABYpSURBVAavrBgQzZgWLVrEaaed5nSbELkY7ErYdZ3Wbdv4uqiI+fPnc7acyu7PnHuu\nSPX705/EBb4bhD0sLIyLLrqI5cuXU1FR0etsGBDFaEOHDrUTMdmh0ShiB+MF1N27d9MEVA8ZYhix\ng7Bjli9fzuHDh4UN09QEb74pmrc5+f+QhVRGyIwYdznstmRkZFgvFM4idhAXvA0bNnjU0qCnCEhh\nbxexQ5sd09hI9LFj3R+xg1hElSeLl+ISHx/Pe++9Z+0zbkTsccfRAOiWYiNDDh3CVF5OdkuLKOUO\nBIKChNcuRakbhB2wDhiJiorirLPO6pZ9esuUKVPs/GSZ6ugo7K4yY2QWTWtqqmHEDkKAq6urMZvN\nzJo1SyQSFBeL/xcnyMwYo9F6MofdW2EH2rUSMNru0KFDlMlGgb2QgBT2iIgI6uvrxQkxfrxIbZPC\nXliISdfZGxREqJx52l3MndsWqXeBuAyz5LK7LFKyREDbaRvAHBBceaVoAAXdJuynnXYaiYmJzJ07\n17h9cC/AsYzeqEMjiHNl2LBhLoU98oQTRHM5g86a8sIwb948Yf298opIKJg92+mxjR8/nrq6OgoM\nhtXk5uYycOBA4uLiPHynbcLu2ErAEXlRy7ctKuxlBKSwy4KO+vp6kfI2enSbsFsyYoq9KfrwFSEh\nokf5oEFdUiAjUx6bXAm75Yu5nW62onqahIS23uHeVtd2kKCgIH7++WeWyIycXohsV7tp0yZAiFlw\ncDDJBjUezjJj8vLyGDJkCGGTJomCIwNBlKL629/+Vjy/bh1cd53LsZSuMmNyc3O9itZtj8GZvy6R\nGWK7PJzi1BMEpLDL6Mjqs9tmxliE/WhPRasPPCCOoQtyxocNG0YOELZ9u/NZq9u3Ux8ZSQkBJuwA\njz0Gjz7adT34DRgyZEiv/pxlwZT02fPz8xk5cqThQm9aWho7duyws0YaGhpYv369iPAtzcCMfPYZ\nM2awbds2zjjjDNHHJzhYtNtwgfT1jRZQd+7c6fHCqSQlJYWLLrqISy+91OV2I0aMwGQyKWHvbciI\n3c5nLygQvbh37aIuKIj6nhJ2kwncDYvuIImJiWQCITU1ThexyMnh6MCBQA+sMfQ0w4fDQw91yUW1\nrxIfH89xxx1nFXbb5l+OpKWlUVNTw/79+wGRQXPjjTeSm5vLrbfeKjqFaprhuadpmkhhrKuDf/xD\npDi6scSio6NJTk5uJ+ylpaUcOXLEa2HXNI0VK1YIj98FYWFhDB8+XAl7b6NdxC4XULOzYfduiiMi\n6NeLo6iOMmTIENbLf6xf334DXYecHEosvmRUF11gFH0LuYDa2trKrl27XAo7tC2gvvDCC7z11ls8\n+OCDogdORIToZuqqxe/y5WJOrotFU1uMMmPeeecdwLuFU28ZNWqU1x57Q0MDTz/9tLCAu5hOC7um\naUmapv1b07TtmqblaJp2my8OrCsxjNhB2DG7d7M/JKRX3x53lNDQUMoHDKA6LMxY2A8fhrIy9kdH\n069fP0xdPfRC0SeYMmUK+/btY8uWLdTV1Rn2ewHRvheEsH/77bfccccdXHDBBTxi29lz7Fjnd4sg\nFk1TU8FF2q4t48aNIzc3l6amJmpra7nyyiv5wx/+wIwZM5gxY4aH79B7Ro0a5VXEnpeXx7Rp07jr\nrrv44osvuuy4JL745jYD/0/X9TTgJOAmTdPSfPC6XUa7iD0xUSxWbt4Me/ZQYDL5rQ2RmJTEdrMZ\nMjPbP2mJpAojI/32/Su8Ry6gyuEeziL2+Ph4Bg0axJo1a7jkkktITU1l6dKl9gFCWhrs3CkWUR3Z\ntAl+/llE6x7aYePHj6exsZEvv/ySadOm8fbbb/PQQw/x1Vdf2bU88DWjR4+mrKzMbcqjruu8/fbb\nnHDCCezdu5fPPvusW5rpdVrYdV0/qOv6L5a/VwE7gG7od9txpLBbI3ZNE1H7V19BQwP5uu6XETuI\nXtabQkNFW17Hk9JyS5sfGuq371/hPZMmTULTNGvTMmcROwg75l//+heapvH555+3P4/GjhXpjgYp\nirzyishS+93vPD422Vrg/PPP58CBA6xZs4ZHH33U+/bSXuJJZkxFRQWXXXYZV1xxBVOmTCE7O5vz\nDKpouwKf3mtrmpYCTAJ+8uXr+hppxdg1N0pPB0vhTm5Tk98Km9ls5kcZDf3k8N+0fTvExLDPj9+/\nwntkGX1xcTHh4eEup2hNnDiRoKAgPvroI+O+RRYfvp3PXl4O778v5uLGxnp8bGPHjiU2NpZf/epX\n/PLLL8ycOdPj3+0M7oRd13V+85vf8OGHH/LnP/+ZdevWeT99rBP4TNg1TesHfAzcrut6u5HjmqZd\nq2naRk3TNvZ0y8t2ETuIlEcLOfX1fmtFmM1mMpubRfaNox2zfTuMG0d1TY3fvn9Fx5B2zKhRo1yu\nvTz00ENs2LBBpC0aITNVHH32O+4Q7aRvusmr44qIiGDPnj3873//82w8oI847rjj0DTNqbAXFBSQ\nnZ3NM888w/3339/ldxCO+ETYNU0LQYj6e7quf2K0ja7rS3Rdn6zr+mSjPsfdidOIHdBDQtjtxxFr\ndHQ0JdXV4kLmuICakwNpaVRVVfnt+1d0DCnsrmwYEM20Jk2a5HyDmBjRjM5W2N95R6Q43ncfWIqE\nvKF///7dLpzh4eEkJSU5zYyRBV3Tu2uOrwO+yIrRgDeBHbqu/1/nD6nrMYzYx42DoCBak5JowX9z\nuM1mM9XV1bROmyasGLmIdeQIHD2qhF1hiBR2ZwunXpGW1mbF7NgB118Pv/5175uL6wZXmTGbNm0i\nJCSECRMmdPNRCXwRsZ8C/BY4Q9O0LZY/rjP8exjDiD08HCZOpMHinfmrsJnNZgDqMzJEV0uZAyx/\njhtHVVWV317YFB0jIyOD2bNnM3fu3M6/mEx5rK2F+fPFgukHH7hsH9AbcSfs48eP79LMHFd0+pPU\ndf0HoE+V6hlG7ACff87e3bth7Vq/F/bysWOJBOGzT5zYFkGlpVFdXe2371/RMUJDQ1m9erVvXiwt\nTQQVl14K27bBl19CNy4s+orRo0dz9OhRysvLibVZ8NV1nU2bNvXojOCArEAJCQkhODi4vbAnJlJh\nucL6a8Qqhf1Y//4wcGCbz56TA2YzrUOGKGFXdC2yZ8zq1XDPPWKCVR/EWWZMYWEhx44d82gweVcR\nkMIODsM2bKiqqgL834qprKqCadPahH37dkhLo8bymfjr+1f0AmTK4ymniOEmfRRnwi4XTpWw9wB2\nwzZsqK6uBvw/Yq+Swp6fLxZOLcLu7+9f0QsYMEBE659+2ud8dVvkGEojYQ8ODu6xhVPwgcfeVwn4\niL2yEk4+WTy4erXoE2NZOAX/ff+KXoKLARp9hYiICBITE9ulPMqF056cFawidgeksPlrxCoFu7Ky\nEiZPFhHTm2+KJy2pjrbbKRQK5zhmxsiF0560YSCAhd1ZxC6tCH8VNruIPSJCFIT873/iyXHjlBWj\nUHiBo7Dv3buXsrIyJew9hauI3WQy9doZlJ3FLmKHNjumXz9ITFQRu0LhBaNHj+bw4cPW75NcOJ08\neXJPHlbgCruriL1fv35ofjpFJzg4mMjIyDZhnzZN/ExLA01Twq5QeIFjZszGjRt7fOEUAljYXUXs\n/i5qZrO5fcRuaX+qrBiFwnMchb03LJxCAAu7q6wYfxc1s9lsjcxJSoKbb7b2wFYRu0LhObYpj71l\n4RQCPN3RWR67v4uaXcSuafDCC9bn/D0rSKHwJVFRUQwdOpT8/Pxes3AKARyxR0ZGWm0HWwLBiomO\njm4Tdgeqq6uJiIjo9jaoCkVfRWbG9IaKU0nACvvxxx9PRUUF+/fvt3tcLp76M3YRuwOBcGFTKHyJ\nrbAHBwcz0WZoT08RsMJ+smXRcL3DsIlAEDYl7AqF7xg9ejQlJSX85z//Ydy4cT2+cAoBLOwTJ04k\nMjKSTIfxcIGyeOrKivH3969Q+BKZGZOZmdkrbBgIYGEPCQlh6tSp7YQ9kBZPdV1v95yK2BUK75DC\nDr3DX4cAFnYQdszmzZutaY8tLS3U1tb6fcRqNptpbm6mvr6+3XNK2BUK71DC3ss4+eSTaW5uZuPG\njQDU1NQA/p/Dbde61wFlxSgU3tGvXz8GDx5MUFBQr1g4hQAX9pNOOgnAascESnGOXSMwB1TErlB4\nT2pqKhMnTuw1PaYCtkAJID4+ntTU1HbC7u8Ra7tGYDYoYVcovOf111+npaWlpw/DSkALOwg75rPP\nPkPXdb9v2StxFrHLz8DfL2wKha+x9dl7AwFtxYAQ9tLSUvLz8wMmYncm7HV1dbS2tvr9hU2h8HeU\nsFsKlTIzMwM+Yg+UNQaFwt8JeGEfM2YM/fv3JzMzM2CETQm7QuHfBLywm0wmpk2bZifsgWLFOKY7\nql7sCoV/EPDCDsKOycnJoaioCPD/iDU8PJzg4GAVsSsUfooSdtp89m+++QYQPZb9GU3TDFv3KmFX\nKPwDJezAlClTCAoKYsOGDURGRgZEL3KjRmDKilEo/AMl7AghS09Pp7W1NWBEzUjYVcSuUPgHStgt\nSDsmUERNCbtC4b8oYbeghF1ZMQqFv6CE3YIU9kARNbPZ3C7dsaqqirCwMEJCQnroqBQKhS9Qwm5h\n+PDhDB061Jrj7e84s2IC5Y5FofBnAr4JmETTNP7xj38EjLAbpTuqBmAKhX+ghN2Gs88+u6cPodsw\nm83U1NTQ0tJiTe9UEbtC4R/4xIrRNG2mpmk7NU3bpWnaPb54TUXXYtRWQAm7QuEfdFrYNU0LAl4C\nzgXSgIWapqV19nUVXYtRIzBlxSgU/oEvIvapwC5d1/fout4ILAPO98HrKroQI2FXEbtC4R/4QtiH\nAftt/l1keUzRi1FWjELhv3RbuqOmaddqmrZR07SNR44c6a7dKpygrBiFwn/xhbAfAJJs/p1oecwO\nXdeX6Lo+Wdf1yQMGDPDBbhWdwVHYdV1XEbtC4Sf4Qtg3AKM1TRuhaVoosAD43Aevq+hCpIBLYW9o\naKC5uVkJu0LhB3Q6j13X9WZN024GvgaCgLd0Xc/p9JEpuhTHiF31iVEo/AefFCjpur4GWOOL11J0\nD44Ru+rsqFD4D6pXTIASFBREVFSUEnaFwg9Rwh7A2DYCk1aMEnaFou+jhD2AsW3dK38qj12h6Pso\nYQ9gbCN2ZcUoFP6DEvYAxrZ1rxJ2hcJ/UMIewBh57MqKUSj6PkrYAxhlxSgU/okS9gDGUdhDQkII\nCwvr4aNSKBSdRQl7ACOFXdd11QBMofAjlLAHMGazmZaWFurr61UDMIXCj1DCHsDY9otRwq5Q+A9K\n2AMY234xyopRKPwHJewBjIrYFQr/RAl7AKOEXaHwT5SwBzC2wq6sGIXCf1DCHsCoiF2h8E+UsAcw\nUtirqqqUsCsUfoQS9gBGCvvRo0dpbGxUVoxC4ScoYQ9gwsLCCAkJobi4GFB9YhQKf0EJewCjaRrR\n0dEcOHAAUMKuUPgLStgDHLPZbI3YlRWjUPgHStgDHLPZrCJ2hcLPUMIe4JjNZg4fPgwoYVco/AUl\n7AGO2WxG13VACbtC4S8oYQ9wZMojKI9dofAXlLAHOLbCriJ2hcI/UMIe4NiKuRJ2hcI/UMIe4MiI\n3WQyER4e3sNHo1AofIES9gBHCnt0dDSapvXw0SgUCl+ghD3AsRV2hULhHyhhD3CksKuMGIXCf1DC\nHuCoiF2h8D+UsAc4StgVCv9DCXuAo6wYhcL/UMIe4MhIXUXsCoX/0Clh1zTtaU3TcjVNy9Y0baWm\nabG+OjBF96CsGIXC/+hsxP4NMF7X9YlAHnBv5w9J0Z1IC0ZZMQqF/xDcmV/WdX2tzT9/BC7u3OEo\nupugoCCeffZZZsyY0dOHolAofIQmW7Z2+oU0bRWwXNf1d508fy1wLcDw4cNP3Lt3r0/2q1AoFIGC\npmmbdF2f7G47txG7pmnrgMEGT92v6/pnlm3uB5qB95y9jq7rS4AlAJMnT/bN1UShUCgU7XAr7Lqu\nu7xH1zTtcmAOcKbuq/BfoVAoFB2mUx67pmkzgbuAU3Vdr/XNISkUCoWiM3Q2K+ZFIBr4RtO0LZqm\nveqDY1IoFApFJ+hsVswoXx2IQqFQKHyDqjxVKBQKP0MJu0KhUPgZStgVCoXCz/BZgZJXO9W0I0BH\nK5QSgKM+PJzupi8ff18+dujbx9+Xjx3U8fuKZF3XB7jbqEeEvTNomrbRk8qr3kpfPv6+fOzQt4+/\nLx87qOPvbpQVo1AoFH6GEnaFQqHwM/qisC/p6QPoJH35+PvysUPfPv6+fOygjr9b6XMeu0KhUChc\n0xcjdoVCoVC4oE8Ju6ZpMzVN26n9//bOJsSqMozjvz+WfVg0WiFDI4yBJLPI0UUpSZRRmESrFkUL\nFy5dKLRxCIKWbSoX0aavTVRkXzKLvibXY5pao4NpNOCINi2SoEVk/Vu8z43D0OTNzTnv5fnB4bzv\nc+7id+Y+97lnnnPuOdJZSfva9rkSkt6QtCBpphFbJekLSWdivbJNx6WQtEbSIUmnJJ2UtCfinfeX\ndL2kw5JOhPvzEV8raTry5z1Jy9t2/S8kLZN0TNJkzKvwlzQn6bu4f9SRiHU+b3pIGpJ0IB77OStp\nS03+UFFhl7QMeAV4FBgDnpI01q7VFXkL2L4otg+Ysr0OmIp5F7kMPGN7DNgM7I6/dw3+vwPbbG8A\nxoHtkjYDLwAvxT2OfgF2tejYD3uA2ca8Jv8HbY83LhGsIW967Ac+tb0e2EB5D2ryB9tVLMAW4LPG\nfAKYaNurD+9RYKYxPw0Mx3gYON22Y5/78QnwcG3+wI3AN8C9lB+YXPNv+dS1BRihFJBtwCSgWvyB\nOeC2RbEq8ga4BfiROP9Ym39vqeaIHbgDONeYz0esNlbbvhDji8DqNmX6QdIosBGYphL/aGMcBxYo\nD13/Abhk+3K8pOv58zLlWQd/xfxW6vE38Lmko/FITKgkb4C1wM/Am9EGe03SCurxBypqxQwiLl//\nnb4sSdJNwAfAXtu/Nrd12d/2n7bHKUe+9wDrW1bqG0mPAQu2j7btcpVstb2J0jbdLen+5sYu5w3l\nVuabgFdtbwR+Y1HbpeP+QF2F/TywpjEfiVht/CRpGCDWCy37LImkaylF/W3bH0a4Gn8A25eAQ5TW\nxZCk3jMIupw/9wGPS5oD3qW0Y/ZTib/t87FeAD6ifLHWkjfzwLzt6ZgfoBT6WvyBugr718C6uDJg\nOfAkcLBlp6vhILAzxjspvevOIUnA68Cs7RcbmzrvL+l2SUMxvoFybmCWUuCfiJd10h3A9oTtEduj\nlDz/yvbTVOAvaYWkm3tj4BFghgryBsD2ReCcpLsi9BBwikr8/6HtJv//PLGxA/ie0i99tm2fPnzf\nAS4Af1COBHZReqVTwBngS2BV255LuG+l/Lv5LXA8lh01+AN3A8fCfQZ4LuJ3AoeBs8D7wHVtu/ax\nLw8Ak7X4h+OJWE72Pqc15E1jH8aBI5E/HwMra/K3nb88TZIkGTRqasUkSZIkfZCFPUmSZMDIwp4k\nSTJgZGFPkiQZMLKwJ0mSDBhZ2JMkSQaMLOxJkiQDRhb2JEmSAeNv5J7x+YJykjMAAAAASUVORK5C\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc1ff9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xce33c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 2.7215737324 \n",
      "Fixed scheme MAE:  1.85786295086\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 1.4873  Test loss = 3.9727  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 1.5660  Test loss = 3.0132  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 1.5842  Test loss = 0.6073  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 1.5814  Test loss = 0.6784  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 1.4094  Test loss = 0.9911  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 1.3992  Test loss = 0.4216  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 1.3897  Test loss = 0.5975  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 1.3855  Test loss = 1.2898  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 1.3352  Test loss = 2.0401  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 1.3554  Test loss = 0.0692  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 1.3226  Test loss = 0.8682  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 1.3263  Test loss = 1.7068  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 1.2923  Test loss = 0.4925  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 1.2885  Test loss = 2.0057  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 1.3123  Test loss = 3.5641  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 1.3848  Test loss = 5.3771  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 1.4297  Test loss = 2.7412  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 1.4695  Test loss = 0.0279  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 1.4695  Test loss = 0.4738  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 1.4172  Test loss = 0.8593  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 1.3663  Test loss = 1.7571  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 1.3764  Test loss = 3.1894  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 1.4288  Test loss = 0.5287  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 1.4231  Test loss = 1.5155  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 1.3913  Test loss = 0.3088  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 1.3898  Test loss = 0.7923  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 1.3906  Test loss = 0.3711  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 1.3741  Test loss = 1.5444  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 1.3110  Test loss = 0.2752  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 1.3056  Test loss = 1.3446  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 1.3091  Test loss = 3.8831  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 1.3609  Test loss = 1.4801  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 1.3191  Test loss = 1.2284  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 1.3171  Test loss = 0.2099  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 1.2789  Test loss = 0.0296  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 1.2778  Test loss = 3.8750  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 1.2661  Test loss = 1.6964  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 1.2092  Test loss = 1.3649  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 1.2205  Test loss = 0.4816  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 1.2191  Test loss = 2.9051  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 1.2294  Test loss = 2.2634  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 1.2610  Test loss = 1.9066  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 1.2828  Test loss = 4.1445  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 1.3817  Test loss = 11.9500  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 2.0095  Test loss = 6.4207  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.1612  Test loss = 0.9556  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.1644  Test loss = 0.4104  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.1643  Test loss = 0.6777  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 1.9894  Test loss = 1.2453  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 1.9927  Test loss = 3.5689  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 2.0353  Test loss = 1.0118  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 2.0367  Test loss = 1.1419  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 1.8993  Test loss = 1.9755  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 1.9121  Test loss = 1.2442  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 1.9182  Test loss = 1.6075  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 1.9262  Test loss = 1.2492  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 1.8820  Test loss = 1.4098  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 1.8899  Test loss = 0.4709  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 1.8906  Test loss = 0.6395  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 1.8922  Test loss = 0.4717  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.8657  Test loss = 2.1859  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.8851  Test loss = 3.4728  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 1.9306  Test loss = 1.4478  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 1.9363  Test loss = 1.5237  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 1.8757  Test loss = 0.0851  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 1.8731  Test loss = 0.6659  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.8650  Test loss = 0.2574  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 1.8635  Test loss = 3.0768  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 1.8742  Test loss = 4.3503  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 1.9450  Test loss = 0.5265  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 1.9396  Test loss = 1.0512  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 1.9433  Test loss = 2.2887  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 1.8823  Test loss = 1.4794  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 1.8911  Test loss = 0.7268  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 1.8902  Test loss = 0.7808  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 1.8873  Test loss = 0.5329  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 1.8253  Test loss = 1.4189  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuLCbmTXMv0cRrDmhzv+OAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc9b0240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.45013955668 \n",
      "Updating scheme MAE:  1.70408317034\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_test_fixed.csv\", yhat_test_fixed)\n",
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_update.csv\", yhat_update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
