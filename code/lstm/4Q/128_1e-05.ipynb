{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"4Q/128_units/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 500\n",
    "learning_rate = 1e-5\n",
    "batch_size = 5\n",
    "early_stop_iters = 15\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 128 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag4',\n",
    "                                       'inflation.lag5',\n",
    "                                       'inflation.lag6',\n",
    "                                       'inflation.lag7']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag4',\n",
    "                                   'unemp.lag5',\n",
    "                                   'unemp.lag6',\n",
    "                                   'unemp.lag7']])\n",
    "train_4lag_oil = np.array(train[['oil.lag4',\n",
    "                                 'oil.lag5',\n",
    "                                 'oil.lag6',\n",
    "                                 'oil.lag7']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag4',\n",
    "                                     'inflation.lag5',\n",
    "                                     'inflation.lag6',\n",
    "                                     'inflation.lag7']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag4',\n",
    "                                 'unemp.lag5',\n",
    "                                 'unemp.lag6',\n",
    "                                 'unemp.lag7']])\n",
    "test_4lag_oil = np.array(test[['oil.lag4',\n",
    "                               'oil.lag5',\n",
    "                               'oil.lag6',\n",
    "                               'oil.lag7']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 128 \n",
      "Learning rate = 1e-05 \n",
      "Epochs = 500 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 15 \n",
      "Learning rate = 1e-05\n",
      "Fold: 1  Epoch: 1  Training loss = 3.3432  Validation loss = 3.5036  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 3.3405  Validation loss = 3.4992  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 3.3374  Validation loss = 3.4942  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 3.3338  Validation loss = 3.4883  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 3.3312  Validation loss = 3.4842  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 3.3276  Validation loss = 3.4784  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 3.3233  Validation loss = 3.4714  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 3.3197  Validation loss = 3.4658  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 3.3176  Validation loss = 3.4622  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 3.3137  Validation loss = 3.4559  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 3.3101  Validation loss = 3.4502  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 3.3068  Validation loss = 3.4448  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 3.3039  Validation loss = 3.4398  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 3.3007  Validation loss = 3.4346  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 3.2971  Validation loss = 3.4287  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 3.2927  Validation loss = 3.4216  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 3.2905  Validation loss = 3.4179  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 3.2872  Validation loss = 3.4124  \n",
      "\n",
      "Fold: 1  Epoch: 19  Training loss = 3.2837  Validation loss = 3.4070  \n",
      "\n",
      "Fold: 1  Epoch: 20  Training loss = 3.2805  Validation loss = 3.4016  \n",
      "\n",
      "Fold: 1  Epoch: 21  Training loss = 3.2780  Validation loss = 3.3975  \n",
      "\n",
      "Fold: 1  Epoch: 22  Training loss = 3.2755  Validation loss = 3.3932  \n",
      "\n",
      "Fold: 1  Epoch: 23  Training loss = 3.2721  Validation loss = 3.3876  \n",
      "\n",
      "Fold: 1  Epoch: 24  Training loss = 3.2677  Validation loss = 3.3803  \n",
      "\n",
      "Fold: 1  Epoch: 25  Training loss = 3.2611  Validation loss = 3.3698  \n",
      "\n",
      "Fold: 1  Epoch: 26  Training loss = 3.2572  Validation loss = 3.3632  \n",
      "\n",
      "Fold: 1  Epoch: 27  Training loss = 3.2544  Validation loss = 3.3586  \n",
      "\n",
      "Fold: 1  Epoch: 28  Training loss = 3.2510  Validation loss = 3.3528  \n",
      "\n",
      "Fold: 1  Epoch: 29  Training loss = 3.2480  Validation loss = 3.3478  \n",
      "\n",
      "Fold: 1  Epoch: 30  Training loss = 3.2446  Validation loss = 3.3420  \n",
      "\n",
      "Fold: 1  Epoch: 31  Training loss = 3.2418  Validation loss = 3.3374  \n",
      "\n",
      "Fold: 1  Epoch: 32  Training loss = 3.2392  Validation loss = 3.3328  \n",
      "\n",
      "Fold: 1  Epoch: 33  Training loss = 3.2356  Validation loss = 3.3268  \n",
      "\n",
      "Fold: 1  Epoch: 34  Training loss = 3.2328  Validation loss = 3.3220  \n",
      "\n",
      "Fold: 1  Epoch: 35  Training loss = 3.2297  Validation loss = 3.3168  \n",
      "\n",
      "Fold: 1  Epoch: 36  Training loss = 3.2266  Validation loss = 3.3116  \n",
      "\n",
      "Fold: 1  Epoch: 37  Training loss = 3.2238  Validation loss = 3.3069  \n",
      "\n",
      "Fold: 1  Epoch: 38  Training loss = 3.2214  Validation loss = 3.3027  \n",
      "\n",
      "Fold: 1  Epoch: 39  Training loss = 3.2189  Validation loss = 3.2985  \n",
      "\n",
      "Fold: 1  Epoch: 40  Training loss = 3.2158  Validation loss = 3.2932  \n",
      "\n",
      "Fold: 1  Epoch: 41  Training loss = 3.2121  Validation loss = 3.2870  \n",
      "\n",
      "Fold: 1  Epoch: 42  Training loss = 3.2094  Validation loss = 3.2823  \n",
      "\n",
      "Fold: 1  Epoch: 43  Training loss = 3.2063  Validation loss = 3.2771  \n",
      "\n",
      "Fold: 1  Epoch: 44  Training loss = 3.2035  Validation loss = 3.2723  \n",
      "\n",
      "Fold: 1  Epoch: 45  Training loss = 3.2005  Validation loss = 3.2675  \n",
      "\n",
      "Fold: 1  Epoch: 46  Training loss = 3.1973  Validation loss = 3.2619  \n",
      "\n",
      "Fold: 1  Epoch: 47  Training loss = 3.1945  Validation loss = 3.2573  \n",
      "\n",
      "Fold: 1  Epoch: 48  Training loss = 3.1916  Validation loss = 3.2521  \n",
      "\n",
      "Fold: 1  Epoch: 49  Training loss = 3.1892  Validation loss = 3.2480  \n",
      "\n",
      "Fold: 1  Epoch: 50  Training loss = 3.1853  Validation loss = 3.2415  \n",
      "\n",
      "Fold: 1  Epoch: 51  Training loss = 3.1826  Validation loss = 3.2369  \n",
      "\n",
      "Fold: 1  Epoch: 52  Training loss = 3.1793  Validation loss = 3.2311  \n",
      "\n",
      "Fold: 1  Epoch: 53  Training loss = 3.1776  Validation loss = 3.2279  \n",
      "\n",
      "Fold: 1  Epoch: 54  Training loss = 3.1753  Validation loss = 3.2240  \n",
      "\n",
      "Fold: 1  Epoch: 55  Training loss = 3.1727  Validation loss = 3.2196  \n",
      "\n",
      "Fold: 1  Epoch: 56  Training loss = 3.1704  Validation loss = 3.2155  \n",
      "\n",
      "Fold: 1  Epoch: 57  Training loss = 3.1679  Validation loss = 3.2112  \n",
      "\n",
      "Fold: 1  Epoch: 58  Training loss = 3.1652  Validation loss = 3.2066  \n",
      "\n",
      "Fold: 1  Epoch: 59  Training loss = 3.1626  Validation loss = 3.2019  \n",
      "\n",
      "Fold: 1  Epoch: 60  Training loss = 3.1603  Validation loss = 3.1979  \n",
      "\n",
      "Fold: 1  Epoch: 61  Training loss = 3.1578  Validation loss = 3.1937  \n",
      "\n",
      "Fold: 1  Epoch: 62  Training loss = 3.1552  Validation loss = 3.1889  \n",
      "\n",
      "Fold: 1  Epoch: 63  Training loss = 3.1535  Validation loss = 3.1857  \n",
      "\n",
      "Fold: 1  Epoch: 64  Training loss = 3.1516  Validation loss = 3.1822  \n",
      "\n",
      "Fold: 1  Epoch: 65  Training loss = 3.1492  Validation loss = 3.1780  \n",
      "\n",
      "Fold: 1  Epoch: 66  Training loss = 3.1463  Validation loss = 3.1727  \n",
      "\n",
      "Fold: 1  Epoch: 67  Training loss = 3.1442  Validation loss = 3.1691  \n",
      "\n",
      "Fold: 1  Epoch: 68  Training loss = 3.1416  Validation loss = 3.1645  \n",
      "\n",
      "Fold: 1  Epoch: 69  Training loss = 3.1395  Validation loss = 3.1607  \n",
      "\n",
      "Fold: 1  Epoch: 70  Training loss = 3.1373  Validation loss = 3.1568  \n",
      "\n",
      "Fold: 1  Epoch: 71  Training loss = 3.1340  Validation loss = 3.1511  \n",
      "\n",
      "Fold: 1  Epoch: 72  Training loss = 3.1310  Validation loss = 3.1459  \n",
      "\n",
      "Fold: 1  Epoch: 73  Training loss = 3.1271  Validation loss = 3.1391  \n",
      "\n",
      "Fold: 1  Epoch: 74  Training loss = 3.1251  Validation loss = 3.1353  \n",
      "\n",
      "Fold: 1  Epoch: 75  Training loss = 3.1223  Validation loss = 3.1302  \n",
      "\n",
      "Fold: 1  Epoch: 76  Training loss = 3.1199  Validation loss = 3.1257  \n",
      "\n",
      "Fold: 1  Epoch: 77  Training loss = 3.1175  Validation loss = 3.1213  \n",
      "\n",
      "Fold: 1  Epoch: 78  Training loss = 3.1153  Validation loss = 3.1173  \n",
      "\n",
      "Fold: 1  Epoch: 79  Training loss = 3.1132  Validation loss = 3.1135  \n",
      "\n",
      "Fold: 1  Epoch: 80  Training loss = 3.1110  Validation loss = 3.1094  \n",
      "\n",
      "Fold: 1  Epoch: 81  Training loss = 3.1091  Validation loss = 3.1059  \n",
      "\n",
      "Fold: 1  Epoch: 82  Training loss = 3.1061  Validation loss = 3.1003  \n",
      "\n",
      "Fold: 1  Epoch: 83  Training loss = 3.1041  Validation loss = 3.0967  \n",
      "\n",
      "Fold: 1  Epoch: 84  Training loss = 3.1019  Validation loss = 3.0926  \n",
      "\n",
      "Fold: 1  Epoch: 85  Training loss = 3.0991  Validation loss = 3.0877  \n",
      "\n",
      "Fold: 1  Epoch: 86  Training loss = 3.0966  Validation loss = 3.0830  \n",
      "\n",
      "Fold: 1  Epoch: 87  Training loss = 3.0947  Validation loss = 3.0795  \n",
      "\n",
      "Fold: 1  Epoch: 88  Training loss = 3.0918  Validation loss = 3.0740  \n",
      "\n",
      "Fold: 1  Epoch: 89  Training loss = 3.0890  Validation loss = 3.0690  \n",
      "\n",
      "Fold: 1  Epoch: 90  Training loss = 3.0865  Validation loss = 3.0642  \n",
      "\n",
      "Fold: 1  Epoch: 91  Training loss = 3.0839  Validation loss = 3.0595  \n",
      "\n",
      "Fold: 1  Epoch: 92  Training loss = 3.0819  Validation loss = 3.0558  \n",
      "\n",
      "Fold: 1  Epoch: 93  Training loss = 3.0795  Validation loss = 3.0514  \n",
      "\n",
      "Fold: 1  Epoch: 94  Training loss = 3.0773  Validation loss = 3.0473  \n",
      "\n",
      "Fold: 1  Epoch: 95  Training loss = 3.0750  Validation loss = 3.0430  \n",
      "\n",
      "Fold: 1  Epoch: 96  Training loss = 3.0731  Validation loss = 3.0395  \n",
      "\n",
      "Fold: 1  Epoch: 97  Training loss = 3.0710  Validation loss = 3.0356  \n",
      "\n",
      "Fold: 1  Epoch: 98  Training loss = 3.0691  Validation loss = 3.0319  \n",
      "\n",
      "Fold: 1  Epoch: 99  Training loss = 3.0674  Validation loss = 3.0284  \n",
      "\n",
      "Fold: 1  Epoch: 100  Training loss = 3.0655  Validation loss = 3.0249  \n",
      "\n",
      "Fold: 1  Epoch: 101  Training loss = 3.0638  Validation loss = 3.0216  \n",
      "\n",
      "Fold: 1  Epoch: 102  Training loss = 3.0623  Validation loss = 3.0187  \n",
      "\n",
      "Fold: 1  Epoch: 103  Training loss = 3.0600  Validation loss = 3.0143  \n",
      "\n",
      "Fold: 1  Epoch: 104  Training loss = 3.0582  Validation loss = 3.0107  \n",
      "\n",
      "Fold: 1  Epoch: 105  Training loss = 3.0562  Validation loss = 3.0070  \n",
      "\n",
      "Fold: 1  Epoch: 106  Training loss = 3.0545  Validation loss = 3.0036  \n",
      "\n",
      "Fold: 1  Epoch: 107  Training loss = 3.0525  Validation loss = 2.9999  \n",
      "\n",
      "Fold: 1  Epoch: 108  Training loss = 3.0505  Validation loss = 2.9960  \n",
      "\n",
      "Fold: 1  Epoch: 109  Training loss = 3.0487  Validation loss = 2.9924  \n",
      "\n",
      "Fold: 1  Epoch: 110  Training loss = 3.0460  Validation loss = 2.9877  \n",
      "\n",
      "Fold: 1  Epoch: 111  Training loss = 3.0435  Validation loss = 2.9828  \n",
      "\n",
      "Fold: 1  Epoch: 112  Training loss = 3.0412  Validation loss = 2.9785  \n",
      "\n",
      "Fold: 1  Epoch: 113  Training loss = 3.0396  Validation loss = 2.9753  \n",
      "\n",
      "Fold: 1  Epoch: 114  Training loss = 3.0370  Validation loss = 2.9702  \n",
      "\n",
      "Fold: 1  Epoch: 115  Training loss = 3.0344  Validation loss = 2.9654  \n",
      "\n",
      "Fold: 1  Epoch: 116  Training loss = 3.0319  Validation loss = 2.9608  \n",
      "\n",
      "Fold: 1  Epoch: 117  Training loss = 3.0300  Validation loss = 2.9570  \n",
      "\n",
      "Fold: 1  Epoch: 118  Training loss = 3.0285  Validation loss = 2.9542  \n",
      "\n",
      "Fold: 1  Epoch: 119  Training loss = 3.0272  Validation loss = 2.9514  \n",
      "\n",
      "Fold: 1  Epoch: 120  Training loss = 3.0247  Validation loss = 2.9467  \n",
      "\n",
      "Fold: 1  Epoch: 121  Training loss = 3.0224  Validation loss = 2.9422  \n",
      "\n",
      "Fold: 1  Epoch: 122  Training loss = 3.0202  Validation loss = 2.9381  \n",
      "\n",
      "Fold: 1  Epoch: 123  Training loss = 3.0185  Validation loss = 2.9346  \n",
      "\n",
      "Fold: 1  Epoch: 124  Training loss = 3.0162  Validation loss = 2.9303  \n",
      "\n",
      "Fold: 1  Epoch: 125  Training loss = 3.0131  Validation loss = 2.9241  \n",
      "\n",
      "Fold: 1  Epoch: 126  Training loss = 3.0112  Validation loss = 2.9205  \n",
      "\n",
      "Fold: 1  Epoch: 127  Training loss = 3.0082  Validation loss = 2.9150  \n",
      "\n",
      "Fold: 1  Epoch: 128  Training loss = 3.0060  Validation loss = 2.9106  \n",
      "\n",
      "Fold: 1  Epoch: 129  Training loss = 3.0043  Validation loss = 2.9074  \n",
      "\n",
      "Fold: 1  Epoch: 130  Training loss = 3.0021  Validation loss = 2.9031  \n",
      "\n",
      "Fold: 1  Epoch: 131  Training loss = 3.0000  Validation loss = 2.8988  \n",
      "\n",
      "Fold: 1  Epoch: 132  Training loss = 2.9984  Validation loss = 2.8955  \n",
      "\n",
      "Fold: 1  Epoch: 133  Training loss = 2.9962  Validation loss = 2.8911  \n",
      "\n",
      "Fold: 1  Epoch: 134  Training loss = 2.9944  Validation loss = 2.8874  \n",
      "\n",
      "Fold: 1  Epoch: 135  Training loss = 2.9932  Validation loss = 2.8849  \n",
      "\n",
      "Fold: 1  Epoch: 136  Training loss = 2.9915  Validation loss = 2.8815  \n",
      "\n",
      "Fold: 1  Epoch: 137  Training loss = 2.9899  Validation loss = 2.8783  \n",
      "\n",
      "Fold: 1  Epoch: 138  Training loss = 2.9876  Validation loss = 2.8739  \n",
      "\n",
      "Fold: 1  Epoch: 139  Training loss = 2.9857  Validation loss = 2.8700  \n",
      "\n",
      "Fold: 1  Epoch: 140  Training loss = 2.9837  Validation loss = 2.8660  \n",
      "\n",
      "Fold: 1  Epoch: 141  Training loss = 2.9820  Validation loss = 2.8624  \n",
      "\n",
      "Fold: 1  Epoch: 142  Training loss = 2.9800  Validation loss = 2.8584  \n",
      "\n",
      "Fold: 1  Epoch: 143  Training loss = 2.9785  Validation loss = 2.8553  \n",
      "\n",
      "Fold: 1  Epoch: 144  Training loss = 2.9767  Validation loss = 2.8514  \n",
      "\n",
      "Fold: 1  Epoch: 145  Training loss = 2.9751  Validation loss = 2.8482  \n",
      "\n",
      "Fold: 1  Epoch: 146  Training loss = 2.9733  Validation loss = 2.8447  \n",
      "\n",
      "Fold: 1  Epoch: 147  Training loss = 2.9719  Validation loss = 2.8418  \n",
      "\n",
      "Fold: 1  Epoch: 148  Training loss = 2.9696  Validation loss = 2.8371  \n",
      "\n",
      "Fold: 1  Epoch: 149  Training loss = 2.9673  Validation loss = 2.8326  \n",
      "\n",
      "Fold: 1  Epoch: 150  Training loss = 2.9654  Validation loss = 2.8289  \n",
      "\n",
      "Fold: 1  Epoch: 151  Training loss = 2.9622  Validation loss = 2.8226  \n",
      "\n",
      "Fold: 1  Epoch: 152  Training loss = 2.9603  Validation loss = 2.8188  \n",
      "\n",
      "Fold: 1  Epoch: 153  Training loss = 2.9587  Validation loss = 2.8155  \n",
      "\n",
      "Fold: 1  Epoch: 154  Training loss = 2.9568  Validation loss = 2.8115  \n",
      "\n",
      "Fold: 1  Epoch: 155  Training loss = 2.9551  Validation loss = 2.8079  \n",
      "\n",
      "Fold: 1  Epoch: 156  Training loss = 2.9530  Validation loss = 2.8034  \n",
      "\n",
      "Fold: 1  Epoch: 157  Training loss = 2.9518  Validation loss = 2.8009  \n",
      "\n",
      "Fold: 1  Epoch: 158  Training loss = 2.9500  Validation loss = 2.7972  \n",
      "\n",
      "Fold: 1  Epoch: 159  Training loss = 2.9486  Validation loss = 2.7942  \n",
      "\n",
      "Fold: 1  Epoch: 160  Training loss = 2.9459  Validation loss = 2.7887  \n",
      "\n",
      "Fold: 1  Epoch: 161  Training loss = 2.9443  Validation loss = 2.7854  \n",
      "\n",
      "Fold: 1  Epoch: 162  Training loss = 2.9422  Validation loss = 2.7812  \n",
      "\n",
      "Fold: 1  Epoch: 163  Training loss = 2.9397  Validation loss = 2.7759  \n",
      "\n",
      "Fold: 1  Epoch: 164  Training loss = 2.9381  Validation loss = 2.7725  \n",
      "\n",
      "Fold: 1  Epoch: 165  Training loss = 2.9365  Validation loss = 2.7691  \n",
      "\n",
      "Fold: 1  Epoch: 166  Training loss = 2.9351  Validation loss = 2.7659  \n",
      "\n",
      "Fold: 1  Epoch: 167  Training loss = 2.9338  Validation loss = 2.7631  \n",
      "\n",
      "Fold: 1  Epoch: 168  Training loss = 2.9325  Validation loss = 2.7604  \n",
      "\n",
      "Fold: 1  Epoch: 169  Training loss = 2.9312  Validation loss = 2.7577  \n",
      "\n",
      "Fold: 1  Epoch: 170  Training loss = 2.9290  Validation loss = 2.7531  \n",
      "\n",
      "Fold: 1  Epoch: 171  Training loss = 2.9277  Validation loss = 2.7503  \n",
      "\n",
      "Fold: 1  Epoch: 172  Training loss = 2.9264  Validation loss = 2.7475  \n",
      "\n",
      "Fold: 1  Epoch: 173  Training loss = 2.9248  Validation loss = 2.7442  \n",
      "\n",
      "Fold: 1  Epoch: 174  Training loss = 2.9236  Validation loss = 2.7413  \n",
      "\n",
      "Fold: 1  Epoch: 175  Training loss = 2.9215  Validation loss = 2.7371  \n",
      "\n",
      "Fold: 1  Epoch: 176  Training loss = 2.9195  Validation loss = 2.7329  \n",
      "\n",
      "Fold: 1  Epoch: 177  Training loss = 2.9183  Validation loss = 2.7302  \n",
      "\n",
      "Fold: 1  Epoch: 178  Training loss = 2.9173  Validation loss = 2.7279  \n",
      "\n",
      "Fold: 1  Epoch: 179  Training loss = 2.9160  Validation loss = 2.7251  \n",
      "\n",
      "Fold: 1  Epoch: 180  Training loss = 2.9148  Validation loss = 2.7225  \n",
      "\n",
      "Fold: 1  Epoch: 181  Training loss = 2.9137  Validation loss = 2.7200  \n",
      "\n",
      "Fold: 1  Epoch: 182  Training loss = 2.9122  Validation loss = 2.7166  \n",
      "\n",
      "Fold: 1  Epoch: 183  Training loss = 2.9106  Validation loss = 2.7132  \n",
      "\n",
      "Fold: 1  Epoch: 184  Training loss = 2.9091  Validation loss = 2.7099  \n",
      "\n",
      "Fold: 1  Epoch: 185  Training loss = 2.9077  Validation loss = 2.7070  \n",
      "\n",
      "Fold: 1  Epoch: 186  Training loss = 2.9066  Validation loss = 2.7045  \n",
      "\n",
      "Fold: 1  Epoch: 187  Training loss = 2.9056  Validation loss = 2.7019  \n",
      "\n",
      "Fold: 1  Epoch: 188  Training loss = 2.9045  Validation loss = 2.6995  \n",
      "\n",
      "Fold: 1  Epoch: 189  Training loss = 2.9031  Validation loss = 2.6963  \n",
      "\n",
      "Fold: 1  Epoch: 190  Training loss = 2.9008  Validation loss = 2.6913  \n",
      "\n",
      "Fold: 1  Epoch: 191  Training loss = 2.8993  Validation loss = 2.6880  \n",
      "\n",
      "Fold: 1  Epoch: 192  Training loss = 2.8973  Validation loss = 2.6835  \n",
      "\n",
      "Fold: 1  Epoch: 193  Training loss = 2.8954  Validation loss = 2.6794  \n",
      "\n",
      "Fold: 1  Epoch: 194  Training loss = 2.8939  Validation loss = 2.6760  \n",
      "\n",
      "Fold: 1  Epoch: 195  Training loss = 2.8931  Validation loss = 2.6741  \n",
      "\n",
      "Fold: 1  Epoch: 196  Training loss = 2.8914  Validation loss = 2.6702  \n",
      "\n",
      "Fold: 1  Epoch: 197  Training loss = 2.8899  Validation loss = 2.6669  \n",
      "\n",
      "Fold: 1  Epoch: 198  Training loss = 2.8875  Validation loss = 2.6617  \n",
      "\n",
      "Fold: 1  Epoch: 199  Training loss = 2.8861  Validation loss = 2.6586  \n",
      "\n",
      "Fold: 1  Epoch: 200  Training loss = 2.8843  Validation loss = 2.6547  \n",
      "\n",
      "Fold: 1  Epoch: 201  Training loss = 2.8832  Validation loss = 2.6522  \n",
      "\n",
      "Fold: 1  Epoch: 202  Training loss = 2.8814  Validation loss = 2.6479  \n",
      "\n",
      "Fold: 1  Epoch: 203  Training loss = 2.8802  Validation loss = 2.6453  \n",
      "\n",
      "Fold: 1  Epoch: 204  Training loss = 2.8788  Validation loss = 2.6421  \n",
      "\n",
      "Fold: 1  Epoch: 205  Training loss = 2.8776  Validation loss = 2.6390  \n",
      "\n",
      "Fold: 1  Epoch: 206  Training loss = 2.8768  Validation loss = 2.6372  \n",
      "\n",
      "Fold: 1  Epoch: 207  Training loss = 2.8754  Validation loss = 2.6338  \n",
      "\n",
      "Fold: 1  Epoch: 208  Training loss = 2.8740  Validation loss = 2.6306  \n",
      "\n",
      "Fold: 1  Epoch: 209  Training loss = 2.8729  Validation loss = 2.6281  \n",
      "\n",
      "Fold: 1  Epoch: 210  Training loss = 2.8713  Validation loss = 2.6244  \n",
      "\n",
      "Fold: 1  Epoch: 211  Training loss = 2.8697  Validation loss = 2.6207  \n",
      "\n",
      "Fold: 1  Epoch: 212  Training loss = 2.8687  Validation loss = 2.6183  \n",
      "\n",
      "Fold: 1  Epoch: 213  Training loss = 2.8677  Validation loss = 2.6159  \n",
      "\n",
      "Fold: 1  Epoch: 214  Training loss = 2.8667  Validation loss = 2.6137  \n",
      "\n",
      "Fold: 1  Epoch: 215  Training loss = 2.8646  Validation loss = 2.6091  \n",
      "\n",
      "Fold: 1  Epoch: 216  Training loss = 2.8629  Validation loss = 2.6052  \n",
      "\n",
      "Fold: 1  Epoch: 217  Training loss = 2.8617  Validation loss = 2.6023  \n",
      "\n",
      "Fold: 1  Epoch: 218  Training loss = 2.8604  Validation loss = 2.5994  \n",
      "\n",
      "Fold: 1  Epoch: 219  Training loss = 2.8585  Validation loss = 2.5949  \n",
      "\n",
      "Fold: 1  Epoch: 220  Training loss = 2.8573  Validation loss = 2.5921  \n",
      "\n",
      "Fold: 1  Epoch: 221  Training loss = 2.8562  Validation loss = 2.5894  \n",
      "\n",
      "Fold: 1  Epoch: 222  Training loss = 2.8543  Validation loss = 2.5851  \n",
      "\n",
      "Fold: 1  Epoch: 223  Training loss = 2.8530  Validation loss = 2.5821  \n",
      "\n",
      "Fold: 1  Epoch: 224  Training loss = 2.8512  Validation loss = 2.5782  \n",
      "\n",
      "Fold: 1  Epoch: 225  Training loss = 2.8505  Validation loss = 2.5767  \n",
      "\n",
      "Fold: 1  Epoch: 226  Training loss = 2.8498  Validation loss = 2.5750  \n",
      "\n",
      "Fold: 1  Epoch: 227  Training loss = 2.8478  Validation loss = 2.5705  \n",
      "\n",
      "Fold: 1  Epoch: 228  Training loss = 2.8466  Validation loss = 2.5676  \n",
      "\n",
      "Fold: 1  Epoch: 229  Training loss = 2.8456  Validation loss = 2.5651  \n",
      "\n",
      "Fold: 1  Epoch: 230  Training loss = 2.8439  Validation loss = 2.5613  \n",
      "\n",
      "Fold: 1  Epoch: 231  Training loss = 2.8430  Validation loss = 2.5589  \n",
      "\n",
      "Fold: 1  Epoch: 232  Training loss = 2.8416  Validation loss = 2.5558  \n",
      "\n",
      "Fold: 1  Epoch: 233  Training loss = 2.8408  Validation loss = 2.5538  \n",
      "\n",
      "Fold: 1  Epoch: 234  Training loss = 2.8381  Validation loss = 2.5479  \n",
      "\n",
      "Fold: 1  Epoch: 235  Training loss = 2.8366  Validation loss = 2.5442  \n",
      "\n",
      "Fold: 1  Epoch: 236  Training loss = 2.8353  Validation loss = 2.5409  \n",
      "\n",
      "Fold: 1  Epoch: 237  Training loss = 2.8344  Validation loss = 2.5386  \n",
      "\n",
      "Fold: 1  Epoch: 238  Training loss = 2.8332  Validation loss = 2.5356  \n",
      "\n",
      "Fold: 1  Epoch: 239  Training loss = 2.8316  Validation loss = 2.5318  \n",
      "\n",
      "Fold: 1  Epoch: 240  Training loss = 2.8306  Validation loss = 2.5292  \n",
      "\n",
      "Fold: 1  Epoch: 241  Training loss = 2.8292  Validation loss = 2.5259  \n",
      "\n",
      "Fold: 1  Epoch: 242  Training loss = 2.8274  Validation loss = 2.5215  \n",
      "\n",
      "Fold: 1  Epoch: 243  Training loss = 2.8257  Validation loss = 2.5174  \n",
      "\n",
      "Fold: 1  Epoch: 244  Training loss = 2.8250  Validation loss = 2.5156  \n",
      "\n",
      "Fold: 1  Epoch: 245  Training loss = 2.8244  Validation loss = 2.5140  \n",
      "\n",
      "Fold: 1  Epoch: 246  Training loss = 2.8230  Validation loss = 2.5108  \n",
      "\n",
      "Fold: 1  Epoch: 247  Training loss = 2.8218  Validation loss = 2.5077  \n",
      "\n",
      "Fold: 1  Epoch: 248  Training loss = 2.8205  Validation loss = 2.5047  \n",
      "\n",
      "Fold: 1  Epoch: 249  Training loss = 2.8193  Validation loss = 2.5014  \n",
      "\n",
      "Fold: 1  Epoch: 250  Training loss = 2.8181  Validation loss = 2.4983  \n",
      "\n",
      "Fold: 1  Epoch: 251  Training loss = 2.8175  Validation loss = 2.4966  \n",
      "\n",
      "Fold: 1  Epoch: 252  Training loss = 2.8163  Validation loss = 2.4937  \n",
      "\n",
      "Fold: 1  Epoch: 253  Training loss = 2.8148  Validation loss = 2.4900  \n",
      "\n",
      "Fold: 1  Epoch: 254  Training loss = 2.8138  Validation loss = 2.4875  \n",
      "\n",
      "Fold: 1  Epoch: 255  Training loss = 2.8126  Validation loss = 2.4844  \n",
      "\n",
      "Fold: 1  Epoch: 256  Training loss = 2.8120  Validation loss = 2.4826  \n",
      "\n",
      "Fold: 1  Epoch: 257  Training loss = 2.8110  Validation loss = 2.4801  \n",
      "\n",
      "Fold: 1  Epoch: 258  Training loss = 2.8101  Validation loss = 2.4778  \n",
      "\n",
      "Fold: 1  Epoch: 259  Training loss = 2.8090  Validation loss = 2.4751  \n",
      "\n",
      "Fold: 1  Epoch: 260  Training loss = 2.8083  Validation loss = 2.4733  \n",
      "\n",
      "Fold: 1  Epoch: 261  Training loss = 2.8073  Validation loss = 2.4710  \n",
      "\n",
      "Fold: 1  Epoch: 262  Training loss = 2.8060  Validation loss = 2.4677  \n",
      "\n",
      "Fold: 1  Epoch: 263  Training loss = 2.8048  Validation loss = 2.4648  \n",
      "\n",
      "Fold: 1  Epoch: 264  Training loss = 2.8044  Validation loss = 2.4638  \n",
      "\n",
      "Fold: 1  Epoch: 265  Training loss = 2.8034  Validation loss = 2.4612  \n",
      "\n",
      "Fold: 1  Epoch: 266  Training loss = 2.8025  Validation loss = 2.4588  \n",
      "\n",
      "Fold: 1  Epoch: 267  Training loss = 2.8010  Validation loss = 2.4552  \n",
      "\n",
      "Fold: 1  Epoch: 268  Training loss = 2.8004  Validation loss = 2.4533  \n",
      "\n",
      "Fold: 1  Epoch: 269  Training loss = 2.7989  Validation loss = 2.4497  \n",
      "\n",
      "Fold: 1  Epoch: 270  Training loss = 2.7975  Validation loss = 2.4463  \n",
      "\n",
      "Fold: 1  Epoch: 271  Training loss = 2.7963  Validation loss = 2.4427  \n",
      "\n",
      "Fold: 1  Epoch: 272  Training loss = 2.7954  Validation loss = 2.4405  \n",
      "\n",
      "Fold: 1  Epoch: 273  Training loss = 2.7949  Validation loss = 2.4390  \n",
      "\n",
      "Fold: 1  Epoch: 274  Training loss = 2.7938  Validation loss = 2.4359  \n",
      "\n",
      "Fold: 1  Epoch: 275  Training loss = 2.7927  Validation loss = 2.4330  \n",
      "\n",
      "Fold: 1  Epoch: 276  Training loss = 2.7912  Validation loss = 2.4292  \n",
      "\n",
      "Fold: 1  Epoch: 277  Training loss = 2.7905  Validation loss = 2.4273  \n",
      "\n",
      "Fold: 1  Epoch: 278  Training loss = 2.7895  Validation loss = 2.4249  \n",
      "\n",
      "Fold: 1  Epoch: 279  Training loss = 2.7886  Validation loss = 2.4226  \n",
      "\n",
      "Fold: 1  Epoch: 280  Training loss = 2.7873  Validation loss = 2.4191  \n",
      "\n",
      "Fold: 1  Epoch: 281  Training loss = 2.7861  Validation loss = 2.4162  \n",
      "\n",
      "Fold: 1  Epoch: 282  Training loss = 2.7852  Validation loss = 2.4137  \n",
      "\n",
      "Fold: 1  Epoch: 283  Training loss = 2.7847  Validation loss = 2.4123  \n",
      "\n",
      "Fold: 1  Epoch: 284  Training loss = 2.7832  Validation loss = 2.4084  \n",
      "\n",
      "Fold: 1  Epoch: 285  Training loss = 2.7825  Validation loss = 2.4066  \n",
      "\n",
      "Fold: 1  Epoch: 286  Training loss = 2.7817  Validation loss = 2.4045  \n",
      "\n",
      "Fold: 1  Epoch: 287  Training loss = 2.7812  Validation loss = 2.4028  \n",
      "\n",
      "Fold: 1  Epoch: 288  Training loss = 2.7800  Validation loss = 2.3997  \n",
      "\n",
      "Fold: 1  Epoch: 289  Training loss = 2.7786  Validation loss = 2.3961  \n",
      "\n",
      "Fold: 1  Epoch: 290  Training loss = 2.7772  Validation loss = 2.3922  \n",
      "\n",
      "Fold: 1  Epoch: 291  Training loss = 2.7764  Validation loss = 2.3900  \n",
      "\n",
      "Fold: 1  Epoch: 292  Training loss = 2.7758  Validation loss = 2.3883  \n",
      "\n",
      "Fold: 1  Epoch: 293  Training loss = 2.7736  Validation loss = 2.3824  \n",
      "\n",
      "Fold: 1  Epoch: 294  Training loss = 2.7731  Validation loss = 2.3812  \n",
      "\n",
      "Fold: 1  Epoch: 295  Training loss = 2.7712  Validation loss = 2.3762  \n",
      "\n",
      "Fold: 1  Epoch: 296  Training loss = 2.7707  Validation loss = 2.3748  \n",
      "\n",
      "Fold: 1  Epoch: 297  Training loss = 2.7694  Validation loss = 2.3714  \n",
      "\n",
      "Fold: 1  Epoch: 298  Training loss = 2.7676  Validation loss = 2.3666  \n",
      "\n",
      "Fold: 1  Epoch: 299  Training loss = 2.7667  Validation loss = 2.3640  \n",
      "\n",
      "Fold: 1  Epoch: 300  Training loss = 2.7655  Validation loss = 2.3607  \n",
      "\n",
      "Fold: 1  Epoch: 301  Training loss = 2.7649  Validation loss = 2.3590  \n",
      "\n",
      "Fold: 1  Epoch: 302  Training loss = 2.7638  Validation loss = 2.3558  \n",
      "\n",
      "Fold: 1  Epoch: 303  Training loss = 2.7624  Validation loss = 2.3521  \n",
      "\n",
      "Fold: 1  Epoch: 304  Training loss = 2.7614  Validation loss = 2.3493  \n",
      "\n",
      "Fold: 1  Epoch: 305  Training loss = 2.7607  Validation loss = 2.3474  \n",
      "\n",
      "Fold: 1  Epoch: 306  Training loss = 2.7599  Validation loss = 2.3451  \n",
      "\n",
      "Fold: 1  Epoch: 307  Training loss = 2.7593  Validation loss = 2.3433  \n",
      "\n",
      "Fold: 1  Epoch: 308  Training loss = 2.7589  Validation loss = 2.3421  \n",
      "\n",
      "Fold: 1  Epoch: 309  Training loss = 2.7584  Validation loss = 2.3406  \n",
      "\n",
      "Fold: 1  Epoch: 310  Training loss = 2.7571  Validation loss = 2.3371  \n",
      "\n",
      "Fold: 1  Epoch: 311  Training loss = 2.7565  Validation loss = 2.3352  \n",
      "\n",
      "Fold: 1  Epoch: 312  Training loss = 2.7563  Validation loss = 2.3345  \n",
      "\n",
      "Fold: 1  Epoch: 313  Training loss = 2.7551  Validation loss = 2.3315  \n",
      "\n",
      "Fold: 1  Epoch: 314  Training loss = 2.7539  Validation loss = 2.3284  \n",
      "\n",
      "Fold: 1  Epoch: 315  Training loss = 2.7532  Validation loss = 2.3262  \n",
      "\n",
      "Fold: 1  Epoch: 316  Training loss = 2.7517  Validation loss = 2.3221  \n",
      "\n",
      "Fold: 1  Epoch: 317  Training loss = 2.7511  Validation loss = 2.3204  \n",
      "\n",
      "Fold: 1  Epoch: 318  Training loss = 2.7501  Validation loss = 2.3173  \n",
      "\n",
      "Fold: 1  Epoch: 319  Training loss = 2.7491  Validation loss = 2.3145  \n",
      "\n",
      "Fold: 1  Epoch: 320  Training loss = 2.7477  Validation loss = 2.3107  \n",
      "\n",
      "Fold: 1  Epoch: 321  Training loss = 2.7471  Validation loss = 2.3086  \n",
      "\n",
      "Fold: 1  Epoch: 322  Training loss = 2.7466  Validation loss = 2.3072  \n",
      "\n",
      "Fold: 1  Epoch: 323  Training loss = 2.7453  Validation loss = 2.3035  \n",
      "\n",
      "Fold: 1  Epoch: 324  Training loss = 2.7445  Validation loss = 2.3007  \n",
      "\n",
      "Fold: 1  Epoch: 325  Training loss = 2.7435  Validation loss = 2.2976  \n",
      "\n",
      "Fold: 1  Epoch: 326  Training loss = 2.7426  Validation loss = 2.2952  \n",
      "\n",
      "Fold: 1  Epoch: 327  Training loss = 2.7422  Validation loss = 2.2938  \n",
      "\n",
      "Fold: 1  Epoch: 328  Training loss = 2.7410  Validation loss = 2.2903  \n",
      "\n",
      "Fold: 1  Epoch: 329  Training loss = 2.7404  Validation loss = 2.2887  \n",
      "\n",
      "Fold: 1  Epoch: 330  Training loss = 2.7394  Validation loss = 2.2858  \n",
      "\n",
      "Fold: 1  Epoch: 331  Training loss = 2.7390  Validation loss = 2.2847  \n",
      "\n",
      "Fold: 1  Epoch: 332  Training loss = 2.7383  Validation loss = 2.2828  \n",
      "\n",
      "Fold: 1  Epoch: 333  Training loss = 2.7375  Validation loss = 2.2802  \n",
      "\n",
      "Fold: 1  Epoch: 334  Training loss = 2.7368  Validation loss = 2.2781  \n",
      "\n",
      "Fold: 1  Epoch: 335  Training loss = 2.7359  Validation loss = 2.2756  \n",
      "\n",
      "Fold: 1  Epoch: 336  Training loss = 2.7349  Validation loss = 2.2727  \n",
      "\n",
      "Fold: 1  Epoch: 337  Training loss = 2.7340  Validation loss = 2.2701  \n",
      "\n",
      "Fold: 1  Epoch: 338  Training loss = 2.7332  Validation loss = 2.2678  \n",
      "\n",
      "Fold: 1  Epoch: 339  Training loss = 2.7319  Validation loss = 2.2642  \n",
      "\n",
      "Fold: 1  Epoch: 340  Training loss = 2.7316  Validation loss = 2.2630  \n",
      "\n",
      "Fold: 1  Epoch: 341  Training loss = 2.7314  Validation loss = 2.2622  \n",
      "\n",
      "Fold: 1  Epoch: 342  Training loss = 2.7311  Validation loss = 2.2614  \n",
      "\n",
      "Fold: 1  Epoch: 343  Training loss = 2.7308  Validation loss = 2.2602  \n",
      "\n",
      "Fold: 1  Epoch: 344  Training loss = 2.7304  Validation loss = 2.2589  \n",
      "\n",
      "Fold: 1  Epoch: 345  Training loss = 2.7292  Validation loss = 2.2552  \n",
      "\n",
      "Fold: 1  Epoch: 346  Training loss = 2.7286  Validation loss = 2.2536  \n",
      "\n",
      "Fold: 1  Epoch: 347  Training loss = 2.7284  Validation loss = 2.2528  \n",
      "\n",
      "Fold: 1  Epoch: 348  Training loss = 2.7274  Validation loss = 2.2498  \n",
      "\n",
      "Fold: 1  Epoch: 349  Training loss = 2.7268  Validation loss = 2.2481  \n",
      "\n",
      "Fold: 1  Epoch: 350  Training loss = 2.7255  Validation loss = 2.2440  \n",
      "\n",
      "Fold: 1  Epoch: 351  Training loss = 2.7246  Validation loss = 2.2411  \n",
      "\n",
      "Fold: 1  Epoch: 352  Training loss = 2.7239  Validation loss = 2.2389  \n",
      "\n",
      "Fold: 1  Epoch: 353  Training loss = 2.7227  Validation loss = 2.2350  \n",
      "\n",
      "Fold: 1  Epoch: 354  Training loss = 2.7218  Validation loss = 2.2323  \n",
      "\n",
      "Fold: 1  Epoch: 355  Training loss = 2.7211  Validation loss = 2.2302  \n",
      "\n",
      "Fold: 1  Epoch: 356  Training loss = 2.7209  Validation loss = 2.2295  \n",
      "\n",
      "Fold: 1  Epoch: 357  Training loss = 2.7201  Validation loss = 2.2274  \n",
      "\n",
      "Fold: 1  Epoch: 358  Training loss = 2.7186  Validation loss = 2.2227  \n",
      "\n",
      "Fold: 1  Epoch: 359  Training loss = 2.7182  Validation loss = 2.2215  \n",
      "\n",
      "Fold: 1  Epoch: 360  Training loss = 2.7176  Validation loss = 2.2193  \n",
      "\n",
      "Fold: 1  Epoch: 361  Training loss = 2.7172  Validation loss = 2.2180  \n",
      "\n",
      "Fold: 1  Epoch: 362  Training loss = 2.7165  Validation loss = 2.2159  \n",
      "\n",
      "Fold: 1  Epoch: 363  Training loss = 2.7154  Validation loss = 2.2128  \n",
      "\n",
      "Fold: 1  Epoch: 364  Training loss = 2.7143  Validation loss = 2.2090  \n",
      "\n",
      "Fold: 1  Epoch: 365  Training loss = 2.7135  Validation loss = 2.2067  \n",
      "\n",
      "Fold: 1  Epoch: 366  Training loss = 2.7124  Validation loss = 2.2033  \n",
      "\n",
      "Fold: 1  Epoch: 367  Training loss = 2.7120  Validation loss = 2.2019  \n",
      "\n",
      "Fold: 1  Epoch: 368  Training loss = 2.7116  Validation loss = 2.2004  \n",
      "\n",
      "Fold: 1  Epoch: 369  Training loss = 2.7108  Validation loss = 2.1978  \n",
      "\n",
      "Fold: 1  Epoch: 370  Training loss = 2.7096  Validation loss = 2.1941  \n",
      "\n",
      "Fold: 1  Epoch: 371  Training loss = 2.7092  Validation loss = 2.1930  \n",
      "\n",
      "Fold: 1  Epoch: 372  Training loss = 2.7086  Validation loss = 2.1910  \n",
      "\n",
      "Fold: 1  Epoch: 373  Training loss = 2.7079  Validation loss = 2.1885  \n",
      "\n",
      "Fold: 1  Epoch: 374  Training loss = 2.7070  Validation loss = 2.1860  \n",
      "\n",
      "Fold: 1  Epoch: 375  Training loss = 2.7064  Validation loss = 2.1841  \n",
      "\n",
      "Fold: 1  Epoch: 376  Training loss = 2.7057  Validation loss = 2.1819  \n",
      "\n",
      "Fold: 1  Epoch: 377  Training loss = 2.7049  Validation loss = 2.1793  \n",
      "\n",
      "Fold: 1  Epoch: 378  Training loss = 2.7043  Validation loss = 2.1777  \n",
      "\n",
      "Fold: 1  Epoch: 379  Training loss = 2.7037  Validation loss = 2.1759  \n",
      "\n",
      "Fold: 1  Epoch: 380  Training loss = 2.7033  Validation loss = 2.1747  \n",
      "\n",
      "Fold: 1  Epoch: 381  Training loss = 2.7029  Validation loss = 2.1736  \n",
      "\n",
      "Fold: 1  Epoch: 382  Training loss = 2.7020  Validation loss = 2.1705  \n",
      "\n",
      "Fold: 1  Epoch: 383  Training loss = 2.7011  Validation loss = 2.1679  \n",
      "\n",
      "Fold: 1  Epoch: 384  Training loss = 2.7009  Validation loss = 2.1673  \n",
      "\n",
      "Fold: 1  Epoch: 385  Training loss = 2.7006  Validation loss = 2.1662  \n",
      "\n",
      "Fold: 1  Epoch: 386  Training loss = 2.7002  Validation loss = 2.1649  \n",
      "\n",
      "Fold: 1  Epoch: 387  Training loss = 2.6997  Validation loss = 2.1632  \n",
      "\n",
      "Fold: 1  Epoch: 388  Training loss = 2.6990  Validation loss = 2.1607  \n",
      "\n",
      "Fold: 1  Epoch: 389  Training loss = 2.6978  Validation loss = 2.1569  \n",
      "\n",
      "Fold: 1  Epoch: 390  Training loss = 2.6973  Validation loss = 2.1552  \n",
      "\n",
      "Fold: 1  Epoch: 391  Training loss = 2.6966  Validation loss = 2.1528  \n",
      "\n",
      "Fold: 1  Epoch: 392  Training loss = 2.6965  Validation loss = 2.1526  \n",
      "\n",
      "Fold: 1  Epoch: 393  Training loss = 2.6961  Validation loss = 2.1516  \n",
      "\n",
      "Fold: 1  Epoch: 394  Training loss = 2.6956  Validation loss = 2.1498  \n",
      "\n",
      "Fold: 1  Epoch: 395  Training loss = 2.6949  Validation loss = 2.1479  \n",
      "\n",
      "Fold: 1  Epoch: 396  Training loss = 2.6941  Validation loss = 2.1453  \n",
      "\n",
      "Fold: 1  Epoch: 397  Training loss = 2.6936  Validation loss = 2.1436  \n",
      "\n",
      "Fold: 1  Epoch: 398  Training loss = 2.6927  Validation loss = 2.1408  \n",
      "\n",
      "Fold: 1  Epoch: 399  Training loss = 2.6925  Validation loss = 2.1400  \n",
      "\n",
      "Fold: 1  Epoch: 400  Training loss = 2.6923  Validation loss = 2.1393  \n",
      "\n",
      "Fold: 1  Epoch: 401  Training loss = 2.6915  Validation loss = 2.1363  \n",
      "\n",
      "Fold: 1  Epoch: 402  Training loss = 2.6909  Validation loss = 2.1346  \n",
      "\n",
      "Fold: 1  Epoch: 403  Training loss = 2.6903  Validation loss = 2.1324  \n",
      "\n",
      "Fold: 1  Epoch: 404  Training loss = 2.6900  Validation loss = 2.1316  \n",
      "\n",
      "Fold: 1  Epoch: 405  Training loss = 2.6898  Validation loss = 2.1312  \n",
      "\n",
      "Fold: 1  Epoch: 406  Training loss = 2.6889  Validation loss = 2.1281  \n",
      "\n",
      "Fold: 1  Epoch: 407  Training loss = 2.6878  Validation loss = 2.1244  \n",
      "\n",
      "Fold: 1  Epoch: 408  Training loss = 2.6875  Validation loss = 2.1233  \n",
      "\n",
      "Fold: 1  Epoch: 409  Training loss = 2.6869  Validation loss = 2.1211  \n",
      "\n",
      "Fold: 1  Epoch: 410  Training loss = 2.6863  Validation loss = 2.1188  \n",
      "\n",
      "Fold: 1  Epoch: 411  Training loss = 2.6859  Validation loss = 2.1173  \n",
      "\n",
      "Fold: 1  Epoch: 412  Training loss = 2.6852  Validation loss = 2.1152  \n",
      "\n",
      "Fold: 1  Epoch: 413  Training loss = 2.6848  Validation loss = 2.1138  \n",
      "\n",
      "Fold: 1  Epoch: 414  Training loss = 2.6842  Validation loss = 2.1117  \n",
      "\n",
      "Fold: 1  Epoch: 415  Training loss = 2.6837  Validation loss = 2.1100  \n",
      "\n",
      "Fold: 1  Epoch: 416  Training loss = 2.6831  Validation loss = 2.1077  \n",
      "\n",
      "Fold: 1  Epoch: 417  Training loss = 2.6826  Validation loss = 2.1060  \n",
      "\n",
      "Fold: 1  Epoch: 418  Training loss = 2.6817  Validation loss = 2.1026  \n",
      "\n",
      "Fold: 1  Epoch: 419  Training loss = 2.6814  Validation loss = 2.1018  \n",
      "\n",
      "Fold: 1  Epoch: 420  Training loss = 2.6807  Validation loss = 2.0997  \n",
      "\n",
      "Fold: 1  Epoch: 421  Training loss = 2.6797  Validation loss = 2.0960  \n",
      "\n",
      "Fold: 1  Epoch: 422  Training loss = 2.6790  Validation loss = 2.0934  \n",
      "\n",
      "Fold: 1  Epoch: 423  Training loss = 2.6781  Validation loss = 2.0902  \n",
      "\n",
      "Fold: 1  Epoch: 424  Training loss = 2.6776  Validation loss = 2.0883  \n",
      "\n",
      "Fold: 1  Epoch: 425  Training loss = 2.6770  Validation loss = 2.0860  \n",
      "\n",
      "Fold: 1  Epoch: 426  Training loss = 2.6767  Validation loss = 2.0853  \n",
      "\n",
      "Fold: 1  Epoch: 427  Training loss = 2.6765  Validation loss = 2.0846  \n",
      "\n",
      "Fold: 1  Epoch: 428  Training loss = 2.6756  Validation loss = 2.0811  \n",
      "\n",
      "Fold: 1  Epoch: 429  Training loss = 2.6750  Validation loss = 2.0789  \n",
      "\n",
      "Fold: 1  Epoch: 430  Training loss = 2.6743  Validation loss = 2.0767  \n",
      "\n",
      "Fold: 1  Epoch: 431  Training loss = 2.6740  Validation loss = 2.0756  \n",
      "\n",
      "Fold: 1  Epoch: 432  Training loss = 2.6737  Validation loss = 2.0747  \n",
      "\n",
      "Fold: 1  Epoch: 433  Training loss = 2.6733  Validation loss = 2.0732  \n",
      "\n",
      "Fold: 1  Epoch: 434  Training loss = 2.6727  Validation loss = 2.0712  \n",
      "\n",
      "Fold: 1  Epoch: 435  Training loss = 2.6721  Validation loss = 2.0688  \n",
      "\n",
      "Fold: 1  Epoch: 436  Training loss = 2.6718  Validation loss = 2.0678  \n",
      "\n",
      "Fold: 1  Epoch: 437  Training loss = 2.6713  Validation loss = 2.0659  \n",
      "\n",
      "Fold: 1  Epoch: 438  Training loss = 2.6707  Validation loss = 2.0636  \n",
      "\n",
      "Fold: 1  Epoch: 439  Training loss = 2.6693  Validation loss = 2.0589  \n",
      "\n",
      "Fold: 1  Epoch: 440  Training loss = 2.6686  Validation loss = 2.0562  \n",
      "\n",
      "Fold: 1  Epoch: 441  Training loss = 2.6682  Validation loss = 2.0546  \n",
      "\n",
      "Fold: 1  Epoch: 442  Training loss = 2.6677  Validation loss = 2.0528  \n",
      "\n",
      "Fold: 1  Epoch: 443  Training loss = 2.6672  Validation loss = 2.0510  \n",
      "\n",
      "Fold: 1  Epoch: 444  Training loss = 2.6670  Validation loss = 2.0503  \n",
      "\n",
      "Fold: 1  Epoch: 445  Training loss = 2.6665  Validation loss = 2.0482  \n",
      "\n",
      "Fold: 1  Epoch: 446  Training loss = 2.6662  Validation loss = 2.0472  \n",
      "\n",
      "Fold: 1  Epoch: 447  Training loss = 2.6655  Validation loss = 2.0444  \n",
      "\n",
      "Fold: 1  Epoch: 448  Training loss = 2.6647  Validation loss = 2.0417  \n",
      "\n",
      "Fold: 1  Epoch: 449  Training loss = 2.6641  Validation loss = 2.0394  \n",
      "\n",
      "Fold: 1  Epoch: 450  Training loss = 2.6635  Validation loss = 2.0373  \n",
      "\n",
      "Fold: 1  Epoch: 451  Training loss = 2.6630  Validation loss = 2.0355  \n",
      "\n",
      "Fold: 1  Epoch: 452  Training loss = 2.6622  Validation loss = 2.0325  \n",
      "\n",
      "Fold: 1  Epoch: 453  Training loss = 2.6620  Validation loss = 2.0315  \n",
      "\n",
      "Fold: 1  Epoch: 454  Training loss = 2.6618  Validation loss = 2.0310  \n",
      "\n",
      "Fold: 1  Epoch: 455  Training loss = 2.6612  Validation loss = 2.0286  \n",
      "\n",
      "Fold: 1  Epoch: 456  Training loss = 2.6612  Validation loss = 2.0287  \n",
      "\n",
      "Fold: 1  Epoch: 457  Training loss = 2.6610  Validation loss = 2.0283  \n",
      "\n",
      "Fold: 1  Epoch: 458  Training loss = 2.6609  Validation loss = 2.0277  \n",
      "\n",
      "Fold: 1  Epoch: 459  Training loss = 2.6610  Validation loss = 2.0283  \n",
      "\n",
      "Fold: 1  Epoch: 460  Training loss = 2.6606  Validation loss = 2.0268  \n",
      "\n",
      "Fold: 1  Epoch: 461  Training loss = 2.6601  Validation loss = 2.0253  \n",
      "\n",
      "Fold: 1  Epoch: 462  Training loss = 2.6600  Validation loss = 2.0247  \n",
      "\n",
      "Fold: 1  Epoch: 463  Training loss = 2.6595  Validation loss = 2.0231  \n",
      "\n",
      "Fold: 1  Epoch: 464  Training loss = 2.6590  Validation loss = 2.0214  \n",
      "\n",
      "Fold: 1  Epoch: 465  Training loss = 2.6584  Validation loss = 2.0191  \n",
      "\n",
      "Fold: 1  Epoch: 466  Training loss = 2.6581  Validation loss = 2.0181  \n",
      "\n",
      "Fold: 1  Epoch: 467  Training loss = 2.6576  Validation loss = 2.0162  \n",
      "\n",
      "Fold: 1  Epoch: 468  Training loss = 2.6573  Validation loss = 2.0153  \n",
      "\n",
      "Fold: 1  Epoch: 469  Training loss = 2.6568  Validation loss = 2.0136  \n",
      "\n",
      "Fold: 1  Epoch: 470  Training loss = 2.6566  Validation loss = 2.0126  \n",
      "\n",
      "Fold: 1  Epoch: 471  Training loss = 2.6561  Validation loss = 2.0107  \n",
      "\n",
      "Fold: 1  Epoch: 472  Training loss = 2.6556  Validation loss = 2.0089  \n",
      "\n",
      "Fold: 1  Epoch: 473  Training loss = 2.6546  Validation loss = 2.0050  \n",
      "\n",
      "Fold: 1  Epoch: 474  Training loss = 2.6541  Validation loss = 2.0028  \n",
      "\n",
      "Fold: 1  Epoch: 475  Training loss = 2.6539  Validation loss = 2.0023  \n",
      "\n",
      "Fold: 1  Epoch: 476  Training loss = 2.6530  Validation loss = 1.9989  \n",
      "\n",
      "Fold: 1  Epoch: 477  Training loss = 2.6524  Validation loss = 1.9965  \n",
      "\n",
      "Fold: 1  Epoch: 478  Training loss = 2.6519  Validation loss = 1.9946  \n",
      "\n",
      "Fold: 1  Epoch: 479  Training loss = 2.6517  Validation loss = 1.9940  \n",
      "\n",
      "Fold: 1  Epoch: 480  Training loss = 2.6513  Validation loss = 1.9927  \n",
      "\n",
      "Fold: 1  Epoch: 481  Training loss = 2.6508  Validation loss = 1.9909  \n",
      "\n",
      "Fold: 1  Epoch: 482  Training loss = 2.6506  Validation loss = 1.9904  \n",
      "\n",
      "Fold: 1  Epoch: 483  Training loss = 2.6504  Validation loss = 1.9899  \n",
      "\n",
      "Fold: 1  Epoch: 484  Training loss = 2.6499  Validation loss = 1.9881  \n",
      "\n",
      "Fold: 1  Epoch: 485  Training loss = 2.6495  Validation loss = 1.9863  \n",
      "\n",
      "Fold: 1  Epoch: 486  Training loss = 2.6492  Validation loss = 1.9851  \n",
      "\n",
      "Fold: 1  Epoch: 487  Training loss = 2.6488  Validation loss = 1.9839  \n",
      "\n",
      "Fold: 1  Epoch: 488  Training loss = 2.6487  Validation loss = 1.9833  \n",
      "\n",
      "Fold: 1  Epoch: 489  Training loss = 2.6484  Validation loss = 1.9824  \n",
      "\n",
      "Fold: 1  Epoch: 490  Training loss = 2.6482  Validation loss = 1.9815  \n",
      "\n",
      "Fold: 1  Epoch: 491  Training loss = 2.6478  Validation loss = 1.9801  \n",
      "\n",
      "Fold: 1  Epoch: 492  Training loss = 2.6478  Validation loss = 1.9802  \n",
      "\n",
      "Fold: 1  Epoch: 493  Training loss = 2.6477  Validation loss = 1.9798  \n",
      "\n",
      "Fold: 1  Epoch: 494  Training loss = 2.6477  Validation loss = 1.9800  \n",
      "\n",
      "Fold: 1  Epoch: 495  Training loss = 2.6474  Validation loss = 1.9787  \n",
      "\n",
      "Fold: 1  Epoch: 496  Training loss = 2.6471  Validation loss = 1.9779  \n",
      "\n",
      "Fold: 1  Epoch: 497  Training loss = 2.6470  Validation loss = 1.9778  \n",
      "\n",
      "Fold: 1  Epoch: 498  Training loss = 2.6464  Validation loss = 1.9754  \n",
      "\n",
      "Fold: 1  Epoch: 499  Training loss = 2.6461  Validation loss = 1.9741  \n",
      "\n",
      "Fold: 1  Epoch: 500  Training loss = 2.6456  Validation loss = 1.9720  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 500  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.5495  Validation loss = 2.1442  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.5493  Validation loss = 2.1436  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.5489  Validation loss = 2.1424  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.5484  Validation loss = 2.1414  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.5480  Validation loss = 2.1403  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.5479  Validation loss = 2.1399  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.5473  Validation loss = 2.1384  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.5472  Validation loss = 2.1380  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.5466  Validation loss = 2.1366  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.5460  Validation loss = 2.1347  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.5460  Validation loss = 2.1349  \n",
      "\n",
      "Fold: 2  Epoch: 12  Training loss = 2.5460  Validation loss = 2.1349  \n",
      "\n",
      "Fold: 2  Epoch: 13  Training loss = 2.5456  Validation loss = 2.1339  \n",
      "\n",
      "Fold: 2  Epoch: 14  Training loss = 2.5451  Validation loss = 2.1329  \n",
      "\n",
      "Fold: 2  Epoch: 15  Training loss = 2.5440  Validation loss = 2.1299  \n",
      "\n",
      "Fold: 2  Epoch: 16  Training loss = 2.5440  Validation loss = 2.1303  \n",
      "\n",
      "Fold: 2  Epoch: 17  Training loss = 2.5439  Validation loss = 2.1299  \n",
      "\n",
      "Fold: 2  Epoch: 18  Training loss = 2.5432  Validation loss = 2.1283  \n",
      "\n",
      "Fold: 2  Epoch: 19  Training loss = 2.5427  Validation loss = 2.1272  \n",
      "\n",
      "Fold: 2  Epoch: 20  Training loss = 2.5419  Validation loss = 2.1251  \n",
      "\n",
      "Fold: 2  Epoch: 21  Training loss = 2.5415  Validation loss = 2.1240  \n",
      "\n",
      "Fold: 2  Epoch: 22  Training loss = 2.5415  Validation loss = 2.1238  \n",
      "\n",
      "Fold: 2  Epoch: 23  Training loss = 2.5414  Validation loss = 2.1236  \n",
      "\n",
      "Fold: 2  Epoch: 24  Training loss = 2.5410  Validation loss = 2.1226  \n",
      "\n",
      "Fold: 2  Epoch: 25  Training loss = 2.5403  Validation loss = 2.1211  \n",
      "\n",
      "Fold: 2  Epoch: 26  Training loss = 2.5397  Validation loss = 2.1199  \n",
      "\n",
      "Fold: 2  Epoch: 27  Training loss = 2.5392  Validation loss = 2.1187  \n",
      "\n",
      "Fold: 2  Epoch: 28  Training loss = 2.5390  Validation loss = 2.1182  \n",
      "\n",
      "Fold: 2  Epoch: 29  Training loss = 2.5385  Validation loss = 2.1171  \n",
      "\n",
      "Fold: 2  Epoch: 30  Training loss = 2.5381  Validation loss = 2.1160  \n",
      "\n",
      "Fold: 2  Epoch: 31  Training loss = 2.5375  Validation loss = 2.1146  \n",
      "\n",
      "Fold: 2  Epoch: 32  Training loss = 2.5374  Validation loss = 2.1142  \n",
      "\n",
      "Fold: 2  Epoch: 33  Training loss = 2.5372  Validation loss = 2.1140  \n",
      "\n",
      "Fold: 2  Epoch: 34  Training loss = 2.5373  Validation loss = 2.1140  \n",
      "\n",
      "Fold: 2  Epoch: 35  Training loss = 2.5371  Validation loss = 2.1136  \n",
      "\n",
      "Fold: 2  Epoch: 36  Training loss = 2.5366  Validation loss = 2.1128  \n",
      "\n",
      "Fold: 2  Epoch: 37  Training loss = 2.5363  Validation loss = 2.1121  \n",
      "\n",
      "Fold: 2  Epoch: 38  Training loss = 2.5361  Validation loss = 2.1115  \n",
      "\n",
      "Fold: 2  Epoch: 39  Training loss = 2.5356  Validation loss = 2.1099  \n",
      "\n",
      "Fold: 2  Epoch: 40  Training loss = 2.5351  Validation loss = 2.1087  \n",
      "\n",
      "Fold: 2  Epoch: 41  Training loss = 2.5351  Validation loss = 2.1086  \n",
      "\n",
      "Fold: 2  Epoch: 42  Training loss = 2.5347  Validation loss = 2.1075  \n",
      "\n",
      "Fold: 2  Epoch: 43  Training loss = 2.5346  Validation loss = 2.1073  \n",
      "\n",
      "Fold: 2  Epoch: 44  Training loss = 2.5342  Validation loss = 2.1063  \n",
      "\n",
      "Fold: 2  Epoch: 45  Training loss = 2.5338  Validation loss = 2.1054  \n",
      "\n",
      "Fold: 2  Epoch: 46  Training loss = 2.5336  Validation loss = 2.1045  \n",
      "\n",
      "Fold: 2  Epoch: 47  Training loss = 2.5333  Validation loss = 2.1041  \n",
      "\n",
      "Fold: 2  Epoch: 48  Training loss = 2.5331  Validation loss = 2.1033  \n",
      "\n",
      "Fold: 2  Epoch: 49  Training loss = 2.5331  Validation loss = 2.1033  \n",
      "\n",
      "Fold: 2  Epoch: 50  Training loss = 2.5328  Validation loss = 2.1024  \n",
      "\n",
      "Fold: 2  Epoch: 51  Training loss = 2.5326  Validation loss = 2.1017  \n",
      "\n",
      "Fold: 2  Epoch: 52  Training loss = 2.5320  Validation loss = 2.1003  \n",
      "\n",
      "Fold: 2  Epoch: 53  Training loss = 2.5316  Validation loss = 2.0991  \n",
      "\n",
      "Fold: 2  Epoch: 54  Training loss = 2.5312  Validation loss = 2.0981  \n",
      "\n",
      "Fold: 2  Epoch: 55  Training loss = 2.5311  Validation loss = 2.0977  \n",
      "\n",
      "Fold: 2  Epoch: 56  Training loss = 2.5305  Validation loss = 2.0965  \n",
      "\n",
      "Fold: 2  Epoch: 57  Training loss = 2.5303  Validation loss = 2.0961  \n",
      "\n",
      "Fold: 2  Epoch: 58  Training loss = 2.5300  Validation loss = 2.0955  \n",
      "\n",
      "Fold: 2  Epoch: 59  Training loss = 2.5298  Validation loss = 2.0950  \n",
      "\n",
      "Fold: 2  Epoch: 60  Training loss = 2.5293  Validation loss = 2.0935  \n",
      "\n",
      "Fold: 2  Epoch: 61  Training loss = 2.5292  Validation loss = 2.0930  \n",
      "\n",
      "Fold: 2  Epoch: 62  Training loss = 2.5285  Validation loss = 2.0911  \n",
      "\n",
      "Fold: 2  Epoch: 63  Training loss = 2.5283  Validation loss = 2.0909  \n",
      "\n",
      "Fold: 2  Epoch: 64  Training loss = 2.5282  Validation loss = 2.0905  \n",
      "\n",
      "Fold: 2  Epoch: 65  Training loss = 2.5279  Validation loss = 2.0892  \n",
      "\n",
      "Fold: 2  Epoch: 66  Training loss = 2.5275  Validation loss = 2.0884  \n",
      "\n",
      "Fold: 2  Epoch: 67  Training loss = 2.5270  Validation loss = 2.0873  \n",
      "\n",
      "Fold: 2  Epoch: 68  Training loss = 2.5264  Validation loss = 2.0860  \n",
      "\n",
      "Fold: 2  Epoch: 69  Training loss = 2.5262  Validation loss = 2.0856  \n",
      "\n",
      "Fold: 2  Epoch: 70  Training loss = 2.5261  Validation loss = 2.0854  \n",
      "\n",
      "Fold: 2  Epoch: 71  Training loss = 2.5259  Validation loss = 2.0843  \n",
      "\n",
      "Fold: 2  Epoch: 72  Training loss = 2.5258  Validation loss = 2.0842  \n",
      "\n",
      "Fold: 2  Epoch: 73  Training loss = 2.5257  Validation loss = 2.0840  \n",
      "\n",
      "Fold: 2  Epoch: 74  Training loss = 2.5256  Validation loss = 2.0834  \n",
      "\n",
      "Fold: 2  Epoch: 75  Training loss = 2.5255  Validation loss = 2.0831  \n",
      "\n",
      "Fold: 2  Epoch: 76  Training loss = 2.5255  Validation loss = 2.0830  \n",
      "\n",
      "Fold: 2  Epoch: 77  Training loss = 2.5251  Validation loss = 2.0820  \n",
      "\n",
      "Fold: 2  Epoch: 78  Training loss = 2.5245  Validation loss = 2.0803  \n",
      "\n",
      "Fold: 2  Epoch: 79  Training loss = 2.5243  Validation loss = 2.0798  \n",
      "\n",
      "Fold: 2  Epoch: 80  Training loss = 2.5238  Validation loss = 2.0788  \n",
      "\n",
      "Fold: 2  Epoch: 81  Training loss = 2.5236  Validation loss = 2.0782  \n",
      "\n",
      "Fold: 2  Epoch: 82  Training loss = 2.5236  Validation loss = 2.0779  \n",
      "\n",
      "Fold: 2  Epoch: 83  Training loss = 2.5237  Validation loss = 2.0782  \n",
      "\n",
      "Fold: 2  Epoch: 84  Training loss = 2.5233  Validation loss = 2.0775  \n",
      "\n",
      "Fold: 2  Epoch: 85  Training loss = 2.5230  Validation loss = 2.0767  \n",
      "\n",
      "Fold: 2  Epoch: 86  Training loss = 2.5225  Validation loss = 2.0753  \n",
      "\n",
      "Fold: 2  Epoch: 87  Training loss = 2.5223  Validation loss = 2.0747  \n",
      "\n",
      "Fold: 2  Epoch: 88  Training loss = 2.5220  Validation loss = 2.0742  \n",
      "\n",
      "Fold: 2  Epoch: 89  Training loss = 2.5216  Validation loss = 2.0734  \n",
      "\n",
      "Fold: 2  Epoch: 90  Training loss = 2.5214  Validation loss = 2.0730  \n",
      "\n",
      "Fold: 2  Epoch: 91  Training loss = 2.5210  Validation loss = 2.0719  \n",
      "\n",
      "Fold: 2  Epoch: 92  Training loss = 2.5204  Validation loss = 2.0703  \n",
      "\n",
      "Fold: 2  Epoch: 93  Training loss = 2.5204  Validation loss = 2.0704  \n",
      "\n",
      "Fold: 2  Epoch: 94  Training loss = 2.5200  Validation loss = 2.0692  \n",
      "\n",
      "Fold: 2  Epoch: 95  Training loss = 2.5200  Validation loss = 2.0691  \n",
      "\n",
      "Fold: 2  Epoch: 96  Training loss = 2.5197  Validation loss = 2.0684  \n",
      "\n",
      "Fold: 2  Epoch: 97  Training loss = 2.5196  Validation loss = 2.0681  \n",
      "\n",
      "Fold: 2  Epoch: 98  Training loss = 2.5193  Validation loss = 2.0671  \n",
      "\n",
      "Fold: 2  Epoch: 99  Training loss = 2.5190  Validation loss = 2.0661  \n",
      "\n",
      "Fold: 2  Epoch: 100  Training loss = 2.5189  Validation loss = 2.0657  \n",
      "\n",
      "Fold: 2  Epoch: 101  Training loss = 2.5188  Validation loss = 2.0651  \n",
      "\n",
      "Fold: 2  Epoch: 102  Training loss = 2.5188  Validation loss = 2.0653  \n",
      "\n",
      "Fold: 2  Epoch: 103  Training loss = 2.5187  Validation loss = 2.0651  \n",
      "\n",
      "Fold: 2  Epoch: 104  Training loss = 2.5182  Validation loss = 2.0637  \n",
      "\n",
      "Fold: 2  Epoch: 105  Training loss = 2.5180  Validation loss = 2.0632  \n",
      "\n",
      "Fold: 2  Epoch: 106  Training loss = 2.5179  Validation loss = 2.0628  \n",
      "\n",
      "Fold: 2  Epoch: 107  Training loss = 2.5176  Validation loss = 2.0617  \n",
      "\n",
      "Fold: 2  Epoch: 108  Training loss = 2.5173  Validation loss = 2.0610  \n",
      "\n",
      "Fold: 2  Epoch: 109  Training loss = 2.5172  Validation loss = 2.0608  \n",
      "\n",
      "Fold: 2  Epoch: 110  Training loss = 2.5168  Validation loss = 2.0596  \n",
      "\n",
      "Fold: 2  Epoch: 111  Training loss = 2.5165  Validation loss = 2.0590  \n",
      "\n",
      "Fold: 2  Epoch: 112  Training loss = 2.5164  Validation loss = 2.0586  \n",
      "\n",
      "Fold: 2  Epoch: 113  Training loss = 2.5162  Validation loss = 2.0585  \n",
      "\n",
      "Fold: 2  Epoch: 114  Training loss = 2.5162  Validation loss = 2.0588  \n",
      "\n",
      "Fold: 2  Epoch: 115  Training loss = 2.5160  Validation loss = 2.0581  \n",
      "\n",
      "Fold: 2  Epoch: 116  Training loss = 2.5158  Validation loss = 2.0577  \n",
      "\n",
      "Fold: 2  Epoch: 117  Training loss = 2.5157  Validation loss = 2.0573  \n",
      "\n",
      "Fold: 2  Epoch: 118  Training loss = 2.5156  Validation loss = 2.0568  \n",
      "\n",
      "Fold: 2  Epoch: 119  Training loss = 2.5156  Validation loss = 2.0567  \n",
      "\n",
      "Fold: 2  Epoch: 120  Training loss = 2.5152  Validation loss = 2.0554  \n",
      "\n",
      "Fold: 2  Epoch: 121  Training loss = 2.5151  Validation loss = 2.0550  \n",
      "\n",
      "Fold: 2  Epoch: 122  Training loss = 2.5146  Validation loss = 2.0534  \n",
      "\n",
      "Fold: 2  Epoch: 123  Training loss = 2.5144  Validation loss = 2.0528  \n",
      "\n",
      "Fold: 2  Epoch: 124  Training loss = 2.5144  Validation loss = 2.0534  \n",
      "\n",
      "Fold: 2  Epoch: 125  Training loss = 2.5139  Validation loss = 2.0520  \n",
      "\n",
      "Fold: 2  Epoch: 126  Training loss = 2.5139  Validation loss = 2.0518  \n",
      "\n",
      "Fold: 2  Epoch: 127  Training loss = 2.5137  Validation loss = 2.0515  \n",
      "\n",
      "Fold: 2  Epoch: 128  Training loss = 2.5135  Validation loss = 2.0514  \n",
      "\n",
      "Fold: 2  Epoch: 129  Training loss = 2.5134  Validation loss = 2.0510  \n",
      "\n",
      "Fold: 2  Epoch: 130  Training loss = 2.5135  Validation loss = 2.0513  \n",
      "\n",
      "Fold: 2  Epoch: 131  Training loss = 2.5130  Validation loss = 2.0499  \n",
      "\n",
      "Fold: 2  Epoch: 132  Training loss = 2.5127  Validation loss = 2.0490  \n",
      "\n",
      "Fold: 2  Epoch: 133  Training loss = 2.5123  Validation loss = 2.0478  \n",
      "\n",
      "Fold: 2  Epoch: 134  Training loss = 2.5120  Validation loss = 2.0467  \n",
      "\n",
      "Fold: 2  Epoch: 135  Training loss = 2.5115  Validation loss = 2.0455  \n",
      "\n",
      "Fold: 2  Epoch: 136  Training loss = 2.5114  Validation loss = 2.0452  \n",
      "\n",
      "Fold: 2  Epoch: 137  Training loss = 2.5112  Validation loss = 2.0448  \n",
      "\n",
      "Fold: 2  Epoch: 138  Training loss = 2.5111  Validation loss = 2.0442  \n",
      "\n",
      "Fold: 2  Epoch: 139  Training loss = 2.5110  Validation loss = 2.0439  \n",
      "\n",
      "Fold: 2  Epoch: 140  Training loss = 2.5107  Validation loss = 2.0431  \n",
      "\n",
      "Fold: 2  Epoch: 141  Training loss = 2.5104  Validation loss = 2.0421  \n",
      "\n",
      "Fold: 2  Epoch: 142  Training loss = 2.5097  Validation loss = 2.0399  \n",
      "\n",
      "Fold: 2  Epoch: 143  Training loss = 2.5091  Validation loss = 2.0385  \n",
      "\n",
      "Fold: 2  Epoch: 144  Training loss = 2.5092  Validation loss = 2.0386  \n",
      "\n",
      "Fold: 2  Epoch: 145  Training loss = 2.5089  Validation loss = 2.0377  \n",
      "\n",
      "Fold: 2  Epoch: 146  Training loss = 2.5088  Validation loss = 2.0375  \n",
      "\n",
      "Fold: 2  Epoch: 147  Training loss = 2.5089  Validation loss = 2.0376  \n",
      "\n",
      "Fold: 2  Epoch: 148  Training loss = 2.5087  Validation loss = 2.0370  \n",
      "\n",
      "Fold: 2  Epoch: 149  Training loss = 2.5084  Validation loss = 2.0359  \n",
      "\n",
      "Fold: 2  Epoch: 150  Training loss = 2.5081  Validation loss = 2.0349  \n",
      "\n",
      "Fold: 2  Epoch: 151  Training loss = 2.5080  Validation loss = 2.0346  \n",
      "\n",
      "Fold: 2  Epoch: 152  Training loss = 2.5078  Validation loss = 2.0343  \n",
      "\n",
      "Fold: 2  Epoch: 153  Training loss = 2.5076  Validation loss = 2.0339  \n",
      "\n",
      "Fold: 2  Epoch: 154  Training loss = 2.5075  Validation loss = 2.0334  \n",
      "\n",
      "Fold: 2  Epoch: 155  Training loss = 2.5073  Validation loss = 2.0333  \n",
      "\n",
      "Fold: 2  Epoch: 156  Training loss = 2.5071  Validation loss = 2.0328  \n",
      "\n",
      "Fold: 2  Epoch: 157  Training loss = 2.5069  Validation loss = 2.0321  \n",
      "\n",
      "Fold: 2  Epoch: 158  Training loss = 2.5068  Validation loss = 2.0317  \n",
      "\n",
      "Fold: 2  Epoch: 159  Training loss = 2.5065  Validation loss = 2.0312  \n",
      "\n",
      "Fold: 2  Epoch: 160  Training loss = 2.5060  Validation loss = 2.0297  \n",
      "\n",
      "Fold: 2  Epoch: 161  Training loss = 2.5060  Validation loss = 2.0297  \n",
      "\n",
      "Fold: 2  Epoch: 162  Training loss = 2.5056  Validation loss = 2.0285  \n",
      "\n",
      "Fold: 2  Epoch: 163  Training loss = 2.5055  Validation loss = 2.0282  \n",
      "\n",
      "Fold: 2  Epoch: 164  Training loss = 2.5051  Validation loss = 2.0265  \n",
      "\n",
      "Fold: 2  Epoch: 165  Training loss = 2.5044  Validation loss = 2.0244  \n",
      "\n",
      "Fold: 2  Epoch: 166  Training loss = 2.5043  Validation loss = 2.0239  \n",
      "\n",
      "Fold: 2  Epoch: 167  Training loss = 2.5043  Validation loss = 2.0241  \n",
      "\n",
      "Fold: 2  Epoch: 168  Training loss = 2.5041  Validation loss = 2.0233  \n",
      "\n",
      "Fold: 2  Epoch: 169  Training loss = 2.5041  Validation loss = 2.0234  \n",
      "\n",
      "Fold: 2  Epoch: 170  Training loss = 2.5040  Validation loss = 2.0234  \n",
      "\n",
      "Fold: 2  Epoch: 171  Training loss = 2.5037  Validation loss = 2.0225  \n",
      "\n",
      "Fold: 2  Epoch: 172  Training loss = 2.5036  Validation loss = 2.0217  \n",
      "\n",
      "Fold: 2  Epoch: 173  Training loss = 2.5034  Validation loss = 2.0213  \n",
      "\n",
      "Fold: 2  Epoch: 174  Training loss = 2.5031  Validation loss = 2.0202  \n",
      "\n",
      "Fold: 2  Epoch: 175  Training loss = 2.5029  Validation loss = 2.0194  \n",
      "\n",
      "Fold: 2  Epoch: 176  Training loss = 2.5027  Validation loss = 2.0186  \n",
      "\n",
      "Fold: 2  Epoch: 177  Training loss = 2.5025  Validation loss = 2.0182  \n",
      "\n",
      "Fold: 2  Epoch: 178  Training loss = 2.5022  Validation loss = 2.0172  \n",
      "\n",
      "Fold: 2  Epoch: 179  Training loss = 2.5019  Validation loss = 2.0166  \n",
      "\n",
      "Fold: 2  Epoch: 180  Training loss = 2.5016  Validation loss = 2.0155  \n",
      "\n",
      "Fold: 2  Epoch: 181  Training loss = 2.5016  Validation loss = 2.0154  \n",
      "\n",
      "Fold: 2  Epoch: 182  Training loss = 2.5015  Validation loss = 2.0152  \n",
      "\n",
      "Fold: 2  Epoch: 183  Training loss = 2.5015  Validation loss = 2.0154  \n",
      "\n",
      "Fold: 2  Epoch: 184  Training loss = 2.5012  Validation loss = 2.0148  \n",
      "\n",
      "Fold: 2  Epoch: 185  Training loss = 2.5011  Validation loss = 2.0143  \n",
      "\n",
      "Fold: 2  Epoch: 186  Training loss = 2.5010  Validation loss = 2.0145  \n",
      "\n",
      "Fold: 2  Epoch: 187  Training loss = 2.5006  Validation loss = 2.0130  \n",
      "\n",
      "Fold: 2  Epoch: 188  Training loss = 2.5006  Validation loss = 2.0133  \n",
      "\n",
      "Fold: 2  Epoch: 189  Training loss = 2.5003  Validation loss = 2.0124  \n",
      "\n",
      "Fold: 2  Epoch: 190  Training loss = 2.5002  Validation loss = 2.0121  \n",
      "\n",
      "Fold: 2  Epoch: 191  Training loss = 2.5000  Validation loss = 2.0115  \n",
      "\n",
      "Fold: 2  Epoch: 192  Training loss = 2.5000  Validation loss = 2.0117  \n",
      "\n",
      "Fold: 2  Epoch: 193  Training loss = 2.4997  Validation loss = 2.0110  \n",
      "\n",
      "Fold: 2  Epoch: 194  Training loss = 2.4997  Validation loss = 2.0111  \n",
      "\n",
      "Fold: 2  Epoch: 195  Training loss = 2.4997  Validation loss = 2.0110  \n",
      "\n",
      "Fold: 2  Epoch: 196  Training loss = 2.4995  Validation loss = 2.0102  \n",
      "\n",
      "Fold: 2  Epoch: 197  Training loss = 2.4993  Validation loss = 2.0094  \n",
      "\n",
      "Fold: 2  Epoch: 198  Training loss = 2.4992  Validation loss = 2.0090  \n",
      "\n",
      "Fold: 2  Epoch: 199  Training loss = 2.4990  Validation loss = 2.0086  \n",
      "\n",
      "Fold: 2  Epoch: 200  Training loss = 2.4988  Validation loss = 2.0083  \n",
      "\n",
      "Fold: 2  Epoch: 201  Training loss = 2.4984  Validation loss = 2.0069  \n",
      "\n",
      "Fold: 2  Epoch: 202  Training loss = 2.4984  Validation loss = 2.0070  \n",
      "\n",
      "Fold: 2  Epoch: 203  Training loss = 2.4982  Validation loss = 2.0063  \n",
      "\n",
      "Fold: 2  Epoch: 204  Training loss = 2.4980  Validation loss = 2.0056  \n",
      "\n",
      "Fold: 2  Epoch: 205  Training loss = 2.4977  Validation loss = 2.0048  \n",
      "\n",
      "Fold: 2  Epoch: 206  Training loss = 2.4975  Validation loss = 2.0044  \n",
      "\n",
      "Fold: 2  Epoch: 207  Training loss = 2.4974  Validation loss = 2.0038  \n",
      "\n",
      "Fold: 2  Epoch: 208  Training loss = 2.4972  Validation loss = 2.0028  \n",
      "\n",
      "Fold: 2  Epoch: 209  Training loss = 2.4972  Validation loss = 2.0032  \n",
      "\n",
      "Fold: 2  Epoch: 210  Training loss = 2.4970  Validation loss = 2.0022  \n",
      "\n",
      "Fold: 2  Epoch: 211  Training loss = 2.4967  Validation loss = 2.0010  \n",
      "\n",
      "Fold: 2  Epoch: 212  Training loss = 2.4966  Validation loss = 2.0010  \n",
      "\n",
      "Fold: 2  Epoch: 213  Training loss = 2.4963  Validation loss = 1.9998  \n",
      "\n",
      "Fold: 2  Epoch: 214  Training loss = 2.4961  Validation loss = 1.9991  \n",
      "\n",
      "Fold: 2  Epoch: 215  Training loss = 2.4962  Validation loss = 1.9995  \n",
      "\n",
      "Fold: 2  Epoch: 216  Training loss = 2.4961  Validation loss = 1.9992  \n",
      "\n",
      "Fold: 2  Epoch: 217  Training loss = 2.4958  Validation loss = 1.9982  \n",
      "\n",
      "Fold: 2  Epoch: 218  Training loss = 2.4956  Validation loss = 1.9977  \n",
      "\n",
      "Fold: 2  Epoch: 219  Training loss = 2.4952  Validation loss = 1.9962  \n",
      "\n",
      "Fold: 2  Epoch: 220  Training loss = 2.4952  Validation loss = 1.9961  \n",
      "\n",
      "Fold: 2  Epoch: 221  Training loss = 2.4948  Validation loss = 1.9952  \n",
      "\n",
      "Fold: 2  Epoch: 222  Training loss = 2.4947  Validation loss = 1.9943  \n",
      "\n",
      "Fold: 2  Epoch: 223  Training loss = 2.4943  Validation loss = 1.9928  \n",
      "\n",
      "Fold: 2  Epoch: 224  Training loss = 2.4940  Validation loss = 1.9920  \n",
      "\n",
      "Fold: 2  Epoch: 225  Training loss = 2.4939  Validation loss = 1.9909  \n",
      "\n",
      "Fold: 2  Epoch: 226  Training loss = 2.4940  Validation loss = 1.9920  \n",
      "\n",
      "Fold: 2  Epoch: 227  Training loss = 2.4939  Validation loss = 1.9917  \n",
      "\n",
      "Fold: 2  Epoch: 228  Training loss = 2.4938  Validation loss = 1.9914  \n",
      "\n",
      "Fold: 2  Epoch: 229  Training loss = 2.4937  Validation loss = 1.9910  \n",
      "\n",
      "Fold: 2  Epoch: 230  Training loss = 2.4933  Validation loss = 1.9902  \n",
      "\n",
      "Fold: 2  Epoch: 231  Training loss = 2.4930  Validation loss = 1.9893  \n",
      "\n",
      "Fold: 2  Epoch: 232  Training loss = 2.4929  Validation loss = 1.9888  \n",
      "\n",
      "Fold: 2  Epoch: 233  Training loss = 2.4928  Validation loss = 1.9887  \n",
      "\n",
      "Fold: 2  Epoch: 234  Training loss = 2.4929  Validation loss = 1.9893  \n",
      "\n",
      "Fold: 2  Epoch: 235  Training loss = 2.4927  Validation loss = 1.9891  \n",
      "\n",
      "Fold: 2  Epoch: 236  Training loss = 2.4927  Validation loss = 1.9889  \n",
      "\n",
      "Fold: 2  Epoch: 237  Training loss = 2.4928  Validation loss = 1.9895  \n",
      "\n",
      "Fold: 2  Epoch: 238  Training loss = 2.4928  Validation loss = 1.9893  \n",
      "\n",
      "Fold: 2  Epoch: 239  Training loss = 2.4927  Validation loss = 1.9891  \n",
      "\n",
      "Fold: 2  Epoch: 240  Training loss = 2.4924  Validation loss = 1.9882  \n",
      "\n",
      "Fold: 2  Epoch: 241  Training loss = 2.4924  Validation loss = 1.9880  \n",
      "\n",
      "Fold: 2  Epoch: 242  Training loss = 2.4922  Validation loss = 1.9878  \n",
      "\n",
      "Fold: 2  Epoch: 243  Training loss = 2.4921  Validation loss = 1.9875  \n",
      "\n",
      "Fold: 2  Epoch: 244  Training loss = 2.4920  Validation loss = 1.9872  \n",
      "\n",
      "Fold: 2  Epoch: 245  Training loss = 2.4918  Validation loss = 1.9867  \n",
      "\n",
      "Fold: 2  Epoch: 246  Training loss = 2.4914  Validation loss = 1.9853  \n",
      "\n",
      "Fold: 2  Epoch: 247  Training loss = 2.4914  Validation loss = 1.9856  \n",
      "\n",
      "Fold: 2  Epoch: 248  Training loss = 2.4910  Validation loss = 1.9845  \n",
      "\n",
      "Fold: 2  Epoch: 249  Training loss = 2.4910  Validation loss = 1.9846  \n",
      "\n",
      "Fold: 2  Epoch: 250  Training loss = 2.4910  Validation loss = 1.9845  \n",
      "\n",
      "Fold: 2  Epoch: 251  Training loss = 2.4910  Validation loss = 1.9848  \n",
      "\n",
      "Fold: 2  Epoch: 252  Training loss = 2.4909  Validation loss = 1.9849  \n",
      "\n",
      "Fold: 2  Epoch: 253  Training loss = 2.4909  Validation loss = 1.9854  \n",
      "\n",
      "Fold: 2  Epoch: 254  Training loss = 2.4908  Validation loss = 1.9847  \n",
      "\n",
      "Fold: 2  Epoch: 255  Training loss = 2.4905  Validation loss = 1.9836  \n",
      "\n",
      "Fold: 2  Epoch: 256  Training loss = 2.4904  Validation loss = 1.9836  \n",
      "\n",
      "Fold: 2  Epoch: 257  Training loss = 2.4903  Validation loss = 1.9832  \n",
      "\n",
      "Fold: 2  Epoch: 258  Training loss = 2.4904  Validation loss = 1.9836  \n",
      "\n",
      "Fold: 2  Epoch: 259  Training loss = 2.4905  Validation loss = 1.9840  \n",
      "\n",
      "Fold: 2  Epoch: 260  Training loss = 2.4902  Validation loss = 1.9832  \n",
      "\n",
      "Fold: 2  Epoch: 261  Training loss = 2.4901  Validation loss = 1.9828  \n",
      "\n",
      "Fold: 2  Epoch: 262  Training loss = 2.4901  Validation loss = 1.9827  \n",
      "\n",
      "Fold: 2  Epoch: 263  Training loss = 2.4900  Validation loss = 1.9826  \n",
      "\n",
      "Fold: 2  Epoch: 264  Training loss = 2.4898  Validation loss = 1.9819  \n",
      "\n",
      "Fold: 2  Epoch: 265  Training loss = 2.4896  Validation loss = 1.9814  \n",
      "\n",
      "Fold: 2  Epoch: 266  Training loss = 2.4897  Validation loss = 1.9817  \n",
      "\n",
      "Fold: 2  Epoch: 267  Training loss = 2.4895  Validation loss = 1.9811  \n",
      "\n",
      "Fold: 2  Epoch: 268  Training loss = 2.4894  Validation loss = 1.9808  \n",
      "\n",
      "Fold: 2  Epoch: 269  Training loss = 2.4893  Validation loss = 1.9807  \n",
      "\n",
      "Fold: 2  Epoch: 270  Training loss = 2.4894  Validation loss = 1.9809  \n",
      "\n",
      "Fold: 2  Epoch: 271  Training loss = 2.4892  Validation loss = 1.9802  \n",
      "\n",
      "Fold: 2  Epoch: 272  Training loss = 2.4891  Validation loss = 1.9802  \n",
      "\n",
      "Fold: 2  Epoch: 273  Training loss = 2.4890  Validation loss = 1.9799  \n",
      "\n",
      "Fold: 2  Epoch: 274  Training loss = 2.4888  Validation loss = 1.9790  \n",
      "\n",
      "Fold: 2  Epoch: 275  Training loss = 2.4887  Validation loss = 1.9785  \n",
      "\n",
      "Fold: 2  Epoch: 276  Training loss = 2.4886  Validation loss = 1.9784  \n",
      "\n",
      "Fold: 2  Epoch: 277  Training loss = 2.4884  Validation loss = 1.9775  \n",
      "\n",
      "Fold: 2  Epoch: 278  Training loss = 2.4884  Validation loss = 1.9782  \n",
      "\n",
      "Fold: 2  Epoch: 279  Training loss = 2.4884  Validation loss = 1.9780  \n",
      "\n",
      "Fold: 2  Epoch: 280  Training loss = 2.4883  Validation loss = 1.9775  \n",
      "\n",
      "Fold: 2  Epoch: 281  Training loss = 2.4881  Validation loss = 1.9764  \n",
      "\n",
      "Fold: 2  Epoch: 282  Training loss = 2.4880  Validation loss = 1.9760  \n",
      "\n",
      "Fold: 2  Epoch: 283  Training loss = 2.4880  Validation loss = 1.9763  \n",
      "\n",
      "Fold: 2  Epoch: 284  Training loss = 2.4880  Validation loss = 1.9767  \n",
      "\n",
      "Fold: 2  Epoch: 285  Training loss = 2.4878  Validation loss = 1.9759  \n",
      "\n",
      "Fold: 2  Epoch: 286  Training loss = 2.4878  Validation loss = 1.9767  \n",
      "\n",
      "Fold: 2  Epoch: 287  Training loss = 2.4876  Validation loss = 1.9757  \n",
      "\n",
      "Fold: 2  Epoch: 288  Training loss = 2.4875  Validation loss = 1.9752  \n",
      "\n",
      "Fold: 2  Epoch: 289  Training loss = 2.4873  Validation loss = 1.9745  \n",
      "\n",
      "Fold: 2  Epoch: 290  Training loss = 2.4872  Validation loss = 1.9745  \n",
      "\n",
      "Fold: 2  Epoch: 291  Training loss = 2.4872  Validation loss = 1.9743  \n",
      "\n",
      "Fold: 2  Epoch: 292  Training loss = 2.4870  Validation loss = 1.9734  \n",
      "\n",
      "Fold: 2  Epoch: 293  Training loss = 2.4869  Validation loss = 1.9728  \n",
      "\n",
      "Fold: 2  Epoch: 294  Training loss = 2.4865  Validation loss = 1.9711  \n",
      "\n",
      "Fold: 2  Epoch: 295  Training loss = 2.4864  Validation loss = 1.9708  \n",
      "\n",
      "Fold: 2  Epoch: 296  Training loss = 2.4862  Validation loss = 1.9703  \n",
      "\n",
      "Fold: 2  Epoch: 297  Training loss = 2.4860  Validation loss = 1.9694  \n",
      "\n",
      "Fold: 2  Epoch: 298  Training loss = 2.4856  Validation loss = 1.9679  \n",
      "\n",
      "Fold: 2  Epoch: 299  Training loss = 2.4856  Validation loss = 1.9683  \n",
      "\n",
      "Fold: 2  Epoch: 300  Training loss = 2.4857  Validation loss = 1.9687  \n",
      "\n",
      "Fold: 2  Epoch: 301  Training loss = 2.4854  Validation loss = 1.9680  \n",
      "\n",
      "Fold: 2  Epoch: 302  Training loss = 2.4853  Validation loss = 1.9679  \n",
      "\n",
      "Fold: 2  Epoch: 303  Training loss = 2.4854  Validation loss = 1.9686  \n",
      "\n",
      "Fold: 2  Epoch: 304  Training loss = 2.4853  Validation loss = 1.9684  \n",
      "\n",
      "Fold: 2  Epoch: 305  Training loss = 2.4852  Validation loss = 1.9678  \n",
      "\n",
      "Fold: 2  Epoch: 306  Training loss = 2.4851  Validation loss = 1.9674  \n",
      "\n",
      "Fold: 2  Epoch: 307  Training loss = 2.4851  Validation loss = 1.9672  \n",
      "\n",
      "Fold: 2  Epoch: 308  Training loss = 2.4850  Validation loss = 1.9668  \n",
      "\n",
      "Fold: 2  Epoch: 309  Training loss = 2.4849  Validation loss = 1.9667  \n",
      "\n",
      "Fold: 2  Epoch: 310  Training loss = 2.4845  Validation loss = 1.9653  \n",
      "\n",
      "Fold: 2  Epoch: 311  Training loss = 2.4844  Validation loss = 1.9647  \n",
      "\n",
      "Fold: 2  Epoch: 312  Training loss = 2.4844  Validation loss = 1.9649  \n",
      "\n",
      "Fold: 2  Epoch: 313  Training loss = 2.4843  Validation loss = 1.9644  \n",
      "\n",
      "Fold: 2  Epoch: 314  Training loss = 2.4842  Validation loss = 1.9643  \n",
      "\n",
      "Fold: 2  Epoch: 315  Training loss = 2.4841  Validation loss = 1.9642  \n",
      "\n",
      "Fold: 2  Epoch: 316  Training loss = 2.4839  Validation loss = 1.9630  \n",
      "\n",
      "Fold: 2  Epoch: 317  Training loss = 2.4837  Validation loss = 1.9627  \n",
      "\n",
      "Fold: 2  Epoch: 318  Training loss = 2.4838  Validation loss = 1.9629  \n",
      "\n",
      "Fold: 2  Epoch: 319  Training loss = 2.4838  Validation loss = 1.9632  \n",
      "\n",
      "Fold: 2  Epoch: 320  Training loss = 2.4837  Validation loss = 1.9631  \n",
      "\n",
      "Fold: 2  Epoch: 321  Training loss = 2.4835  Validation loss = 1.9626  \n",
      "\n",
      "Fold: 2  Epoch: 322  Training loss = 2.4835  Validation loss = 1.9621  \n",
      "\n",
      "Fold: 2  Epoch: 323  Training loss = 2.4834  Validation loss = 1.9621  \n",
      "\n",
      "Fold: 2  Epoch: 324  Training loss = 2.4834  Validation loss = 1.9618  \n",
      "\n",
      "Fold: 2  Epoch: 325  Training loss = 2.4833  Validation loss = 1.9618  \n",
      "\n",
      "Fold: 2  Epoch: 326  Training loss = 2.4833  Validation loss = 1.9616  \n",
      "\n",
      "Fold: 2  Epoch: 327  Training loss = 2.4831  Validation loss = 1.9612  \n",
      "\n",
      "Fold: 2  Epoch: 328  Training loss = 2.4831  Validation loss = 1.9610  \n",
      "\n",
      "Fold: 2  Epoch: 329  Training loss = 2.4830  Validation loss = 1.9609  \n",
      "\n",
      "Fold: 2  Epoch: 330  Training loss = 2.4829  Validation loss = 1.9605  \n",
      "\n",
      "Fold: 2  Epoch: 331  Training loss = 2.4829  Validation loss = 1.9607  \n",
      "\n",
      "Fold: 2  Epoch: 332  Training loss = 2.4828  Validation loss = 1.9604  \n",
      "\n",
      "Fold: 2  Epoch: 333  Training loss = 2.4826  Validation loss = 1.9598  \n",
      "\n",
      "Fold: 2  Epoch: 334  Training loss = 2.4824  Validation loss = 1.9591  \n",
      "\n",
      "Fold: 2  Epoch: 335  Training loss = 2.4825  Validation loss = 1.9597  \n",
      "\n",
      "Fold: 2  Epoch: 336  Training loss = 2.4824  Validation loss = 1.9599  \n",
      "\n",
      "Fold: 2  Epoch: 337  Training loss = 2.4823  Validation loss = 1.9598  \n",
      "\n",
      "Fold: 2  Epoch: 338  Training loss = 2.4822  Validation loss = 1.9594  \n",
      "\n",
      "Fold: 2  Epoch: 339  Training loss = 2.4821  Validation loss = 1.9591  \n",
      "\n",
      "Fold: 2  Epoch: 340  Training loss = 2.4820  Validation loss = 1.9583  \n",
      "\n",
      "Fold: 2  Epoch: 341  Training loss = 2.4819  Validation loss = 1.9579  \n",
      "\n",
      "Fold: 2  Epoch: 342  Training loss = 2.4818  Validation loss = 1.9578  \n",
      "\n",
      "Fold: 2  Epoch: 343  Training loss = 2.4818  Validation loss = 1.9576  \n",
      "\n",
      "Fold: 2  Epoch: 344  Training loss = 2.4817  Validation loss = 1.9577  \n",
      "\n",
      "Fold: 2  Epoch: 345  Training loss = 2.4816  Validation loss = 1.9573  \n",
      "\n",
      "Fold: 2  Epoch: 346  Training loss = 2.4814  Validation loss = 1.9564  \n",
      "\n",
      "Fold: 2  Epoch: 347  Training loss = 2.4813  Validation loss = 1.9561  \n",
      "\n",
      "Fold: 2  Epoch: 348  Training loss = 2.4811  Validation loss = 1.9553  \n",
      "\n",
      "Fold: 2  Epoch: 349  Training loss = 2.4811  Validation loss = 1.9553  \n",
      "\n",
      "Fold: 2  Epoch: 350  Training loss = 2.4810  Validation loss = 1.9552  \n",
      "\n",
      "Fold: 2  Epoch: 351  Training loss = 2.4810  Validation loss = 1.9552  \n",
      "\n",
      "Fold: 2  Epoch: 352  Training loss = 2.4810  Validation loss = 1.9554  \n",
      "\n",
      "Fold: 2  Epoch: 353  Training loss = 2.4809  Validation loss = 1.9551  \n",
      "\n",
      "Fold: 2  Epoch: 354  Training loss = 2.4810  Validation loss = 1.9556  \n",
      "\n",
      "Fold: 2  Epoch: 355  Training loss = 2.4810  Validation loss = 1.9561  \n",
      "\n",
      "Fold: 2  Epoch: 356  Training loss = 2.4810  Validation loss = 1.9564  \n",
      "\n",
      "Fold: 2  Epoch: 357  Training loss = 2.4810  Validation loss = 1.9561  \n",
      "\n",
      "Fold: 2  Epoch: 358  Training loss = 2.4807  Validation loss = 1.9552  \n",
      "\n",
      "Fold: 2  Epoch: 359  Training loss = 2.4807  Validation loss = 1.9551  \n",
      "\n",
      "Fold: 2  Epoch: 360  Training loss = 2.4805  Validation loss = 1.9547  \n",
      "\n",
      "Fold: 2  Epoch: 361  Training loss = 2.4805  Validation loss = 1.9549  \n",
      "\n",
      "Fold: 2  Epoch: 362  Training loss = 2.4802  Validation loss = 1.9539  \n",
      "\n",
      "Fold: 2  Epoch: 363  Training loss = 2.4802  Validation loss = 1.9539  \n",
      "\n",
      "Fold: 2  Epoch: 364  Training loss = 2.4802  Validation loss = 1.9544  \n",
      "\n",
      "Fold: 2  Epoch: 365  Training loss = 2.4799  Validation loss = 1.9534  \n",
      "\n",
      "Fold: 2  Epoch: 366  Training loss = 2.4798  Validation loss = 1.9536  \n",
      "\n",
      "Fold: 2  Epoch: 367  Training loss = 2.4799  Validation loss = 1.9544  \n",
      "\n",
      "Fold: 2  Epoch: 368  Training loss = 2.4796  Validation loss = 1.9538  \n",
      "\n",
      "Fold: 2  Epoch: 369  Training loss = 2.4795  Validation loss = 1.9531  \n",
      "\n",
      "Fold: 2  Epoch: 370  Training loss = 2.4793  Validation loss = 1.9522  \n",
      "\n",
      "Fold: 2  Epoch: 371  Training loss = 2.4791  Validation loss = 1.9514  \n",
      "\n",
      "Fold: 2  Epoch: 372  Training loss = 2.4790  Validation loss = 1.9513  \n",
      "\n",
      "Fold: 2  Epoch: 373  Training loss = 2.4791  Validation loss = 1.9515  \n",
      "\n",
      "Fold: 2  Epoch: 374  Training loss = 2.4790  Validation loss = 1.9518  \n",
      "\n",
      "Fold: 2  Epoch: 375  Training loss = 2.4790  Validation loss = 1.9517  \n",
      "\n",
      "Fold: 2  Epoch: 376  Training loss = 2.4789  Validation loss = 1.9516  \n",
      "\n",
      "Fold: 2  Epoch: 377  Training loss = 2.4790  Validation loss = 1.9520  \n",
      "\n",
      "Fold: 2  Epoch: 378  Training loss = 2.4790  Validation loss = 1.9525  \n",
      "\n",
      "Fold: 2  Epoch: 379  Training loss = 2.4789  Validation loss = 1.9527  \n",
      "\n",
      "Fold: 2  Epoch: 380  Training loss = 2.4789  Validation loss = 1.9527  \n",
      "\n",
      "Fold: 2  Epoch: 381  Training loss = 2.4788  Validation loss = 1.9522  \n",
      "\n",
      "Fold: 2  Epoch: 382  Training loss = 2.4787  Validation loss = 1.9518  \n",
      "\n",
      "Fold: 2  Epoch: 383  Training loss = 2.4788  Validation loss = 1.9524  \n",
      "\n",
      "Fold: 2  Epoch: 384  Training loss = 2.4788  Validation loss = 1.9529  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 372  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.4738  Validation loss = 3.2156  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.4737  Validation loss = 3.2160  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.4737  Validation loss = 3.2162  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.4736  Validation loss = 3.2173  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.4735  Validation loss = 3.2176  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.4735  Validation loss = 3.2188  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.4734  Validation loss = 3.2194  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.4734  Validation loss = 3.2197  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.4733  Validation loss = 3.2199  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.4733  Validation loss = 3.2204  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.4733  Validation loss = 3.2200  \n",
      "\n",
      "Fold: 3  Epoch: 12  Training loss = 1.4732  Validation loss = 3.2207  \n",
      "\n",
      "Fold: 3  Epoch: 13  Training loss = 1.4732  Validation loss = 3.2204  \n",
      "\n",
      "Fold: 3  Epoch: 14  Training loss = 1.4732  Validation loss = 3.2203  \n",
      "\n",
      "Fold: 3  Epoch: 15  Training loss = 1.4731  Validation loss = 3.2206  \n",
      "\n",
      "Fold: 3  Epoch: 16  Training loss = 1.4731  Validation loss = 3.2207  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 1  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.5822  Validation loss = 4.3622  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.5821  Validation loss = 4.3616  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.5821  Validation loss = 4.3611  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.5821  Validation loss = 4.3610  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.5819  Validation loss = 4.3593  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.5819  Validation loss = 4.3587  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.5818  Validation loss = 4.3575  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.5818  Validation loss = 4.3561  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.5817  Validation loss = 4.3556  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 1.5816  Validation loss = 4.3542  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.5815  Validation loss = 4.3544  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 1.5815  Validation loss = 4.3544  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.5814  Validation loss = 4.3542  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 1.5814  Validation loss = 4.3545  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.5813  Validation loss = 4.3528  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 1.5813  Validation loss = 4.3533  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 1.5813  Validation loss = 4.3527  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 1.5812  Validation loss = 4.3525  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 1.5812  Validation loss = 4.3522  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 1.5811  Validation loss = 4.3509  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 1.5810  Validation loss = 4.3510  \n",
      "\n",
      "Fold: 4  Epoch: 22  Training loss = 1.5810  Validation loss = 4.3509  \n",
      "\n",
      "Fold: 4  Epoch: 23  Training loss = 1.5809  Validation loss = 4.3500  \n",
      "\n",
      "Fold: 4  Epoch: 24  Training loss = 1.5809  Validation loss = 4.3497  \n",
      "\n",
      "Fold: 4  Epoch: 25  Training loss = 1.5808  Validation loss = 4.3487  \n",
      "\n",
      "Fold: 4  Epoch: 26  Training loss = 1.5808  Validation loss = 4.3490  \n",
      "\n",
      "Fold: 4  Epoch: 27  Training loss = 1.5807  Validation loss = 4.3488  \n",
      "\n",
      "Fold: 4  Epoch: 28  Training loss = 1.5807  Validation loss = 4.3495  \n",
      "\n",
      "Fold: 4  Epoch: 29  Training loss = 1.5807  Validation loss = 4.3497  \n",
      "\n",
      "Fold: 4  Epoch: 30  Training loss = 1.5807  Validation loss = 4.3499  \n",
      "\n",
      "Fold: 4  Epoch: 31  Training loss = 1.5806  Validation loss = 4.3508  \n",
      "\n",
      "Fold: 4  Epoch: 32  Training loss = 1.5806  Validation loss = 4.3511  \n",
      "\n",
      "Fold: 4  Epoch: 33  Training loss = 1.5806  Validation loss = 4.3506  \n",
      "\n",
      "Fold: 4  Epoch: 34  Training loss = 1.5805  Validation loss = 4.3511  \n",
      "\n",
      "Fold: 4  Epoch: 35  Training loss = 1.5805  Validation loss = 4.3499  \n",
      "\n",
      "Fold: 4  Epoch: 36  Training loss = 1.5804  Validation loss = 4.3501  \n",
      "\n",
      "Fold: 4  Epoch: 37  Training loss = 1.5804  Validation loss = 4.3497  \n",
      "\n",
      "Fold: 4  Epoch: 38  Training loss = 1.5803  Validation loss = 4.3493  \n",
      "\n",
      "Fold: 4  Epoch: 39  Training loss = 1.5803  Validation loss = 4.3486  \n",
      "\n",
      "Fold: 4  Epoch: 40  Training loss = 1.5802  Validation loss = 4.3481  \n",
      "\n",
      "Fold: 4  Epoch: 41  Training loss = 1.5802  Validation loss = 4.3480  \n",
      "\n",
      "Fold: 4  Epoch: 42  Training loss = 1.5801  Validation loss = 4.3473  \n",
      "\n",
      "Fold: 4  Epoch: 43  Training loss = 1.5801  Validation loss = 4.3487  \n",
      "\n",
      "Fold: 4  Epoch: 44  Training loss = 1.5801  Validation loss = 4.3487  \n",
      "\n",
      "Fold: 4  Epoch: 45  Training loss = 1.5800  Validation loss = 4.3479  \n",
      "\n",
      "Fold: 4  Epoch: 46  Training loss = 1.5800  Validation loss = 4.3471  \n",
      "\n",
      "Fold: 4  Epoch: 47  Training loss = 1.5799  Validation loss = 4.3467  \n",
      "\n",
      "Fold: 4  Epoch: 48  Training loss = 1.5799  Validation loss = 4.3461  \n",
      "\n",
      "Fold: 4  Epoch: 49  Training loss = 1.5798  Validation loss = 4.3459  \n",
      "\n",
      "Fold: 4  Epoch: 50  Training loss = 1.5798  Validation loss = 4.3462  \n",
      "\n",
      "Fold: 4  Epoch: 51  Training loss = 1.5797  Validation loss = 4.3454  \n",
      "\n",
      "Fold: 4  Epoch: 52  Training loss = 1.5797  Validation loss = 4.3455  \n",
      "\n",
      "Fold: 4  Epoch: 53  Training loss = 1.5796  Validation loss = 4.3449  \n",
      "\n",
      "Fold: 4  Epoch: 54  Training loss = 1.5796  Validation loss = 4.3450  \n",
      "\n",
      "Fold: 4  Epoch: 55  Training loss = 1.5796  Validation loss = 4.3457  \n",
      "\n",
      "Fold: 4  Epoch: 56  Training loss = 1.5795  Validation loss = 4.3457  \n",
      "\n",
      "Fold: 4  Epoch: 57  Training loss = 1.5795  Validation loss = 4.3457  \n",
      "\n",
      "Fold: 4  Epoch: 58  Training loss = 1.5795  Validation loss = 4.3449  \n",
      "\n",
      "Fold: 4  Epoch: 59  Training loss = 1.5794  Validation loss = 4.3436  \n",
      "\n",
      "Fold: 4  Epoch: 60  Training loss = 1.5793  Validation loss = 4.3429  \n",
      "\n",
      "Fold: 4  Epoch: 61  Training loss = 1.5793  Validation loss = 4.3431  \n",
      "\n",
      "Fold: 4  Epoch: 62  Training loss = 1.5792  Validation loss = 4.3423  \n",
      "\n",
      "Fold: 4  Epoch: 63  Training loss = 1.5792  Validation loss = 4.3416  \n",
      "\n",
      "Fold: 4  Epoch: 64  Training loss = 1.5791  Validation loss = 4.3409  \n",
      "\n",
      "Fold: 4  Epoch: 65  Training loss = 1.5791  Validation loss = 4.3402  \n",
      "\n",
      "Fold: 4  Epoch: 66  Training loss = 1.5791  Validation loss = 4.3406  \n",
      "\n",
      "Fold: 4  Epoch: 67  Training loss = 1.5790  Validation loss = 4.3402  \n",
      "\n",
      "Fold: 4  Epoch: 68  Training loss = 1.5790  Validation loss = 4.3400  \n",
      "\n",
      "Fold: 4  Epoch: 69  Training loss = 1.5789  Validation loss = 4.3392  \n",
      "\n",
      "Fold: 4  Epoch: 70  Training loss = 1.5789  Validation loss = 4.3386  \n",
      "\n",
      "Fold: 4  Epoch: 71  Training loss = 1.5788  Validation loss = 4.3376  \n",
      "\n",
      "Fold: 4  Epoch: 72  Training loss = 1.5788  Validation loss = 4.3379  \n",
      "\n",
      "Fold: 4  Epoch: 73  Training loss = 1.5787  Validation loss = 4.3373  \n",
      "\n",
      "Fold: 4  Epoch: 74  Training loss = 1.5787  Validation loss = 4.3370  \n",
      "\n",
      "Fold: 4  Epoch: 75  Training loss = 1.5787  Validation loss = 4.3387  \n",
      "\n",
      "Fold: 4  Epoch: 76  Training loss = 1.5786  Validation loss = 4.3376  \n",
      "\n",
      "Fold: 4  Epoch: 77  Training loss = 1.5786  Validation loss = 4.3378  \n",
      "\n",
      "Fold: 4  Epoch: 78  Training loss = 1.5786  Validation loss = 4.3389  \n",
      "\n",
      "Fold: 4  Epoch: 79  Training loss = 1.5786  Validation loss = 4.3390  \n",
      "\n",
      "Fold: 4  Epoch: 80  Training loss = 1.5786  Validation loss = 4.3380  \n",
      "\n",
      "Fold: 4  Epoch: 81  Training loss = 1.5785  Validation loss = 4.3371  \n",
      "\n",
      "Fold: 4  Epoch: 82  Training loss = 1.5785  Validation loss = 4.3368  \n",
      "\n",
      "Fold: 4  Epoch: 83  Training loss = 1.5784  Validation loss = 4.3361  \n",
      "\n",
      "Fold: 4  Epoch: 84  Training loss = 1.5784  Validation loss = 4.3364  \n",
      "\n",
      "Fold: 4  Epoch: 85  Training loss = 1.5783  Validation loss = 4.3352  \n",
      "\n",
      "Fold: 4  Epoch: 86  Training loss = 1.5782  Validation loss = 4.3337  \n",
      "\n",
      "Fold: 4  Epoch: 87  Training loss = 1.5782  Validation loss = 4.3343  \n",
      "\n",
      "Fold: 4  Epoch: 88  Training loss = 1.5782  Validation loss = 4.3344  \n",
      "\n",
      "Fold: 4  Epoch: 89  Training loss = 1.5781  Validation loss = 4.3350  \n",
      "\n",
      "Fold: 4  Epoch: 90  Training loss = 1.5781  Validation loss = 4.3346  \n",
      "\n",
      "Fold: 4  Epoch: 91  Training loss = 1.5780  Validation loss = 4.3335  \n",
      "\n",
      "Fold: 4  Epoch: 92  Training loss = 1.5779  Validation loss = 4.3334  \n",
      "\n",
      "Fold: 4  Epoch: 93  Training loss = 1.5779  Validation loss = 4.3328  \n",
      "\n",
      "Fold: 4  Epoch: 94  Training loss = 1.5778  Validation loss = 4.3331  \n",
      "\n",
      "Fold: 4  Epoch: 95  Training loss = 1.5778  Validation loss = 4.3330  \n",
      "\n",
      "Fold: 4  Epoch: 96  Training loss = 1.5777  Validation loss = 4.3316  \n",
      "\n",
      "Fold: 4  Epoch: 97  Training loss = 1.5777  Validation loss = 4.3315  \n",
      "\n",
      "Fold: 4  Epoch: 98  Training loss = 1.5776  Validation loss = 4.3305  \n",
      "\n",
      "Fold: 4  Epoch: 99  Training loss = 1.5776  Validation loss = 4.3309  \n",
      "\n",
      "Fold: 4  Epoch: 100  Training loss = 1.5775  Validation loss = 4.3300  \n",
      "\n",
      "Fold: 4  Epoch: 101  Training loss = 1.5774  Validation loss = 4.3291  \n",
      "\n",
      "Fold: 4  Epoch: 102  Training loss = 1.5774  Validation loss = 4.3290  \n",
      "\n",
      "Fold: 4  Epoch: 103  Training loss = 1.5774  Validation loss = 4.3290  \n",
      "\n",
      "Fold: 4  Epoch: 104  Training loss = 1.5773  Validation loss = 4.3275  \n",
      "\n",
      "Fold: 4  Epoch: 105  Training loss = 1.5773  Validation loss = 4.3271  \n",
      "\n",
      "Fold: 4  Epoch: 106  Training loss = 1.5772  Validation loss = 4.3269  \n",
      "\n",
      "Fold: 4  Epoch: 107  Training loss = 1.5772  Validation loss = 4.3266  \n",
      "\n",
      "Fold: 4  Epoch: 108  Training loss = 1.5771  Validation loss = 4.3258  \n",
      "\n",
      "Fold: 4  Epoch: 109  Training loss = 1.5771  Validation loss = 4.3269  \n",
      "\n",
      "Fold: 4  Epoch: 110  Training loss = 1.5770  Validation loss = 4.3273  \n",
      "\n",
      "Fold: 4  Epoch: 111  Training loss = 1.5770  Validation loss = 4.3278  \n",
      "\n",
      "Fold: 4  Epoch: 112  Training loss = 1.5770  Validation loss = 4.3277  \n",
      "\n",
      "Fold: 4  Epoch: 113  Training loss = 1.5769  Validation loss = 4.3279  \n",
      "\n",
      "Fold: 4  Epoch: 114  Training loss = 1.5769  Validation loss = 4.3280  \n",
      "\n",
      "Fold: 4  Epoch: 115  Training loss = 1.5768  Validation loss = 4.3266  \n",
      "\n",
      "Fold: 4  Epoch: 116  Training loss = 1.5768  Validation loss = 4.3255  \n",
      "\n",
      "Fold: 4  Epoch: 117  Training loss = 1.5767  Validation loss = 4.3249  \n",
      "\n",
      "Fold: 4  Epoch: 118  Training loss = 1.5767  Validation loss = 4.3254  \n",
      "\n",
      "Fold: 4  Epoch: 119  Training loss = 1.5766  Validation loss = 4.3240  \n",
      "\n",
      "Fold: 4  Epoch: 120  Training loss = 1.5766  Validation loss = 4.3244  \n",
      "\n",
      "Fold: 4  Epoch: 121  Training loss = 1.5766  Validation loss = 4.3238  \n",
      "\n",
      "Fold: 4  Epoch: 122  Training loss = 1.5765  Validation loss = 4.3243  \n",
      "\n",
      "Fold: 4  Epoch: 123  Training loss = 1.5765  Validation loss = 4.3235  \n",
      "\n",
      "Fold: 4  Epoch: 124  Training loss = 1.5765  Validation loss = 4.3228  \n",
      "\n",
      "Fold: 4  Epoch: 125  Training loss = 1.5764  Validation loss = 4.3218  \n",
      "\n",
      "Fold: 4  Epoch: 126  Training loss = 1.5764  Validation loss = 4.3214  \n",
      "\n",
      "Fold: 4  Epoch: 127  Training loss = 1.5763  Validation loss = 4.3206  \n",
      "\n",
      "Fold: 4  Epoch: 128  Training loss = 1.5763  Validation loss = 4.3204  \n",
      "\n",
      "Fold: 4  Epoch: 129  Training loss = 1.5762  Validation loss = 4.3199  \n",
      "\n",
      "Fold: 4  Epoch: 130  Training loss = 1.5762  Validation loss = 4.3201  \n",
      "\n",
      "Fold: 4  Epoch: 131  Training loss = 1.5762  Validation loss = 4.3203  \n",
      "\n",
      "Fold: 4  Epoch: 132  Training loss = 1.5761  Validation loss = 4.3204  \n",
      "\n",
      "Fold: 4  Epoch: 133  Training loss = 1.5760  Validation loss = 4.3198  \n",
      "\n",
      "Fold: 4  Epoch: 134  Training loss = 1.5760  Validation loss = 4.3188  \n",
      "\n",
      "Fold: 4  Epoch: 135  Training loss = 1.5760  Validation loss = 4.3187  \n",
      "\n",
      "Fold: 4  Epoch: 136  Training loss = 1.5759  Validation loss = 4.3188  \n",
      "\n",
      "Fold: 4  Epoch: 137  Training loss = 1.5759  Validation loss = 4.3186  \n",
      "\n",
      "Fold: 4  Epoch: 138  Training loss = 1.5759  Validation loss = 4.3187  \n",
      "\n",
      "Fold: 4  Epoch: 139  Training loss = 1.5758  Validation loss = 4.3181  \n",
      "\n",
      "Fold: 4  Epoch: 140  Training loss = 1.5758  Validation loss = 4.3173  \n",
      "\n",
      "Fold: 4  Epoch: 141  Training loss = 1.5758  Validation loss = 4.3173  \n",
      "\n",
      "Fold: 4  Epoch: 142  Training loss = 1.5757  Validation loss = 4.3179  \n",
      "\n",
      "Fold: 4  Epoch: 143  Training loss = 1.5757  Validation loss = 4.3170  \n",
      "\n",
      "Fold: 4  Epoch: 144  Training loss = 1.5756  Validation loss = 4.3159  \n",
      "\n",
      "Fold: 4  Epoch: 145  Training loss = 1.5756  Validation loss = 4.3145  \n",
      "\n",
      "Fold: 4  Epoch: 146  Training loss = 1.5755  Validation loss = 4.3150  \n",
      "\n",
      "Fold: 4  Epoch: 147  Training loss = 1.5755  Validation loss = 4.3158  \n",
      "\n",
      "Fold: 4  Epoch: 148  Training loss = 1.5755  Validation loss = 4.3148  \n",
      "\n",
      "Fold: 4  Epoch: 149  Training loss = 1.5754  Validation loss = 4.3145  \n",
      "\n",
      "Fold: 4  Epoch: 150  Training loss = 1.5754  Validation loss = 4.3151  \n",
      "\n",
      "Fold: 4  Epoch: 151  Training loss = 1.5754  Validation loss = 4.3148  \n",
      "\n",
      "Fold: 4  Epoch: 152  Training loss = 1.5753  Validation loss = 4.3147  \n",
      "\n",
      "Fold: 4  Epoch: 153  Training loss = 1.5753  Validation loss = 4.3146  \n",
      "\n",
      "Fold: 4  Epoch: 154  Training loss = 1.5753  Validation loss = 4.3152  \n",
      "\n",
      "Fold: 4  Epoch: 155  Training loss = 1.5752  Validation loss = 4.3146  \n",
      "\n",
      "Fold: 4  Epoch: 156  Training loss = 1.5752  Validation loss = 4.3152  \n",
      "\n",
      "Fold: 4  Epoch: 157  Training loss = 1.5752  Validation loss = 4.3148  \n",
      "\n",
      "Fold: 4  Epoch: 158  Training loss = 1.5752  Validation loss = 4.3148  \n",
      "\n",
      "Fold: 4  Epoch: 159  Training loss = 1.5751  Validation loss = 4.3158  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 149  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.8629  Validation loss = 4.5532  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.8628  Validation loss = 4.5521  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.8620  Validation loss = 4.5477  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.8616  Validation loss = 4.5461  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.8615  Validation loss = 4.5456  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 1.8612  Validation loss = 4.5450  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 1.8609  Validation loss = 4.5438  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.8607  Validation loss = 4.5428  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 1.8603  Validation loss = 4.5408  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.8596  Validation loss = 4.5369  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 1.8592  Validation loss = 4.5349  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 1.8586  Validation loss = 4.5319  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 1.8583  Validation loss = 4.5304  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.8580  Validation loss = 4.5282  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 1.8575  Validation loss = 4.5252  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 1.8573  Validation loss = 4.5244  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 1.8571  Validation loss = 4.5229  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 1.8569  Validation loss = 4.5225  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 1.8566  Validation loss = 4.5213  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 1.8565  Validation loss = 4.5210  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 1.8563  Validation loss = 4.5196  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 1.8558  Validation loss = 4.5171  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 1.8554  Validation loss = 4.5153  \n",
      "\n",
      "Fold: 5  Epoch: 24  Training loss = 1.8552  Validation loss = 4.5140  \n",
      "\n",
      "Fold: 5  Epoch: 25  Training loss = 1.8546  Validation loss = 4.5106  \n",
      "\n",
      "Fold: 5  Epoch: 26  Training loss = 1.8540  Validation loss = 4.5075  \n",
      "\n",
      "Fold: 5  Epoch: 27  Training loss = 1.8536  Validation loss = 4.5052  \n",
      "\n",
      "Fold: 5  Epoch: 28  Training loss = 1.8532  Validation loss = 4.5032  \n",
      "\n",
      "Fold: 5  Epoch: 29  Training loss = 1.8529  Validation loss = 4.5013  \n",
      "\n",
      "Fold: 5  Epoch: 30  Training loss = 1.8522  Validation loss = 4.4972  \n",
      "\n",
      "Fold: 5  Epoch: 31  Training loss = 1.8519  Validation loss = 4.4958  \n",
      "\n",
      "Fold: 5  Epoch: 32  Training loss = 1.8517  Validation loss = 4.4946  \n",
      "\n",
      "Fold: 5  Epoch: 33  Training loss = 1.8514  Validation loss = 4.4933  \n",
      "\n",
      "Fold: 5  Epoch: 34  Training loss = 1.8511  Validation loss = 4.4918  \n",
      "\n",
      "Fold: 5  Epoch: 35  Training loss = 1.8509  Validation loss = 4.4910  \n",
      "\n",
      "Fold: 5  Epoch: 36  Training loss = 1.8505  Validation loss = 4.4884  \n",
      "\n",
      "Fold: 5  Epoch: 37  Training loss = 1.8503  Validation loss = 4.4871  \n",
      "\n",
      "Fold: 5  Epoch: 38  Training loss = 1.8500  Validation loss = 4.4853  \n",
      "\n",
      "Fold: 5  Epoch: 39  Training loss = 1.8498  Validation loss = 4.4844  \n",
      "\n",
      "Fold: 5  Epoch: 40  Training loss = 1.8493  Validation loss = 4.4818  \n",
      "\n",
      "Fold: 5  Epoch: 41  Training loss = 1.8486  Validation loss = 4.4773  \n",
      "\n",
      "Fold: 5  Epoch: 42  Training loss = 1.8484  Validation loss = 4.4763  \n",
      "\n",
      "Fold: 5  Epoch: 43  Training loss = 1.8484  Validation loss = 4.4765  \n",
      "\n",
      "Fold: 5  Epoch: 44  Training loss = 1.8482  Validation loss = 4.4758  \n",
      "\n",
      "Fold: 5  Epoch: 45  Training loss = 1.8480  Validation loss = 4.4746  \n",
      "\n",
      "Fold: 5  Epoch: 46  Training loss = 1.8477  Validation loss = 4.4730  \n",
      "\n",
      "Fold: 5  Epoch: 47  Training loss = 1.8476  Validation loss = 4.4726  \n",
      "\n",
      "Fold: 5  Epoch: 48  Training loss = 1.8472  Validation loss = 4.4702  \n",
      "\n",
      "Fold: 5  Epoch: 49  Training loss = 1.8466  Validation loss = 4.4668  \n",
      "\n",
      "Fold: 5  Epoch: 50  Training loss = 1.8462  Validation loss = 4.4647  \n",
      "\n",
      "Fold: 5  Epoch: 51  Training loss = 1.8458  Validation loss = 4.4624  \n",
      "\n",
      "Fold: 5  Epoch: 52  Training loss = 1.8457  Validation loss = 4.4618  \n",
      "\n",
      "Fold: 5  Epoch: 53  Training loss = 1.8453  Validation loss = 4.4595  \n",
      "\n",
      "Fold: 5  Epoch: 54  Training loss = 1.8449  Validation loss = 4.4571  \n",
      "\n",
      "Fold: 5  Epoch: 55  Training loss = 1.8447  Validation loss = 4.4563  \n",
      "\n",
      "Fold: 5  Epoch: 56  Training loss = 1.8444  Validation loss = 4.4548  \n",
      "\n",
      "Fold: 5  Epoch: 57  Training loss = 1.8442  Validation loss = 4.4535  \n",
      "\n",
      "Fold: 5  Epoch: 58  Training loss = 1.8441  Validation loss = 4.4530  \n",
      "\n",
      "Fold: 5  Epoch: 59  Training loss = 1.8438  Validation loss = 4.4509  \n",
      "\n",
      "Fold: 5  Epoch: 60  Training loss = 1.8437  Validation loss = 4.4509  \n",
      "\n",
      "Fold: 5  Epoch: 61  Training loss = 1.8434  Validation loss = 4.4491  \n",
      "\n",
      "Fold: 5  Epoch: 62  Training loss = 1.8431  Validation loss = 4.4471  \n",
      "\n",
      "Fold: 5  Epoch: 63  Training loss = 1.8428  Validation loss = 4.4456  \n",
      "\n",
      "Fold: 5  Epoch: 64  Training loss = 1.8425  Validation loss = 4.4436  \n",
      "\n",
      "Fold: 5  Epoch: 65  Training loss = 1.8422  Validation loss = 4.4427  \n",
      "\n",
      "Fold: 5  Epoch: 66  Training loss = 1.8419  Validation loss = 4.4411  \n",
      "\n",
      "Fold: 5  Epoch: 67  Training loss = 1.8416  Validation loss = 4.4390  \n",
      "\n",
      "Fold: 5  Epoch: 68  Training loss = 1.8415  Validation loss = 4.4384  \n",
      "\n",
      "Fold: 5  Epoch: 69  Training loss = 1.8415  Validation loss = 4.4388  \n",
      "\n",
      "Fold: 5  Epoch: 70  Training loss = 1.8412  Validation loss = 4.4375  \n",
      "\n",
      "Fold: 5  Epoch: 71  Training loss = 1.8410  Validation loss = 4.4362  \n",
      "\n",
      "Fold: 5  Epoch: 72  Training loss = 1.8409  Validation loss = 4.4357  \n",
      "\n",
      "Fold: 5  Epoch: 73  Training loss = 1.8406  Validation loss = 4.4336  \n",
      "\n",
      "Fold: 5  Epoch: 74  Training loss = 1.8402  Validation loss = 4.4318  \n",
      "\n",
      "Fold: 5  Epoch: 75  Training loss = 1.8398  Validation loss = 4.4289  \n",
      "\n",
      "Fold: 5  Epoch: 76  Training loss = 1.8394  Validation loss = 4.4267  \n",
      "\n",
      "Fold: 5  Epoch: 77  Training loss = 1.8392  Validation loss = 4.4256  \n",
      "\n",
      "Fold: 5  Epoch: 78  Training loss = 1.8388  Validation loss = 4.4231  \n",
      "\n",
      "Fold: 5  Epoch: 79  Training loss = 1.8388  Validation loss = 4.4231  \n",
      "\n",
      "Fold: 5  Epoch: 80  Training loss = 1.8385  Validation loss = 4.4219  \n",
      "\n",
      "Fold: 5  Epoch: 81  Training loss = 1.8381  Validation loss = 4.4197  \n",
      "\n",
      "Fold: 5  Epoch: 82  Training loss = 1.8378  Validation loss = 4.4175  \n",
      "\n",
      "Fold: 5  Epoch: 83  Training loss = 1.8375  Validation loss = 4.4163  \n",
      "\n",
      "Fold: 5  Epoch: 84  Training loss = 1.8375  Validation loss = 4.4164  \n",
      "\n",
      "Fold: 5  Epoch: 85  Training loss = 1.8372  Validation loss = 4.4145  \n",
      "\n",
      "Fold: 5  Epoch: 86  Training loss = 1.8369  Validation loss = 4.4121  \n",
      "\n",
      "Fold: 5  Epoch: 87  Training loss = 1.8368  Validation loss = 4.4120  \n",
      "\n",
      "Fold: 5  Epoch: 88  Training loss = 1.8367  Validation loss = 4.4122  \n",
      "\n",
      "Fold: 5  Epoch: 89  Training loss = 1.8365  Validation loss = 4.4109  \n",
      "\n",
      "Fold: 5  Epoch: 90  Training loss = 1.8362  Validation loss = 4.4101  \n",
      "\n",
      "Fold: 5  Epoch: 91  Training loss = 1.8362  Validation loss = 4.4094  \n",
      "\n",
      "Fold: 5  Epoch: 92  Training loss = 1.8360  Validation loss = 4.4094  \n",
      "\n",
      "Fold: 5  Epoch: 93  Training loss = 1.8359  Validation loss = 4.4089  \n",
      "\n",
      "Fold: 5  Epoch: 94  Training loss = 1.8358  Validation loss = 4.4088  \n",
      "\n",
      "Fold: 5  Epoch: 95  Training loss = 1.8356  Validation loss = 4.4071  \n",
      "\n",
      "Fold: 5  Epoch: 96  Training loss = 1.8355  Validation loss = 4.4071  \n",
      "\n",
      "Fold: 5  Epoch: 97  Training loss = 1.8353  Validation loss = 4.4058  \n",
      "\n",
      "Fold: 5  Epoch: 98  Training loss = 1.8351  Validation loss = 4.4050  \n",
      "\n",
      "Fold: 5  Epoch: 99  Training loss = 1.8351  Validation loss = 4.4056  \n",
      "\n",
      "Fold: 5  Epoch: 100  Training loss = 1.8350  Validation loss = 4.4057  \n",
      "\n",
      "Fold: 5  Epoch: 101  Training loss = 1.8349  Validation loss = 4.4050  \n",
      "\n",
      "Fold: 5  Epoch: 102  Training loss = 1.8346  Validation loss = 4.4032  \n",
      "\n",
      "Fold: 5  Epoch: 103  Training loss = 1.8347  Validation loss = 4.4040  \n",
      "\n",
      "Fold: 5  Epoch: 104  Training loss = 1.8346  Validation loss = 4.4035  \n",
      "\n",
      "Fold: 5  Epoch: 105  Training loss = 1.8345  Validation loss = 4.4036  \n",
      "\n",
      "Fold: 5  Epoch: 106  Training loss = 1.8343  Validation loss = 4.4024  \n",
      "\n",
      "Fold: 5  Epoch: 107  Training loss = 1.8341  Validation loss = 4.4013  \n",
      "\n",
      "Fold: 5  Epoch: 108  Training loss = 1.8340  Validation loss = 4.4008  \n",
      "\n",
      "Fold: 5  Epoch: 109  Training loss = 1.8338  Validation loss = 4.3998  \n",
      "\n",
      "Fold: 5  Epoch: 110  Training loss = 1.8334  Validation loss = 4.3975  \n",
      "\n",
      "Fold: 5  Epoch: 111  Training loss = 1.8331  Validation loss = 4.3957  \n",
      "\n",
      "Fold: 5  Epoch: 112  Training loss = 1.8332  Validation loss = 4.3967  \n",
      "\n",
      "Fold: 5  Epoch: 113  Training loss = 1.8331  Validation loss = 4.3961  \n",
      "\n",
      "Fold: 5  Epoch: 114  Training loss = 1.8327  Validation loss = 4.3940  \n",
      "\n",
      "Fold: 5  Epoch: 115  Training loss = 1.8325  Validation loss = 4.3927  \n",
      "\n",
      "Fold: 5  Epoch: 116  Training loss = 1.8324  Validation loss = 4.3925  \n",
      "\n",
      "Fold: 5  Epoch: 117  Training loss = 1.8322  Validation loss = 4.3916  \n",
      "\n",
      "Fold: 5  Epoch: 118  Training loss = 1.8322  Validation loss = 4.3919  \n",
      "\n",
      "Fold: 5  Epoch: 119  Training loss = 1.8318  Validation loss = 4.3895  \n",
      "\n",
      "Fold: 5  Epoch: 120  Training loss = 1.8316  Validation loss = 4.3884  \n",
      "\n",
      "Fold: 5  Epoch: 121  Training loss = 1.8314  Validation loss = 4.3877  \n",
      "\n",
      "Fold: 5  Epoch: 122  Training loss = 1.8314  Validation loss = 4.3879  \n",
      "\n",
      "Fold: 5  Epoch: 123  Training loss = 1.8313  Validation loss = 4.3871  \n",
      "\n",
      "Fold: 5  Epoch: 124  Training loss = 1.8312  Validation loss = 4.3870  \n",
      "\n",
      "Fold: 5  Epoch: 125  Training loss = 1.8310  Validation loss = 4.3862  \n",
      "\n",
      "Fold: 5  Epoch: 126  Training loss = 1.8309  Validation loss = 4.3859  \n",
      "\n",
      "Fold: 5  Epoch: 127  Training loss = 1.8309  Validation loss = 4.3858  \n",
      "\n",
      "Fold: 5  Epoch: 128  Training loss = 1.8304  Validation loss = 4.3825  \n",
      "\n",
      "Fold: 5  Epoch: 129  Training loss = 1.8300  Validation loss = 4.3802  \n",
      "\n",
      "Fold: 5  Epoch: 130  Training loss = 1.8297  Validation loss = 4.3780  \n",
      "\n",
      "Fold: 5  Epoch: 131  Training loss = 1.8293  Validation loss = 4.3744  \n",
      "\n",
      "Fold: 5  Epoch: 132  Training loss = 1.8290  Validation loss = 4.3725  \n",
      "\n",
      "Fold: 5  Epoch: 133  Training loss = 1.8286  Validation loss = 4.3697  \n",
      "\n",
      "Fold: 5  Epoch: 134  Training loss = 1.8285  Validation loss = 4.3694  \n",
      "\n",
      "Fold: 5  Epoch: 135  Training loss = 1.8280  Validation loss = 4.3664  \n",
      "\n",
      "Fold: 5  Epoch: 136  Training loss = 1.8278  Validation loss = 4.3654  \n",
      "\n",
      "Fold: 5  Epoch: 137  Training loss = 1.8276  Validation loss = 4.3641  \n",
      "\n",
      "Fold: 5  Epoch: 138  Training loss = 1.8274  Validation loss = 4.3636  \n",
      "\n",
      "Fold: 5  Epoch: 139  Training loss = 1.8273  Validation loss = 4.3628  \n",
      "\n",
      "Fold: 5  Epoch: 140  Training loss = 1.8270  Validation loss = 4.3607  \n",
      "\n",
      "Fold: 5  Epoch: 141  Training loss = 1.8269  Validation loss = 4.3605  \n",
      "\n",
      "Fold: 5  Epoch: 142  Training loss = 1.8268  Validation loss = 4.3600  \n",
      "\n",
      "Fold: 5  Epoch: 143  Training loss = 1.8266  Validation loss = 4.3591  \n",
      "\n",
      "Fold: 5  Epoch: 144  Training loss = 1.8266  Validation loss = 4.3602  \n",
      "\n",
      "Fold: 5  Epoch: 145  Training loss = 1.8262  Validation loss = 4.3574  \n",
      "\n",
      "Fold: 5  Epoch: 146  Training loss = 1.8259  Validation loss = 4.3555  \n",
      "\n",
      "Fold: 5  Epoch: 147  Training loss = 1.8256  Validation loss = 4.3534  \n",
      "\n",
      "Fold: 5  Epoch: 148  Training loss = 1.8254  Validation loss = 4.3525  \n",
      "\n",
      "Fold: 5  Epoch: 149  Training loss = 1.8251  Validation loss = 4.3511  \n",
      "\n",
      "Fold: 5  Epoch: 150  Training loss = 1.8247  Validation loss = 4.3483  \n",
      "\n",
      "Fold: 5  Epoch: 151  Training loss = 1.8245  Validation loss = 4.3469  \n",
      "\n",
      "Fold: 5  Epoch: 152  Training loss = 1.8244  Validation loss = 4.3465  \n",
      "\n",
      "Fold: 5  Epoch: 153  Training loss = 1.8242  Validation loss = 4.3454  \n",
      "\n",
      "Fold: 5  Epoch: 154  Training loss = 1.8240  Validation loss = 4.3448  \n",
      "\n",
      "Fold: 5  Epoch: 155  Training loss = 1.8239  Validation loss = 4.3446  \n",
      "\n",
      "Fold: 5  Epoch: 156  Training loss = 1.8236  Validation loss = 4.3432  \n",
      "\n",
      "Fold: 5  Epoch: 157  Training loss = 1.8233  Validation loss = 4.3409  \n",
      "\n",
      "Fold: 5  Epoch: 158  Training loss = 1.8232  Validation loss = 4.3409  \n",
      "\n",
      "Fold: 5  Epoch: 159  Training loss = 1.8231  Validation loss = 4.3403  \n",
      "\n",
      "Fold: 5  Epoch: 160  Training loss = 1.8230  Validation loss = 4.3404  \n",
      "\n",
      "Fold: 5  Epoch: 161  Training loss = 1.8229  Validation loss = 4.3399  \n",
      "\n",
      "Fold: 5  Epoch: 162  Training loss = 1.8226  Validation loss = 4.3381  \n",
      "\n",
      "Fold: 5  Epoch: 163  Training loss = 1.8224  Validation loss = 4.3363  \n",
      "\n",
      "Fold: 5  Epoch: 164  Training loss = 1.8221  Validation loss = 4.3340  \n",
      "\n",
      "Fold: 5  Epoch: 165  Training loss = 1.8219  Validation loss = 4.3342  \n",
      "\n",
      "Fold: 5  Epoch: 166  Training loss = 1.8218  Validation loss = 4.3332  \n",
      "\n",
      "Fold: 5  Epoch: 167  Training loss = 1.8216  Validation loss = 4.3321  \n",
      "\n",
      "Fold: 5  Epoch: 168  Training loss = 1.8214  Validation loss = 4.3316  \n",
      "\n",
      "Fold: 5  Epoch: 169  Training loss = 1.8211  Validation loss = 4.3298  \n",
      "\n",
      "Fold: 5  Epoch: 170  Training loss = 1.8207  Validation loss = 4.3271  \n",
      "\n",
      "Fold: 5  Epoch: 171  Training loss = 1.8204  Validation loss = 4.3254  \n",
      "\n",
      "Fold: 5  Epoch: 172  Training loss = 1.8201  Validation loss = 4.3232  \n",
      "\n",
      "Fold: 5  Epoch: 173  Training loss = 1.8199  Validation loss = 4.3220  \n",
      "\n",
      "Fold: 5  Epoch: 174  Training loss = 1.8197  Validation loss = 4.3206  \n",
      "\n",
      "Fold: 5  Epoch: 175  Training loss = 1.8195  Validation loss = 4.3190  \n",
      "\n",
      "Fold: 5  Epoch: 176  Training loss = 1.8193  Validation loss = 4.3181  \n",
      "\n",
      "Fold: 5  Epoch: 177  Training loss = 1.8190  Validation loss = 4.3156  \n",
      "\n",
      "Fold: 5  Epoch: 178  Training loss = 1.8188  Validation loss = 4.3142  \n",
      "\n",
      "Fold: 5  Epoch: 179  Training loss = 1.8187  Validation loss = 4.3138  \n",
      "\n",
      "Fold: 5  Epoch: 180  Training loss = 1.8184  Validation loss = 4.3127  \n",
      "\n",
      "Fold: 5  Epoch: 181  Training loss = 1.8181  Validation loss = 4.3107  \n",
      "\n",
      "Fold: 5  Epoch: 182  Training loss = 1.8180  Validation loss = 4.3105  \n",
      "\n",
      "Fold: 5  Epoch: 183  Training loss = 1.8178  Validation loss = 4.3091  \n",
      "\n",
      "Fold: 5  Epoch: 184  Training loss = 1.8175  Validation loss = 4.3070  \n",
      "\n",
      "Fold: 5  Epoch: 185  Training loss = 1.8172  Validation loss = 4.3046  \n",
      "\n",
      "Fold: 5  Epoch: 186  Training loss = 1.8170  Validation loss = 4.3033  \n",
      "\n",
      "Fold: 5  Epoch: 187  Training loss = 1.8168  Validation loss = 4.3021  \n",
      "\n",
      "Fold: 5  Epoch: 188  Training loss = 1.8167  Validation loss = 4.3030  \n",
      "\n",
      "Fold: 5  Epoch: 189  Training loss = 1.8166  Validation loss = 4.3016  \n",
      "\n",
      "Fold: 5  Epoch: 190  Training loss = 1.8163  Validation loss = 4.2997  \n",
      "\n",
      "Fold: 5  Epoch: 191  Training loss = 1.8161  Validation loss = 4.2991  \n",
      "\n",
      "Fold: 5  Epoch: 192  Training loss = 1.8158  Validation loss = 4.2966  \n",
      "\n",
      "Fold: 5  Epoch: 193  Training loss = 1.8155  Validation loss = 4.2944  \n",
      "\n",
      "Fold: 5  Epoch: 194  Training loss = 1.8153  Validation loss = 4.2939  \n",
      "\n",
      "Fold: 5  Epoch: 195  Training loss = 1.8150  Validation loss = 4.2913  \n",
      "\n",
      "Fold: 5  Epoch: 196  Training loss = 1.8147  Validation loss = 4.2892  \n",
      "\n",
      "Fold: 5  Epoch: 197  Training loss = 1.8144  Validation loss = 4.2868  \n",
      "\n",
      "Fold: 5  Epoch: 198  Training loss = 1.8143  Validation loss = 4.2866  \n",
      "\n",
      "Fold: 5  Epoch: 199  Training loss = 1.8141  Validation loss = 4.2861  \n",
      "\n",
      "Fold: 5  Epoch: 200  Training loss = 1.8140  Validation loss = 4.2859  \n",
      "\n",
      "Fold: 5  Epoch: 201  Training loss = 1.8138  Validation loss = 4.2850  \n",
      "\n",
      "Fold: 5  Epoch: 202  Training loss = 1.8135  Validation loss = 4.2828  \n",
      "\n",
      "Fold: 5  Epoch: 203  Training loss = 1.8133  Validation loss = 4.2810  \n",
      "\n",
      "Fold: 5  Epoch: 204  Training loss = 1.8133  Validation loss = 4.2815  \n",
      "\n",
      "Fold: 5  Epoch: 205  Training loss = 1.8130  Validation loss = 4.2798  \n",
      "\n",
      "Fold: 5  Epoch: 206  Training loss = 1.8128  Validation loss = 4.2782  \n",
      "\n",
      "Fold: 5  Epoch: 207  Training loss = 1.8127  Validation loss = 4.2774  \n",
      "\n",
      "Fold: 5  Epoch: 208  Training loss = 1.8126  Validation loss = 4.2776  \n",
      "\n",
      "Fold: 5  Epoch: 209  Training loss = 1.8124  Validation loss = 4.2762  \n",
      "\n",
      "Fold: 5  Epoch: 210  Training loss = 1.8122  Validation loss = 4.2752  \n",
      "\n",
      "Fold: 5  Epoch: 211  Training loss = 1.8120  Validation loss = 4.2736  \n",
      "\n",
      "Fold: 5  Epoch: 212  Training loss = 1.8119  Validation loss = 4.2728  \n",
      "\n",
      "Fold: 5  Epoch: 213  Training loss = 1.8117  Validation loss = 4.2717  \n",
      "\n",
      "Fold: 5  Epoch: 214  Training loss = 1.8115  Validation loss = 4.2709  \n",
      "\n",
      "Fold: 5  Epoch: 215  Training loss = 1.8112  Validation loss = 4.2687  \n",
      "\n",
      "Fold: 5  Epoch: 216  Training loss = 1.8110  Validation loss = 4.2670  \n",
      "\n",
      "Fold: 5  Epoch: 217  Training loss = 1.8108  Validation loss = 4.2661  \n",
      "\n",
      "Fold: 5  Epoch: 218  Training loss = 1.8108  Validation loss = 4.2658  \n",
      "\n",
      "Fold: 5  Epoch: 219  Training loss = 1.8106  Validation loss = 4.2652  \n",
      "\n",
      "Fold: 5  Epoch: 220  Training loss = 1.8104  Validation loss = 4.2642  \n",
      "\n",
      "Fold: 5  Epoch: 221  Training loss = 1.8103  Validation loss = 4.2633  \n",
      "\n",
      "Fold: 5  Epoch: 222  Training loss = 1.8101  Validation loss = 4.2628  \n",
      "\n",
      "Fold: 5  Epoch: 223  Training loss = 1.8099  Validation loss = 4.2613  \n",
      "\n",
      "Fold: 5  Epoch: 224  Training loss = 1.8097  Validation loss = 4.2601  \n",
      "\n",
      "Fold: 5  Epoch: 225  Training loss = 1.8095  Validation loss = 4.2586  \n",
      "\n",
      "Fold: 5  Epoch: 226  Training loss = 1.8094  Validation loss = 4.2586  \n",
      "\n",
      "Fold: 5  Epoch: 227  Training loss = 1.8093  Validation loss = 4.2586  \n",
      "\n",
      "Fold: 5  Epoch: 228  Training loss = 1.8090  Validation loss = 4.2568  \n",
      "\n",
      "Fold: 5  Epoch: 229  Training loss = 1.8088  Validation loss = 4.2551  \n",
      "\n",
      "Fold: 5  Epoch: 230  Training loss = 1.8088  Validation loss = 4.2554  \n",
      "\n",
      "Fold: 5  Epoch: 231  Training loss = 1.8085  Validation loss = 4.2540  \n",
      "\n",
      "Fold: 5  Epoch: 232  Training loss = 1.8084  Validation loss = 4.2540  \n",
      "\n",
      "Fold: 5  Epoch: 233  Training loss = 1.8082  Validation loss = 4.2523  \n",
      "\n",
      "Fold: 5  Epoch: 234  Training loss = 1.8079  Validation loss = 4.2505  \n",
      "\n",
      "Fold: 5  Epoch: 235  Training loss = 1.8077  Validation loss = 4.2495  \n",
      "\n",
      "Fold: 5  Epoch: 236  Training loss = 1.8076  Validation loss = 4.2489  \n",
      "\n",
      "Fold: 5  Epoch: 237  Training loss = 1.8075  Validation loss = 4.2477  \n",
      "\n",
      "Fold: 5  Epoch: 238  Training loss = 1.8071  Validation loss = 4.2452  \n",
      "\n",
      "Fold: 5  Epoch: 239  Training loss = 1.8069  Validation loss = 4.2441  \n",
      "\n",
      "Fold: 5  Epoch: 240  Training loss = 1.8069  Validation loss = 4.2440  \n",
      "\n",
      "Fold: 5  Epoch: 241  Training loss = 1.8066  Validation loss = 4.2422  \n",
      "\n",
      "Fold: 5  Epoch: 242  Training loss = 1.8064  Validation loss = 4.2412  \n",
      "\n",
      "Fold: 5  Epoch: 243  Training loss = 1.8062  Validation loss = 4.2394  \n",
      "\n",
      "Fold: 5  Epoch: 244  Training loss = 1.8062  Validation loss = 4.2405  \n",
      "\n",
      "Fold: 5  Epoch: 245  Training loss = 1.8059  Validation loss = 4.2386  \n",
      "\n",
      "Fold: 5  Epoch: 246  Training loss = 1.8058  Validation loss = 4.2372  \n",
      "\n",
      "Fold: 5  Epoch: 247  Training loss = 1.8056  Validation loss = 4.2369  \n",
      "\n",
      "Fold: 5  Epoch: 248  Training loss = 1.8054  Validation loss = 4.2357  \n",
      "\n",
      "Fold: 5  Epoch: 249  Training loss = 1.8052  Validation loss = 4.2338  \n",
      "\n",
      "Fold: 5  Epoch: 250  Training loss = 1.8050  Validation loss = 4.2326  \n",
      "\n",
      "Fold: 5  Epoch: 251  Training loss = 1.8048  Validation loss = 4.2318  \n",
      "\n",
      "Fold: 5  Epoch: 252  Training loss = 1.8045  Validation loss = 4.2301  \n",
      "\n",
      "Fold: 5  Epoch: 253  Training loss = 1.8044  Validation loss = 4.2293  \n",
      "\n",
      "Fold: 5  Epoch: 254  Training loss = 1.8043  Validation loss = 4.2281  \n",
      "\n",
      "Fold: 5  Epoch: 255  Training loss = 1.8041  Validation loss = 4.2269  \n",
      "\n",
      "Fold: 5  Epoch: 256  Training loss = 1.8040  Validation loss = 4.2273  \n",
      "\n",
      "Fold: 5  Epoch: 257  Training loss = 1.8040  Validation loss = 4.2271  \n",
      "\n",
      "Fold: 5  Epoch: 258  Training loss = 1.8037  Validation loss = 4.2259  \n",
      "\n",
      "Fold: 5  Epoch: 259  Training loss = 1.8036  Validation loss = 4.2253  \n",
      "\n",
      "Fold: 5  Epoch: 260  Training loss = 1.8035  Validation loss = 4.2250  \n",
      "\n",
      "Fold: 5  Epoch: 261  Training loss = 1.8035  Validation loss = 4.2258  \n",
      "\n",
      "Fold: 5  Epoch: 262  Training loss = 1.8033  Validation loss = 4.2244  \n",
      "\n",
      "Fold: 5  Epoch: 263  Training loss = 1.8030  Validation loss = 4.2225  \n",
      "\n",
      "Fold: 5  Epoch: 264  Training loss = 1.8028  Validation loss = 4.2207  \n",
      "\n",
      "Fold: 5  Epoch: 265  Training loss = 1.8027  Validation loss = 4.2212  \n",
      "\n",
      "Fold: 5  Epoch: 266  Training loss = 1.8025  Validation loss = 4.2195  \n",
      "\n",
      "Fold: 5  Epoch: 267  Training loss = 1.8023  Validation loss = 4.2190  \n",
      "\n",
      "Fold: 5  Epoch: 268  Training loss = 1.8021  Validation loss = 4.2170  \n",
      "\n",
      "Fold: 5  Epoch: 269  Training loss = 1.8020  Validation loss = 4.2165  \n",
      "\n",
      "Fold: 5  Epoch: 270  Training loss = 1.8017  Validation loss = 4.2148  \n",
      "\n",
      "Fold: 5  Epoch: 271  Training loss = 1.8016  Validation loss = 4.2140  \n",
      "\n",
      "Fold: 5  Epoch: 272  Training loss = 1.8013  Validation loss = 4.2120  \n",
      "\n",
      "Fold: 5  Epoch: 273  Training loss = 1.8012  Validation loss = 4.2109  \n",
      "\n",
      "Fold: 5  Epoch: 274  Training loss = 1.8010  Validation loss = 4.2104  \n",
      "\n",
      "Fold: 5  Epoch: 275  Training loss = 1.8009  Validation loss = 4.2100  \n",
      "\n",
      "Fold: 5  Epoch: 276  Training loss = 1.8007  Validation loss = 4.2083  \n",
      "\n",
      "Fold: 5  Epoch: 277  Training loss = 1.8005  Validation loss = 4.2070  \n",
      "\n",
      "Fold: 5  Epoch: 278  Training loss = 1.8004  Validation loss = 4.2070  \n",
      "\n",
      "Fold: 5  Epoch: 279  Training loss = 1.8003  Validation loss = 4.2073  \n",
      "\n",
      "Fold: 5  Epoch: 280  Training loss = 1.8001  Validation loss = 4.2059  \n",
      "\n",
      "Fold: 5  Epoch: 281  Training loss = 1.7998  Validation loss = 4.2036  \n",
      "\n",
      "Fold: 5  Epoch: 282  Training loss = 1.7996  Validation loss = 4.2034  \n",
      "\n",
      "Fold: 5  Epoch: 283  Training loss = 1.7994  Validation loss = 4.2017  \n",
      "\n",
      "Fold: 5  Epoch: 284  Training loss = 1.7993  Validation loss = 4.2015  \n",
      "\n",
      "Fold: 5  Epoch: 285  Training loss = 1.7992  Validation loss = 4.2010  \n",
      "\n",
      "Fold: 5  Epoch: 286  Training loss = 1.7990  Validation loss = 4.2000  \n",
      "\n",
      "Fold: 5  Epoch: 287  Training loss = 1.7988  Validation loss = 4.1986  \n",
      "\n",
      "Fold: 5  Epoch: 288  Training loss = 1.7987  Validation loss = 4.1973  \n",
      "\n",
      "Fold: 5  Epoch: 289  Training loss = 1.7987  Validation loss = 4.1984  \n",
      "\n",
      "Fold: 5  Epoch: 290  Training loss = 1.7985  Validation loss = 4.1972  \n",
      "\n",
      "Fold: 5  Epoch: 291  Training loss = 1.7984  Validation loss = 4.1970  \n",
      "\n",
      "Fold: 5  Epoch: 292  Training loss = 1.7984  Validation loss = 4.1974  \n",
      "\n",
      "Fold: 5  Epoch: 293  Training loss = 1.7983  Validation loss = 4.1973  \n",
      "\n",
      "Fold: 5  Epoch: 294  Training loss = 1.7982  Validation loss = 4.1979  \n",
      "\n",
      "Fold: 5  Epoch: 295  Training loss = 1.7981  Validation loss = 4.1971  \n",
      "\n",
      "Fold: 5  Epoch: 296  Training loss = 1.7981  Validation loss = 4.1982  \n",
      "\n",
      "Fold: 5  Epoch: 297  Training loss = 1.7981  Validation loss = 4.1991  \n",
      "\n",
      "Fold: 5  Epoch: 298  Training loss = 1.7980  Validation loss = 4.1986  \n",
      "\n",
      "Fold: 5  Epoch: 299  Training loss = 1.7978  Validation loss = 4.1982  \n",
      "\n",
      "Fold: 5  Epoch: 300  Training loss = 1.7978  Validation loss = 4.1980  \n",
      "\n",
      "Fold: 5  Epoch: 301  Training loss = 1.7975  Validation loss = 4.1959  \n",
      "\n",
      "Fold: 5  Epoch: 302  Training loss = 1.7973  Validation loss = 4.1944  \n",
      "\n",
      "Fold: 5  Epoch: 303  Training loss = 1.7971  Validation loss = 4.1942  \n",
      "\n",
      "Fold: 5  Epoch: 304  Training loss = 1.7969  Validation loss = 4.1927  \n",
      "\n",
      "Fold: 5  Epoch: 305  Training loss = 1.7968  Validation loss = 4.1924  \n",
      "\n",
      "Fold: 5  Epoch: 306  Training loss = 1.7967  Validation loss = 4.1919  \n",
      "\n",
      "Fold: 5  Epoch: 307  Training loss = 1.7966  Validation loss = 4.1921  \n",
      "\n",
      "Fold: 5  Epoch: 308  Training loss = 1.7963  Validation loss = 4.1902  \n",
      "\n",
      "Fold: 5  Epoch: 309  Training loss = 1.7963  Validation loss = 4.1904  \n",
      "\n",
      "Fold: 5  Epoch: 310  Training loss = 1.7961  Validation loss = 4.1891  \n",
      "\n",
      "Fold: 5  Epoch: 311  Training loss = 1.7958  Validation loss = 4.1873  \n",
      "\n",
      "Fold: 5  Epoch: 312  Training loss = 1.7957  Validation loss = 4.1866  \n",
      "\n",
      "Fold: 5  Epoch: 313  Training loss = 1.7956  Validation loss = 4.1862  \n",
      "\n",
      "Fold: 5  Epoch: 314  Training loss = 1.7955  Validation loss = 4.1847  \n",
      "\n",
      "Fold: 5  Epoch: 315  Training loss = 1.7953  Validation loss = 4.1838  \n",
      "\n",
      "Fold: 5  Epoch: 316  Training loss = 1.7953  Validation loss = 4.1836  \n",
      "\n",
      "Fold: 5  Epoch: 317  Training loss = 1.7950  Validation loss = 4.1812  \n",
      "\n",
      "Fold: 5  Epoch: 318  Training loss = 1.7948  Validation loss = 4.1799  \n",
      "\n",
      "Fold: 5  Epoch: 319  Training loss = 1.7947  Validation loss = 4.1793  \n",
      "\n",
      "Fold: 5  Epoch: 320  Training loss = 1.7945  Validation loss = 4.1777  \n",
      "\n",
      "Fold: 5  Epoch: 321  Training loss = 1.7941  Validation loss = 4.1746  \n",
      "\n",
      "Fold: 5  Epoch: 322  Training loss = 1.7941  Validation loss = 4.1746  \n",
      "\n",
      "Fold: 5  Epoch: 323  Training loss = 1.7940  Validation loss = 4.1739  \n",
      "\n",
      "Fold: 5  Epoch: 324  Training loss = 1.7938  Validation loss = 4.1727  \n",
      "\n",
      "Fold: 5  Epoch: 325  Training loss = 1.7935  Validation loss = 4.1713  \n",
      "\n",
      "Fold: 5  Epoch: 326  Training loss = 1.7935  Validation loss = 4.1715  \n",
      "\n",
      "Fold: 5  Epoch: 327  Training loss = 1.7935  Validation loss = 4.1722  \n",
      "\n",
      "Fold: 5  Epoch: 328  Training loss = 1.7932  Validation loss = 4.1701  \n",
      "\n",
      "Fold: 5  Epoch: 329  Training loss = 1.7931  Validation loss = 4.1695  \n",
      "\n",
      "Fold: 5  Epoch: 330  Training loss = 1.7929  Validation loss = 4.1687  \n",
      "\n",
      "Fold: 5  Epoch: 331  Training loss = 1.7929  Validation loss = 4.1689  \n",
      "\n",
      "Fold: 5  Epoch: 332  Training loss = 1.7927  Validation loss = 4.1680  \n",
      "\n",
      "Fold: 5  Epoch: 333  Training loss = 1.7925  Validation loss = 4.1670  \n",
      "\n",
      "Fold: 5  Epoch: 334  Training loss = 1.7923  Validation loss = 4.1654  \n",
      "\n",
      "Fold: 5  Epoch: 335  Training loss = 1.7921  Validation loss = 4.1641  \n",
      "\n",
      "Fold: 5  Epoch: 336  Training loss = 1.7919  Validation loss = 4.1635  \n",
      "\n",
      "Fold: 5  Epoch: 337  Training loss = 1.7917  Validation loss = 4.1608  \n",
      "\n",
      "Fold: 5  Epoch: 338  Training loss = 1.7915  Validation loss = 4.1599  \n",
      "\n",
      "Fold: 5  Epoch: 339  Training loss = 1.7913  Validation loss = 4.1580  \n",
      "\n",
      "Fold: 5  Epoch: 340  Training loss = 1.7913  Validation loss = 4.1590  \n",
      "\n",
      "Fold: 5  Epoch: 341  Training loss = 1.7911  Validation loss = 4.1566  \n",
      "\n",
      "Fold: 5  Epoch: 342  Training loss = 1.7909  Validation loss = 4.1550  \n",
      "\n",
      "Fold: 5  Epoch: 343  Training loss = 1.7907  Validation loss = 4.1540  \n",
      "\n",
      "Fold: 5  Epoch: 344  Training loss = 1.7907  Validation loss = 4.1537  \n",
      "\n",
      "Fold: 5  Epoch: 345  Training loss = 1.7905  Validation loss = 4.1525  \n",
      "\n",
      "Fold: 5  Epoch: 346  Training loss = 1.7904  Validation loss = 4.1522  \n",
      "\n",
      "Fold: 5  Epoch: 347  Training loss = 1.7903  Validation loss = 4.1511  \n",
      "\n",
      "Fold: 5  Epoch: 348  Training loss = 1.7902  Validation loss = 4.1512  \n",
      "\n",
      "Fold: 5  Epoch: 349  Training loss = 1.7902  Validation loss = 4.1518  \n",
      "\n",
      "Fold: 5  Epoch: 350  Training loss = 1.7900  Validation loss = 4.1506  \n",
      "\n",
      "Fold: 5  Epoch: 351  Training loss = 1.7899  Validation loss = 4.1496  \n",
      "\n",
      "Fold: 5  Epoch: 352  Training loss = 1.7898  Validation loss = 4.1495  \n",
      "\n",
      "Fold: 5  Epoch: 353  Training loss = 1.7896  Validation loss = 4.1483  \n",
      "\n",
      "Fold: 5  Epoch: 354  Training loss = 1.7895  Validation loss = 4.1484  \n",
      "\n",
      "Fold: 5  Epoch: 355  Training loss = 1.7894  Validation loss = 4.1481  \n",
      "\n",
      "Fold: 5  Epoch: 356  Training loss = 1.7892  Validation loss = 4.1470  \n",
      "\n",
      "Fold: 5  Epoch: 357  Training loss = 1.7890  Validation loss = 4.1459  \n",
      "\n",
      "Fold: 5  Epoch: 358  Training loss = 1.7888  Validation loss = 4.1440  \n",
      "\n",
      "Fold: 5  Epoch: 359  Training loss = 1.7887  Validation loss = 4.1434  \n",
      "\n",
      "Fold: 5  Epoch: 360  Training loss = 1.7886  Validation loss = 4.1440  \n",
      "\n",
      "Fold: 5  Epoch: 361  Training loss = 1.7884  Validation loss = 4.1419  \n",
      "\n",
      "Fold: 5  Epoch: 362  Training loss = 1.7882  Validation loss = 4.1409  \n",
      "\n",
      "Fold: 5  Epoch: 363  Training loss = 1.7880  Validation loss = 4.1393  \n",
      "\n",
      "Fold: 5  Epoch: 364  Training loss = 1.7878  Validation loss = 4.1380  \n",
      "\n",
      "Fold: 5  Epoch: 365  Training loss = 1.7877  Validation loss = 4.1381  \n",
      "\n",
      "Fold: 5  Epoch: 366  Training loss = 1.7875  Validation loss = 4.1364  \n",
      "\n",
      "Fold: 5  Epoch: 367  Training loss = 1.7873  Validation loss = 4.1354  \n",
      "\n",
      "Fold: 5  Epoch: 368  Training loss = 1.7872  Validation loss = 4.1350  \n",
      "\n",
      "Fold: 5  Epoch: 369  Training loss = 1.7870  Validation loss = 4.1330  \n",
      "\n",
      "Fold: 5  Epoch: 370  Training loss = 1.7868  Validation loss = 4.1313  \n",
      "\n",
      "Fold: 5  Epoch: 371  Training loss = 1.7866  Validation loss = 4.1299  \n",
      "\n",
      "Fold: 5  Epoch: 372  Training loss = 1.7865  Validation loss = 4.1296  \n",
      "\n",
      "Fold: 5  Epoch: 373  Training loss = 1.7864  Validation loss = 4.1293  \n",
      "\n",
      "Fold: 5  Epoch: 374  Training loss = 1.7862  Validation loss = 4.1285  \n",
      "\n",
      "Fold: 5  Epoch: 375  Training loss = 1.7861  Validation loss = 4.1290  \n",
      "\n",
      "Fold: 5  Epoch: 376  Training loss = 1.7859  Validation loss = 4.1275  \n",
      "\n",
      "Fold: 5  Epoch: 377  Training loss = 1.7858  Validation loss = 4.1268  \n",
      "\n",
      "Fold: 5  Epoch: 378  Training loss = 1.7857  Validation loss = 4.1272  \n",
      "\n",
      "Fold: 5  Epoch: 379  Training loss = 1.7857  Validation loss = 4.1270  \n",
      "\n",
      "Fold: 5  Epoch: 380  Training loss = 1.7856  Validation loss = 4.1270  \n",
      "\n",
      "Fold: 5  Epoch: 381  Training loss = 1.7855  Validation loss = 4.1270  \n",
      "\n",
      "Fold: 5  Epoch: 382  Training loss = 1.7854  Validation loss = 4.1277  \n",
      "\n",
      "Fold: 5  Epoch: 383  Training loss = 1.7853  Validation loss = 4.1263  \n",
      "\n",
      "Fold: 5  Epoch: 384  Training loss = 1.7852  Validation loss = 4.1258  \n",
      "\n",
      "Fold: 5  Epoch: 385  Training loss = 1.7849  Validation loss = 4.1241  \n",
      "\n",
      "Fold: 5  Epoch: 386  Training loss = 1.7848  Validation loss = 4.1229  \n",
      "\n",
      "Fold: 5  Epoch: 387  Training loss = 1.7847  Validation loss = 4.1230  \n",
      "\n",
      "Fold: 5  Epoch: 388  Training loss = 1.7846  Validation loss = 4.1227  \n",
      "\n",
      "Fold: 5  Epoch: 389  Training loss = 1.7844  Validation loss = 4.1223  \n",
      "\n",
      "Fold: 5  Epoch: 390  Training loss = 1.7842  Validation loss = 4.1212  \n",
      "\n",
      "Fold: 5  Epoch: 391  Training loss = 1.7841  Validation loss = 4.1201  \n",
      "\n",
      "Fold: 5  Epoch: 392  Training loss = 1.7839  Validation loss = 4.1185  \n",
      "\n",
      "Fold: 5  Epoch: 393  Training loss = 1.7838  Validation loss = 4.1178  \n",
      "\n",
      "Fold: 5  Epoch: 394  Training loss = 1.7836  Validation loss = 4.1160  \n",
      "\n",
      "Fold: 5  Epoch: 395  Training loss = 1.7834  Validation loss = 4.1151  \n",
      "\n",
      "Fold: 5  Epoch: 396  Training loss = 1.7833  Validation loss = 4.1139  \n",
      "\n",
      "Fold: 5  Epoch: 397  Training loss = 1.7830  Validation loss = 4.1110  \n",
      "\n",
      "Fold: 5  Epoch: 398  Training loss = 1.7828  Validation loss = 4.1098  \n",
      "\n",
      "Fold: 5  Epoch: 399  Training loss = 1.7827  Validation loss = 4.1088  \n",
      "\n",
      "Fold: 5  Epoch: 400  Training loss = 1.7825  Validation loss = 4.1075  \n",
      "\n",
      "Fold: 5  Epoch: 401  Training loss = 1.7825  Validation loss = 4.1085  \n",
      "\n",
      "Fold: 5  Epoch: 402  Training loss = 1.7824  Validation loss = 4.1081  \n",
      "\n",
      "Fold: 5  Epoch: 403  Training loss = 1.7823  Validation loss = 4.1077  \n",
      "\n",
      "Fold: 5  Epoch: 404  Training loss = 1.7822  Validation loss = 4.1074  \n",
      "\n",
      "Fold: 5  Epoch: 405  Training loss = 1.7821  Validation loss = 4.1070  \n",
      "\n",
      "Fold: 5  Epoch: 406  Training loss = 1.7818  Validation loss = 4.1051  \n",
      "\n",
      "Fold: 5  Epoch: 407  Training loss = 1.7817  Validation loss = 4.1040  \n",
      "\n",
      "Fold: 5  Epoch: 408  Training loss = 1.7815  Validation loss = 4.1038  \n",
      "\n",
      "Fold: 5  Epoch: 409  Training loss = 1.7814  Validation loss = 4.1029  \n",
      "\n",
      "Fold: 5  Epoch: 410  Training loss = 1.7813  Validation loss = 4.1018  \n",
      "\n",
      "Fold: 5  Epoch: 411  Training loss = 1.7812  Validation loss = 4.1022  \n",
      "\n",
      "Fold: 5  Epoch: 412  Training loss = 1.7811  Validation loss = 4.1006  \n",
      "\n",
      "Fold: 5  Epoch: 413  Training loss = 1.7810  Validation loss = 4.1008  \n",
      "\n",
      "Fold: 5  Epoch: 414  Training loss = 1.7808  Validation loss = 4.0998  \n",
      "\n",
      "Fold: 5  Epoch: 415  Training loss = 1.7807  Validation loss = 4.0988  \n",
      "\n",
      "Fold: 5  Epoch: 416  Training loss = 1.7806  Validation loss = 4.0987  \n",
      "\n",
      "Fold: 5  Epoch: 417  Training loss = 1.7804  Validation loss = 4.0976  \n",
      "\n",
      "Fold: 5  Epoch: 418  Training loss = 1.7803  Validation loss = 4.0968  \n",
      "\n",
      "Fold: 5  Epoch: 419  Training loss = 1.7801  Validation loss = 4.0947  \n",
      "\n",
      "Fold: 5  Epoch: 420  Training loss = 1.7801  Validation loss = 4.0964  \n",
      "\n",
      "Fold: 5  Epoch: 421  Training loss = 1.7800  Validation loss = 4.0963  \n",
      "\n",
      "Fold: 5  Epoch: 422  Training loss = 1.7798  Validation loss = 4.0944  \n",
      "\n",
      "Fold: 5  Epoch: 423  Training loss = 1.7796  Validation loss = 4.0925  \n",
      "\n",
      "Fold: 5  Epoch: 424  Training loss = 1.7794  Validation loss = 4.0915  \n",
      "\n",
      "Fold: 5  Epoch: 425  Training loss = 1.7792  Validation loss = 4.0906  \n",
      "\n",
      "Fold: 5  Epoch: 426  Training loss = 1.7791  Validation loss = 4.0895  \n",
      "\n",
      "Fold: 5  Epoch: 427  Training loss = 1.7790  Validation loss = 4.0885  \n",
      "\n",
      "Fold: 5  Epoch: 428  Training loss = 1.7788  Validation loss = 4.0881  \n",
      "\n",
      "Fold: 5  Epoch: 429  Training loss = 1.7787  Validation loss = 4.0867  \n",
      "\n",
      "Fold: 5  Epoch: 430  Training loss = 1.7786  Validation loss = 4.0875  \n",
      "\n",
      "Fold: 5  Epoch: 431  Training loss = 1.7785  Validation loss = 4.0871  \n",
      "\n",
      "Fold: 5  Epoch: 432  Training loss = 1.7784  Validation loss = 4.0867  \n",
      "\n",
      "Fold: 5  Epoch: 433  Training loss = 1.7783  Validation loss = 4.0859  \n",
      "\n",
      "Fold: 5  Epoch: 434  Training loss = 1.7782  Validation loss = 4.0848  \n",
      "\n",
      "Fold: 5  Epoch: 435  Training loss = 1.7781  Validation loss = 4.0854  \n",
      "\n",
      "Fold: 5  Epoch: 436  Training loss = 1.7779  Validation loss = 4.0836  \n",
      "\n",
      "Fold: 5  Epoch: 437  Training loss = 1.7778  Validation loss = 4.0830  \n",
      "\n",
      "Fold: 5  Epoch: 438  Training loss = 1.7776  Validation loss = 4.0824  \n",
      "\n",
      "Fold: 5  Epoch: 439  Training loss = 1.7776  Validation loss = 4.0831  \n",
      "\n",
      "Fold: 5  Epoch: 440  Training loss = 1.7774  Validation loss = 4.0816  \n",
      "\n",
      "Fold: 5  Epoch: 441  Training loss = 1.7772  Validation loss = 4.0809  \n",
      "\n",
      "Fold: 5  Epoch: 442  Training loss = 1.7769  Validation loss = 4.0788  \n",
      "\n",
      "Fold: 5  Epoch: 443  Training loss = 1.7768  Validation loss = 4.0775  \n",
      "\n",
      "Fold: 5  Epoch: 444  Training loss = 1.7766  Validation loss = 4.0766  \n",
      "\n",
      "Fold: 5  Epoch: 445  Training loss = 1.7765  Validation loss = 4.0767  \n",
      "\n",
      "Fold: 5  Epoch: 446  Training loss = 1.7763  Validation loss = 4.0753  \n",
      "\n",
      "Fold: 5  Epoch: 447  Training loss = 1.7761  Validation loss = 4.0733  \n",
      "\n",
      "Fold: 5  Epoch: 448  Training loss = 1.7760  Validation loss = 4.0726  \n",
      "\n",
      "Fold: 5  Epoch: 449  Training loss = 1.7759  Validation loss = 4.0726  \n",
      "\n",
      "Fold: 5  Epoch: 450  Training loss = 1.7758  Validation loss = 4.0723  \n",
      "\n",
      "Fold: 5  Epoch: 451  Training loss = 1.7756  Validation loss = 4.0713  \n",
      "\n",
      "Fold: 5  Epoch: 452  Training loss = 1.7755  Validation loss = 4.0719  \n",
      "\n",
      "Fold: 5  Epoch: 453  Training loss = 1.7754  Validation loss = 4.0716  \n",
      "\n",
      "Fold: 5  Epoch: 454  Training loss = 1.7753  Validation loss = 4.0714  \n",
      "\n",
      "Fold: 5  Epoch: 455  Training loss = 1.7751  Validation loss = 4.0696  \n",
      "\n",
      "Fold: 5  Epoch: 456  Training loss = 1.7750  Validation loss = 4.0700  \n",
      "\n",
      "Fold: 5  Epoch: 457  Training loss = 1.7749  Validation loss = 4.0698  \n",
      "\n",
      "Fold: 5  Epoch: 458  Training loss = 1.7748  Validation loss = 4.0681  \n",
      "\n",
      "Fold: 5  Epoch: 459  Training loss = 1.7747  Validation loss = 4.0680  \n",
      "\n",
      "Fold: 5  Epoch: 460  Training loss = 1.7746  Validation loss = 4.0672  \n",
      "\n",
      "Fold: 5  Epoch: 461  Training loss = 1.7745  Validation loss = 4.0677  \n",
      "\n",
      "Fold: 5  Epoch: 462  Training loss = 1.7744  Validation loss = 4.0684  \n",
      "\n",
      "Fold: 5  Epoch: 463  Training loss = 1.7743  Validation loss = 4.0672  \n",
      "\n",
      "Fold: 5  Epoch: 464  Training loss = 1.7742  Validation loss = 4.0685  \n",
      "\n",
      "Fold: 5  Epoch: 465  Training loss = 1.7740  Validation loss = 4.0668  \n",
      "\n",
      "Fold: 5  Epoch: 466  Training loss = 1.7739  Validation loss = 4.0670  \n",
      "\n",
      "Fold: 5  Epoch: 467  Training loss = 1.7738  Validation loss = 4.0663  \n",
      "\n",
      "Fold: 5  Epoch: 468  Training loss = 1.7738  Validation loss = 4.0679  \n",
      "\n",
      "Fold: 5  Epoch: 469  Training loss = 1.7736  Validation loss = 4.0667  \n",
      "\n",
      "Fold: 5  Epoch: 470  Training loss = 1.7734  Validation loss = 4.0655  \n",
      "\n",
      "Fold: 5  Epoch: 471  Training loss = 1.7732  Validation loss = 4.0642  \n",
      "\n",
      "Fold: 5  Epoch: 472  Training loss = 1.7731  Validation loss = 4.0639  \n",
      "\n",
      "Fold: 5  Epoch: 473  Training loss = 1.7730  Validation loss = 4.0639  \n",
      "\n",
      "Fold: 5  Epoch: 474  Training loss = 1.7728  Validation loss = 4.0626  \n",
      "\n",
      "Fold: 5  Epoch: 475  Training loss = 1.7726  Validation loss = 4.0614  \n",
      "\n",
      "Fold: 5  Epoch: 476  Training loss = 1.7725  Validation loss = 4.0605  \n",
      "\n",
      "Fold: 5  Epoch: 477  Training loss = 1.7723  Validation loss = 4.0605  \n",
      "\n",
      "Fold: 5  Epoch: 478  Training loss = 1.7721  Validation loss = 4.0601  \n",
      "\n",
      "Fold: 5  Epoch: 479  Training loss = 1.7721  Validation loss = 4.0609  \n",
      "\n",
      "Fold: 5  Epoch: 480  Training loss = 1.7721  Validation loss = 4.0615  \n",
      "\n",
      "Fold: 5  Epoch: 481  Training loss = 1.7718  Validation loss = 4.0593  \n",
      "\n",
      "Fold: 5  Epoch: 482  Training loss = 1.7717  Validation loss = 4.0583  \n",
      "\n",
      "Fold: 5  Epoch: 483  Training loss = 1.7715  Validation loss = 4.0567  \n",
      "\n",
      "Fold: 5  Epoch: 484  Training loss = 1.7713  Validation loss = 4.0549  \n",
      "\n",
      "Fold: 5  Epoch: 485  Training loss = 1.7712  Validation loss = 4.0549  \n",
      "\n",
      "Fold: 5  Epoch: 486  Training loss = 1.7712  Validation loss = 4.0546  \n",
      "\n",
      "Fold: 5  Epoch: 487  Training loss = 1.7711  Validation loss = 4.0544  \n",
      "\n",
      "Fold: 5  Epoch: 488  Training loss = 1.7710  Validation loss = 4.0540  \n",
      "\n",
      "Fold: 5  Epoch: 489  Training loss = 1.7709  Validation loss = 4.0545  \n",
      "\n",
      "Fold: 5  Epoch: 490  Training loss = 1.7708  Validation loss = 4.0539  \n",
      "\n",
      "Fold: 5  Epoch: 491  Training loss = 1.7707  Validation loss = 4.0535  \n",
      "\n",
      "Fold: 5  Epoch: 492  Training loss = 1.7705  Validation loss = 4.0524  \n",
      "\n",
      "Fold: 5  Epoch: 493  Training loss = 1.7705  Validation loss = 4.0524  \n",
      "\n",
      "Fold: 5  Epoch: 494  Training loss = 1.7704  Validation loss = 4.0536  \n",
      "\n",
      "Fold: 5  Epoch: 495  Training loss = 1.7703  Validation loss = 4.0534  \n",
      "\n",
      "Fold: 5  Epoch: 496  Training loss = 1.7703  Validation loss = 4.0539  \n",
      "\n",
      "Fold: 5  Epoch: 497  Training loss = 1.7701  Validation loss = 4.0539  \n",
      "\n",
      "Fold: 5  Epoch: 498  Training loss = 1.7700  Validation loss = 4.0534  \n",
      "\n",
      "Fold: 5  Epoch: 499  Training loss = 1.7699  Validation loss = 4.0533  \n",
      "\n",
      "Fold: 5  Epoch: 500  Training loss = 1.7698  Validation loss = 4.0536  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 492  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 2.0007  Validation loss = 1.6974  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 2.0002  Validation loss = 1.6955  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 2.0000  Validation loss = 1.6950  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 1.9994  Validation loss = 1.6925  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 1.9983  Validation loss = 1.6877  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 1.9975  Validation loss = 1.6848  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 1.9970  Validation loss = 1.6829  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 1.9965  Validation loss = 1.6809  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 1.9960  Validation loss = 1.6790  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 1.9955  Validation loss = 1.6771  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 1.9952  Validation loss = 1.6761  \n",
      "\n",
      "Fold: 6  Epoch: 12  Training loss = 1.9948  Validation loss = 1.6748  \n",
      "\n",
      "Fold: 6  Epoch: 13  Training loss = 1.9942  Validation loss = 1.6726  \n",
      "\n",
      "Fold: 6  Epoch: 14  Training loss = 1.9936  Validation loss = 1.6701  \n",
      "\n",
      "Fold: 6  Epoch: 15  Training loss = 1.9931  Validation loss = 1.6682  \n",
      "\n",
      "Fold: 6  Epoch: 16  Training loss = 1.9928  Validation loss = 1.6672  \n",
      "\n",
      "Fold: 6  Epoch: 17  Training loss = 1.9919  Validation loss = 1.6634  \n",
      "\n",
      "Fold: 6  Epoch: 18  Training loss = 1.9916  Validation loss = 1.6623  \n",
      "\n",
      "Fold: 6  Epoch: 19  Training loss = 1.9909  Validation loss = 1.6594  \n",
      "\n",
      "Fold: 6  Epoch: 20  Training loss = 1.9902  Validation loss = 1.6566  \n",
      "\n",
      "Fold: 6  Epoch: 21  Training loss = 1.9894  Validation loss = 1.6535  \n",
      "\n",
      "Fold: 6  Epoch: 22  Training loss = 1.9887  Validation loss = 1.6503  \n",
      "\n",
      "Fold: 6  Epoch: 23  Training loss = 1.9880  Validation loss = 1.6473  \n",
      "\n",
      "Fold: 6  Epoch: 24  Training loss = 1.9873  Validation loss = 1.6447  \n",
      "\n",
      "Fold: 6  Epoch: 25  Training loss = 1.9864  Validation loss = 1.6407  \n",
      "\n",
      "Fold: 6  Epoch: 26  Training loss = 1.9858  Validation loss = 1.6385  \n",
      "\n",
      "Fold: 6  Epoch: 27  Training loss = 1.9851  Validation loss = 1.6356  \n",
      "\n",
      "Fold: 6  Epoch: 28  Training loss = 1.9849  Validation loss = 1.6349  \n",
      "\n",
      "Fold: 6  Epoch: 29  Training loss = 1.9845  Validation loss = 1.6334  \n",
      "\n",
      "Fold: 6  Epoch: 30  Training loss = 1.9840  Validation loss = 1.6313  \n",
      "\n",
      "Fold: 6  Epoch: 31  Training loss = 1.9834  Validation loss = 1.6286  \n",
      "\n",
      "Fold: 6  Epoch: 32  Training loss = 1.9830  Validation loss = 1.6274  \n",
      "\n",
      "Fold: 6  Epoch: 33  Training loss = 1.9830  Validation loss = 1.6277  \n",
      "\n",
      "Fold: 6  Epoch: 34  Training loss = 1.9825  Validation loss = 1.6257  \n",
      "\n",
      "Fold: 6  Epoch: 35  Training loss = 1.9821  Validation loss = 1.6240  \n",
      "\n",
      "Fold: 6  Epoch: 36  Training loss = 1.9813  Validation loss = 1.6211  \n",
      "\n",
      "Fold: 6  Epoch: 37  Training loss = 1.9806  Validation loss = 1.6181  \n",
      "\n",
      "Fold: 6  Epoch: 38  Training loss = 1.9798  Validation loss = 1.6149  \n",
      "\n",
      "Fold: 6  Epoch: 39  Training loss = 1.9794  Validation loss = 1.6131  \n",
      "\n",
      "Fold: 6  Epoch: 40  Training loss = 1.9791  Validation loss = 1.6123  \n",
      "\n",
      "Fold: 6  Epoch: 41  Training loss = 1.9787  Validation loss = 1.6109  \n",
      "\n",
      "Fold: 6  Epoch: 42  Training loss = 1.9780  Validation loss = 1.6078  \n",
      "\n",
      "Fold: 6  Epoch: 43  Training loss = 1.9773  Validation loss = 1.6048  \n",
      "\n",
      "Fold: 6  Epoch: 44  Training loss = 1.9764  Validation loss = 1.6007  \n",
      "\n",
      "Fold: 6  Epoch: 45  Training loss = 1.9761  Validation loss = 1.5994  \n",
      "\n",
      "Fold: 6  Epoch: 46  Training loss = 1.9759  Validation loss = 1.5986  \n",
      "\n",
      "Fold: 6  Epoch: 47  Training loss = 1.9755  Validation loss = 1.5975  \n",
      "\n",
      "Fold: 6  Epoch: 48  Training loss = 1.9751  Validation loss = 1.5958  \n",
      "\n",
      "Fold: 6  Epoch: 49  Training loss = 1.9746  Validation loss = 1.5934  \n",
      "\n",
      "Fold: 6  Epoch: 50  Training loss = 1.9742  Validation loss = 1.5918  \n",
      "\n",
      "Fold: 6  Epoch: 51  Training loss = 1.9735  Validation loss = 1.5890  \n",
      "\n",
      "Fold: 6  Epoch: 52  Training loss = 1.9728  Validation loss = 1.5858  \n",
      "\n",
      "Fold: 6  Epoch: 53  Training loss = 1.9722  Validation loss = 1.5832  \n",
      "\n",
      "Fold: 6  Epoch: 54  Training loss = 1.9717  Validation loss = 1.5810  \n",
      "\n",
      "Fold: 6  Epoch: 55  Training loss = 1.9714  Validation loss = 1.5803  \n",
      "\n",
      "Fold: 6  Epoch: 56  Training loss = 1.9710  Validation loss = 1.5786  \n",
      "\n",
      "Fold: 6  Epoch: 57  Training loss = 1.9702  Validation loss = 1.5749  \n",
      "\n",
      "Fold: 6  Epoch: 58  Training loss = 1.9697  Validation loss = 1.5729  \n",
      "\n",
      "Fold: 6  Epoch: 59  Training loss = 1.9692  Validation loss = 1.5706  \n",
      "\n",
      "Fold: 6  Epoch: 60  Training loss = 1.9686  Validation loss = 1.5681  \n",
      "\n",
      "Fold: 6  Epoch: 61  Training loss = 1.9681  Validation loss = 1.5661  \n",
      "\n",
      "Fold: 6  Epoch: 62  Training loss = 1.9672  Validation loss = 1.5618  \n",
      "\n",
      "Fold: 6  Epoch: 63  Training loss = 1.9666  Validation loss = 1.5594  \n",
      "\n",
      "Fold: 6  Epoch: 64  Training loss = 1.9662  Validation loss = 1.5575  \n",
      "\n",
      "Fold: 6  Epoch: 65  Training loss = 1.9659  Validation loss = 1.5565  \n",
      "\n",
      "Fold: 6  Epoch: 66  Training loss = 1.9653  Validation loss = 1.5535  \n",
      "\n",
      "Fold: 6  Epoch: 67  Training loss = 1.9644  Validation loss = 1.5496  \n",
      "\n",
      "Fold: 6  Epoch: 68  Training loss = 1.9641  Validation loss = 1.5483  \n",
      "\n",
      "Fold: 6  Epoch: 69  Training loss = 1.9638  Validation loss = 1.5471  \n",
      "\n",
      "Fold: 6  Epoch: 70  Training loss = 1.9632  Validation loss = 1.5445  \n",
      "\n",
      "Fold: 6  Epoch: 71  Training loss = 1.9630  Validation loss = 1.5436  \n",
      "\n",
      "Fold: 6  Epoch: 72  Training loss = 1.9629  Validation loss = 1.5437  \n",
      "\n",
      "Fold: 6  Epoch: 73  Training loss = 1.9624  Validation loss = 1.5412  \n",
      "\n",
      "Fold: 6  Epoch: 74  Training loss = 1.9617  Validation loss = 1.5381  \n",
      "\n",
      "Fold: 6  Epoch: 75  Training loss = 1.9613  Validation loss = 1.5365  \n",
      "\n",
      "Fold: 6  Epoch: 76  Training loss = 1.9610  Validation loss = 1.5351  \n",
      "\n",
      "Fold: 6  Epoch: 77  Training loss = 1.9604  Validation loss = 1.5326  \n",
      "\n",
      "Fold: 6  Epoch: 78  Training loss = 1.9599  Validation loss = 1.5300  \n",
      "\n",
      "Fold: 6  Epoch: 79  Training loss = 1.9594  Validation loss = 1.5278  \n",
      "\n",
      "Fold: 6  Epoch: 80  Training loss = 1.9589  Validation loss = 1.5254  \n",
      "\n",
      "Fold: 6  Epoch: 81  Training loss = 1.9584  Validation loss = 1.5232  \n",
      "\n",
      "Fold: 6  Epoch: 82  Training loss = 1.9582  Validation loss = 1.5225  \n",
      "\n",
      "Fold: 6  Epoch: 83  Training loss = 1.9579  Validation loss = 1.5220  \n",
      "\n",
      "Fold: 6  Epoch: 84  Training loss = 1.9572  Validation loss = 1.5186  \n",
      "\n",
      "Fold: 6  Epoch: 85  Training loss = 1.9569  Validation loss = 1.5176  \n",
      "\n",
      "Fold: 6  Epoch: 86  Training loss = 1.9565  Validation loss = 1.5159  \n",
      "\n",
      "Fold: 6  Epoch: 87  Training loss = 1.9562  Validation loss = 1.5147  \n",
      "\n",
      "Fold: 6  Epoch: 88  Training loss = 1.9555  Validation loss = 1.5118  \n",
      "\n",
      "Fold: 6  Epoch: 89  Training loss = 1.9552  Validation loss = 1.5105  \n",
      "\n",
      "Fold: 6  Epoch: 90  Training loss = 1.9548  Validation loss = 1.5087  \n",
      "\n",
      "Fold: 6  Epoch: 91  Training loss = 1.9543  Validation loss = 1.5062  \n",
      "\n",
      "Fold: 6  Epoch: 92  Training loss = 1.9538  Validation loss = 1.5038  \n",
      "\n",
      "Fold: 6  Epoch: 93  Training loss = 1.9536  Validation loss = 1.5032  \n",
      "\n",
      "Fold: 6  Epoch: 94  Training loss = 1.9532  Validation loss = 1.5015  \n",
      "\n",
      "Fold: 6  Epoch: 95  Training loss = 1.9525  Validation loss = 1.4983  \n",
      "\n",
      "Fold: 6  Epoch: 96  Training loss = 1.9523  Validation loss = 1.4978  \n",
      "\n",
      "Fold: 6  Epoch: 97  Training loss = 1.9520  Validation loss = 1.4967  \n",
      "\n",
      "Fold: 6  Epoch: 98  Training loss = 1.9514  Validation loss = 1.4934  \n",
      "\n",
      "Fold: 6  Epoch: 99  Training loss = 1.9512  Validation loss = 1.4926  \n",
      "\n",
      "Fold: 6  Epoch: 100  Training loss = 1.9504  Validation loss = 1.4887  \n",
      "\n",
      "Fold: 6  Epoch: 101  Training loss = 1.9497  Validation loss = 1.4851  \n",
      "\n",
      "Fold: 6  Epoch: 102  Training loss = 1.9495  Validation loss = 1.4840  \n",
      "\n",
      "Fold: 6  Epoch: 103  Training loss = 1.9491  Validation loss = 1.4823  \n",
      "\n",
      "Fold: 6  Epoch: 104  Training loss = 1.9487  Validation loss = 1.4806  \n",
      "\n",
      "Fold: 6  Epoch: 105  Training loss = 1.9484  Validation loss = 1.4793  \n",
      "\n",
      "Fold: 6  Epoch: 106  Training loss = 1.9480  Validation loss = 1.4776  \n",
      "\n",
      "Fold: 6  Epoch: 107  Training loss = 1.9474  Validation loss = 1.4745  \n",
      "\n",
      "Fold: 6  Epoch: 108  Training loss = 1.9469  Validation loss = 1.4726  \n",
      "\n",
      "Fold: 6  Epoch: 109  Training loss = 1.9464  Validation loss = 1.4697  \n",
      "\n",
      "Fold: 6  Epoch: 110  Training loss = 1.9459  Validation loss = 1.4676  \n",
      "\n",
      "Fold: 6  Epoch: 111  Training loss = 1.9452  Validation loss = 1.4636  \n",
      "\n",
      "Fold: 6  Epoch: 112  Training loss = 1.9449  Validation loss = 1.4624  \n",
      "\n",
      "Fold: 6  Epoch: 113  Training loss = 1.9445  Validation loss = 1.4611  \n",
      "\n",
      "Fold: 6  Epoch: 114  Training loss = 1.9441  Validation loss = 1.4590  \n",
      "\n",
      "Fold: 6  Epoch: 115  Training loss = 1.9436  Validation loss = 1.4567  \n",
      "\n",
      "Fold: 6  Epoch: 116  Training loss = 1.9430  Validation loss = 1.4538  \n",
      "\n",
      "Fold: 6  Epoch: 117  Training loss = 1.9427  Validation loss = 1.4523  \n",
      "\n",
      "Fold: 6  Epoch: 118  Training loss = 1.9423  Validation loss = 1.4506  \n",
      "\n",
      "Fold: 6  Epoch: 119  Training loss = 1.9419  Validation loss = 1.4487  \n",
      "\n",
      "Fold: 6  Epoch: 120  Training loss = 1.9415  Validation loss = 1.4469  \n",
      "\n",
      "Fold: 6  Epoch: 121  Training loss = 1.9413  Validation loss = 1.4459  \n",
      "\n",
      "Fold: 6  Epoch: 122  Training loss = 1.9406  Validation loss = 1.4424  \n",
      "\n",
      "Fold: 6  Epoch: 123  Training loss = 1.9406  Validation loss = 1.4431  \n",
      "\n",
      "Fold: 6  Epoch: 124  Training loss = 1.9403  Validation loss = 1.4417  \n",
      "\n",
      "Fold: 6  Epoch: 125  Training loss = 1.9401  Validation loss = 1.4408  \n",
      "\n",
      "Fold: 6  Epoch: 126  Training loss = 1.9396  Validation loss = 1.4383  \n",
      "\n",
      "Fold: 6  Epoch: 127  Training loss = 1.9390  Validation loss = 1.4351  \n",
      "\n",
      "Fold: 6  Epoch: 128  Training loss = 1.9384  Validation loss = 1.4323  \n",
      "\n",
      "Fold: 6  Epoch: 129  Training loss = 1.9383  Validation loss = 1.4318  \n",
      "\n",
      "Fold: 6  Epoch: 130  Training loss = 1.9379  Validation loss = 1.4299  \n",
      "\n",
      "Fold: 6  Epoch: 131  Training loss = 1.9373  Validation loss = 1.4267  \n",
      "\n",
      "Fold: 6  Epoch: 132  Training loss = 1.9369  Validation loss = 1.4251  \n",
      "\n",
      "Fold: 6  Epoch: 133  Training loss = 1.9366  Validation loss = 1.4237  \n",
      "\n",
      "Fold: 6  Epoch: 134  Training loss = 1.9360  Validation loss = 1.4207  \n",
      "\n",
      "Fold: 6  Epoch: 135  Training loss = 1.9358  Validation loss = 1.4197  \n",
      "\n",
      "Fold: 6  Epoch: 136  Training loss = 1.9356  Validation loss = 1.4192  \n",
      "\n",
      "Fold: 6  Epoch: 137  Training loss = 1.9352  Validation loss = 1.4170  \n",
      "\n",
      "Fold: 6  Epoch: 138  Training loss = 1.9348  Validation loss = 1.4156  \n",
      "\n",
      "Fold: 6  Epoch: 139  Training loss = 1.9344  Validation loss = 1.4133  \n",
      "\n",
      "Fold: 6  Epoch: 140  Training loss = 1.9340  Validation loss = 1.4115  \n",
      "\n",
      "Fold: 6  Epoch: 141  Training loss = 1.9336  Validation loss = 1.4097  \n",
      "\n",
      "Fold: 6  Epoch: 142  Training loss = 1.9334  Validation loss = 1.4089  \n",
      "\n",
      "Fold: 6  Epoch: 143  Training loss = 1.9331  Validation loss = 1.4078  \n",
      "\n",
      "Fold: 6  Epoch: 144  Training loss = 1.9327  Validation loss = 1.4058  \n",
      "\n",
      "Fold: 6  Epoch: 145  Training loss = 1.9324  Validation loss = 1.4042  \n",
      "\n",
      "Fold: 6  Epoch: 146  Training loss = 1.9322  Validation loss = 1.4042  \n",
      "\n",
      "Fold: 6  Epoch: 147  Training loss = 1.9319  Validation loss = 1.4028  \n",
      "\n",
      "Fold: 6  Epoch: 148  Training loss = 1.9317  Validation loss = 1.4026  \n",
      "\n",
      "Fold: 6  Epoch: 149  Training loss = 1.9312  Validation loss = 1.3998  \n",
      "\n",
      "Fold: 6  Epoch: 150  Training loss = 1.9310  Validation loss = 1.3988  \n",
      "\n",
      "Fold: 6  Epoch: 151  Training loss = 1.9306  Validation loss = 1.3967  \n",
      "\n",
      "Fold: 6  Epoch: 152  Training loss = 1.9303  Validation loss = 1.3950  \n",
      "\n",
      "Fold: 6  Epoch: 153  Training loss = 1.9299  Validation loss = 1.3935  \n",
      "\n",
      "Fold: 6  Epoch: 154  Training loss = 1.9298  Validation loss = 1.3933  \n",
      "\n",
      "Fold: 6  Epoch: 155  Training loss = 1.9297  Validation loss = 1.3936  \n",
      "\n",
      "Fold: 6  Epoch: 156  Training loss = 1.9296  Validation loss = 1.3934  \n",
      "\n",
      "Fold: 6  Epoch: 157  Training loss = 1.9293  Validation loss = 1.3922  \n",
      "\n",
      "Fold: 6  Epoch: 158  Training loss = 1.9288  Validation loss = 1.3894  \n",
      "\n",
      "Fold: 6  Epoch: 159  Training loss = 1.9281  Validation loss = 1.3848  \n",
      "\n",
      "Fold: 6  Epoch: 160  Training loss = 1.9277  Validation loss = 1.3829  \n",
      "\n",
      "Fold: 6  Epoch: 161  Training loss = 1.9275  Validation loss = 1.3814  \n",
      "\n",
      "Fold: 6  Epoch: 162  Training loss = 1.9271  Validation loss = 1.3798  \n",
      "\n",
      "Fold: 6  Epoch: 163  Training loss = 1.9267  Validation loss = 1.3777  \n",
      "\n",
      "Fold: 6  Epoch: 164  Training loss = 1.9264  Validation loss = 1.3763  \n",
      "\n",
      "Fold: 6  Epoch: 165  Training loss = 1.9264  Validation loss = 1.3767  \n",
      "\n",
      "Fold: 6  Epoch: 166  Training loss = 1.9260  Validation loss = 1.3751  \n",
      "\n",
      "Fold: 6  Epoch: 167  Training loss = 1.9257  Validation loss = 1.3736  \n",
      "\n",
      "Fold: 6  Epoch: 168  Training loss = 1.9253  Validation loss = 1.3712  \n",
      "\n",
      "Fold: 6  Epoch: 169  Training loss = 1.9249  Validation loss = 1.3692  \n",
      "\n",
      "Fold: 6  Epoch: 170  Training loss = 1.9246  Validation loss = 1.3681  \n",
      "\n",
      "Fold: 6  Epoch: 171  Training loss = 1.9245  Validation loss = 1.3679  \n",
      "\n",
      "Fold: 6  Epoch: 172  Training loss = 1.9244  Validation loss = 1.3678  \n",
      "\n",
      "Fold: 6  Epoch: 173  Training loss = 1.9238  Validation loss = 1.3643  \n",
      "\n",
      "Fold: 6  Epoch: 174  Training loss = 1.9235  Validation loss = 1.3632  \n",
      "\n",
      "Fold: 6  Epoch: 175  Training loss = 1.9234  Validation loss = 1.3631  \n",
      "\n",
      "Fold: 6  Epoch: 176  Training loss = 1.9231  Validation loss = 1.3617  \n",
      "\n",
      "Fold: 6  Epoch: 177  Training loss = 1.9224  Validation loss = 1.3579  \n",
      "\n",
      "Fold: 6  Epoch: 178  Training loss = 1.9223  Validation loss = 1.3580  \n",
      "\n",
      "Fold: 6  Epoch: 179  Training loss = 1.9220  Validation loss = 1.3567  \n",
      "\n",
      "Fold: 6  Epoch: 180  Training loss = 1.9217  Validation loss = 1.3549  \n",
      "\n",
      "Fold: 6  Epoch: 181  Training loss = 1.9212  Validation loss = 1.3520  \n",
      "\n",
      "Fold: 6  Epoch: 182  Training loss = 1.9210  Validation loss = 1.3516  \n",
      "\n",
      "Fold: 6  Epoch: 183  Training loss = 1.9208  Validation loss = 1.3508  \n",
      "\n",
      "Fold: 6  Epoch: 184  Training loss = 1.9205  Validation loss = 1.3493  \n",
      "\n",
      "Fold: 6  Epoch: 185  Training loss = 1.9202  Validation loss = 1.3485  \n",
      "\n",
      "Fold: 6  Epoch: 186  Training loss = 1.9198  Validation loss = 1.3460  \n",
      "\n",
      "Fold: 6  Epoch: 187  Training loss = 1.9195  Validation loss = 1.3448  \n",
      "\n",
      "Fold: 6  Epoch: 188  Training loss = 1.9194  Validation loss = 1.3451  \n",
      "\n",
      "Fold: 6  Epoch: 189  Training loss = 1.9191  Validation loss = 1.3442  \n",
      "\n",
      "Fold: 6  Epoch: 190  Training loss = 1.9189  Validation loss = 1.3429  \n",
      "\n",
      "Fold: 6  Epoch: 191  Training loss = 1.9189  Validation loss = 1.3438  \n",
      "\n",
      "Fold: 6  Epoch: 192  Training loss = 1.9185  Validation loss = 1.3416  \n",
      "\n",
      "Fold: 6  Epoch: 193  Training loss = 1.9182  Validation loss = 1.3404  \n",
      "\n",
      "Fold: 6  Epoch: 194  Training loss = 1.9179  Validation loss = 1.3391  \n",
      "\n",
      "Fold: 6  Epoch: 195  Training loss = 1.9176  Validation loss = 1.3383  \n",
      "\n",
      "Fold: 6  Epoch: 196  Training loss = 1.9172  Validation loss = 1.3355  \n",
      "\n",
      "Fold: 6  Epoch: 197  Training loss = 1.9171  Validation loss = 1.3356  \n",
      "\n",
      "Fold: 6  Epoch: 198  Training loss = 1.9166  Validation loss = 1.3333  \n",
      "\n",
      "Fold: 6  Epoch: 199  Training loss = 1.9165  Validation loss = 1.3331  \n",
      "\n",
      "Fold: 6  Epoch: 200  Training loss = 1.9162  Validation loss = 1.3315  \n",
      "\n",
      "Fold: 6  Epoch: 201  Training loss = 1.9157  Validation loss = 1.3286  \n",
      "\n",
      "Fold: 6  Epoch: 202  Training loss = 1.9155  Validation loss = 1.3277  \n",
      "\n",
      "Fold: 6  Epoch: 203  Training loss = 1.9150  Validation loss = 1.3249  \n",
      "\n",
      "Fold: 6  Epoch: 204  Training loss = 1.9147  Validation loss = 1.3237  \n",
      "\n",
      "Fold: 6  Epoch: 205  Training loss = 1.9144  Validation loss = 1.3216  \n",
      "\n",
      "Fold: 6  Epoch: 206  Training loss = 1.9140  Validation loss = 1.3200  \n",
      "\n",
      "Fold: 6  Epoch: 207  Training loss = 1.9137  Validation loss = 1.3187  \n",
      "\n",
      "Fold: 6  Epoch: 208  Training loss = 1.9134  Validation loss = 1.3178  \n",
      "\n",
      "Fold: 6  Epoch: 209  Training loss = 1.9132  Validation loss = 1.3169  \n",
      "\n",
      "Fold: 6  Epoch: 210  Training loss = 1.9127  Validation loss = 1.3140  \n",
      "\n",
      "Fold: 6  Epoch: 211  Training loss = 1.9124  Validation loss = 1.3126  \n",
      "\n",
      "Fold: 6  Epoch: 212  Training loss = 1.9121  Validation loss = 1.3114  \n",
      "\n",
      "Fold: 6  Epoch: 213  Training loss = 1.9119  Validation loss = 1.3104  \n",
      "\n",
      "Fold: 6  Epoch: 214  Training loss = 1.9116  Validation loss = 1.3099  \n",
      "\n",
      "Fold: 6  Epoch: 215  Training loss = 1.9115  Validation loss = 1.3102  \n",
      "\n",
      "Fold: 6  Epoch: 216  Training loss = 1.9112  Validation loss = 1.3089  \n",
      "\n",
      "Fold: 6  Epoch: 217  Training loss = 1.9108  Validation loss = 1.3068  \n",
      "\n",
      "Fold: 6  Epoch: 218  Training loss = 1.9105  Validation loss = 1.3053  \n",
      "\n",
      "Fold: 6  Epoch: 219  Training loss = 1.9104  Validation loss = 1.3051  \n",
      "\n",
      "Fold: 6  Epoch: 220  Training loss = 1.9100  Validation loss = 1.3036  \n",
      "\n",
      "Fold: 6  Epoch: 221  Training loss = 1.9099  Validation loss = 1.3031  \n",
      "\n",
      "Fold: 6  Epoch: 222  Training loss = 1.9096  Validation loss = 1.3021  \n",
      "\n",
      "Fold: 6  Epoch: 223  Training loss = 1.9094  Validation loss = 1.3011  \n",
      "\n",
      "Fold: 6  Epoch: 224  Training loss = 1.9091  Validation loss = 1.3001  \n",
      "\n",
      "Fold: 6  Epoch: 225  Training loss = 1.9089  Validation loss = 1.2992  \n",
      "\n",
      "Fold: 6  Epoch: 226  Training loss = 1.9085  Validation loss = 1.2969  \n",
      "\n",
      "Fold: 6  Epoch: 227  Training loss = 1.9083  Validation loss = 1.2955  \n",
      "\n",
      "Fold: 6  Epoch: 228  Training loss = 1.9077  Validation loss = 1.2922  \n",
      "\n",
      "Fold: 6  Epoch: 229  Training loss = 1.9074  Validation loss = 1.2910  \n",
      "\n",
      "Fold: 6  Epoch: 230  Training loss = 1.9071  Validation loss = 1.2895  \n",
      "\n",
      "Fold: 6  Epoch: 231  Training loss = 1.9068  Validation loss = 1.2880  \n",
      "\n",
      "Fold: 6  Epoch: 232  Training loss = 1.9064  Validation loss = 1.2857  \n",
      "\n",
      "Fold: 6  Epoch: 233  Training loss = 1.9062  Validation loss = 1.2855  \n",
      "\n",
      "Fold: 6  Epoch: 234  Training loss = 1.9059  Validation loss = 1.2846  \n",
      "\n",
      "Fold: 6  Epoch: 235  Training loss = 1.9057  Validation loss = 1.2835  \n",
      "\n",
      "Fold: 6  Epoch: 236  Training loss = 1.9054  Validation loss = 1.2826  \n",
      "\n",
      "Fold: 6  Epoch: 237  Training loss = 1.9050  Validation loss = 1.2804  \n",
      "\n",
      "Fold: 6  Epoch: 238  Training loss = 1.9050  Validation loss = 1.2805  \n",
      "\n",
      "Fold: 6  Epoch: 239  Training loss = 1.9047  Validation loss = 1.2793  \n",
      "\n",
      "Fold: 6  Epoch: 240  Training loss = 1.9044  Validation loss = 1.2780  \n",
      "\n",
      "Fold: 6  Epoch: 241  Training loss = 1.9040  Validation loss = 1.2759  \n",
      "\n",
      "Fold: 6  Epoch: 242  Training loss = 1.9039  Validation loss = 1.2769  \n",
      "\n",
      "Fold: 6  Epoch: 243  Training loss = 1.9035  Validation loss = 1.2739  \n",
      "\n",
      "Fold: 6  Epoch: 244  Training loss = 1.9033  Validation loss = 1.2734  \n",
      "\n",
      "Fold: 6  Epoch: 245  Training loss = 1.9030  Validation loss = 1.2716  \n",
      "\n",
      "Fold: 6  Epoch: 246  Training loss = 1.9027  Validation loss = 1.2705  \n",
      "\n",
      "Fold: 6  Epoch: 247  Training loss = 1.9024  Validation loss = 1.2694  \n",
      "\n",
      "Fold: 6  Epoch: 248  Training loss = 1.9021  Validation loss = 1.2682  \n",
      "\n",
      "Fold: 6  Epoch: 249  Training loss = 1.9017  Validation loss = 1.2657  \n",
      "\n",
      "Fold: 6  Epoch: 250  Training loss = 1.9015  Validation loss = 1.2648  \n",
      "\n",
      "Fold: 6  Epoch: 251  Training loss = 1.9013  Validation loss = 1.2637  \n",
      "\n",
      "Fold: 6  Epoch: 252  Training loss = 1.9010  Validation loss = 1.2622  \n",
      "\n",
      "Fold: 6  Epoch: 253  Training loss = 1.9006  Validation loss = 1.2607  \n",
      "\n",
      "Fold: 6  Epoch: 254  Training loss = 1.9002  Validation loss = 1.2578  \n",
      "\n",
      "Fold: 6  Epoch: 255  Training loss = 1.8999  Validation loss = 1.2559  \n",
      "\n",
      "Fold: 6  Epoch: 256  Training loss = 1.8996  Validation loss = 1.2543  \n",
      "\n",
      "Fold: 6  Epoch: 257  Training loss = 1.8994  Validation loss = 1.2539  \n",
      "\n",
      "Fold: 6  Epoch: 258  Training loss = 1.8994  Validation loss = 1.2545  \n",
      "\n",
      "Fold: 6  Epoch: 259  Training loss = 1.8992  Validation loss = 1.2539  \n",
      "\n",
      "Fold: 6  Epoch: 260  Training loss = 1.8990  Validation loss = 1.2539  \n",
      "\n",
      "Fold: 6  Epoch: 261  Training loss = 1.8988  Validation loss = 1.2525  \n",
      "\n",
      "Fold: 6  Epoch: 262  Training loss = 1.8985  Validation loss = 1.2524  \n",
      "\n",
      "Fold: 6  Epoch: 263  Training loss = 1.8984  Validation loss = 1.2521  \n",
      "\n",
      "Fold: 6  Epoch: 264  Training loss = 1.8982  Validation loss = 1.2505  \n",
      "\n",
      "Fold: 6  Epoch: 265  Training loss = 1.8979  Validation loss = 1.2497  \n",
      "\n",
      "Fold: 6  Epoch: 266  Training loss = 1.8976  Validation loss = 1.2476  \n",
      "\n",
      "Fold: 6  Epoch: 267  Training loss = 1.8973  Validation loss = 1.2463  \n",
      "\n",
      "Fold: 6  Epoch: 268  Training loss = 1.8971  Validation loss = 1.2460  \n",
      "\n",
      "Fold: 6  Epoch: 269  Training loss = 1.8969  Validation loss = 1.2456  \n",
      "\n",
      "Fold: 6  Epoch: 270  Training loss = 1.8967  Validation loss = 1.2449  \n",
      "\n",
      "Fold: 6  Epoch: 271  Training loss = 1.8965  Validation loss = 1.2442  \n",
      "\n",
      "Fold: 6  Epoch: 272  Training loss = 1.8962  Validation loss = 1.2420  \n",
      "\n",
      "Fold: 6  Epoch: 273  Training loss = 1.8959  Validation loss = 1.2408  \n",
      "\n",
      "Fold: 6  Epoch: 274  Training loss = 1.8957  Validation loss = 1.2408  \n",
      "\n",
      "Fold: 6  Epoch: 275  Training loss = 1.8953  Validation loss = 1.2390  \n",
      "\n",
      "Fold: 6  Epoch: 276  Training loss = 1.8952  Validation loss = 1.2396  \n",
      "\n",
      "Fold: 6  Epoch: 277  Training loss = 1.8948  Validation loss = 1.2376  \n",
      "\n",
      "Fold: 6  Epoch: 278  Training loss = 1.8945  Validation loss = 1.2353  \n",
      "\n",
      "Fold: 6  Epoch: 279  Training loss = 1.8942  Validation loss = 1.2343  \n",
      "\n",
      "Fold: 6  Epoch: 280  Training loss = 1.8941  Validation loss = 1.2338  \n",
      "\n",
      "Fold: 6  Epoch: 281  Training loss = 1.8939  Validation loss = 1.2330  \n",
      "\n",
      "Fold: 6  Epoch: 282  Training loss = 1.8936  Validation loss = 1.2316  \n",
      "\n",
      "Fold: 6  Epoch: 283  Training loss = 1.8934  Validation loss = 1.2308  \n",
      "\n",
      "Fold: 6  Epoch: 284  Training loss = 1.8931  Validation loss = 1.2290  \n",
      "\n",
      "Fold: 6  Epoch: 285  Training loss = 1.8927  Validation loss = 1.2270  \n",
      "\n",
      "Fold: 6  Epoch: 286  Training loss = 1.8923  Validation loss = 1.2246  \n",
      "\n",
      "Fold: 6  Epoch: 287  Training loss = 1.8920  Validation loss = 1.2233  \n",
      "\n",
      "Fold: 6  Epoch: 288  Training loss = 1.8918  Validation loss = 1.2230  \n",
      "\n",
      "Fold: 6  Epoch: 289  Training loss = 1.8915  Validation loss = 1.2214  \n",
      "\n",
      "Fold: 6  Epoch: 290  Training loss = 1.8913  Validation loss = 1.2199  \n",
      "\n",
      "Fold: 6  Epoch: 291  Training loss = 1.8909  Validation loss = 1.2172  \n",
      "\n",
      "Fold: 6  Epoch: 292  Training loss = 1.8904  Validation loss = 1.2133  \n",
      "\n",
      "Fold: 6  Epoch: 293  Training loss = 1.8901  Validation loss = 1.2125  \n",
      "\n",
      "Fold: 6  Epoch: 294  Training loss = 1.8899  Validation loss = 1.2111  \n",
      "\n",
      "Fold: 6  Epoch: 295  Training loss = 1.8897  Validation loss = 1.2110  \n",
      "\n",
      "Fold: 6  Epoch: 296  Training loss = 1.8896  Validation loss = 1.2122  \n",
      "\n",
      "Fold: 6  Epoch: 297  Training loss = 1.8894  Validation loss = 1.2115  \n",
      "\n",
      "Fold: 6  Epoch: 298  Training loss = 1.8892  Validation loss = 1.2109  \n",
      "\n",
      "Fold: 6  Epoch: 299  Training loss = 1.8890  Validation loss = 1.2100  \n",
      "\n",
      "Fold: 6  Epoch: 300  Training loss = 1.8887  Validation loss = 1.2087  \n",
      "\n",
      "Fold: 6  Epoch: 301  Training loss = 1.8884  Validation loss = 1.2077  \n",
      "\n",
      "Fold: 6  Epoch: 302  Training loss = 1.8882  Validation loss = 1.2062  \n",
      "\n",
      "Fold: 6  Epoch: 303  Training loss = 1.8879  Validation loss = 1.2052  \n",
      "\n",
      "Fold: 6  Epoch: 304  Training loss = 1.8877  Validation loss = 1.2048  \n",
      "\n",
      "Fold: 6  Epoch: 305  Training loss = 1.8874  Validation loss = 1.2024  \n",
      "\n",
      "Fold: 6  Epoch: 306  Training loss = 1.8872  Validation loss = 1.2017  \n",
      "\n",
      "Fold: 6  Epoch: 307  Training loss = 1.8871  Validation loss = 1.2024  \n",
      "\n",
      "Fold: 6  Epoch: 308  Training loss = 1.8869  Validation loss = 1.2015  \n",
      "\n",
      "Fold: 6  Epoch: 309  Training loss = 1.8866  Validation loss = 1.1997  \n",
      "\n",
      "Fold: 6  Epoch: 310  Training loss = 1.8863  Validation loss = 1.1987  \n",
      "\n",
      "Fold: 6  Epoch: 311  Training loss = 1.8861  Validation loss = 1.1982  \n",
      "\n",
      "Fold: 6  Epoch: 312  Training loss = 1.8858  Validation loss = 1.1974  \n",
      "\n",
      "Fold: 6  Epoch: 313  Training loss = 1.8856  Validation loss = 1.1969  \n",
      "\n",
      "Fold: 6  Epoch: 314  Training loss = 1.8853  Validation loss = 1.1955  \n",
      "\n",
      "Fold: 6  Epoch: 315  Training loss = 1.8852  Validation loss = 1.1958  \n",
      "\n",
      "Fold: 6  Epoch: 316  Training loss = 1.8851  Validation loss = 1.1950  \n",
      "\n",
      "Fold: 6  Epoch: 317  Training loss = 1.8847  Validation loss = 1.1930  \n",
      "\n",
      "Fold: 6  Epoch: 318  Training loss = 1.8846  Validation loss = 1.1933  \n",
      "\n",
      "Fold: 6  Epoch: 319  Training loss = 1.8844  Validation loss = 1.1927  \n",
      "\n",
      "Fold: 6  Epoch: 320  Training loss = 1.8842  Validation loss = 1.1919  \n",
      "\n",
      "Fold: 6  Epoch: 321  Training loss = 1.8840  Validation loss = 1.1917  \n",
      "\n",
      "Fold: 6  Epoch: 322  Training loss = 1.8837  Validation loss = 1.1901  \n",
      "\n",
      "Fold: 6  Epoch: 323  Training loss = 1.8834  Validation loss = 1.1889  \n",
      "\n",
      "Fold: 6  Epoch: 324  Training loss = 1.8833  Validation loss = 1.1896  \n",
      "\n",
      "Fold: 6  Epoch: 325  Training loss = 1.8832  Validation loss = 1.1894  \n",
      "\n",
      "Fold: 6  Epoch: 326  Training loss = 1.8830  Validation loss = 1.1881  \n",
      "\n",
      "Fold: 6  Epoch: 327  Training loss = 1.8828  Validation loss = 1.1872  \n",
      "\n",
      "Fold: 6  Epoch: 328  Training loss = 1.8824  Validation loss = 1.1843  \n",
      "\n",
      "Fold: 6  Epoch: 329  Training loss = 1.8823  Validation loss = 1.1850  \n",
      "\n",
      "Fold: 6  Epoch: 330  Training loss = 1.8821  Validation loss = 1.1842  \n",
      "\n",
      "Fold: 6  Epoch: 331  Training loss = 1.8819  Validation loss = 1.1829  \n",
      "\n",
      "Fold: 6  Epoch: 332  Training loss = 1.8817  Validation loss = 1.1822  \n",
      "\n",
      "Fold: 6  Epoch: 333  Training loss = 1.8816  Validation loss = 1.1827  \n",
      "\n",
      "Fold: 6  Epoch: 334  Training loss = 1.8811  Validation loss = 1.1798  \n",
      "\n",
      "Fold: 6  Epoch: 335  Training loss = 1.8809  Validation loss = 1.1794  \n",
      "\n",
      "Fold: 6  Epoch: 336  Training loss = 1.8807  Validation loss = 1.1792  \n",
      "\n",
      "Fold: 6  Epoch: 337  Training loss = 1.8806  Validation loss = 1.1788  \n",
      "\n",
      "Fold: 6  Epoch: 338  Training loss = 1.8803  Validation loss = 1.1773  \n",
      "\n",
      "Fold: 6  Epoch: 339  Training loss = 1.8800  Validation loss = 1.1755  \n",
      "\n",
      "Fold: 6  Epoch: 340  Training loss = 1.8796  Validation loss = 1.1727  \n",
      "\n",
      "Fold: 6  Epoch: 341  Training loss = 1.8794  Validation loss = 1.1727  \n",
      "\n",
      "Fold: 6  Epoch: 342  Training loss = 1.8792  Validation loss = 1.1722  \n",
      "\n",
      "Fold: 6  Epoch: 343  Training loss = 1.8791  Validation loss = 1.1714  \n",
      "\n",
      "Fold: 6  Epoch: 344  Training loss = 1.8788  Validation loss = 1.1700  \n",
      "\n",
      "Fold: 6  Epoch: 345  Training loss = 1.8786  Validation loss = 1.1688  \n",
      "\n",
      "Fold: 6  Epoch: 346  Training loss = 1.8782  Validation loss = 1.1667  \n",
      "\n",
      "Fold: 6  Epoch: 347  Training loss = 1.8781  Validation loss = 1.1663  \n",
      "\n",
      "Fold: 6  Epoch: 348  Training loss = 1.8778  Validation loss = 1.1651  \n",
      "\n",
      "Fold: 6  Epoch: 349  Training loss = 1.8775  Validation loss = 1.1640  \n",
      "\n",
      "Fold: 6  Epoch: 350  Training loss = 1.8773  Validation loss = 1.1637  \n",
      "\n",
      "Fold: 6  Epoch: 351  Training loss = 1.8771  Validation loss = 1.1627  \n",
      "\n",
      "Fold: 6  Epoch: 352  Training loss = 1.8769  Validation loss = 1.1615  \n",
      "\n",
      "Fold: 6  Epoch: 353  Training loss = 1.8766  Validation loss = 1.1606  \n",
      "\n",
      "Fold: 6  Epoch: 354  Training loss = 1.8765  Validation loss = 1.1598  \n",
      "\n",
      "Fold: 6  Epoch: 355  Training loss = 1.8764  Validation loss = 1.1604  \n",
      "\n",
      "Fold: 6  Epoch: 356  Training loss = 1.8762  Validation loss = 1.1593  \n",
      "\n",
      "Fold: 6  Epoch: 357  Training loss = 1.8759  Validation loss = 1.1584  \n",
      "\n",
      "Fold: 6  Epoch: 358  Training loss = 1.8758  Validation loss = 1.1577  \n",
      "\n",
      "Fold: 6  Epoch: 359  Training loss = 1.8756  Validation loss = 1.1572  \n",
      "\n",
      "Fold: 6  Epoch: 360  Training loss = 1.8754  Validation loss = 1.1571  \n",
      "\n",
      "Fold: 6  Epoch: 361  Training loss = 1.8752  Validation loss = 1.1563  \n",
      "\n",
      "Fold: 6  Epoch: 362  Training loss = 1.8749  Validation loss = 1.1551  \n",
      "\n",
      "Fold: 6  Epoch: 363  Training loss = 1.8747  Validation loss = 1.1539  \n",
      "\n",
      "Fold: 6  Epoch: 364  Training loss = 1.8745  Validation loss = 1.1535  \n",
      "\n",
      "Fold: 6  Epoch: 365  Training loss = 1.8743  Validation loss = 1.1528  \n",
      "\n",
      "Fold: 6  Epoch: 366  Training loss = 1.8741  Validation loss = 1.1525  \n",
      "\n",
      "Fold: 6  Epoch: 367  Training loss = 1.8739  Validation loss = 1.1517  \n",
      "\n",
      "Fold: 6  Epoch: 368  Training loss = 1.8737  Validation loss = 1.1507  \n",
      "\n",
      "Fold: 6  Epoch: 369  Training loss = 1.8736  Validation loss = 1.1501  \n",
      "\n",
      "Fold: 6  Epoch: 370  Training loss = 1.8734  Validation loss = 1.1498  \n",
      "\n",
      "Fold: 6  Epoch: 371  Training loss = 1.8732  Validation loss = 1.1496  \n",
      "\n",
      "Fold: 6  Epoch: 372  Training loss = 1.8729  Validation loss = 1.1471  \n",
      "\n",
      "Fold: 6  Epoch: 373  Training loss = 1.8726  Validation loss = 1.1458  \n",
      "\n",
      "Fold: 6  Epoch: 374  Training loss = 1.8723  Validation loss = 1.1449  \n",
      "\n",
      "Fold: 6  Epoch: 375  Training loss = 1.8718  Validation loss = 1.1394  \n",
      "\n",
      "Fold: 6  Epoch: 376  Training loss = 1.8715  Validation loss = 1.1387  \n",
      "\n",
      "Fold: 6  Epoch: 377  Training loss = 1.8712  Validation loss = 1.1366  \n",
      "\n",
      "Fold: 6  Epoch: 378  Training loss = 1.8710  Validation loss = 1.1353  \n",
      "\n",
      "Fold: 6  Epoch: 379  Training loss = 1.8707  Validation loss = 1.1335  \n",
      "\n",
      "Fold: 6  Epoch: 380  Training loss = 1.8706  Validation loss = 1.1342  \n",
      "\n",
      "Fold: 6  Epoch: 381  Training loss = 1.8704  Validation loss = 1.1331  \n",
      "\n",
      "Fold: 6  Epoch: 382  Training loss = 1.8702  Validation loss = 1.1321  \n",
      "\n",
      "Fold: 6  Epoch: 383  Training loss = 1.8700  Validation loss = 1.1311  \n",
      "\n",
      "Fold: 6  Epoch: 384  Training loss = 1.8697  Validation loss = 1.1300  \n",
      "\n",
      "Fold: 6  Epoch: 385  Training loss = 1.8696  Validation loss = 1.1294  \n",
      "\n",
      "Fold: 6  Epoch: 386  Training loss = 1.8693  Validation loss = 1.1281  \n",
      "\n",
      "Fold: 6  Epoch: 387  Training loss = 1.8690  Validation loss = 1.1266  \n",
      "\n",
      "Fold: 6  Epoch: 388  Training loss = 1.8687  Validation loss = 1.1247  \n",
      "\n",
      "Fold: 6  Epoch: 389  Training loss = 1.8685  Validation loss = 1.1249  \n",
      "\n",
      "Fold: 6  Epoch: 390  Training loss = 1.8683  Validation loss = 1.1240  \n",
      "\n",
      "Fold: 6  Epoch: 391  Training loss = 1.8681  Validation loss = 1.1239  \n",
      "\n",
      "Fold: 6  Epoch: 392  Training loss = 1.8678  Validation loss = 1.1225  \n",
      "\n",
      "Fold: 6  Epoch: 393  Training loss = 1.8677  Validation loss = 1.1218  \n",
      "\n",
      "Fold: 6  Epoch: 394  Training loss = 1.8674  Validation loss = 1.1212  \n",
      "\n",
      "Fold: 6  Epoch: 395  Training loss = 1.8672  Validation loss = 1.1198  \n",
      "\n",
      "Fold: 6  Epoch: 396  Training loss = 1.8669  Validation loss = 1.1183  \n",
      "\n",
      "Fold: 6  Epoch: 397  Training loss = 1.8667  Validation loss = 1.1172  \n",
      "\n",
      "Fold: 6  Epoch: 398  Training loss = 1.8664  Validation loss = 1.1158  \n",
      "\n",
      "Fold: 6  Epoch: 399  Training loss = 1.8662  Validation loss = 1.1149  \n",
      "\n",
      "Fold: 6  Epoch: 400  Training loss = 1.8659  Validation loss = 1.1135  \n",
      "\n",
      "Fold: 6  Epoch: 401  Training loss = 1.8658  Validation loss = 1.1138  \n",
      "\n",
      "Fold: 6  Epoch: 402  Training loss = 1.8654  Validation loss = 1.1112  \n",
      "\n",
      "Fold: 6  Epoch: 403  Training loss = 1.8651  Validation loss = 1.1097  \n",
      "\n",
      "Fold: 6  Epoch: 404  Training loss = 1.8649  Validation loss = 1.1091  \n",
      "\n",
      "Fold: 6  Epoch: 405  Training loss = 1.8647  Validation loss = 1.1085  \n",
      "\n",
      "Fold: 6  Epoch: 406  Training loss = 1.8646  Validation loss = 1.1083  \n",
      "\n",
      "Fold: 6  Epoch: 407  Training loss = 1.8644  Validation loss = 1.1074  \n",
      "\n",
      "Fold: 6  Epoch: 408  Training loss = 1.8643  Validation loss = 1.1077  \n",
      "\n",
      "Fold: 6  Epoch: 409  Training loss = 1.8640  Validation loss = 1.1066  \n",
      "\n",
      "Fold: 6  Epoch: 410  Training loss = 1.8639  Validation loss = 1.1062  \n",
      "\n",
      "Fold: 6  Epoch: 411  Training loss = 1.8636  Validation loss = 1.1041  \n",
      "\n",
      "Fold: 6  Epoch: 412  Training loss = 1.8634  Validation loss = 1.1035  \n",
      "\n",
      "Fold: 6  Epoch: 413  Training loss = 1.8632  Validation loss = 1.1025  \n",
      "\n",
      "Fold: 6  Epoch: 414  Training loss = 1.8630  Validation loss = 1.1023  \n",
      "\n",
      "Fold: 6  Epoch: 415  Training loss = 1.8628  Validation loss = 1.1027  \n",
      "\n",
      "Fold: 6  Epoch: 416  Training loss = 1.8625  Validation loss = 1.1004  \n",
      "\n",
      "Fold: 6  Epoch: 417  Training loss = 1.8622  Validation loss = 1.0991  \n",
      "\n",
      "Fold: 6  Epoch: 418  Training loss = 1.8620  Validation loss = 1.0980  \n",
      "\n",
      "Fold: 6  Epoch: 419  Training loss = 1.8617  Validation loss = 1.0969  \n",
      "\n",
      "Fold: 6  Epoch: 420  Training loss = 1.8614  Validation loss = 1.0961  \n",
      "\n",
      "Fold: 6  Epoch: 421  Training loss = 1.8611  Validation loss = 1.0948  \n",
      "\n",
      "Fold: 6  Epoch: 422  Training loss = 1.8610  Validation loss = 1.0946  \n",
      "\n",
      "Fold: 6  Epoch: 423  Training loss = 1.8608  Validation loss = 1.0943  \n",
      "\n",
      "Fold: 6  Epoch: 424  Training loss = 1.8607  Validation loss = 1.0948  \n",
      "\n",
      "Fold: 6  Epoch: 425  Training loss = 1.8604  Validation loss = 1.0932  \n",
      "\n",
      "Fold: 6  Epoch: 426  Training loss = 1.8600  Validation loss = 1.0904  \n",
      "\n",
      "Fold: 6  Epoch: 427  Training loss = 1.8599  Validation loss = 1.0898  \n",
      "\n",
      "Fold: 6  Epoch: 428  Training loss = 1.8597  Validation loss = 1.0897  \n",
      "\n",
      "Fold: 6  Epoch: 429  Training loss = 1.8594  Validation loss = 1.0872  \n",
      "\n",
      "Fold: 6  Epoch: 430  Training loss = 1.8591  Validation loss = 1.0851  \n",
      "\n",
      "Fold: 6  Epoch: 431  Training loss = 1.8590  Validation loss = 1.0859  \n",
      "\n",
      "Fold: 6  Epoch: 432  Training loss = 1.8587  Validation loss = 1.0842  \n",
      "\n",
      "Fold: 6  Epoch: 433  Training loss = 1.8586  Validation loss = 1.0844  \n",
      "\n",
      "Fold: 6  Epoch: 434  Training loss = 1.8584  Validation loss = 1.0837  \n",
      "\n",
      "Fold: 6  Epoch: 435  Training loss = 1.8581  Validation loss = 1.0830  \n",
      "\n",
      "Fold: 6  Epoch: 436  Training loss = 1.8578  Validation loss = 1.0821  \n",
      "\n",
      "Fold: 6  Epoch: 437  Training loss = 1.8576  Validation loss = 1.0809  \n",
      "\n",
      "Fold: 6  Epoch: 438  Training loss = 1.8574  Validation loss = 1.0792  \n",
      "\n",
      "Fold: 6  Epoch: 439  Training loss = 1.8573  Validation loss = 1.0785  \n",
      "\n",
      "Fold: 6  Epoch: 440  Training loss = 1.8570  Validation loss = 1.0769  \n",
      "\n",
      "Fold: 6  Epoch: 441  Training loss = 1.8569  Validation loss = 1.0778  \n",
      "\n",
      "Fold: 6  Epoch: 442  Training loss = 1.8567  Validation loss = 1.0767  \n",
      "\n",
      "Fold: 6  Epoch: 443  Training loss = 1.8565  Validation loss = 1.0763  \n",
      "\n",
      "Fold: 6  Epoch: 444  Training loss = 1.8563  Validation loss = 1.0755  \n",
      "\n",
      "Fold: 6  Epoch: 445  Training loss = 1.8561  Validation loss = 1.0735  \n",
      "\n",
      "Fold: 6  Epoch: 446  Training loss = 1.8558  Validation loss = 1.0708  \n",
      "\n",
      "Fold: 6  Epoch: 447  Training loss = 1.8557  Validation loss = 1.0724  \n",
      "\n",
      "Fold: 6  Epoch: 448  Training loss = 1.8555  Validation loss = 1.0708  \n",
      "\n",
      "Fold: 6  Epoch: 449  Training loss = 1.8553  Validation loss = 1.0701  \n",
      "\n",
      "Fold: 6  Epoch: 450  Training loss = 1.8551  Validation loss = 1.0687  \n",
      "\n",
      "Fold: 6  Epoch: 451  Training loss = 1.8550  Validation loss = 1.0693  \n",
      "\n",
      "Fold: 6  Epoch: 452  Training loss = 1.8548  Validation loss = 1.0689  \n",
      "\n",
      "Fold: 6  Epoch: 453  Training loss = 1.8545  Validation loss = 1.0670  \n",
      "\n",
      "Fold: 6  Epoch: 454  Training loss = 1.8543  Validation loss = 1.0669  \n",
      "\n",
      "Fold: 6  Epoch: 455  Training loss = 1.8541  Validation loss = 1.0651  \n",
      "\n",
      "Fold: 6  Epoch: 456  Training loss = 1.8539  Validation loss = 1.0650  \n",
      "\n",
      "Fold: 6  Epoch: 457  Training loss = 1.8537  Validation loss = 1.0648  \n",
      "\n",
      "Fold: 6  Epoch: 458  Training loss = 1.8535  Validation loss = 1.0648  \n",
      "\n",
      "Fold: 6  Epoch: 459  Training loss = 1.8533  Validation loss = 1.0643  \n",
      "\n",
      "Fold: 6  Epoch: 460  Training loss = 1.8532  Validation loss = 1.0646  \n",
      "\n",
      "Fold: 6  Epoch: 461  Training loss = 1.8530  Validation loss = 1.0629  \n",
      "\n",
      "Fold: 6  Epoch: 462  Training loss = 1.8528  Validation loss = 1.0623  \n",
      "\n",
      "Fold: 6  Epoch: 463  Training loss = 1.8526  Validation loss = 1.0618  \n",
      "\n",
      "Fold: 6  Epoch: 464  Training loss = 1.8524  Validation loss = 1.0595  \n",
      "\n",
      "Fold: 6  Epoch: 465  Training loss = 1.8521  Validation loss = 1.0583  \n",
      "\n",
      "Fold: 6  Epoch: 466  Training loss = 1.8519  Validation loss = 1.0581  \n",
      "\n",
      "Fold: 6  Epoch: 467  Training loss = 1.8518  Validation loss = 1.0586  \n",
      "\n",
      "Fold: 6  Epoch: 468  Training loss = 1.8517  Validation loss = 1.0594  \n",
      "\n",
      "Fold: 6  Epoch: 469  Training loss = 1.8516  Validation loss = 1.0593  \n",
      "\n",
      "Fold: 6  Epoch: 470  Training loss = 1.8515  Validation loss = 1.0590  \n",
      "\n",
      "Fold: 6  Epoch: 471  Training loss = 1.8512  Validation loss = 1.0574  \n",
      "\n",
      "Fold: 6  Epoch: 472  Training loss = 1.8510  Validation loss = 1.0575  \n",
      "\n",
      "Fold: 6  Epoch: 473  Training loss = 1.8508  Validation loss = 1.0558  \n",
      "\n",
      "Fold: 6  Epoch: 474  Training loss = 1.8505  Validation loss = 1.0542  \n",
      "\n",
      "Fold: 6  Epoch: 475  Training loss = 1.8502  Validation loss = 1.0518  \n",
      "\n",
      "Fold: 6  Epoch: 476  Training loss = 1.8501  Validation loss = 1.0523  \n",
      "\n",
      "Fold: 6  Epoch: 477  Training loss = 1.8498  Validation loss = 1.0506  \n",
      "\n",
      "Fold: 6  Epoch: 478  Training loss = 1.8496  Validation loss = 1.0493  \n",
      "\n",
      "Fold: 6  Epoch: 479  Training loss = 1.8494  Validation loss = 1.0474  \n",
      "\n",
      "Fold: 6  Epoch: 480  Training loss = 1.8491  Validation loss = 1.0453  \n",
      "\n",
      "Fold: 6  Epoch: 481  Training loss = 1.8489  Validation loss = 1.0443  \n",
      "\n",
      "Fold: 6  Epoch: 482  Training loss = 1.8488  Validation loss = 1.0445  \n",
      "\n",
      "Fold: 6  Epoch: 483  Training loss = 1.8486  Validation loss = 1.0443  \n",
      "\n",
      "Fold: 6  Epoch: 484  Training loss = 1.8484  Validation loss = 1.0435  \n",
      "\n",
      "Fold: 6  Epoch: 485  Training loss = 1.8482  Validation loss = 1.0420  \n",
      "\n",
      "Fold: 6  Epoch: 486  Training loss = 1.8479  Validation loss = 1.0392  \n",
      "\n",
      "Fold: 6  Epoch: 487  Training loss = 1.8476  Validation loss = 1.0373  \n",
      "\n",
      "Fold: 6  Epoch: 488  Training loss = 1.8474  Validation loss = 1.0363  \n",
      "\n",
      "Fold: 6  Epoch: 489  Training loss = 1.8473  Validation loss = 1.0364  \n",
      "\n",
      "Fold: 6  Epoch: 490  Training loss = 1.8471  Validation loss = 1.0353  \n",
      "\n",
      "Fold: 6  Epoch: 491  Training loss = 1.8468  Validation loss = 1.0332  \n",
      "\n",
      "Fold: 6  Epoch: 492  Training loss = 1.8466  Validation loss = 1.0330  \n",
      "\n",
      "Fold: 6  Epoch: 493  Training loss = 1.8463  Validation loss = 1.0308  \n",
      "\n",
      "Fold: 6  Epoch: 494  Training loss = 1.8461  Validation loss = 1.0302  \n",
      "\n",
      "Fold: 6  Epoch: 495  Training loss = 1.8459  Validation loss = 1.0285  \n",
      "\n",
      "Fold: 6  Epoch: 496  Training loss = 1.8456  Validation loss = 1.0267  \n",
      "\n",
      "Fold: 6  Epoch: 497  Training loss = 1.8454  Validation loss = 1.0251  \n",
      "\n",
      "Fold: 6  Epoch: 498  Training loss = 1.8452  Validation loss = 1.0253  \n",
      "\n",
      "Fold: 6  Epoch: 499  Training loss = 1.8449  Validation loss = 1.0223  \n",
      "\n",
      "Fold: 6  Epoch: 500  Training loss = 1.8446  Validation loss = 1.0217  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 500  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 1.8031  Validation loss = 0.8842  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 1.8025  Validation loss = 0.8818  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 1.8019  Validation loss = 0.8796  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.8016  Validation loss = 0.8786  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 1.8011  Validation loss = 0.8763  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 1.8007  Validation loss = 0.8747  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 1.8004  Validation loss = 0.8735  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 1.8000  Validation loss = 0.8716  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 1.7996  Validation loss = 0.8702  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 1.7995  Validation loss = 0.8706  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 1.7989  Validation loss = 0.8677  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 1.7986  Validation loss = 0.8663  \n",
      "\n",
      "Fold: 7  Epoch: 13  Training loss = 1.7982  Validation loss = 0.8652  \n",
      "\n",
      "Fold: 7  Epoch: 14  Training loss = 1.7979  Validation loss = 0.8639  \n",
      "\n",
      "Fold: 7  Epoch: 15  Training loss = 1.7976  Validation loss = 0.8629  \n",
      "\n",
      "Fold: 7  Epoch: 16  Training loss = 1.7973  Validation loss = 0.8622  \n",
      "\n",
      "Fold: 7  Epoch: 17  Training loss = 1.7971  Validation loss = 0.8616  \n",
      "\n",
      "Fold: 7  Epoch: 18  Training loss = 1.7968  Validation loss = 0.8601  \n",
      "\n",
      "Fold: 7  Epoch: 19  Training loss = 1.7963  Validation loss = 0.8580  \n",
      "\n",
      "Fold: 7  Epoch: 20  Training loss = 1.7960  Validation loss = 0.8570  \n",
      "\n",
      "Fold: 7  Epoch: 21  Training loss = 1.7958  Validation loss = 0.8569  \n",
      "\n",
      "Fold: 7  Epoch: 22  Training loss = 1.7954  Validation loss = 0.8558  \n",
      "\n",
      "Fold: 7  Epoch: 23  Training loss = 1.7951  Validation loss = 0.8544  \n",
      "\n",
      "Fold: 7  Epoch: 24  Training loss = 1.7949  Validation loss = 0.8539  \n",
      "\n",
      "Fold: 7  Epoch: 25  Training loss = 1.7944  Validation loss = 0.8516  \n",
      "\n",
      "Fold: 7  Epoch: 26  Training loss = 1.7943  Validation loss = 0.8519  \n",
      "\n",
      "Fold: 7  Epoch: 27  Training loss = 1.7939  Validation loss = 0.8508  \n",
      "\n",
      "Fold: 7  Epoch: 28  Training loss = 1.7937  Validation loss = 0.8504  \n",
      "\n",
      "Fold: 7  Epoch: 29  Training loss = 1.7931  Validation loss = 0.8473  \n",
      "\n",
      "Fold: 7  Epoch: 30  Training loss = 1.7928  Validation loss = 0.8463  \n",
      "\n",
      "Fold: 7  Epoch: 31  Training loss = 1.7924  Validation loss = 0.8446  \n",
      "\n",
      "Fold: 7  Epoch: 32  Training loss = 1.7919  Validation loss = 0.8428  \n",
      "\n",
      "Fold: 7  Epoch: 33  Training loss = 1.7917  Validation loss = 0.8430  \n",
      "\n",
      "Fold: 7  Epoch: 34  Training loss = 1.7914  Validation loss = 0.8414  \n",
      "\n",
      "Fold: 7  Epoch: 35  Training loss = 1.7909  Validation loss = 0.8399  \n",
      "\n",
      "Fold: 7  Epoch: 36  Training loss = 1.7906  Validation loss = 0.8389  \n",
      "\n",
      "Fold: 7  Epoch: 37  Training loss = 1.7901  Validation loss = 0.8369  \n",
      "\n",
      "Fold: 7  Epoch: 38  Training loss = 1.7897  Validation loss = 0.8353  \n",
      "\n",
      "Fold: 7  Epoch: 39  Training loss = 1.7893  Validation loss = 0.8344  \n",
      "\n",
      "Fold: 7  Epoch: 40  Training loss = 1.7888  Validation loss = 0.8322  \n",
      "\n",
      "Fold: 7  Epoch: 41  Training loss = 1.7887  Validation loss = 0.8325  \n",
      "\n",
      "Fold: 7  Epoch: 42  Training loss = 1.7882  Validation loss = 0.8302  \n",
      "\n",
      "Fold: 7  Epoch: 43  Training loss = 1.7877  Validation loss = 0.8278  \n",
      "\n",
      "Fold: 7  Epoch: 44  Training loss = 1.7873  Validation loss = 0.8266  \n",
      "\n",
      "Fold: 7  Epoch: 45  Training loss = 1.7870  Validation loss = 0.8248  \n",
      "\n",
      "Fold: 7  Epoch: 46  Training loss = 1.7867  Validation loss = 0.8235  \n",
      "\n",
      "Fold: 7  Epoch: 47  Training loss = 1.7862  Validation loss = 0.8213  \n",
      "\n",
      "Fold: 7  Epoch: 48  Training loss = 1.7858  Validation loss = 0.8205  \n",
      "\n",
      "Fold: 7  Epoch: 49  Training loss = 1.7854  Validation loss = 0.8184  \n",
      "\n",
      "Fold: 7  Epoch: 50  Training loss = 1.7850  Validation loss = 0.8167  \n",
      "\n",
      "Fold: 7  Epoch: 51  Training loss = 1.7849  Validation loss = 0.8168  \n",
      "\n",
      "Fold: 7  Epoch: 52  Training loss = 1.7846  Validation loss = 0.8163  \n",
      "\n",
      "Fold: 7  Epoch: 53  Training loss = 1.7843  Validation loss = 0.8142  \n",
      "\n",
      "Fold: 7  Epoch: 54  Training loss = 1.7840  Validation loss = 0.8135  \n",
      "\n",
      "Fold: 7  Epoch: 55  Training loss = 1.7836  Validation loss = 0.8121  \n",
      "\n",
      "Fold: 7  Epoch: 56  Training loss = 1.7835  Validation loss = 0.8124  \n",
      "\n",
      "Fold: 7  Epoch: 57  Training loss = 1.7832  Validation loss = 0.8110  \n",
      "\n",
      "Fold: 7  Epoch: 58  Training loss = 1.7829  Validation loss = 0.8103  \n",
      "\n",
      "Fold: 7  Epoch: 59  Training loss = 1.7827  Validation loss = 0.8096  \n",
      "\n",
      "Fold: 7  Epoch: 60  Training loss = 1.7823  Validation loss = 0.8083  \n",
      "\n",
      "Fold: 7  Epoch: 61  Training loss = 1.7819  Validation loss = 0.8068  \n",
      "\n",
      "Fold: 7  Epoch: 62  Training loss = 1.7816  Validation loss = 0.8060  \n",
      "\n",
      "Fold: 7  Epoch: 63  Training loss = 1.7814  Validation loss = 0.8053  \n",
      "\n",
      "Fold: 7  Epoch: 64  Training loss = 1.7812  Validation loss = 0.8053  \n",
      "\n",
      "Fold: 7  Epoch: 65  Training loss = 1.7810  Validation loss = 0.8046  \n",
      "\n",
      "Fold: 7  Epoch: 66  Training loss = 1.7807  Validation loss = 0.8036  \n",
      "\n",
      "Fold: 7  Epoch: 67  Training loss = 1.7803  Validation loss = 0.8017  \n",
      "\n",
      "Fold: 7  Epoch: 68  Training loss = 1.7799  Validation loss = 0.8006  \n",
      "\n",
      "Fold: 7  Epoch: 69  Training loss = 1.7795  Validation loss = 0.7992  \n",
      "\n",
      "Fold: 7  Epoch: 70  Training loss = 1.7793  Validation loss = 0.7986  \n",
      "\n",
      "Fold: 7  Epoch: 71  Training loss = 1.7789  Validation loss = 0.7975  \n",
      "\n",
      "Fold: 7  Epoch: 72  Training loss = 1.7786  Validation loss = 0.7969  \n",
      "\n",
      "Fold: 7  Epoch: 73  Training loss = 1.7784  Validation loss = 0.7966  \n",
      "\n",
      "Fold: 7  Epoch: 74  Training loss = 1.7780  Validation loss = 0.7947  \n",
      "\n",
      "Fold: 7  Epoch: 75  Training loss = 1.7777  Validation loss = 0.7945  \n",
      "\n",
      "Fold: 7  Epoch: 76  Training loss = 1.7775  Validation loss = 0.7936  \n",
      "\n",
      "Fold: 7  Epoch: 77  Training loss = 1.7772  Validation loss = 0.7923  \n",
      "\n",
      "Fold: 7  Epoch: 78  Training loss = 1.7771  Validation loss = 0.7921  \n",
      "\n",
      "Fold: 7  Epoch: 79  Training loss = 1.7767  Validation loss = 0.7903  \n",
      "\n",
      "Fold: 7  Epoch: 80  Training loss = 1.7763  Validation loss = 0.7884  \n",
      "\n",
      "Fold: 7  Epoch: 81  Training loss = 1.7760  Validation loss = 0.7877  \n",
      "\n",
      "Fold: 7  Epoch: 82  Training loss = 1.7758  Validation loss = 0.7870  \n",
      "\n",
      "Fold: 7  Epoch: 83  Training loss = 1.7753  Validation loss = 0.7855  \n",
      "\n",
      "Fold: 7  Epoch: 84  Training loss = 1.7750  Validation loss = 0.7846  \n",
      "\n",
      "Fold: 7  Epoch: 85  Training loss = 1.7747  Validation loss = 0.7836  \n",
      "\n",
      "Fold: 7  Epoch: 86  Training loss = 1.7743  Validation loss = 0.7820  \n",
      "\n",
      "Fold: 7  Epoch: 87  Training loss = 1.7741  Validation loss = 0.7814  \n",
      "\n",
      "Fold: 7  Epoch: 88  Training loss = 1.7736  Validation loss = 0.7795  \n",
      "\n",
      "Fold: 7  Epoch: 89  Training loss = 1.7734  Validation loss = 0.7787  \n",
      "\n",
      "Fold: 7  Epoch: 90  Training loss = 1.7730  Validation loss = 0.7770  \n",
      "\n",
      "Fold: 7  Epoch: 91  Training loss = 1.7729  Validation loss = 0.7771  \n",
      "\n",
      "Fold: 7  Epoch: 92  Training loss = 1.7728  Validation loss = 0.7777  \n",
      "\n",
      "Fold: 7  Epoch: 93  Training loss = 1.7727  Validation loss = 0.7777  \n",
      "\n",
      "Fold: 7  Epoch: 94  Training loss = 1.7722  Validation loss = 0.7756  \n",
      "\n",
      "Fold: 7  Epoch: 95  Training loss = 1.7719  Validation loss = 0.7742  \n",
      "\n",
      "Fold: 7  Epoch: 96  Training loss = 1.7713  Validation loss = 0.7717  \n",
      "\n",
      "Fold: 7  Epoch: 97  Training loss = 1.7709  Validation loss = 0.7701  \n",
      "\n",
      "Fold: 7  Epoch: 98  Training loss = 1.7707  Validation loss = 0.7688  \n",
      "\n",
      "Fold: 7  Epoch: 99  Training loss = 1.7706  Validation loss = 0.7685  \n",
      "\n",
      "Fold: 7  Epoch: 100  Training loss = 1.7702  Validation loss = 0.7671  \n",
      "\n",
      "Fold: 7  Epoch: 101  Training loss = 1.7700  Validation loss = 0.7668  \n",
      "\n",
      "Fold: 7  Epoch: 102  Training loss = 1.7695  Validation loss = 0.7646  \n",
      "\n",
      "Fold: 7  Epoch: 103  Training loss = 1.7692  Validation loss = 0.7640  \n",
      "\n",
      "Fold: 7  Epoch: 104  Training loss = 1.7690  Validation loss = 0.7636  \n",
      "\n",
      "Fold: 7  Epoch: 105  Training loss = 1.7687  Validation loss = 0.7629  \n",
      "\n",
      "Fold: 7  Epoch: 106  Training loss = 1.7685  Validation loss = 0.7623  \n",
      "\n",
      "Fold: 7  Epoch: 107  Training loss = 1.7681  Validation loss = 0.7609  \n",
      "\n",
      "Fold: 7  Epoch: 108  Training loss = 1.7679  Validation loss = 0.7607  \n",
      "\n",
      "Fold: 7  Epoch: 109  Training loss = 1.7675  Validation loss = 0.7590  \n",
      "\n",
      "Fold: 7  Epoch: 110  Training loss = 1.7671  Validation loss = 0.7571  \n",
      "\n",
      "Fold: 7  Epoch: 111  Training loss = 1.7668  Validation loss = 0.7561  \n",
      "\n",
      "Fold: 7  Epoch: 112  Training loss = 1.7665  Validation loss = 0.7549  \n",
      "\n",
      "Fold: 7  Epoch: 113  Training loss = 1.7661  Validation loss = 0.7537  \n",
      "\n",
      "Fold: 7  Epoch: 114  Training loss = 1.7659  Validation loss = 0.7534  \n",
      "\n",
      "Fold: 7  Epoch: 115  Training loss = 1.7656  Validation loss = 0.7523  \n",
      "\n",
      "Fold: 7  Epoch: 116  Training loss = 1.7652  Validation loss = 0.7504  \n",
      "\n",
      "Fold: 7  Epoch: 117  Training loss = 1.7650  Validation loss = 0.7501  \n",
      "\n",
      "Fold: 7  Epoch: 118  Training loss = 1.7648  Validation loss = 0.7501  \n",
      "\n",
      "Fold: 7  Epoch: 119  Training loss = 1.7647  Validation loss = 0.7504  \n",
      "\n",
      "Fold: 7  Epoch: 120  Training loss = 1.7644  Validation loss = 0.7495  \n",
      "\n",
      "Fold: 7  Epoch: 121  Training loss = 1.7641  Validation loss = 0.7480  \n",
      "\n",
      "Fold: 7  Epoch: 122  Training loss = 1.7637  Validation loss = 0.7469  \n",
      "\n",
      "Fold: 7  Epoch: 123  Training loss = 1.7635  Validation loss = 0.7460  \n",
      "\n",
      "Fold: 7  Epoch: 124  Training loss = 1.7633  Validation loss = 0.7454  \n",
      "\n",
      "Fold: 7  Epoch: 125  Training loss = 1.7630  Validation loss = 0.7449  \n",
      "\n",
      "Fold: 7  Epoch: 126  Training loss = 1.7627  Validation loss = 0.7441  \n",
      "\n",
      "Fold: 7  Epoch: 127  Training loss = 1.7622  Validation loss = 0.7423  \n",
      "\n",
      "Fold: 7  Epoch: 128  Training loss = 1.7620  Validation loss = 0.7414  \n",
      "\n",
      "Fold: 7  Epoch: 129  Training loss = 1.7618  Validation loss = 0.7408  \n",
      "\n",
      "Fold: 7  Epoch: 130  Training loss = 1.7616  Validation loss = 0.7399  \n",
      "\n",
      "Fold: 7  Epoch: 131  Training loss = 1.7614  Validation loss = 0.7397  \n",
      "\n",
      "Fold: 7  Epoch: 132  Training loss = 1.7612  Validation loss = 0.7389  \n",
      "\n",
      "Fold: 7  Epoch: 133  Training loss = 1.7610  Validation loss = 0.7389  \n",
      "\n",
      "Fold: 7  Epoch: 134  Training loss = 1.7608  Validation loss = 0.7379  \n",
      "\n",
      "Fold: 7  Epoch: 135  Training loss = 1.7606  Validation loss = 0.7374  \n",
      "\n",
      "Fold: 7  Epoch: 136  Training loss = 1.7603  Validation loss = 0.7362  \n",
      "\n",
      "Fold: 7  Epoch: 137  Training loss = 1.7600  Validation loss = 0.7353  \n",
      "\n",
      "Fold: 7  Epoch: 138  Training loss = 1.7595  Validation loss = 0.7331  \n",
      "\n",
      "Fold: 7  Epoch: 139  Training loss = 1.7593  Validation loss = 0.7330  \n",
      "\n",
      "Fold: 7  Epoch: 140  Training loss = 1.7591  Validation loss = 0.7322  \n",
      "\n",
      "Fold: 7  Epoch: 141  Training loss = 1.7588  Validation loss = 0.7313  \n",
      "\n",
      "Fold: 7  Epoch: 142  Training loss = 1.7588  Validation loss = 0.7317  \n",
      "\n",
      "Fold: 7  Epoch: 143  Training loss = 1.7586  Validation loss = 0.7321  \n",
      "\n",
      "Fold: 7  Epoch: 144  Training loss = 1.7584  Validation loss = 0.7319  \n",
      "\n",
      "Fold: 7  Epoch: 145  Training loss = 1.7582  Validation loss = 0.7308  \n",
      "\n",
      "Fold: 7  Epoch: 146  Training loss = 1.7579  Validation loss = 0.7305  \n",
      "\n",
      "Fold: 7  Epoch: 147  Training loss = 1.7578  Validation loss = 0.7312  \n",
      "\n",
      "Fold: 7  Epoch: 148  Training loss = 1.7574  Validation loss = 0.7297  \n",
      "\n",
      "Fold: 7  Epoch: 149  Training loss = 1.7572  Validation loss = 0.7299  \n",
      "\n",
      "Fold: 7  Epoch: 150  Training loss = 1.7567  Validation loss = 0.7279  \n",
      "\n",
      "Fold: 7  Epoch: 151  Training loss = 1.7567  Validation loss = 0.7284  \n",
      "\n",
      "Fold: 7  Epoch: 152  Training loss = 1.7565  Validation loss = 0.7288  \n",
      "\n",
      "Fold: 7  Epoch: 153  Training loss = 1.7563  Validation loss = 0.7286  \n",
      "\n",
      "Fold: 7  Epoch: 154  Training loss = 1.7562  Validation loss = 0.7279  \n",
      "\n",
      "Fold: 7  Epoch: 155  Training loss = 1.7557  Validation loss = 0.7256  \n",
      "\n",
      "Fold: 7  Epoch: 156  Training loss = 1.7556  Validation loss = 0.7259  \n",
      "\n",
      "Fold: 7  Epoch: 157  Training loss = 1.7554  Validation loss = 0.7254  \n",
      "\n",
      "Fold: 7  Epoch: 158  Training loss = 1.7553  Validation loss = 0.7255  \n",
      "\n",
      "Fold: 7  Epoch: 159  Training loss = 1.7551  Validation loss = 0.7250  \n",
      "\n",
      "Fold: 7  Epoch: 160  Training loss = 1.7548  Validation loss = 0.7237  \n",
      "\n",
      "Fold: 7  Epoch: 161  Training loss = 1.7545  Validation loss = 0.7225  \n",
      "\n",
      "Fold: 7  Epoch: 162  Training loss = 1.7541  Validation loss = 0.7210  \n",
      "\n",
      "Fold: 7  Epoch: 163  Training loss = 1.7539  Validation loss = 0.7211  \n",
      "\n",
      "Fold: 7  Epoch: 164  Training loss = 1.7537  Validation loss = 0.7209  \n",
      "\n",
      "Fold: 7  Epoch: 165  Training loss = 1.7535  Validation loss = 0.7201  \n",
      "\n",
      "Fold: 7  Epoch: 166  Training loss = 1.7535  Validation loss = 0.7214  \n",
      "\n",
      "Fold: 7  Epoch: 167  Training loss = 1.7532  Validation loss = 0.7201  \n",
      "\n",
      "Fold: 7  Epoch: 168  Training loss = 1.7529  Validation loss = 0.7201  \n",
      "\n",
      "Fold: 7  Epoch: 169  Training loss = 1.7529  Validation loss = 0.7206  \n",
      "\n",
      "Fold: 7  Epoch: 170  Training loss = 1.7527  Validation loss = 0.7206  \n",
      "\n",
      "Fold: 7  Epoch: 171  Training loss = 1.7524  Validation loss = 0.7199  \n",
      "\n",
      "Fold: 7  Epoch: 172  Training loss = 1.7522  Validation loss = 0.7193  \n",
      "\n",
      "Fold: 7  Epoch: 173  Training loss = 1.7520  Validation loss = 0.7197  \n",
      "\n",
      "Fold: 7  Epoch: 174  Training loss = 1.7517  Validation loss = 0.7187  \n",
      "\n",
      "Fold: 7  Epoch: 175  Training loss = 1.7515  Validation loss = 0.7183  \n",
      "\n",
      "Fold: 7  Epoch: 176  Training loss = 1.7513  Validation loss = 0.7181  \n",
      "\n",
      "Fold: 7  Epoch: 177  Training loss = 1.7510  Validation loss = 0.7173  \n",
      "\n",
      "Fold: 7  Epoch: 178  Training loss = 1.7508  Validation loss = 0.7170  \n",
      "\n",
      "Fold: 7  Epoch: 179  Training loss = 1.7505  Validation loss = 0.7158  \n",
      "\n",
      "Fold: 7  Epoch: 180  Training loss = 1.7502  Validation loss = 0.7146  \n",
      "\n",
      "Fold: 7  Epoch: 181  Training loss = 1.7499  Validation loss = 0.7127  \n",
      "\n",
      "Fold: 7  Epoch: 182  Training loss = 1.7497  Validation loss = 0.7124  \n",
      "\n",
      "Fold: 7  Epoch: 183  Training loss = 1.7495  Validation loss = 0.7121  \n",
      "\n",
      "Fold: 7  Epoch: 184  Training loss = 1.7493  Validation loss = 0.7117  \n",
      "\n",
      "Fold: 7  Epoch: 185  Training loss = 1.7491  Validation loss = 0.7106  \n",
      "\n",
      "Fold: 7  Epoch: 186  Training loss = 1.7489  Validation loss = 0.7111  \n",
      "\n",
      "Fold: 7  Epoch: 187  Training loss = 1.7488  Validation loss = 0.7112  \n",
      "\n",
      "Fold: 7  Epoch: 188  Training loss = 1.7485  Validation loss = 0.7110  \n",
      "\n",
      "Fold: 7  Epoch: 189  Training loss = 1.7484  Validation loss = 0.7116  \n",
      "\n",
      "Fold: 7  Epoch: 190  Training loss = 1.7483  Validation loss = 0.7114  \n",
      "\n",
      "Fold: 7  Epoch: 191  Training loss = 1.7479  Validation loss = 0.7101  \n",
      "\n",
      "Fold: 7  Epoch: 192  Training loss = 1.7475  Validation loss = 0.7084  \n",
      "\n",
      "Fold: 7  Epoch: 193  Training loss = 1.7472  Validation loss = 0.7074  \n",
      "\n",
      "Fold: 7  Epoch: 194  Training loss = 1.7469  Validation loss = 0.7061  \n",
      "\n",
      "Fold: 7  Epoch: 195  Training loss = 1.7465  Validation loss = 0.7044  \n",
      "\n",
      "Fold: 7  Epoch: 196  Training loss = 1.7462  Validation loss = 0.7035  \n",
      "\n",
      "Fold: 7  Epoch: 197  Training loss = 1.7460  Validation loss = 0.7027  \n",
      "\n",
      "Fold: 7  Epoch: 198  Training loss = 1.7458  Validation loss = 0.7024  \n",
      "\n",
      "Fold: 7  Epoch: 199  Training loss = 1.7455  Validation loss = 0.7011  \n",
      "\n",
      "Fold: 7  Epoch: 200  Training loss = 1.7454  Validation loss = 0.7015  \n",
      "\n",
      "Fold: 7  Epoch: 201  Training loss = 1.7451  Validation loss = 0.7003  \n",
      "\n",
      "Fold: 7  Epoch: 202  Training loss = 1.7447  Validation loss = 0.6986  \n",
      "\n",
      "Fold: 7  Epoch: 203  Training loss = 1.7446  Validation loss = 0.6996  \n",
      "\n",
      "Fold: 7  Epoch: 204  Training loss = 1.7444  Validation loss = 0.6987  \n",
      "\n",
      "Fold: 7  Epoch: 205  Training loss = 1.7442  Validation loss = 0.6991  \n",
      "\n",
      "Fold: 7  Epoch: 206  Training loss = 1.7439  Validation loss = 0.6984  \n",
      "\n",
      "Fold: 7  Epoch: 207  Training loss = 1.7436  Validation loss = 0.6973  \n",
      "\n",
      "Fold: 7  Epoch: 208  Training loss = 1.7434  Validation loss = 0.6963  \n",
      "\n",
      "Fold: 7  Epoch: 209  Training loss = 1.7431  Validation loss = 0.6958  \n",
      "\n",
      "Fold: 7  Epoch: 210  Training loss = 1.7427  Validation loss = 0.6937  \n",
      "\n",
      "Fold: 7  Epoch: 211  Training loss = 1.7426  Validation loss = 0.6936  \n",
      "\n",
      "Fold: 7  Epoch: 212  Training loss = 1.7423  Validation loss = 0.6922  \n",
      "\n",
      "Fold: 7  Epoch: 213  Training loss = 1.7421  Validation loss = 0.6924  \n",
      "\n",
      "Fold: 7  Epoch: 214  Training loss = 1.7419  Validation loss = 0.6919  \n",
      "\n",
      "Fold: 7  Epoch: 215  Training loss = 1.7416  Validation loss = 0.6910  \n",
      "\n",
      "Fold: 7  Epoch: 216  Training loss = 1.7413  Validation loss = 0.6905  \n",
      "\n",
      "Fold: 7  Epoch: 217  Training loss = 1.7411  Validation loss = 0.6905  \n",
      "\n",
      "Fold: 7  Epoch: 218  Training loss = 1.7407  Validation loss = 0.6884  \n",
      "\n",
      "Fold: 7  Epoch: 219  Training loss = 1.7405  Validation loss = 0.6881  \n",
      "\n",
      "Fold: 7  Epoch: 220  Training loss = 1.7402  Validation loss = 0.6864  \n",
      "\n",
      "Fold: 7  Epoch: 221  Training loss = 1.7399  Validation loss = 0.6850  \n",
      "\n",
      "Fold: 7  Epoch: 222  Training loss = 1.7397  Validation loss = 0.6847  \n",
      "\n",
      "Fold: 7  Epoch: 223  Training loss = 1.7395  Validation loss = 0.6846  \n",
      "\n",
      "Fold: 7  Epoch: 224  Training loss = 1.7393  Validation loss = 0.6845  \n",
      "\n",
      "Fold: 7  Epoch: 225  Training loss = 1.7391  Validation loss = 0.6838  \n",
      "\n",
      "Fold: 7  Epoch: 226  Training loss = 1.7388  Validation loss = 0.6821  \n",
      "\n",
      "Fold: 7  Epoch: 227  Training loss = 1.7385  Validation loss = 0.6810  \n",
      "\n",
      "Fold: 7  Epoch: 228  Training loss = 1.7383  Validation loss = 0.6807  \n",
      "\n",
      "Fold: 7  Epoch: 229  Training loss = 1.7382  Validation loss = 0.6806  \n",
      "\n",
      "Fold: 7  Epoch: 230  Training loss = 1.7380  Validation loss = 0.6805  \n",
      "\n",
      "Fold: 7  Epoch: 231  Training loss = 1.7377  Validation loss = 0.6800  \n",
      "\n",
      "Fold: 7  Epoch: 232  Training loss = 1.7375  Validation loss = 0.6792  \n",
      "\n",
      "Fold: 7  Epoch: 233  Training loss = 1.7373  Validation loss = 0.6794  \n",
      "\n",
      "Fold: 7  Epoch: 234  Training loss = 1.7372  Validation loss = 0.6802  \n",
      "\n",
      "Fold: 7  Epoch: 235  Training loss = 1.7370  Validation loss = 0.6798  \n",
      "\n",
      "Fold: 7  Epoch: 236  Training loss = 1.7368  Validation loss = 0.6788  \n",
      "\n",
      "Fold: 7  Epoch: 237  Training loss = 1.7365  Validation loss = 0.6778  \n",
      "\n",
      "Fold: 7  Epoch: 238  Training loss = 1.7363  Validation loss = 0.6782  \n",
      "\n",
      "Fold: 7  Epoch: 239  Training loss = 1.7362  Validation loss = 0.6783  \n",
      "\n",
      "Fold: 7  Epoch: 240  Training loss = 1.7358  Validation loss = 0.6768  \n",
      "\n",
      "Fold: 7  Epoch: 241  Training loss = 1.7356  Validation loss = 0.6767  \n",
      "\n",
      "Fold: 7  Epoch: 242  Training loss = 1.7354  Validation loss = 0.6763  \n",
      "\n",
      "Fold: 7  Epoch: 243  Training loss = 1.7352  Validation loss = 0.6766  \n",
      "\n",
      "Fold: 7  Epoch: 244  Training loss = 1.7352  Validation loss = 0.6772  \n",
      "\n",
      "Fold: 7  Epoch: 245  Training loss = 1.7350  Validation loss = 0.6772  \n",
      "\n",
      "Fold: 7  Epoch: 246  Training loss = 1.7347  Validation loss = 0.6771  \n",
      "\n",
      "Fold: 7  Epoch: 247  Training loss = 1.7344  Validation loss = 0.6754  \n",
      "\n",
      "Fold: 7  Epoch: 248  Training loss = 1.7343  Validation loss = 0.6756  \n",
      "\n",
      "Fold: 7  Epoch: 249  Training loss = 1.7342  Validation loss = 0.6759  \n",
      "\n",
      "Fold: 7  Epoch: 250  Training loss = 1.7339  Validation loss = 0.6749  \n",
      "\n",
      "Fold: 7  Epoch: 251  Training loss = 1.7337  Validation loss = 0.6749  \n",
      "\n",
      "Fold: 7  Epoch: 252  Training loss = 1.7334  Validation loss = 0.6738  \n",
      "\n",
      "Fold: 7  Epoch: 253  Training loss = 1.7332  Validation loss = 0.6730  \n",
      "\n",
      "Fold: 7  Epoch: 254  Training loss = 1.7330  Validation loss = 0.6738  \n",
      "\n",
      "Fold: 7  Epoch: 255  Training loss = 1.7328  Validation loss = 0.6732  \n",
      "\n",
      "Fold: 7  Epoch: 256  Training loss = 1.7324  Validation loss = 0.6717  \n",
      "\n",
      "Fold: 7  Epoch: 257  Training loss = 1.7323  Validation loss = 0.6722  \n",
      "\n",
      "Fold: 7  Epoch: 258  Training loss = 1.7320  Validation loss = 0.6718  \n",
      "\n",
      "Fold: 7  Epoch: 259  Training loss = 1.7319  Validation loss = 0.6715  \n",
      "\n",
      "Fold: 7  Epoch: 260  Training loss = 1.7317  Validation loss = 0.6712  \n",
      "\n",
      "Fold: 7  Epoch: 261  Training loss = 1.7316  Validation loss = 0.6715  \n",
      "\n",
      "Fold: 7  Epoch: 262  Training loss = 1.7314  Validation loss = 0.6713  \n",
      "\n",
      "Fold: 7  Epoch: 263  Training loss = 1.7312  Validation loss = 0.6710  \n",
      "\n",
      "Fold: 7  Epoch: 264  Training loss = 1.7309  Validation loss = 0.6701  \n",
      "\n",
      "Fold: 7  Epoch: 265  Training loss = 1.7306  Validation loss = 0.6689  \n",
      "\n",
      "Fold: 7  Epoch: 266  Training loss = 1.7305  Validation loss = 0.6688  \n",
      "\n",
      "Fold: 7  Epoch: 267  Training loss = 1.7302  Validation loss = 0.6676  \n",
      "\n",
      "Fold: 7  Epoch: 268  Training loss = 1.7300  Validation loss = 0.6670  \n",
      "\n",
      "Fold: 7  Epoch: 269  Training loss = 1.7297  Validation loss = 0.6665  \n",
      "\n",
      "Fold: 7  Epoch: 270  Training loss = 1.7296  Validation loss = 0.6664  \n",
      "\n",
      "Fold: 7  Epoch: 271  Training loss = 1.7294  Validation loss = 0.6657  \n",
      "\n",
      "Fold: 7  Epoch: 272  Training loss = 1.7292  Validation loss = 0.6651  \n",
      "\n",
      "Fold: 7  Epoch: 273  Training loss = 1.7290  Validation loss = 0.6646  \n",
      "\n",
      "Fold: 7  Epoch: 274  Training loss = 1.7287  Validation loss = 0.6633  \n",
      "\n",
      "Fold: 7  Epoch: 275  Training loss = 1.7285  Validation loss = 0.6633  \n",
      "\n",
      "Fold: 7  Epoch: 276  Training loss = 1.7283  Validation loss = 0.6641  \n",
      "\n",
      "Fold: 7  Epoch: 277  Training loss = 1.7280  Validation loss = 0.6634  \n",
      "\n",
      "Fold: 7  Epoch: 278  Training loss = 1.7278  Validation loss = 0.6617  \n",
      "\n",
      "Fold: 7  Epoch: 279  Training loss = 1.7275  Validation loss = 0.6607  \n",
      "\n",
      "Fold: 7  Epoch: 280  Training loss = 1.7273  Validation loss = 0.6604  \n",
      "\n",
      "Fold: 7  Epoch: 281  Training loss = 1.7272  Validation loss = 0.6600  \n",
      "\n",
      "Fold: 7  Epoch: 282  Training loss = 1.7269  Validation loss = 0.6591  \n",
      "\n",
      "Fold: 7  Epoch: 283  Training loss = 1.7268  Validation loss = 0.6592  \n",
      "\n",
      "Fold: 7  Epoch: 284  Training loss = 1.7266  Validation loss = 0.6590  \n",
      "\n",
      "Fold: 7  Epoch: 285  Training loss = 1.7264  Validation loss = 0.6588  \n",
      "\n",
      "Fold: 7  Epoch: 286  Training loss = 1.7262  Validation loss = 0.6579  \n",
      "\n",
      "Fold: 7  Epoch: 287  Training loss = 1.7259  Validation loss = 0.6579  \n",
      "\n",
      "Fold: 7  Epoch: 288  Training loss = 1.7257  Validation loss = 0.6568  \n",
      "\n",
      "Fold: 7  Epoch: 289  Training loss = 1.7254  Validation loss = 0.6563  \n",
      "\n",
      "Fold: 7  Epoch: 290  Training loss = 1.7251  Validation loss = 0.6554  \n",
      "\n",
      "Fold: 7  Epoch: 291  Training loss = 1.7249  Validation loss = 0.6546  \n",
      "\n",
      "Fold: 7  Epoch: 292  Training loss = 1.7247  Validation loss = 0.6535  \n",
      "\n",
      "Fold: 7  Epoch: 293  Training loss = 1.7245  Validation loss = 0.6537  \n",
      "\n",
      "Fold: 7  Epoch: 294  Training loss = 1.7243  Validation loss = 0.6530  \n",
      "\n",
      "Fold: 7  Epoch: 295  Training loss = 1.7241  Validation loss = 0.6526  \n",
      "\n",
      "Fold: 7  Epoch: 296  Training loss = 1.7239  Validation loss = 0.6524  \n",
      "\n",
      "Fold: 7  Epoch: 297  Training loss = 1.7238  Validation loss = 0.6527  \n",
      "\n",
      "Fold: 7  Epoch: 298  Training loss = 1.7235  Validation loss = 0.6521  \n",
      "\n",
      "Fold: 7  Epoch: 299  Training loss = 1.7233  Validation loss = 0.6522  \n",
      "\n",
      "Fold: 7  Epoch: 300  Training loss = 1.7232  Validation loss = 0.6520  \n",
      "\n",
      "Fold: 7  Epoch: 301  Training loss = 1.7229  Validation loss = 0.6513  \n",
      "\n",
      "Fold: 7  Epoch: 302  Training loss = 1.7227  Validation loss = 0.6510  \n",
      "\n",
      "Fold: 7  Epoch: 303  Training loss = 1.7225  Validation loss = 0.6512  \n",
      "\n",
      "Fold: 7  Epoch: 304  Training loss = 1.7223  Validation loss = 0.6503  \n",
      "\n",
      "Fold: 7  Epoch: 305  Training loss = 1.7221  Validation loss = 0.6502  \n",
      "\n",
      "Fold: 7  Epoch: 306  Training loss = 1.7219  Validation loss = 0.6493  \n",
      "\n",
      "Fold: 7  Epoch: 307  Training loss = 1.7216  Validation loss = 0.6488  \n",
      "\n",
      "Fold: 7  Epoch: 308  Training loss = 1.7214  Validation loss = 0.6489  \n",
      "\n",
      "Fold: 7  Epoch: 309  Training loss = 1.7212  Validation loss = 0.6486  \n",
      "\n",
      "Fold: 7  Epoch: 310  Training loss = 1.7209  Validation loss = 0.6472  \n",
      "\n",
      "Fold: 7  Epoch: 311  Training loss = 1.7206  Validation loss = 0.6468  \n",
      "\n",
      "Fold: 7  Epoch: 312  Training loss = 1.7204  Validation loss = 0.6462  \n",
      "\n",
      "Fold: 7  Epoch: 313  Training loss = 1.7202  Validation loss = 0.6461  \n",
      "\n",
      "Fold: 7  Epoch: 314  Training loss = 1.7201  Validation loss = 0.6464  \n",
      "\n",
      "Fold: 7  Epoch: 315  Training loss = 1.7199  Validation loss = 0.6468  \n",
      "\n",
      "Fold: 7  Epoch: 316  Training loss = 1.7197  Validation loss = 0.6466  \n",
      "\n",
      "Fold: 7  Epoch: 317  Training loss = 1.7196  Validation loss = 0.6474  \n",
      "\n",
      "Fold: 7  Epoch: 318  Training loss = 1.7195  Validation loss = 0.6474  \n",
      "\n",
      "Fold: 7  Epoch: 319  Training loss = 1.7193  Validation loss = 0.6476  \n",
      "\n",
      "Fold: 7  Epoch: 320  Training loss = 1.7191  Validation loss = 0.6476  \n",
      "\n",
      "Fold: 7  Epoch: 321  Training loss = 1.7189  Validation loss = 0.6472  \n",
      "\n",
      "Fold: 7  Epoch: 322  Training loss = 1.7186  Validation loss = 0.6459  \n",
      "\n",
      "Fold: 7  Epoch: 323  Training loss = 1.7183  Validation loss = 0.6451  \n",
      "\n",
      "Fold: 7  Epoch: 324  Training loss = 1.7182  Validation loss = 0.6452  \n",
      "\n",
      "Fold: 7  Epoch: 325  Training loss = 1.7180  Validation loss = 0.6454  \n",
      "\n",
      "Fold: 7  Epoch: 326  Training loss = 1.7177  Validation loss = 0.6443  \n",
      "\n",
      "Fold: 7  Epoch: 327  Training loss = 1.7175  Validation loss = 0.6442  \n",
      "\n",
      "Fold: 7  Epoch: 328  Training loss = 1.7174  Validation loss = 0.6440  \n",
      "\n",
      "Fold: 7  Epoch: 329  Training loss = 1.7172  Validation loss = 0.6445  \n",
      "\n",
      "Fold: 7  Epoch: 330  Training loss = 1.7170  Validation loss = 0.6439  \n",
      "\n",
      "Fold: 7  Epoch: 331  Training loss = 1.7168  Validation loss = 0.6440  \n",
      "\n",
      "Fold: 7  Epoch: 332  Training loss = 1.7166  Validation loss = 0.6437  \n",
      "\n",
      "Fold: 7  Epoch: 333  Training loss = 1.7165  Validation loss = 0.6443  \n",
      "\n",
      "Fold: 7  Epoch: 334  Training loss = 1.7163  Validation loss = 0.6441  \n",
      "\n",
      "Fold: 7  Epoch: 335  Training loss = 1.7160  Validation loss = 0.6436  \n",
      "\n",
      "Fold: 7  Epoch: 336  Training loss = 1.7158  Validation loss = 0.6440  \n",
      "\n",
      "Fold: 7  Epoch: 337  Training loss = 1.7156  Validation loss = 0.6445  \n",
      "\n",
      "Fold: 7  Epoch: 338  Training loss = 1.7154  Validation loss = 0.6446  \n",
      "\n",
      "Fold: 7  Epoch: 339  Training loss = 1.7152  Validation loss = 0.6443  \n",
      "\n",
      "Fold: 7  Epoch: 340  Training loss = 1.7151  Validation loss = 0.6453  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 335  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 1.6663  Validation loss = 5.6859  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 1.6662  Validation loss = 5.6864  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 1.6660  Validation loss = 5.6857  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 1.6659  Validation loss = 5.6857  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 1.6657  Validation loss = 5.6849  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 1.6654  Validation loss = 5.6834  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 1.6653  Validation loss = 5.6843  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 1.6649  Validation loss = 5.6824  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 1.6647  Validation loss = 5.6816  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 1.6644  Validation loss = 5.6799  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 1.6641  Validation loss = 5.6778  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 1.6640  Validation loss = 5.6768  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 1.6639  Validation loss = 5.6777  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 1.6636  Validation loss = 5.6772  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 1.6636  Validation loss = 5.6786  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 1.6633  Validation loss = 5.6779  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 1.6632  Validation loss = 5.6785  \n",
      "\n",
      "Fold: 8  Epoch: 18  Training loss = 1.6630  Validation loss = 5.6780  \n",
      "\n",
      "Fold: 8  Epoch: 19  Training loss = 1.6628  Validation loss = 5.6768  \n",
      "\n",
      "Fold: 8  Epoch: 20  Training loss = 1.6624  Validation loss = 5.6753  \n",
      "\n",
      "Fold: 8  Epoch: 21  Training loss = 1.6622  Validation loss = 5.6751  \n",
      "\n",
      "Fold: 8  Epoch: 22  Training loss = 1.6621  Validation loss = 5.6770  \n",
      "\n",
      "Fold: 8  Epoch: 23  Training loss = 1.6620  Validation loss = 5.6769  \n",
      "\n",
      "Fold: 8  Epoch: 24  Training loss = 1.6617  Validation loss = 5.6765  \n",
      "\n",
      "Fold: 8  Epoch: 25  Training loss = 1.6615  Validation loss = 5.6755  \n",
      "\n",
      "Fold: 8  Epoch: 26  Training loss = 1.6612  Validation loss = 5.6738  \n",
      "\n",
      "Fold: 8  Epoch: 27  Training loss = 1.6610  Validation loss = 5.6750  \n",
      "\n",
      "Fold: 8  Epoch: 28  Training loss = 1.6608  Validation loss = 5.6747  \n",
      "\n",
      "Fold: 8  Epoch: 29  Training loss = 1.6605  Validation loss = 5.6732  \n",
      "\n",
      "Fold: 8  Epoch: 30  Training loss = 1.6604  Validation loss = 5.6738  \n",
      "\n",
      "Fold: 8  Epoch: 31  Training loss = 1.6601  Validation loss = 5.6724  \n",
      "\n",
      "Fold: 8  Epoch: 32  Training loss = 1.6599  Validation loss = 5.6715  \n",
      "\n",
      "Fold: 8  Epoch: 33  Training loss = 1.6597  Validation loss = 5.6714  \n",
      "\n",
      "Fold: 8  Epoch: 34  Training loss = 1.6594  Validation loss = 5.6715  \n",
      "\n",
      "Fold: 8  Epoch: 35  Training loss = 1.6591  Validation loss = 5.6709  \n",
      "\n",
      "Fold: 8  Epoch: 36  Training loss = 1.6591  Validation loss = 5.6726  \n",
      "\n",
      "Fold: 8  Epoch: 37  Training loss = 1.6587  Validation loss = 5.6719  \n",
      "\n",
      "Fold: 8  Epoch: 38  Training loss = 1.6585  Validation loss = 5.6714  \n",
      "\n",
      "Fold: 8  Epoch: 39  Training loss = 1.6584  Validation loss = 5.6715  \n",
      "\n",
      "Fold: 8  Epoch: 40  Training loss = 1.6582  Validation loss = 5.6713  \n",
      "\n",
      "Fold: 8  Epoch: 41  Training loss = 1.6580  Validation loss = 5.6716  \n",
      "\n",
      "Fold: 8  Epoch: 42  Training loss = 1.6577  Validation loss = 5.6713  \n",
      "\n",
      "Fold: 8  Epoch: 43  Training loss = 1.6575  Validation loss = 5.6715  \n",
      "\n",
      "Fold: 8  Epoch: 44  Training loss = 1.6574  Validation loss = 5.6712  \n",
      "\n",
      "Fold: 8  Epoch: 45  Training loss = 1.6572  Validation loss = 5.6708  \n",
      "\n",
      "Fold: 8  Epoch: 46  Training loss = 1.6569  Validation loss = 5.6693  \n",
      "\n",
      "Fold: 8  Epoch: 47  Training loss = 1.6569  Validation loss = 5.6703  \n",
      "\n",
      "Fold: 8  Epoch: 48  Training loss = 1.6566  Validation loss = 5.6701  \n",
      "\n",
      "Fold: 8  Epoch: 49  Training loss = 1.6563  Validation loss = 5.6693  \n",
      "\n",
      "Fold: 8  Epoch: 50  Training loss = 1.6561  Validation loss = 5.6699  \n",
      "\n",
      "Fold: 8  Epoch: 51  Training loss = 1.6558  Validation loss = 5.6695  \n",
      "\n",
      "Fold: 8  Epoch: 52  Training loss = 1.6555  Validation loss = 5.6692  \n",
      "\n",
      "Fold: 8  Epoch: 53  Training loss = 1.6553  Validation loss = 5.6685  \n",
      "\n",
      "Fold: 8  Epoch: 54  Training loss = 1.6550  Validation loss = 5.6672  \n",
      "\n",
      "Fold: 8  Epoch: 55  Training loss = 1.6548  Validation loss = 5.6674  \n",
      "\n",
      "Fold: 8  Epoch: 56  Training loss = 1.6546  Validation loss = 5.6669  \n",
      "\n",
      "Fold: 8  Epoch: 57  Training loss = 1.6545  Validation loss = 5.6677  \n",
      "\n",
      "Fold: 8  Epoch: 58  Training loss = 1.6544  Validation loss = 5.6669  \n",
      "\n",
      "Fold: 8  Epoch: 59  Training loss = 1.6541  Validation loss = 5.6670  \n",
      "\n",
      "Fold: 8  Epoch: 60  Training loss = 1.6540  Validation loss = 5.6685  \n",
      "\n",
      "Fold: 8  Epoch: 61  Training loss = 1.6538  Validation loss = 5.6684  \n",
      "\n",
      "Fold: 8  Epoch: 62  Training loss = 1.6536  Validation loss = 5.6686  \n",
      "\n",
      "Fold: 8  Epoch: 63  Training loss = 1.6535  Validation loss = 5.6686  \n",
      "\n",
      "Fold: 8  Epoch: 64  Training loss = 1.6533  Validation loss = 5.6691  \n",
      "\n",
      "Fold: 8  Epoch: 65  Training loss = 1.6531  Validation loss = 5.6687  \n",
      "\n",
      "Fold: 8  Epoch: 66  Training loss = 1.6529  Validation loss = 5.6689  \n",
      "\n",
      "Fold: 8  Epoch: 67  Training loss = 1.6526  Validation loss = 5.6681  \n",
      "\n",
      "Fold: 8  Epoch: 68  Training loss = 1.6525  Validation loss = 5.6682  \n",
      "\n",
      "Fold: 8  Epoch: 69  Training loss = 1.6523  Validation loss = 5.6676  \n",
      "\n",
      "Fold: 8  Epoch: 70  Training loss = 1.6521  Validation loss = 5.6675  \n",
      "\n",
      "Fold: 8  Epoch: 71  Training loss = 1.6519  Validation loss = 5.6679  \n",
      "\n",
      "Fold: 8  Epoch: 72  Training loss = 1.6517  Validation loss = 5.6674  \n",
      "\n",
      "Fold: 8  Epoch: 73  Training loss = 1.6515  Validation loss = 5.6661  \n",
      "\n",
      "Fold: 8  Epoch: 74  Training loss = 1.6513  Validation loss = 5.6664  \n",
      "\n",
      "Fold: 8  Epoch: 75  Training loss = 1.6510  Validation loss = 5.6660  \n",
      "\n",
      "Fold: 8  Epoch: 76  Training loss = 1.6507  Validation loss = 5.6646  \n",
      "\n",
      "Fold: 8  Epoch: 77  Training loss = 1.6505  Validation loss = 5.6655  \n",
      "\n",
      "Fold: 8  Epoch: 78  Training loss = 1.6503  Validation loss = 5.6654  \n",
      "\n",
      "Fold: 8  Epoch: 79  Training loss = 1.6501  Validation loss = 5.6642  \n",
      "\n",
      "Fold: 8  Epoch: 80  Training loss = 1.6498  Validation loss = 5.6633  \n",
      "\n",
      "Fold: 8  Epoch: 81  Training loss = 1.6497  Validation loss = 5.6639  \n",
      "\n",
      "Fold: 8  Epoch: 82  Training loss = 1.6494  Validation loss = 5.6628  \n",
      "\n",
      "Fold: 8  Epoch: 83  Training loss = 1.6492  Validation loss = 5.6621  \n",
      "\n",
      "Fold: 8  Epoch: 84  Training loss = 1.6491  Validation loss = 5.6642  \n",
      "\n",
      "Fold: 8  Epoch: 85  Training loss = 1.6489  Validation loss = 5.6630  \n",
      "\n",
      "Fold: 8  Epoch: 86  Training loss = 1.6486  Validation loss = 5.6611  \n",
      "\n",
      "Fold: 8  Epoch: 87  Training loss = 1.6484  Validation loss = 5.6608  \n",
      "\n",
      "Fold: 8  Epoch: 88  Training loss = 1.6483  Validation loss = 5.6605  \n",
      "\n",
      "Fold: 8  Epoch: 89  Training loss = 1.6482  Validation loss = 5.6602  \n",
      "\n",
      "Fold: 8  Epoch: 90  Training loss = 1.6481  Validation loss = 5.6608  \n",
      "\n",
      "Fold: 8  Epoch: 91  Training loss = 1.6480  Validation loss = 5.6623  \n",
      "\n",
      "Fold: 8  Epoch: 92  Training loss = 1.6478  Validation loss = 5.6628  \n",
      "\n",
      "Fold: 8  Epoch: 93  Training loss = 1.6476  Validation loss = 5.6634  \n",
      "\n",
      "Fold: 8  Epoch: 94  Training loss = 1.6474  Validation loss = 5.6639  \n",
      "\n",
      "Fold: 8  Epoch: 95  Training loss = 1.6471  Validation loss = 5.6635  \n",
      "\n",
      "Fold: 8  Epoch: 96  Training loss = 1.6469  Validation loss = 5.6632  \n",
      "\n",
      "Fold: 8  Epoch: 97  Training loss = 1.6468  Validation loss = 5.6620  \n",
      "\n",
      "Fold: 8  Epoch: 98  Training loss = 1.6467  Validation loss = 5.6622  \n",
      "\n",
      "Fold: 8  Epoch: 99  Training loss = 1.6465  Validation loss = 5.6605  \n",
      "\n",
      "Fold: 8  Epoch: 100  Training loss = 1.6464  Validation loss = 5.6615  \n",
      "\n",
      "Fold: 8  Epoch: 101  Training loss = 1.6463  Validation loss = 5.6617  \n",
      "\n",
      "Fold: 8  Epoch: 102  Training loss = 1.6461  Validation loss = 5.6631  \n",
      "\n",
      "Fold: 8  Epoch: 103  Training loss = 1.6459  Validation loss = 5.6619  \n",
      "\n",
      "Fold: 8  Epoch: 104  Training loss = 1.6457  Validation loss = 5.6602  \n",
      "\n",
      "Fold: 8  Epoch: 105  Training loss = 1.6454  Validation loss = 5.6597  \n",
      "\n",
      "Fold: 8  Epoch: 106  Training loss = 1.6451  Validation loss = 5.6581  \n",
      "\n",
      "Fold: 8  Epoch: 107  Training loss = 1.6450  Validation loss = 5.6583  \n",
      "\n",
      "Fold: 8  Epoch: 108  Training loss = 1.6448  Validation loss = 5.6578  \n",
      "\n",
      "Fold: 8  Epoch: 109  Training loss = 1.6445  Validation loss = 5.6565  \n",
      "\n",
      "Fold: 8  Epoch: 110  Training loss = 1.6442  Validation loss = 5.6559  \n",
      "\n",
      "Fold: 8  Epoch: 111  Training loss = 1.6441  Validation loss = 5.6570  \n",
      "\n",
      "Fold: 8  Epoch: 112  Training loss = 1.6439  Validation loss = 5.6582  \n",
      "\n",
      "Fold: 8  Epoch: 113  Training loss = 1.6438  Validation loss = 5.6579  \n",
      "\n",
      "Fold: 8  Epoch: 114  Training loss = 1.6435  Validation loss = 5.6566  \n",
      "\n",
      "Fold: 8  Epoch: 115  Training loss = 1.6433  Validation loss = 5.6570  \n",
      "\n",
      "Fold: 8  Epoch: 116  Training loss = 1.6432  Validation loss = 5.6552  \n",
      "\n",
      "Fold: 8  Epoch: 117  Training loss = 1.6430  Validation loss = 5.6540  \n",
      "\n",
      "Fold: 8  Epoch: 118  Training loss = 1.6428  Validation loss = 5.6539  \n",
      "\n",
      "Fold: 8  Epoch: 119  Training loss = 1.6426  Validation loss = 5.6536  \n",
      "\n",
      "Fold: 8  Epoch: 120  Training loss = 1.6424  Validation loss = 5.6531  \n",
      "\n",
      "Fold: 8  Epoch: 121  Training loss = 1.6422  Validation loss = 5.6530  \n",
      "\n",
      "Fold: 8  Epoch: 122  Training loss = 1.6420  Validation loss = 5.6533  \n",
      "\n",
      "Fold: 8  Epoch: 123  Training loss = 1.6417  Validation loss = 5.6528  \n",
      "\n",
      "Fold: 8  Epoch: 124  Training loss = 1.6416  Validation loss = 5.6511  \n",
      "\n",
      "Fold: 8  Epoch: 125  Training loss = 1.6414  Validation loss = 5.6499  \n",
      "\n",
      "Fold: 8  Epoch: 126  Training loss = 1.6412  Validation loss = 5.6512  \n",
      "\n",
      "Fold: 8  Epoch: 127  Training loss = 1.6410  Validation loss = 5.6503  \n",
      "\n",
      "Fold: 8  Epoch: 128  Training loss = 1.6408  Validation loss = 5.6489  \n",
      "\n",
      "Fold: 8  Epoch: 129  Training loss = 1.6408  Validation loss = 5.6508  \n",
      "\n",
      "Fold: 8  Epoch: 130  Training loss = 1.6405  Validation loss = 5.6518  \n",
      "\n",
      "Fold: 8  Epoch: 131  Training loss = 1.6404  Validation loss = 5.6518  \n",
      "\n",
      "Fold: 8  Epoch: 132  Training loss = 1.6403  Validation loss = 5.6520  \n",
      "\n",
      "Fold: 8  Epoch: 133  Training loss = 1.6401  Validation loss = 5.6518  \n",
      "\n",
      "Fold: 8  Epoch: 134  Training loss = 1.6399  Validation loss = 5.6519  \n",
      "\n",
      "Fold: 8  Epoch: 135  Training loss = 1.6398  Validation loss = 5.6524  \n",
      "\n",
      "Fold: 8  Epoch: 136  Training loss = 1.6396  Validation loss = 5.6533  \n",
      "\n",
      "Fold: 8  Epoch: 137  Training loss = 1.6395  Validation loss = 5.6521  \n",
      "\n",
      "Fold: 8  Epoch: 138  Training loss = 1.6394  Validation loss = 5.6522  \n",
      "\n",
      "Fold: 8  Epoch: 139  Training loss = 1.6393  Validation loss = 5.6524  \n",
      "\n",
      "Fold: 8  Epoch: 140  Training loss = 1.6391  Validation loss = 5.6537  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 128  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 2.1304  Validation loss = 8.9494  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 2.1300  Validation loss = 8.9472  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 2.1294  Validation loss = 8.9436  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 2.1289  Validation loss = 8.9411  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 2.1289  Validation loss = 8.9412  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 2.1285  Validation loss = 8.9390  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 2.1279  Validation loss = 8.9357  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 2.1275  Validation loss = 8.9340  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 2.1274  Validation loss = 8.9333  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 2.1267  Validation loss = 8.9296  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 2.1264  Validation loss = 8.9282  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 2.1261  Validation loss = 8.9262  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 2.1255  Validation loss = 8.9232  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 2.1255  Validation loss = 8.9235  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 2.1251  Validation loss = 8.9215  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 2.1249  Validation loss = 8.9209  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 2.1245  Validation loss = 8.9186  \n",
      "\n",
      "Fold: 9  Epoch: 18  Training loss = 2.1241  Validation loss = 8.9162  \n",
      "\n",
      "Fold: 9  Epoch: 19  Training loss = 2.1236  Validation loss = 8.9135  \n",
      "\n",
      "Fold: 9  Epoch: 20  Training loss = 2.1232  Validation loss = 8.9116  \n",
      "\n",
      "Fold: 9  Epoch: 21  Training loss = 2.1227  Validation loss = 8.9088  \n",
      "\n",
      "Fold: 9  Epoch: 22  Training loss = 2.1225  Validation loss = 8.9077  \n",
      "\n",
      "Fold: 9  Epoch: 23  Training loss = 2.1224  Validation loss = 8.9080  \n",
      "\n",
      "Fold: 9  Epoch: 24  Training loss = 2.1221  Validation loss = 8.9057  \n",
      "\n",
      "Fold: 9  Epoch: 25  Training loss = 2.1217  Validation loss = 8.9042  \n",
      "\n",
      "Fold: 9  Epoch: 26  Training loss = 2.1212  Validation loss = 8.9010  \n",
      "\n",
      "Fold: 9  Epoch: 27  Training loss = 2.1207  Validation loss = 8.8982  \n",
      "\n",
      "Fold: 9  Epoch: 28  Training loss = 2.1203  Validation loss = 8.8955  \n",
      "\n",
      "Fold: 9  Epoch: 29  Training loss = 2.1197  Validation loss = 8.8920  \n",
      "\n",
      "Fold: 9  Epoch: 30  Training loss = 2.1194  Validation loss = 8.8903  \n",
      "\n",
      "Fold: 9  Epoch: 31  Training loss = 2.1193  Validation loss = 8.8902  \n",
      "\n",
      "Fold: 9  Epoch: 32  Training loss = 2.1194  Validation loss = 8.8916  \n",
      "\n",
      "Fold: 9  Epoch: 33  Training loss = 2.1193  Validation loss = 8.8921  \n",
      "\n",
      "Fold: 9  Epoch: 34  Training loss = 2.1192  Validation loss = 8.8919  \n",
      "\n",
      "Fold: 9  Epoch: 35  Training loss = 2.1191  Validation loss = 8.8920  \n",
      "\n",
      "Fold: 9  Epoch: 36  Training loss = 2.1190  Validation loss = 8.8919  \n",
      "\n",
      "Fold: 9  Epoch: 37  Training loss = 2.1186  Validation loss = 8.8897  \n",
      "\n",
      "Fold: 9  Epoch: 38  Training loss = 2.1181  Validation loss = 8.8867  \n",
      "\n",
      "Fold: 9  Epoch: 39  Training loss = 2.1177  Validation loss = 8.8851  \n",
      "\n",
      "Fold: 9  Epoch: 40  Training loss = 2.1173  Validation loss = 8.8827  \n",
      "\n",
      "Fold: 9  Epoch: 41  Training loss = 2.1169  Validation loss = 8.8801  \n",
      "\n",
      "Fold: 9  Epoch: 42  Training loss = 2.1166  Validation loss = 8.8787  \n",
      "\n",
      "Fold: 9  Epoch: 43  Training loss = 2.1163  Validation loss = 8.8769  \n",
      "\n",
      "Fold: 9  Epoch: 44  Training loss = 2.1159  Validation loss = 8.8744  \n",
      "\n",
      "Fold: 9  Epoch: 45  Training loss = 2.1156  Validation loss = 8.8727  \n",
      "\n",
      "Fold: 9  Epoch: 46  Training loss = 2.1155  Validation loss = 8.8729  \n",
      "\n",
      "Fold: 9  Epoch: 47  Training loss = 2.1153  Validation loss = 8.8724  \n",
      "\n",
      "Fold: 9  Epoch: 48  Training loss = 2.1150  Validation loss = 8.8710  \n",
      "\n",
      "Fold: 9  Epoch: 49  Training loss = 2.1145  Validation loss = 8.8673  \n",
      "\n",
      "Fold: 9  Epoch: 50  Training loss = 2.1138  Validation loss = 8.8633  \n",
      "\n",
      "Fold: 9  Epoch: 51  Training loss = 2.1137  Validation loss = 8.8629  \n",
      "\n",
      "Fold: 9  Epoch: 52  Training loss = 2.1133  Validation loss = 8.8608  \n",
      "\n",
      "Fold: 9  Epoch: 53  Training loss = 2.1132  Validation loss = 8.8608  \n",
      "\n",
      "Fold: 9  Epoch: 54  Training loss = 2.1129  Validation loss = 8.8591  \n",
      "\n",
      "Fold: 9  Epoch: 55  Training loss = 2.1128  Validation loss = 8.8587  \n",
      "\n",
      "Fold: 9  Epoch: 56  Training loss = 2.1127  Validation loss = 8.8594  \n",
      "\n",
      "Fold: 9  Epoch: 57  Training loss = 2.1125  Validation loss = 8.8583  \n",
      "\n",
      "Fold: 9  Epoch: 58  Training loss = 2.1124  Validation loss = 8.8586  \n",
      "\n",
      "Fold: 9  Epoch: 59  Training loss = 2.1122  Validation loss = 8.8577  \n",
      "\n",
      "Fold: 9  Epoch: 60  Training loss = 2.1120  Validation loss = 8.8566  \n",
      "\n",
      "Fold: 9  Epoch: 61  Training loss = 2.1118  Validation loss = 8.8560  \n",
      "\n",
      "Fold: 9  Epoch: 62  Training loss = 2.1115  Validation loss = 8.8546  \n",
      "\n",
      "Fold: 9  Epoch: 63  Training loss = 2.1113  Validation loss = 8.8534  \n",
      "\n",
      "Fold: 9  Epoch: 64  Training loss = 2.1114  Validation loss = 8.8547  \n",
      "\n",
      "Fold: 9  Epoch: 65  Training loss = 2.1110  Validation loss = 8.8528  \n",
      "\n",
      "Fold: 9  Epoch: 66  Training loss = 2.1109  Validation loss = 8.8524  \n",
      "\n",
      "Fold: 9  Epoch: 67  Training loss = 2.1108  Validation loss = 8.8529  \n",
      "\n",
      "Fold: 9  Epoch: 68  Training loss = 2.1105  Validation loss = 8.8509  \n",
      "\n",
      "Fold: 9  Epoch: 69  Training loss = 2.1103  Validation loss = 8.8500  \n",
      "\n",
      "Fold: 9  Epoch: 70  Training loss = 2.1100  Validation loss = 8.8483  \n",
      "\n",
      "Fold: 9  Epoch: 71  Training loss = 2.1096  Validation loss = 8.8464  \n",
      "\n",
      "Fold: 9  Epoch: 72  Training loss = 2.1092  Validation loss = 8.8435  \n",
      "\n",
      "Fold: 9  Epoch: 73  Training loss = 2.1088  Validation loss = 8.8413  \n",
      "\n",
      "Fold: 9  Epoch: 74  Training loss = 2.1085  Validation loss = 8.8395  \n",
      "\n",
      "Fold: 9  Epoch: 75  Training loss = 2.1082  Validation loss = 8.8380  \n",
      "\n",
      "Fold: 9  Epoch: 76  Training loss = 2.1080  Validation loss = 8.8369  \n",
      "\n",
      "Fold: 9  Epoch: 77  Training loss = 2.1079  Validation loss = 8.8371  \n",
      "\n",
      "Fold: 9  Epoch: 78  Training loss = 2.1076  Validation loss = 8.8355  \n",
      "\n",
      "Fold: 9  Epoch: 79  Training loss = 2.1074  Validation loss = 8.8344  \n",
      "\n",
      "Fold: 9  Epoch: 80  Training loss = 2.1069  Validation loss = 8.8314  \n",
      "\n",
      "Fold: 9  Epoch: 81  Training loss = 2.1066  Validation loss = 8.8297  \n",
      "\n",
      "Fold: 9  Epoch: 82  Training loss = 2.1064  Validation loss = 8.8289  \n",
      "\n",
      "Fold: 9  Epoch: 83  Training loss = 2.1060  Validation loss = 8.8261  \n",
      "\n",
      "Fold: 9  Epoch: 84  Training loss = 2.1056  Validation loss = 8.8238  \n",
      "\n",
      "Fold: 9  Epoch: 85  Training loss = 2.1053  Validation loss = 8.8218  \n",
      "\n",
      "Fold: 9  Epoch: 86  Training loss = 2.1048  Validation loss = 8.8186  \n",
      "\n",
      "Fold: 9  Epoch: 87  Training loss = 2.1045  Validation loss = 8.8166  \n",
      "\n",
      "Fold: 9  Epoch: 88  Training loss = 2.1043  Validation loss = 8.8163  \n",
      "\n",
      "Fold: 9  Epoch: 89  Training loss = 2.1040  Validation loss = 8.8146  \n",
      "\n",
      "Fold: 9  Epoch: 90  Training loss = 2.1037  Validation loss = 8.8125  \n",
      "\n",
      "Fold: 9  Epoch: 91  Training loss = 2.1034  Validation loss = 8.8111  \n",
      "\n",
      "Fold: 9  Epoch: 92  Training loss = 2.1033  Validation loss = 8.8114  \n",
      "\n",
      "Fold: 9  Epoch: 93  Training loss = 2.1030  Validation loss = 8.8099  \n",
      "\n",
      "Fold: 9  Epoch: 94  Training loss = 2.1027  Validation loss = 8.8076  \n",
      "\n",
      "Fold: 9  Epoch: 95  Training loss = 2.1024  Validation loss = 8.8065  \n",
      "\n",
      "Fold: 9  Epoch: 96  Training loss = 2.1020  Validation loss = 8.8038  \n",
      "\n",
      "Fold: 9  Epoch: 97  Training loss = 2.1016  Validation loss = 8.8012  \n",
      "\n",
      "Fold: 9  Epoch: 98  Training loss = 2.1013  Validation loss = 8.7997  \n",
      "\n",
      "Fold: 9  Epoch: 99  Training loss = 2.1013  Validation loss = 8.7999  \n",
      "\n",
      "Fold: 9  Epoch: 100  Training loss = 2.1013  Validation loss = 8.8010  \n",
      "\n",
      "Fold: 9  Epoch: 101  Training loss = 2.1010  Validation loss = 8.7988  \n",
      "\n",
      "Fold: 9  Epoch: 102  Training loss = 2.1007  Validation loss = 8.7975  \n",
      "\n",
      "Fold: 9  Epoch: 103  Training loss = 2.1005  Validation loss = 8.7964  \n",
      "\n",
      "Fold: 9  Epoch: 104  Training loss = 2.1002  Validation loss = 8.7944  \n",
      "\n",
      "Fold: 9  Epoch: 105  Training loss = 2.0997  Validation loss = 8.7907  \n",
      "\n",
      "Fold: 9  Epoch: 106  Training loss = 2.0994  Validation loss = 8.7888  \n",
      "\n",
      "Fold: 9  Epoch: 107  Training loss = 2.0991  Validation loss = 8.7870  \n",
      "\n",
      "Fold: 9  Epoch: 108  Training loss = 2.0989  Validation loss = 8.7861  \n",
      "\n",
      "Fold: 9  Epoch: 109  Training loss = 2.0987  Validation loss = 8.7854  \n",
      "\n",
      "Fold: 9  Epoch: 110  Training loss = 2.0984  Validation loss = 8.7835  \n",
      "\n",
      "Fold: 9  Epoch: 111  Training loss = 2.0981  Validation loss = 8.7818  \n",
      "\n",
      "Fold: 9  Epoch: 112  Training loss = 2.0979  Validation loss = 8.7805  \n",
      "\n",
      "Fold: 9  Epoch: 113  Training loss = 2.0977  Validation loss = 8.7790  \n",
      "\n",
      "Fold: 9  Epoch: 114  Training loss = 2.0974  Validation loss = 8.7768  \n",
      "\n",
      "Fold: 9  Epoch: 115  Training loss = 2.0971  Validation loss = 8.7754  \n",
      "\n",
      "Fold: 9  Epoch: 116  Training loss = 2.0970  Validation loss = 8.7753  \n",
      "\n",
      "Fold: 9  Epoch: 117  Training loss = 2.0969  Validation loss = 8.7749  \n",
      "\n",
      "Fold: 9  Epoch: 118  Training loss = 2.0964  Validation loss = 8.7717  \n",
      "\n",
      "Fold: 9  Epoch: 119  Training loss = 2.0961  Validation loss = 8.7699  \n",
      "\n",
      "Fold: 9  Epoch: 120  Training loss = 2.0957  Validation loss = 8.7668  \n",
      "\n",
      "Fold: 9  Epoch: 121  Training loss = 2.0954  Validation loss = 8.7653  \n",
      "\n",
      "Fold: 9  Epoch: 122  Training loss = 2.0952  Validation loss = 8.7641  \n",
      "\n",
      "Fold: 9  Epoch: 123  Training loss = 2.0946  Validation loss = 8.7587  \n",
      "\n",
      "Fold: 9  Epoch: 124  Training loss = 2.0945  Validation loss = 8.7584  \n",
      "\n",
      "Fold: 9  Epoch: 125  Training loss = 2.0941  Validation loss = 8.7557  \n",
      "\n",
      "Fold: 9  Epoch: 126  Training loss = 2.0939  Validation loss = 8.7549  \n",
      "\n",
      "Fold: 9  Epoch: 127  Training loss = 2.0935  Validation loss = 8.7523  \n",
      "\n",
      "Fold: 9  Epoch: 128  Training loss = 2.0934  Validation loss = 8.7517  \n",
      "\n",
      "Fold: 9  Epoch: 129  Training loss = 2.0933  Validation loss = 8.7518  \n",
      "\n",
      "Fold: 9  Epoch: 130  Training loss = 2.0931  Validation loss = 8.7515  \n",
      "\n",
      "Fold: 9  Epoch: 131  Training loss = 2.0928  Validation loss = 8.7495  \n",
      "\n",
      "Fold: 9  Epoch: 132  Training loss = 2.0929  Validation loss = 8.7511  \n",
      "\n",
      "Fold: 9  Epoch: 133  Training loss = 2.0927  Validation loss = 8.7515  \n",
      "\n",
      "Fold: 9  Epoch: 134  Training loss = 2.0926  Validation loss = 8.7508  \n",
      "\n",
      "Fold: 9  Epoch: 135  Training loss = 2.0923  Validation loss = 8.7487  \n",
      "\n",
      "Fold: 9  Epoch: 136  Training loss = 2.0923  Validation loss = 8.7495  \n",
      "\n",
      "Fold: 9  Epoch: 137  Training loss = 2.0922  Validation loss = 8.7500  \n",
      "\n",
      "Fold: 9  Epoch: 138  Training loss = 2.0921  Validation loss = 8.7496  \n",
      "\n",
      "Fold: 9  Epoch: 139  Training loss = 2.0919  Validation loss = 8.7487  \n",
      "\n",
      "Fold: 9  Epoch: 140  Training loss = 2.0915  Validation loss = 8.7453  \n",
      "\n",
      "Fold: 9  Epoch: 141  Training loss = 2.0912  Validation loss = 8.7432  \n",
      "\n",
      "Fold: 9  Epoch: 142  Training loss = 2.0909  Validation loss = 8.7420  \n",
      "\n",
      "Fold: 9  Epoch: 143  Training loss = 2.0908  Validation loss = 8.7414  \n",
      "\n",
      "Fold: 9  Epoch: 144  Training loss = 2.0908  Validation loss = 8.7419  \n",
      "\n",
      "Fold: 9  Epoch: 145  Training loss = 2.0906  Validation loss = 8.7407  \n",
      "\n",
      "Fold: 9  Epoch: 146  Training loss = 2.0904  Validation loss = 8.7403  \n",
      "\n",
      "Fold: 9  Epoch: 147  Training loss = 2.0902  Validation loss = 8.7392  \n",
      "\n",
      "Fold: 9  Epoch: 148  Training loss = 2.0900  Validation loss = 8.7383  \n",
      "\n",
      "Fold: 9  Epoch: 149  Training loss = 2.0898  Validation loss = 8.7375  \n",
      "\n",
      "Fold: 9  Epoch: 150  Training loss = 2.0896  Validation loss = 8.7364  \n",
      "\n",
      "Fold: 9  Epoch: 151  Training loss = 2.0894  Validation loss = 8.7362  \n",
      "\n",
      "Fold: 9  Epoch: 152  Training loss = 2.0893  Validation loss = 8.7360  \n",
      "\n",
      "Fold: 9  Epoch: 153  Training loss = 2.0892  Validation loss = 8.7360  \n",
      "\n",
      "Fold: 9  Epoch: 154  Training loss = 2.0892  Validation loss = 8.7368  \n",
      "\n",
      "Fold: 9  Epoch: 155  Training loss = 2.0888  Validation loss = 8.7337  \n",
      "\n",
      "Fold: 9  Epoch: 156  Training loss = 2.0884  Validation loss = 8.7313  \n",
      "\n",
      "Fold: 9  Epoch: 157  Training loss = 2.0881  Validation loss = 8.7287  \n",
      "\n",
      "Fold: 9  Epoch: 158  Training loss = 2.0876  Validation loss = 8.7238  \n",
      "\n",
      "Fold: 9  Epoch: 159  Training loss = 2.0874  Validation loss = 8.7233  \n",
      "\n",
      "Fold: 9  Epoch: 160  Training loss = 2.0872  Validation loss = 8.7221  \n",
      "\n",
      "Fold: 9  Epoch: 161  Training loss = 2.0870  Validation loss = 8.7207  \n",
      "\n",
      "Fold: 9  Epoch: 162  Training loss = 2.0868  Validation loss = 8.7197  \n",
      "\n",
      "Fold: 9  Epoch: 163  Training loss = 2.0866  Validation loss = 8.7178  \n",
      "\n",
      "Fold: 9  Epoch: 164  Training loss = 2.0865  Validation loss = 8.7182  \n",
      "\n",
      "Fold: 9  Epoch: 165  Training loss = 2.0863  Validation loss = 8.7171  \n",
      "\n",
      "Fold: 9  Epoch: 166  Training loss = 2.0862  Validation loss = 8.7168  \n",
      "\n",
      "Fold: 9  Epoch: 167  Training loss = 2.0859  Validation loss = 8.7155  \n",
      "\n",
      "Fold: 9  Epoch: 168  Training loss = 2.0857  Validation loss = 8.7147  \n",
      "\n",
      "Fold: 9  Epoch: 169  Training loss = 2.0854  Validation loss = 8.7125  \n",
      "\n",
      "Fold: 9  Epoch: 170  Training loss = 2.0852  Validation loss = 8.7110  \n",
      "\n",
      "Fold: 9  Epoch: 171  Training loss = 2.0850  Validation loss = 8.7103  \n",
      "\n",
      "Fold: 9  Epoch: 172  Training loss = 2.0848  Validation loss = 8.7095  \n",
      "\n",
      "Fold: 9  Epoch: 173  Training loss = 2.0846  Validation loss = 8.7081  \n",
      "\n",
      "Fold: 9  Epoch: 174  Training loss = 2.0842  Validation loss = 8.7054  \n",
      "\n",
      "Fold: 9  Epoch: 175  Training loss = 2.0840  Validation loss = 8.7033  \n",
      "\n",
      "Fold: 9  Epoch: 176  Training loss = 2.0839  Validation loss = 8.7041  \n",
      "\n",
      "Fold: 9  Epoch: 177  Training loss = 2.0838  Validation loss = 8.7049  \n",
      "\n",
      "Fold: 9  Epoch: 178  Training loss = 2.0835  Validation loss = 8.7023  \n",
      "\n",
      "Fold: 9  Epoch: 179  Training loss = 2.0832  Validation loss = 8.7007  \n",
      "\n",
      "Fold: 9  Epoch: 180  Training loss = 2.0831  Validation loss = 8.6997  \n",
      "\n",
      "Fold: 9  Epoch: 181  Training loss = 2.0829  Validation loss = 8.6993  \n",
      "\n",
      "Fold: 9  Epoch: 182  Training loss = 2.0826  Validation loss = 8.6976  \n",
      "\n",
      "Fold: 9  Epoch: 183  Training loss = 2.0823  Validation loss = 8.6949  \n",
      "\n",
      "Fold: 9  Epoch: 184  Training loss = 2.0821  Validation loss = 8.6940  \n",
      "\n",
      "Fold: 9  Epoch: 185  Training loss = 2.0819  Validation loss = 8.6930  \n",
      "\n",
      "Fold: 9  Epoch: 186  Training loss = 2.0817  Validation loss = 8.6914  \n",
      "\n",
      "Fold: 9  Epoch: 187  Training loss = 2.0814  Validation loss = 8.6899  \n",
      "\n",
      "Fold: 9  Epoch: 188  Training loss = 2.0813  Validation loss = 8.6893  \n",
      "\n",
      "Fold: 9  Epoch: 189  Training loss = 2.0811  Validation loss = 8.6883  \n",
      "\n",
      "Fold: 9  Epoch: 190  Training loss = 2.0810  Validation loss = 8.6894  \n",
      "\n",
      "Fold: 9  Epoch: 191  Training loss = 2.0808  Validation loss = 8.6872  \n",
      "\n",
      "Fold: 9  Epoch: 192  Training loss = 2.0805  Validation loss = 8.6848  \n",
      "\n",
      "Fold: 9  Epoch: 193  Training loss = 2.0802  Validation loss = 8.6821  \n",
      "\n",
      "Fold: 9  Epoch: 194  Training loss = 2.0801  Validation loss = 8.6834  \n",
      "\n",
      "Fold: 9  Epoch: 195  Training loss = 2.0800  Validation loss = 8.6837  \n",
      "\n",
      "Fold: 9  Epoch: 196  Training loss = 2.0797  Validation loss = 8.6814  \n",
      "\n",
      "Fold: 9  Epoch: 197  Training loss = 2.0794  Validation loss = 8.6792  \n",
      "\n",
      "Fold: 9  Epoch: 198  Training loss = 2.0791  Validation loss = 8.6766  \n",
      "\n",
      "Fold: 9  Epoch: 199  Training loss = 2.0791  Validation loss = 8.6768  \n",
      "\n",
      "Fold: 9  Epoch: 200  Training loss = 2.0788  Validation loss = 8.6751  \n",
      "\n",
      "Fold: 9  Epoch: 201  Training loss = 2.0786  Validation loss = 8.6743  \n",
      "\n",
      "Fold: 9  Epoch: 202  Training loss = 2.0784  Validation loss = 8.6734  \n",
      "\n",
      "Fold: 9  Epoch: 203  Training loss = 2.0782  Validation loss = 8.6716  \n",
      "\n",
      "Fold: 9  Epoch: 204  Training loss = 2.0780  Validation loss = 8.6707  \n",
      "\n",
      "Fold: 9  Epoch: 205  Training loss = 2.0779  Validation loss = 8.6720  \n",
      "\n",
      "Fold: 9  Epoch: 206  Training loss = 2.0778  Validation loss = 8.6716  \n",
      "\n",
      "Fold: 9  Epoch: 207  Training loss = 2.0774  Validation loss = 8.6681  \n",
      "\n",
      "Fold: 9  Epoch: 208  Training loss = 2.0774  Validation loss = 8.6693  \n",
      "\n",
      "Fold: 9  Epoch: 209  Training loss = 2.0775  Validation loss = 8.6720  \n",
      "\n",
      "Fold: 9  Epoch: 210  Training loss = 2.0772  Validation loss = 8.6702  \n",
      "\n",
      "Fold: 9  Epoch: 211  Training loss = 2.0769  Validation loss = 8.6663  \n",
      "\n",
      "Fold: 9  Epoch: 212  Training loss = 2.0767  Validation loss = 8.6659  \n",
      "\n",
      "Fold: 9  Epoch: 213  Training loss = 2.0766  Validation loss = 8.6660  \n",
      "\n",
      "Fold: 9  Epoch: 214  Training loss = 2.0764  Validation loss = 8.6646  \n",
      "\n",
      "Fold: 9  Epoch: 215  Training loss = 2.0762  Validation loss = 8.6636  \n",
      "\n",
      "Fold: 9  Epoch: 216  Training loss = 2.0759  Validation loss = 8.6623  \n",
      "\n",
      "Fold: 9  Epoch: 217  Training loss = 2.0757  Validation loss = 8.6605  \n",
      "\n",
      "Fold: 9  Epoch: 218  Training loss = 2.0756  Validation loss = 8.6603  \n",
      "\n",
      "Fold: 9  Epoch: 219  Training loss = 2.0755  Validation loss = 8.6602  \n",
      "\n",
      "Fold: 9  Epoch: 220  Training loss = 2.0754  Validation loss = 8.6612  \n",
      "\n",
      "Fold: 9  Epoch: 221  Training loss = 2.0753  Validation loss = 8.6607  \n",
      "\n",
      "Fold: 9  Epoch: 222  Training loss = 2.0751  Validation loss = 8.6593  \n",
      "\n",
      "Fold: 9  Epoch: 223  Training loss = 2.0748  Validation loss = 8.6572  \n",
      "\n",
      "Fold: 9  Epoch: 224  Training loss = 2.0746  Validation loss = 8.6545  \n",
      "\n",
      "Fold: 9  Epoch: 225  Training loss = 2.0744  Validation loss = 8.6536  \n",
      "\n",
      "Fold: 9  Epoch: 226  Training loss = 2.0741  Validation loss = 8.6521  \n",
      "\n",
      "Fold: 9  Epoch: 227  Training loss = 2.0739  Validation loss = 8.6508  \n",
      "\n",
      "Fold: 9  Epoch: 228  Training loss = 2.0738  Validation loss = 8.6509  \n",
      "\n",
      "Fold: 9  Epoch: 229  Training loss = 2.0735  Validation loss = 8.6490  \n",
      "\n",
      "Fold: 9  Epoch: 230  Training loss = 2.0733  Validation loss = 8.6467  \n",
      "\n",
      "Fold: 9  Epoch: 231  Training loss = 2.0730  Validation loss = 8.6434  \n",
      "\n",
      "Fold: 9  Epoch: 232  Training loss = 2.0727  Validation loss = 8.6420  \n",
      "\n",
      "Fold: 9  Epoch: 233  Training loss = 2.0725  Validation loss = 8.6403  \n",
      "\n",
      "Fold: 9  Epoch: 234  Training loss = 2.0724  Validation loss = 8.6408  \n",
      "\n",
      "Fold: 9  Epoch: 235  Training loss = 2.0721  Validation loss = 8.6382  \n",
      "\n",
      "Fold: 9  Epoch: 236  Training loss = 2.0719  Validation loss = 8.6364  \n",
      "\n",
      "Fold: 9  Epoch: 237  Training loss = 2.0718  Validation loss = 8.6362  \n",
      "\n",
      "Fold: 9  Epoch: 238  Training loss = 2.0716  Validation loss = 8.6347  \n",
      "\n",
      "Fold: 9  Epoch: 239  Training loss = 2.0714  Validation loss = 8.6342  \n",
      "\n",
      "Fold: 9  Epoch: 240  Training loss = 2.0712  Validation loss = 8.6330  \n",
      "\n",
      "Fold: 9  Epoch: 241  Training loss = 2.0712  Validation loss = 8.6342  \n",
      "\n",
      "Fold: 9  Epoch: 242  Training loss = 2.0711  Validation loss = 8.6350  \n",
      "\n",
      "Fold: 9  Epoch: 243  Training loss = 2.0709  Validation loss = 8.6346  \n",
      "\n",
      "Fold: 9  Epoch: 244  Training loss = 2.0708  Validation loss = 8.6339  \n",
      "\n",
      "Fold: 9  Epoch: 245  Training loss = 2.0706  Validation loss = 8.6336  \n",
      "\n",
      "Fold: 9  Epoch: 246  Training loss = 2.0703  Validation loss = 8.6313  \n",
      "\n",
      "Fold: 9  Epoch: 247  Training loss = 2.0702  Validation loss = 8.6308  \n",
      "\n",
      "Fold: 9  Epoch: 248  Training loss = 2.0700  Validation loss = 8.6298  \n",
      "\n",
      "Fold: 9  Epoch: 249  Training loss = 2.0698  Validation loss = 8.6288  \n",
      "\n",
      "Fold: 9  Epoch: 250  Training loss = 2.0695  Validation loss = 8.6263  \n",
      "\n",
      "Fold: 9  Epoch: 251  Training loss = 2.0693  Validation loss = 8.6247  \n",
      "\n",
      "Fold: 9  Epoch: 252  Training loss = 2.0690  Validation loss = 8.6227  \n",
      "\n",
      "Fold: 9  Epoch: 253  Training loss = 2.0688  Validation loss = 8.6212  \n",
      "\n",
      "Fold: 9  Epoch: 254  Training loss = 2.0686  Validation loss = 8.6199  \n",
      "\n",
      "Fold: 9  Epoch: 255  Training loss = 2.0685  Validation loss = 8.6190  \n",
      "\n",
      "Fold: 9  Epoch: 256  Training loss = 2.0683  Validation loss = 8.6190  \n",
      "\n",
      "Fold: 9  Epoch: 257  Training loss = 2.0680  Validation loss = 8.6166  \n",
      "\n",
      "Fold: 9  Epoch: 258  Training loss = 2.0678  Validation loss = 8.6154  \n",
      "\n",
      "Fold: 9  Epoch: 259  Training loss = 2.0675  Validation loss = 8.6128  \n",
      "\n",
      "Fold: 9  Epoch: 260  Training loss = 2.0673  Validation loss = 8.6104  \n",
      "\n",
      "Fold: 9  Epoch: 261  Training loss = 2.0671  Validation loss = 8.6103  \n",
      "\n",
      "Fold: 9  Epoch: 262  Training loss = 2.0669  Validation loss = 8.6085  \n",
      "\n",
      "Fold: 9  Epoch: 263  Training loss = 2.0668  Validation loss = 8.6087  \n",
      "\n",
      "Fold: 9  Epoch: 264  Training loss = 2.0666  Validation loss = 8.6072  \n",
      "\n",
      "Fold: 9  Epoch: 265  Training loss = 2.0665  Validation loss = 8.6068  \n",
      "\n",
      "Fold: 9  Epoch: 266  Training loss = 2.0661  Validation loss = 8.6033  \n",
      "\n",
      "Fold: 9  Epoch: 267  Training loss = 2.0659  Validation loss = 8.6023  \n",
      "\n",
      "Fold: 9  Epoch: 268  Training loss = 2.0658  Validation loss = 8.6019  \n",
      "\n",
      "Fold: 9  Epoch: 269  Training loss = 2.0655  Validation loss = 8.5996  \n",
      "\n",
      "Fold: 9  Epoch: 270  Training loss = 2.0653  Validation loss = 8.5977  \n",
      "\n",
      "Fold: 9  Epoch: 271  Training loss = 2.0653  Validation loss = 8.5986  \n",
      "\n",
      "Fold: 9  Epoch: 272  Training loss = 2.0650  Validation loss = 8.5969  \n",
      "\n",
      "Fold: 9  Epoch: 273  Training loss = 2.0649  Validation loss = 8.5963  \n",
      "\n",
      "Fold: 9  Epoch: 274  Training loss = 2.0647  Validation loss = 8.5964  \n",
      "\n",
      "Fold: 9  Epoch: 275  Training loss = 2.0645  Validation loss = 8.5954  \n",
      "\n",
      "Fold: 9  Epoch: 276  Training loss = 2.0643  Validation loss = 8.5934  \n",
      "\n",
      "Fold: 9  Epoch: 277  Training loss = 2.0641  Validation loss = 8.5919  \n",
      "\n",
      "Fold: 9  Epoch: 278  Training loss = 2.0639  Validation loss = 8.5899  \n",
      "\n",
      "Fold: 9  Epoch: 279  Training loss = 2.0637  Validation loss = 8.5890  \n",
      "\n",
      "Fold: 9  Epoch: 280  Training loss = 2.0636  Validation loss = 8.5895  \n",
      "\n",
      "Fold: 9  Epoch: 281  Training loss = 2.0634  Validation loss = 8.5869  \n",
      "\n",
      "Fold: 9  Epoch: 282  Training loss = 2.0631  Validation loss = 8.5850  \n",
      "\n",
      "Fold: 9  Epoch: 283  Training loss = 2.0630  Validation loss = 8.5849  \n",
      "\n",
      "Fold: 9  Epoch: 284  Training loss = 2.0628  Validation loss = 8.5843  \n",
      "\n",
      "Fold: 9  Epoch: 285  Training loss = 2.0626  Validation loss = 8.5830  \n",
      "\n",
      "Fold: 9  Epoch: 286  Training loss = 2.0624  Validation loss = 8.5812  \n",
      "\n",
      "Fold: 9  Epoch: 287  Training loss = 2.0622  Validation loss = 8.5799  \n",
      "\n",
      "Fold: 9  Epoch: 288  Training loss = 2.0619  Validation loss = 8.5783  \n",
      "\n",
      "Fold: 9  Epoch: 289  Training loss = 2.0617  Validation loss = 8.5760  \n",
      "\n",
      "Fold: 9  Epoch: 290  Training loss = 2.0616  Validation loss = 8.5758  \n",
      "\n",
      "Fold: 9  Epoch: 291  Training loss = 2.0614  Validation loss = 8.5762  \n",
      "\n",
      "Fold: 9  Epoch: 292  Training loss = 2.0613  Validation loss = 8.5772  \n",
      "\n",
      "Fold: 9  Epoch: 293  Training loss = 2.0612  Validation loss = 8.5769  \n",
      "\n",
      "Fold: 9  Epoch: 294  Training loss = 2.0611  Validation loss = 8.5770  \n",
      "\n",
      "Fold: 9  Epoch: 295  Training loss = 2.0609  Validation loss = 8.5767  \n",
      "\n",
      "Fold: 9  Epoch: 296  Training loss = 2.0608  Validation loss = 8.5774  \n",
      "\n",
      "Fold: 9  Epoch: 297  Training loss = 2.0606  Validation loss = 8.5751  \n",
      "\n",
      "Fold: 9  Epoch: 298  Training loss = 2.0603  Validation loss = 8.5731  \n",
      "\n",
      "Fold: 9  Epoch: 299  Training loss = 2.0601  Validation loss = 8.5699  \n",
      "\n",
      "Fold: 9  Epoch: 300  Training loss = 2.0600  Validation loss = 8.5697  \n",
      "\n",
      "Fold: 9  Epoch: 301  Training loss = 2.0597  Validation loss = 8.5685  \n",
      "\n",
      "Fold: 9  Epoch: 302  Training loss = 2.0596  Validation loss = 8.5678  \n",
      "\n",
      "Fold: 9  Epoch: 303  Training loss = 2.0594  Validation loss = 8.5661  \n",
      "\n",
      "Fold: 9  Epoch: 304  Training loss = 2.0592  Validation loss = 8.5652  \n",
      "\n",
      "Fold: 9  Epoch: 305  Training loss = 2.0590  Validation loss = 8.5640  \n",
      "\n",
      "Fold: 9  Epoch: 306  Training loss = 2.0588  Validation loss = 8.5620  \n",
      "\n",
      "Fold: 9  Epoch: 307  Training loss = 2.0586  Validation loss = 8.5610  \n",
      "\n",
      "Fold: 9  Epoch: 308  Training loss = 2.0585  Validation loss = 8.5617  \n",
      "\n",
      "Fold: 9  Epoch: 309  Training loss = 2.0584  Validation loss = 8.5630  \n",
      "\n",
      "Fold: 9  Epoch: 310  Training loss = 2.0583  Validation loss = 8.5625  \n",
      "\n",
      "Fold: 9  Epoch: 311  Training loss = 2.0581  Validation loss = 8.5617  \n",
      "\n",
      "Fold: 9  Epoch: 312  Training loss = 2.0579  Validation loss = 8.5604  \n",
      "\n",
      "Fold: 9  Epoch: 313  Training loss = 2.0578  Validation loss = 8.5595  \n",
      "\n",
      "Fold: 9  Epoch: 314  Training loss = 2.0575  Validation loss = 8.5572  \n",
      "\n",
      "Fold: 9  Epoch: 315  Training loss = 2.0572  Validation loss = 8.5529  \n",
      "\n",
      "Fold: 9  Epoch: 316  Training loss = 2.0571  Validation loss = 8.5532  \n",
      "\n",
      "Fold: 9  Epoch: 317  Training loss = 2.0569  Validation loss = 8.5517  \n",
      "\n",
      "Fold: 9  Epoch: 318  Training loss = 2.0567  Validation loss = 8.5501  \n",
      "\n",
      "Fold: 9  Epoch: 319  Training loss = 2.0565  Validation loss = 8.5492  \n",
      "\n",
      "Fold: 9  Epoch: 320  Training loss = 2.0564  Validation loss = 8.5486  \n",
      "\n",
      "Fold: 9  Epoch: 321  Training loss = 2.0563  Validation loss = 8.5493  \n",
      "\n",
      "Fold: 9  Epoch: 322  Training loss = 2.0560  Validation loss = 8.5458  \n",
      "\n",
      "Fold: 9  Epoch: 323  Training loss = 2.0559  Validation loss = 8.5462  \n",
      "\n",
      "Fold: 9  Epoch: 324  Training loss = 2.0558  Validation loss = 8.5478  \n",
      "\n",
      "Fold: 9  Epoch: 325  Training loss = 2.0557  Validation loss = 8.5476  \n",
      "\n",
      "Fold: 9  Epoch: 326  Training loss = 2.0556  Validation loss = 8.5487  \n",
      "\n",
      "Fold: 9  Epoch: 327  Training loss = 2.0554  Validation loss = 8.5488  \n",
      "\n",
      "Fold: 9  Epoch: 328  Training loss = 2.0552  Validation loss = 8.5471  \n",
      "\n",
      "Fold: 9  Epoch: 329  Training loss = 2.0551  Validation loss = 8.5482  \n",
      "\n",
      "Fold: 9  Epoch: 330  Training loss = 2.0550  Validation loss = 8.5490  \n",
      "\n",
      "Fold: 9  Epoch: 331  Training loss = 2.0548  Validation loss = 8.5478  \n",
      "\n",
      "Fold: 9  Epoch: 332  Training loss = 2.0547  Validation loss = 8.5479  \n",
      "\n",
      "Fold: 9  Epoch: 333  Training loss = 2.0546  Validation loss = 8.5481  \n",
      "\n",
      "Fold: 9  Epoch: 334  Training loss = 2.0544  Validation loss = 8.5468  \n",
      "\n",
      "Fold: 9  Epoch: 335  Training loss = 2.0544  Validation loss = 8.5486  \n",
      "\n",
      "Fold: 9  Epoch: 336  Training loss = 2.0543  Validation loss = 8.5510  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 322  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 2.9174  Validation loss = 5.3495  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 2.9161  Validation loss = 5.3442  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 2.9152  Validation loss = 5.3393  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 2.9144  Validation loss = 5.3354  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 2.9129  Validation loss = 5.3299  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 2.9126  Validation loss = 5.3284  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 2.9119  Validation loss = 5.3237  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 2.9107  Validation loss = 5.3173  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 2.9091  Validation loss = 5.3076  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 2.9080  Validation loss = 5.3012  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 2.9071  Validation loss = 5.2968  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 2.9057  Validation loss = 5.2903  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 2.9046  Validation loss = 5.2843  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 2.9038  Validation loss = 5.2802  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 2.9021  Validation loss = 5.2690  \n",
      "\n",
      "Fold: 10  Epoch: 16  Training loss = 2.9018  Validation loss = 5.2681  \n",
      "\n",
      "Fold: 10  Epoch: 17  Training loss = 2.9008  Validation loss = 5.2638  \n",
      "\n",
      "Fold: 10  Epoch: 18  Training loss = 2.9005  Validation loss = 5.2607  \n",
      "\n",
      "Fold: 10  Epoch: 19  Training loss = 2.8994  Validation loss = 5.2547  \n",
      "\n",
      "Fold: 10  Epoch: 20  Training loss = 2.8981  Validation loss = 5.2463  \n",
      "\n",
      "Fold: 10  Epoch: 21  Training loss = 2.8976  Validation loss = 5.2440  \n",
      "\n",
      "Fold: 10  Epoch: 22  Training loss = 2.8967  Validation loss = 5.2403  \n",
      "\n",
      "Fold: 10  Epoch: 23  Training loss = 2.8961  Validation loss = 5.2354  \n",
      "\n",
      "Fold: 10  Epoch: 24  Training loss = 2.8948  Validation loss = 5.2308  \n",
      "\n",
      "Fold: 10  Epoch: 25  Training loss = 2.8937  Validation loss = 5.2254  \n",
      "\n",
      "Fold: 10  Epoch: 26  Training loss = 2.8932  Validation loss = 5.2226  \n",
      "\n",
      "Fold: 10  Epoch: 27  Training loss = 2.8924  Validation loss = 5.2179  \n",
      "\n",
      "Fold: 10  Epoch: 28  Training loss = 2.8915  Validation loss = 5.2131  \n",
      "\n",
      "Fold: 10  Epoch: 29  Training loss = 2.8905  Validation loss = 5.2091  \n",
      "\n",
      "Fold: 10  Epoch: 30  Training loss = 2.8896  Validation loss = 5.2028  \n",
      "\n",
      "Fold: 10  Epoch: 31  Training loss = 2.8889  Validation loss = 5.1994  \n",
      "\n",
      "Fold: 10  Epoch: 32  Training loss = 2.8889  Validation loss = 5.1982  \n",
      "\n",
      "Fold: 10  Epoch: 33  Training loss = 2.8880  Validation loss = 5.1922  \n",
      "\n",
      "Fold: 10  Epoch: 34  Training loss = 2.8867  Validation loss = 5.1848  \n",
      "\n",
      "Fold: 10  Epoch: 35  Training loss = 2.8858  Validation loss = 5.1772  \n",
      "\n",
      "Fold: 10  Epoch: 36  Training loss = 2.8852  Validation loss = 5.1733  \n",
      "\n",
      "Fold: 10  Epoch: 37  Training loss = 2.8845  Validation loss = 5.1710  \n",
      "\n",
      "Fold: 10  Epoch: 38  Training loss = 2.8834  Validation loss = 5.1647  \n",
      "\n",
      "Fold: 10  Epoch: 39  Training loss = 2.8831  Validation loss = 5.1642  \n",
      "\n",
      "Fold: 10  Epoch: 40  Training loss = 2.8824  Validation loss = 5.1586  \n",
      "\n",
      "Fold: 10  Epoch: 41  Training loss = 2.8809  Validation loss = 5.1524  \n",
      "\n",
      "Fold: 10  Epoch: 42  Training loss = 2.8802  Validation loss = 5.1470  \n",
      "\n",
      "Fold: 10  Epoch: 43  Training loss = 2.8795  Validation loss = 5.1419  \n",
      "\n",
      "Fold: 10  Epoch: 44  Training loss = 2.8777  Validation loss = 5.1309  \n",
      "\n",
      "Fold: 10  Epoch: 45  Training loss = 2.8770  Validation loss = 5.1267  \n",
      "\n",
      "Fold: 10  Epoch: 46  Training loss = 2.8768  Validation loss = 5.1245  \n",
      "\n",
      "Fold: 10  Epoch: 47  Training loss = 2.8756  Validation loss = 5.1196  \n",
      "\n",
      "Fold: 10  Epoch: 48  Training loss = 2.8746  Validation loss = 5.1147  \n",
      "\n",
      "Fold: 10  Epoch: 49  Training loss = 2.8741  Validation loss = 5.1126  \n",
      "\n",
      "Fold: 10  Epoch: 50  Training loss = 2.8735  Validation loss = 5.1120  \n",
      "\n",
      "Fold: 10  Epoch: 51  Training loss = 2.8722  Validation loss = 5.1051  \n",
      "\n",
      "Fold: 10  Epoch: 52  Training loss = 2.8714  Validation loss = 5.0998  \n",
      "\n",
      "Fold: 10  Epoch: 53  Training loss = 2.8705  Validation loss = 5.0948  \n",
      "\n",
      "Fold: 10  Epoch: 54  Training loss = 2.8694  Validation loss = 5.0902  \n",
      "\n",
      "Fold: 10  Epoch: 55  Training loss = 2.8680  Validation loss = 5.0843  \n",
      "\n",
      "Fold: 10  Epoch: 56  Training loss = 2.8668  Validation loss = 5.0782  \n",
      "\n",
      "Fold: 10  Epoch: 57  Training loss = 2.8660  Validation loss = 5.0749  \n",
      "\n",
      "Fold: 10  Epoch: 58  Training loss = 2.8647  Validation loss = 5.0699  \n",
      "\n",
      "Fold: 10  Epoch: 59  Training loss = 2.8640  Validation loss = 5.0667  \n",
      "\n",
      "Fold: 10  Epoch: 60  Training loss = 2.8632  Validation loss = 5.0634  \n",
      "\n",
      "Fold: 10  Epoch: 61  Training loss = 2.8622  Validation loss = 5.0588  \n",
      "\n",
      "Fold: 10  Epoch: 62  Training loss = 2.8615  Validation loss = 5.0549  \n",
      "\n",
      "Fold: 10  Epoch: 63  Training loss = 2.8604  Validation loss = 5.0493  \n",
      "\n",
      "Fold: 10  Epoch: 64  Training loss = 2.8594  Validation loss = 5.0444  \n",
      "\n",
      "Fold: 10  Epoch: 65  Training loss = 2.8587  Validation loss = 5.0428  \n",
      "\n",
      "Fold: 10  Epoch: 66  Training loss = 2.8578  Validation loss = 5.0377  \n",
      "\n",
      "Fold: 10  Epoch: 67  Training loss = 2.8564  Validation loss = 5.0293  \n",
      "\n",
      "Fold: 10  Epoch: 68  Training loss = 2.8550  Validation loss = 5.0220  \n",
      "\n",
      "Fold: 10  Epoch: 69  Training loss = 2.8546  Validation loss = 5.0214  \n",
      "\n",
      "Fold: 10  Epoch: 70  Training loss = 2.8535  Validation loss = 5.0161  \n",
      "\n",
      "Fold: 10  Epoch: 71  Training loss = 2.8530  Validation loss = 5.0145  \n",
      "\n",
      "Fold: 10  Epoch: 72  Training loss = 2.8528  Validation loss = 5.0143  \n",
      "\n",
      "Fold: 10  Epoch: 73  Training loss = 2.8517  Validation loss = 5.0104  \n",
      "\n",
      "Fold: 10  Epoch: 74  Training loss = 2.8505  Validation loss = 5.0041  \n",
      "\n",
      "Fold: 10  Epoch: 75  Training loss = 2.8498  Validation loss = 5.0010  \n",
      "\n",
      "Fold: 10  Epoch: 76  Training loss = 2.8484  Validation loss = 4.9926  \n",
      "\n",
      "Fold: 10  Epoch: 77  Training loss = 2.8475  Validation loss = 4.9872  \n",
      "\n",
      "Fold: 10  Epoch: 78  Training loss = 2.8467  Validation loss = 4.9828  \n",
      "\n",
      "Fold: 10  Epoch: 79  Training loss = 2.8457  Validation loss = 4.9778  \n",
      "\n",
      "Fold: 10  Epoch: 80  Training loss = 2.8449  Validation loss = 4.9732  \n",
      "\n",
      "Fold: 10  Epoch: 81  Training loss = 2.8443  Validation loss = 4.9707  \n",
      "\n",
      "Fold: 10  Epoch: 82  Training loss = 2.8430  Validation loss = 4.9621  \n",
      "\n",
      "Fold: 10  Epoch: 83  Training loss = 2.8416  Validation loss = 4.9538  \n",
      "\n",
      "Fold: 10  Epoch: 84  Training loss = 2.8407  Validation loss = 4.9494  \n",
      "\n",
      "Fold: 10  Epoch: 85  Training loss = 2.8403  Validation loss = 4.9480  \n",
      "\n",
      "Fold: 10  Epoch: 86  Training loss = 2.8396  Validation loss = 4.9451  \n",
      "\n",
      "Fold: 10  Epoch: 87  Training loss = 2.8392  Validation loss = 4.9446  \n",
      "\n",
      "Fold: 10  Epoch: 88  Training loss = 2.8382  Validation loss = 4.9412  \n",
      "\n",
      "Fold: 10  Epoch: 89  Training loss = 2.8376  Validation loss = 4.9384  \n",
      "\n",
      "Fold: 10  Epoch: 90  Training loss = 2.8366  Validation loss = 4.9330  \n",
      "\n",
      "Fold: 10  Epoch: 91  Training loss = 2.8360  Validation loss = 4.9300  \n",
      "\n",
      "Fold: 10  Epoch: 92  Training loss = 2.8349  Validation loss = 4.9237  \n",
      "\n",
      "Fold: 10  Epoch: 93  Training loss = 2.8344  Validation loss = 4.9215  \n",
      "\n",
      "Fold: 10  Epoch: 94  Training loss = 2.8336  Validation loss = 4.9177  \n",
      "\n",
      "Fold: 10  Epoch: 95  Training loss = 2.8327  Validation loss = 4.9123  \n",
      "\n",
      "Fold: 10  Epoch: 96  Training loss = 2.8317  Validation loss = 4.9074  \n",
      "\n",
      "Fold: 10  Epoch: 97  Training loss = 2.8310  Validation loss = 4.9029  \n",
      "\n",
      "Fold: 10  Epoch: 98  Training loss = 2.8304  Validation loss = 4.9007  \n",
      "\n",
      "Fold: 10  Epoch: 99  Training loss = 2.8297  Validation loss = 4.8974  \n",
      "\n",
      "Fold: 10  Epoch: 100  Training loss = 2.8295  Validation loss = 4.8972  \n",
      "\n",
      "Fold: 10  Epoch: 101  Training loss = 2.8279  Validation loss = 4.8878  \n",
      "\n",
      "Fold: 10  Epoch: 102  Training loss = 2.8273  Validation loss = 4.8842  \n",
      "\n",
      "Fold: 10  Epoch: 103  Training loss = 2.8268  Validation loss = 4.8817  \n",
      "\n",
      "Fold: 10  Epoch: 104  Training loss = 2.8260  Validation loss = 4.8791  \n",
      "\n",
      "Fold: 10  Epoch: 105  Training loss = 2.8251  Validation loss = 4.8725  \n",
      "\n",
      "Fold: 10  Epoch: 106  Training loss = 2.8240  Validation loss = 4.8654  \n",
      "\n",
      "Fold: 10  Epoch: 107  Training loss = 2.8239  Validation loss = 4.8640  \n",
      "\n",
      "Fold: 10  Epoch: 108  Training loss = 2.8233  Validation loss = 4.8603  \n",
      "\n",
      "Fold: 10  Epoch: 109  Training loss = 2.8228  Validation loss = 4.8574  \n",
      "\n",
      "Fold: 10  Epoch: 110  Training loss = 2.8227  Validation loss = 4.8564  \n",
      "\n",
      "Fold: 10  Epoch: 111  Training loss = 2.8220  Validation loss = 4.8505  \n",
      "\n",
      "Fold: 10  Epoch: 112  Training loss = 2.8212  Validation loss = 4.8474  \n",
      "\n",
      "Fold: 10  Epoch: 113  Training loss = 2.8210  Validation loss = 4.8475  \n",
      "\n",
      "Fold: 10  Epoch: 114  Training loss = 2.8203  Validation loss = 4.8436  \n",
      "\n",
      "Fold: 10  Epoch: 115  Training loss = 2.8197  Validation loss = 4.8391  \n",
      "\n",
      "Fold: 10  Epoch: 116  Training loss = 2.8192  Validation loss = 4.8355  \n",
      "\n",
      "Fold: 10  Epoch: 117  Training loss = 2.8187  Validation loss = 4.8337  \n",
      "\n",
      "Fold: 10  Epoch: 118  Training loss = 2.8186  Validation loss = 4.8338  \n",
      "\n",
      "Fold: 10  Epoch: 119  Training loss = 2.8180  Validation loss = 4.8289  \n",
      "\n",
      "Fold: 10  Epoch: 120  Training loss = 2.8178  Validation loss = 4.8282  \n",
      "\n",
      "Fold: 10  Epoch: 121  Training loss = 2.8167  Validation loss = 4.8202  \n",
      "\n",
      "Fold: 10  Epoch: 122  Training loss = 2.8163  Validation loss = 4.8168  \n",
      "\n",
      "Fold: 10  Epoch: 123  Training loss = 2.8153  Validation loss = 4.8089  \n",
      "\n",
      "Fold: 10  Epoch: 124  Training loss = 2.8146  Validation loss = 4.8061  \n",
      "\n",
      "Fold: 10  Epoch: 125  Training loss = 2.8143  Validation loss = 4.8034  \n",
      "\n",
      "Fold: 10  Epoch: 126  Training loss = 2.8138  Validation loss = 4.8004  \n",
      "\n",
      "Fold: 10  Epoch: 127  Training loss = 2.8129  Validation loss = 4.7934  \n",
      "\n",
      "Fold: 10  Epoch: 128  Training loss = 2.8123  Validation loss = 4.7880  \n",
      "\n",
      "Fold: 10  Epoch: 129  Training loss = 2.8117  Validation loss = 4.7820  \n",
      "\n",
      "Fold: 10  Epoch: 130  Training loss = 2.8109  Validation loss = 4.7761  \n",
      "\n",
      "Fold: 10  Epoch: 131  Training loss = 2.8107  Validation loss = 4.7734  \n",
      "\n",
      "Fold: 10  Epoch: 132  Training loss = 2.8105  Validation loss = 4.7736  \n",
      "\n",
      "Fold: 10  Epoch: 133  Training loss = 2.8099  Validation loss = 4.7694  \n",
      "\n",
      "Fold: 10  Epoch: 134  Training loss = 2.8090  Validation loss = 4.7621  \n",
      "\n",
      "Fold: 10  Epoch: 135  Training loss = 2.8088  Validation loss = 4.7588  \n",
      "\n",
      "Fold: 10  Epoch: 136  Training loss = 2.8083  Validation loss = 4.7565  \n",
      "\n",
      "Fold: 10  Epoch: 137  Training loss = 2.8079  Validation loss = 4.7540  \n",
      "\n",
      "Fold: 10  Epoch: 138  Training loss = 2.8070  Validation loss = 4.7490  \n",
      "\n",
      "Fold: 10  Epoch: 139  Training loss = 2.8064  Validation loss = 4.7447  \n",
      "\n",
      "Fold: 10  Epoch: 140  Training loss = 2.8058  Validation loss = 4.7408  \n",
      "\n",
      "Fold: 10  Epoch: 141  Training loss = 2.8049  Validation loss = 4.7323  \n",
      "\n",
      "Fold: 10  Epoch: 142  Training loss = 2.8043  Validation loss = 4.7287  \n",
      "\n",
      "Fold: 10  Epoch: 143  Training loss = 2.8040  Validation loss = 4.7279  \n",
      "\n",
      "Fold: 10  Epoch: 144  Training loss = 2.8030  Validation loss = 4.7212  \n",
      "\n",
      "Fold: 10  Epoch: 145  Training loss = 2.8024  Validation loss = 4.7162  \n",
      "\n",
      "Fold: 10  Epoch: 146  Training loss = 2.8015  Validation loss = 4.7097  \n",
      "\n",
      "Fold: 10  Epoch: 147  Training loss = 2.8011  Validation loss = 4.7077  \n",
      "\n",
      "Fold: 10  Epoch: 148  Training loss = 2.8004  Validation loss = 4.7024  \n",
      "\n",
      "Fold: 10  Epoch: 149  Training loss = 2.7998  Validation loss = 4.6991  \n",
      "\n",
      "Fold: 10  Epoch: 150  Training loss = 2.7994  Validation loss = 4.6947  \n",
      "\n",
      "Fold: 10  Epoch: 151  Training loss = 2.7993  Validation loss = 4.6934  \n",
      "\n",
      "Fold: 10  Epoch: 152  Training loss = 2.7987  Validation loss = 4.6923  \n",
      "\n",
      "Fold: 10  Epoch: 153  Training loss = 2.7983  Validation loss = 4.6912  \n",
      "\n",
      "Fold: 10  Epoch: 154  Training loss = 2.7977  Validation loss = 4.6868  \n",
      "\n",
      "Fold: 10  Epoch: 155  Training loss = 2.7973  Validation loss = 4.6836  \n",
      "\n",
      "Fold: 10  Epoch: 156  Training loss = 2.7968  Validation loss = 4.6758  \n",
      "\n",
      "Fold: 10  Epoch: 157  Training loss = 2.7961  Validation loss = 4.6689  \n",
      "\n",
      "Fold: 10  Epoch: 158  Training loss = 2.7960  Validation loss = 4.6656  \n",
      "\n",
      "Fold: 10  Epoch: 159  Training loss = 2.7955  Validation loss = 4.6583  \n",
      "\n",
      "Fold: 10  Epoch: 160  Training loss = 2.7952  Validation loss = 4.6586  \n",
      "\n",
      "Fold: 10  Epoch: 161  Training loss = 2.7950  Validation loss = 4.6574  \n",
      "\n",
      "Fold: 10  Epoch: 162  Training loss = 2.7943  Validation loss = 4.6441  \n",
      "\n",
      "Fold: 10  Epoch: 163  Training loss = 2.7940  Validation loss = 4.6406  \n",
      "\n",
      "Fold: 10  Epoch: 164  Training loss = 2.7935  Validation loss = 4.6379  \n",
      "\n",
      "Fold: 10  Epoch: 165  Training loss = 2.7929  Validation loss = 4.6355  \n",
      "\n",
      "Fold: 10  Epoch: 166  Training loss = 2.7925  Validation loss = 4.6357  \n",
      "\n",
      "Fold: 10  Epoch: 167  Training loss = 2.7919  Validation loss = 4.6309  \n",
      "\n",
      "Fold: 10  Epoch: 168  Training loss = 2.7918  Validation loss = 4.6290  \n",
      "\n",
      "Fold: 10  Epoch: 169  Training loss = 2.7916  Validation loss = 4.6281  \n",
      "\n",
      "Fold: 10  Epoch: 170  Training loss = 2.7913  Validation loss = 4.6275  \n",
      "\n",
      "Fold: 10  Epoch: 171  Training loss = 2.7907  Validation loss = 4.6199  \n",
      "\n",
      "Fold: 10  Epoch: 172  Training loss = 2.7903  Validation loss = 4.6133  \n",
      "\n",
      "Fold: 10  Epoch: 173  Training loss = 2.7904  Validation loss = 4.6131  \n",
      "\n",
      "Fold: 10  Epoch: 174  Training loss = 2.7893  Validation loss = 4.5999  \n",
      "\n",
      "Fold: 10  Epoch: 175  Training loss = 2.7891  Validation loss = 4.6001  \n",
      "\n",
      "Fold: 10  Epoch: 176  Training loss = 2.7883  Validation loss = 4.5900  \n",
      "\n",
      "Fold: 10  Epoch: 177  Training loss = 2.7878  Validation loss = 4.5879  \n",
      "\n",
      "Fold: 10  Epoch: 178  Training loss = 2.7869  Validation loss = 4.5821  \n",
      "\n",
      "Fold: 10  Epoch: 179  Training loss = 2.7866  Validation loss = 4.5800  \n",
      "\n",
      "Fold: 10  Epoch: 180  Training loss = 2.7862  Validation loss = 4.5774  \n",
      "\n",
      "Fold: 10  Epoch: 181  Training loss = 2.7858  Validation loss = 4.5740  \n",
      "\n",
      "Fold: 10  Epoch: 182  Training loss = 2.7854  Validation loss = 4.5702  \n",
      "\n",
      "Fold: 10  Epoch: 183  Training loss = 2.7848  Validation loss = 4.5652  \n",
      "\n",
      "Fold: 10  Epoch: 184  Training loss = 2.7844  Validation loss = 4.5626  \n",
      "\n",
      "Fold: 10  Epoch: 185  Training loss = 2.7841  Validation loss = 4.5603  \n",
      "\n",
      "Fold: 10  Epoch: 186  Training loss = 2.7836  Validation loss = 4.5565  \n",
      "\n",
      "Fold: 10  Epoch: 187  Training loss = 2.7831  Validation loss = 4.5548  \n",
      "\n",
      "Fold: 10  Epoch: 188  Training loss = 2.7823  Validation loss = 4.5481  \n",
      "\n",
      "Fold: 10  Epoch: 189  Training loss = 2.7819  Validation loss = 4.5443  \n",
      "\n",
      "Fold: 10  Epoch: 190  Training loss = 2.7812  Validation loss = 4.5401  \n",
      "\n",
      "Fold: 10  Epoch: 191  Training loss = 2.7803  Validation loss = 4.5338  \n",
      "\n",
      "Fold: 10  Epoch: 192  Training loss = 2.7797  Validation loss = 4.5293  \n",
      "\n",
      "Fold: 10  Epoch: 193  Training loss = 2.7791  Validation loss = 4.5224  \n",
      "\n",
      "Fold: 10  Epoch: 194  Training loss = 2.7786  Validation loss = 4.5196  \n",
      "\n",
      "Fold: 10  Epoch: 195  Training loss = 2.7777  Validation loss = 4.5154  \n",
      "\n",
      "Fold: 10  Epoch: 196  Training loss = 2.7775  Validation loss = 4.5143  \n",
      "\n",
      "Fold: 10  Epoch: 197  Training loss = 2.7766  Validation loss = 4.5077  \n",
      "\n",
      "Fold: 10  Epoch: 198  Training loss = 2.7759  Validation loss = 4.5026  \n",
      "\n",
      "Fold: 10  Epoch: 199  Training loss = 2.7752  Validation loss = 4.4970  \n",
      "\n",
      "Fold: 10  Epoch: 200  Training loss = 2.7746  Validation loss = 4.4945  \n",
      "\n",
      "Fold: 10  Epoch: 201  Training loss = 2.7742  Validation loss = 4.4930  \n",
      "\n",
      "Fold: 10  Epoch: 202  Training loss = 2.7735  Validation loss = 4.4865  \n",
      "\n",
      "Fold: 10  Epoch: 203  Training loss = 2.7726  Validation loss = 4.4788  \n",
      "\n",
      "Fold: 10  Epoch: 204  Training loss = 2.7720  Validation loss = 4.4753  \n",
      "\n",
      "Fold: 10  Epoch: 205  Training loss = 2.7713  Validation loss = 4.4668  \n",
      "\n",
      "Fold: 10  Epoch: 206  Training loss = 2.7709  Validation loss = 4.4667  \n",
      "\n",
      "Fold: 10  Epoch: 207  Training loss = 2.7703  Validation loss = 4.4623  \n",
      "\n",
      "Fold: 10  Epoch: 208  Training loss = 2.7696  Validation loss = 4.4560  \n",
      "\n",
      "Fold: 10  Epoch: 209  Training loss = 2.7692  Validation loss = 4.4505  \n",
      "\n",
      "Fold: 10  Epoch: 210  Training loss = 2.7688  Validation loss = 4.4506  \n",
      "\n",
      "Fold: 10  Epoch: 211  Training loss = 2.7684  Validation loss = 4.4482  \n",
      "\n",
      "Fold: 10  Epoch: 212  Training loss = 2.7682  Validation loss = 4.4488  \n",
      "\n",
      "Fold: 10  Epoch: 213  Training loss = 2.7678  Validation loss = 4.4463  \n",
      "\n",
      "Fold: 10  Epoch: 214  Training loss = 2.7674  Validation loss = 4.4433  \n",
      "\n",
      "Fold: 10  Epoch: 215  Training loss = 2.7665  Validation loss = 4.4326  \n",
      "\n",
      "Fold: 10  Epoch: 216  Training loss = 2.7659  Validation loss = 4.4282  \n",
      "\n",
      "Fold: 10  Epoch: 217  Training loss = 2.7654  Validation loss = 4.4254  \n",
      "\n",
      "Fold: 10  Epoch: 218  Training loss = 2.7646  Validation loss = 4.4177  \n",
      "\n",
      "Fold: 10  Epoch: 219  Training loss = 2.7643  Validation loss = 4.4159  \n",
      "\n",
      "Fold: 10  Epoch: 220  Training loss = 2.7643  Validation loss = 4.4173  \n",
      "\n",
      "Fold: 10  Epoch: 221  Training loss = 2.7636  Validation loss = 4.4122  \n",
      "\n",
      "Fold: 10  Epoch: 222  Training loss = 2.7634  Validation loss = 4.4113  \n",
      "\n",
      "Fold: 10  Epoch: 223  Training loss = 2.7631  Validation loss = 4.4113  \n",
      "\n",
      "Fold: 10  Epoch: 224  Training loss = 2.7626  Validation loss = 4.4071  \n",
      "\n",
      "Fold: 10  Epoch: 225  Training loss = 2.7622  Validation loss = 4.4044  \n",
      "\n",
      "Fold: 10  Epoch: 226  Training loss = 2.7618  Validation loss = 4.4011  \n",
      "\n",
      "Fold: 10  Epoch: 227  Training loss = 2.7613  Validation loss = 4.3960  \n",
      "\n",
      "Fold: 10  Epoch: 228  Training loss = 2.7608  Validation loss = 4.3955  \n",
      "\n",
      "Fold: 10  Epoch: 229  Training loss = 2.7603  Validation loss = 4.3927  \n",
      "\n",
      "Fold: 10  Epoch: 230  Training loss = 2.7593  Validation loss = 4.3835  \n",
      "\n",
      "Fold: 10  Epoch: 231  Training loss = 2.7588  Validation loss = 4.3807  \n",
      "\n",
      "Fold: 10  Epoch: 232  Training loss = 2.7587  Validation loss = 4.3822  \n",
      "\n",
      "Fold: 10  Epoch: 233  Training loss = 2.7584  Validation loss = 4.3819  \n",
      "\n",
      "Fold: 10  Epoch: 234  Training loss = 2.7580  Validation loss = 4.3801  \n",
      "\n",
      "Fold: 10  Epoch: 235  Training loss = 2.7575  Validation loss = 4.3758  \n",
      "\n",
      "Fold: 10  Epoch: 236  Training loss = 2.7569  Validation loss = 4.3718  \n",
      "\n",
      "Fold: 10  Epoch: 237  Training loss = 2.7565  Validation loss = 4.3709  \n",
      "\n",
      "Fold: 10  Epoch: 238  Training loss = 2.7561  Validation loss = 4.3694  \n",
      "\n",
      "Fold: 10  Epoch: 239  Training loss = 2.7557  Validation loss = 4.3679  \n",
      "\n",
      "Fold: 10  Epoch: 240  Training loss = 2.7555  Validation loss = 4.3686  \n",
      "\n",
      "Fold: 10  Epoch: 241  Training loss = 2.7547  Validation loss = 4.3604  \n",
      "\n",
      "Fold: 10  Epoch: 242  Training loss = 2.7544  Validation loss = 4.3590  \n",
      "\n",
      "Fold: 10  Epoch: 243  Training loss = 2.7541  Validation loss = 4.3584  \n",
      "\n",
      "Fold: 10  Epoch: 244  Training loss = 2.7537  Validation loss = 4.3575  \n",
      "\n",
      "Fold: 10  Epoch: 245  Training loss = 2.7535  Validation loss = 4.3579  \n",
      "\n",
      "Fold: 10  Epoch: 246  Training loss = 2.7531  Validation loss = 4.3566  \n",
      "\n",
      "Fold: 10  Epoch: 247  Training loss = 2.7525  Validation loss = 4.3533  \n",
      "\n",
      "Fold: 10  Epoch: 248  Training loss = 2.7521  Validation loss = 4.3492  \n",
      "\n",
      "Fold: 10  Epoch: 249  Training loss = 2.7516  Validation loss = 4.3464  \n",
      "\n",
      "Fold: 10  Epoch: 250  Training loss = 2.7512  Validation loss = 4.3452  \n",
      "\n",
      "Fold: 10  Epoch: 251  Training loss = 2.7507  Validation loss = 4.3424  \n",
      "\n",
      "Fold: 10  Epoch: 252  Training loss = 2.7502  Validation loss = 4.3399  \n",
      "\n",
      "Fold: 10  Epoch: 253  Training loss = 2.7498  Validation loss = 4.3380  \n",
      "\n",
      "Fold: 10  Epoch: 254  Training loss = 2.7496  Validation loss = 4.3381  \n",
      "\n",
      "Fold: 10  Epoch: 255  Training loss = 2.7492  Validation loss = 4.3352  \n",
      "\n",
      "Fold: 10  Epoch: 256  Training loss = 2.7489  Validation loss = 4.3338  \n",
      "\n",
      "Fold: 10  Epoch: 257  Training loss = 2.7487  Validation loss = 4.3344  \n",
      "\n",
      "Fold: 10  Epoch: 258  Training loss = 2.7483  Validation loss = 4.3319  \n",
      "\n",
      "Fold: 10  Epoch: 259  Training loss = 2.7480  Validation loss = 4.3314  \n",
      "\n",
      "Fold: 10  Epoch: 260  Training loss = 2.7471  Validation loss = 4.3240  \n",
      "\n",
      "Fold: 10  Epoch: 261  Training loss = 2.7465  Validation loss = 4.3193  \n",
      "\n",
      "Fold: 10  Epoch: 262  Training loss = 2.7464  Validation loss = 4.3203  \n",
      "\n",
      "Fold: 10  Epoch: 263  Training loss = 2.7459  Validation loss = 4.3178  \n",
      "\n",
      "Fold: 10  Epoch: 264  Training loss = 2.7454  Validation loss = 4.3139  \n",
      "\n",
      "Fold: 10  Epoch: 265  Training loss = 2.7448  Validation loss = 4.3101  \n",
      "\n",
      "Fold: 10  Epoch: 266  Training loss = 2.7440  Validation loss = 4.3041  \n",
      "\n",
      "Fold: 10  Epoch: 267  Training loss = 2.7432  Validation loss = 4.2977  \n",
      "\n",
      "Fold: 10  Epoch: 268  Training loss = 2.7425  Validation loss = 4.2920  \n",
      "\n",
      "Fold: 10  Epoch: 269  Training loss = 2.7420  Validation loss = 4.2894  \n",
      "\n",
      "Fold: 10  Epoch: 270  Training loss = 2.7415  Validation loss = 4.2846  \n",
      "\n",
      "Fold: 10  Epoch: 271  Training loss = 2.7409  Validation loss = 4.2825  \n",
      "\n",
      "Fold: 10  Epoch: 272  Training loss = 2.7404  Validation loss = 4.2787  \n",
      "\n",
      "Fold: 10  Epoch: 273  Training loss = 2.7401  Validation loss = 4.2778  \n",
      "\n",
      "Fold: 10  Epoch: 274  Training loss = 2.7398  Validation loss = 4.2756  \n",
      "\n",
      "Fold: 10  Epoch: 275  Training loss = 2.7393  Validation loss = 4.2748  \n",
      "\n",
      "Fold: 10  Epoch: 276  Training loss = 2.7391  Validation loss = 4.2750  \n",
      "\n",
      "Fold: 10  Epoch: 277  Training loss = 2.7386  Validation loss = 4.2709  \n",
      "\n",
      "Fold: 10  Epoch: 278  Training loss = 2.7381  Validation loss = 4.2682  \n",
      "\n",
      "Fold: 10  Epoch: 279  Training loss = 2.7376  Validation loss = 4.2652  \n",
      "\n",
      "Fold: 10  Epoch: 280  Training loss = 2.7371  Validation loss = 4.2640  \n",
      "\n",
      "Fold: 10  Epoch: 281  Training loss = 2.7369  Validation loss = 4.2639  \n",
      "\n",
      "Fold: 10  Epoch: 282  Training loss = 2.7366  Validation loss = 4.2616  \n",
      "\n",
      "Fold: 10  Epoch: 283  Training loss = 2.7362  Validation loss = 4.2598  \n",
      "\n",
      "Fold: 10  Epoch: 284  Training loss = 2.7358  Validation loss = 4.2570  \n",
      "\n",
      "Fold: 10  Epoch: 285  Training loss = 2.7352  Validation loss = 4.2533  \n",
      "\n",
      "Fold: 10  Epoch: 286  Training loss = 2.7350  Validation loss = 4.2548  \n",
      "\n",
      "Fold: 10  Epoch: 287  Training loss = 2.7348  Validation loss = 4.2547  \n",
      "\n",
      "Fold: 10  Epoch: 288  Training loss = 2.7343  Validation loss = 4.2529  \n",
      "\n",
      "Fold: 10  Epoch: 289  Training loss = 2.7336  Validation loss = 4.2471  \n",
      "\n",
      "Fold: 10  Epoch: 290  Training loss = 2.7330  Validation loss = 4.2443  \n",
      "\n",
      "Fold: 10  Epoch: 291  Training loss = 2.7325  Validation loss = 4.2416  \n",
      "\n",
      "Fold: 10  Epoch: 292  Training loss = 2.7320  Validation loss = 4.2381  \n",
      "\n",
      "Fold: 10  Epoch: 293  Training loss = 2.7318  Validation loss = 4.2385  \n",
      "\n",
      "Fold: 10  Epoch: 294  Training loss = 2.7312  Validation loss = 4.2339  \n",
      "\n",
      "Fold: 10  Epoch: 295  Training loss = 2.7309  Validation loss = 4.2334  \n",
      "\n",
      "Fold: 10  Epoch: 296  Training loss = 2.7303  Validation loss = 4.2297  \n",
      "\n",
      "Fold: 10  Epoch: 297  Training loss = 2.7302  Validation loss = 4.2302  \n",
      "\n",
      "Fold: 10  Epoch: 298  Training loss = 2.7298  Validation loss = 4.2295  \n",
      "\n",
      "Fold: 10  Epoch: 299  Training loss = 2.7295  Validation loss = 4.2281  \n",
      "\n",
      "Fold: 10  Epoch: 300  Training loss = 2.7291  Validation loss = 4.2265  \n",
      "\n",
      "Fold: 10  Epoch: 301  Training loss = 2.7285  Validation loss = 4.2217  \n",
      "\n",
      "Fold: 10  Epoch: 302  Training loss = 2.7283  Validation loss = 4.2224  \n",
      "\n",
      "Fold: 10  Epoch: 303  Training loss = 2.7278  Validation loss = 4.2192  \n",
      "\n",
      "Fold: 10  Epoch: 304  Training loss = 2.7273  Validation loss = 4.2164  \n",
      "\n",
      "Fold: 10  Epoch: 305  Training loss = 2.7270  Validation loss = 4.2157  \n",
      "\n",
      "Fold: 10  Epoch: 306  Training loss = 2.7265  Validation loss = 4.2134  \n",
      "\n",
      "Fold: 10  Epoch: 307  Training loss = 2.7259  Validation loss = 4.2091  \n",
      "\n",
      "Fold: 10  Epoch: 308  Training loss = 2.7254  Validation loss = 4.2074  \n",
      "\n",
      "Fold: 10  Epoch: 309  Training loss = 2.7251  Validation loss = 4.2067  \n",
      "\n",
      "Fold: 10  Epoch: 310  Training loss = 2.7246  Validation loss = 4.2027  \n",
      "\n",
      "Fold: 10  Epoch: 311  Training loss = 2.7241  Validation loss = 4.2008  \n",
      "\n",
      "Fold: 10  Epoch: 312  Training loss = 2.7239  Validation loss = 4.2011  \n",
      "\n",
      "Fold: 10  Epoch: 313  Training loss = 2.7236  Validation loss = 4.2027  \n",
      "\n",
      "Fold: 10  Epoch: 314  Training loss = 2.7232  Validation loss = 4.2000  \n",
      "\n",
      "Fold: 10  Epoch: 315  Training loss = 2.7227  Validation loss = 4.1956  \n",
      "\n",
      "Fold: 10  Epoch: 316  Training loss = 2.7223  Validation loss = 4.1952  \n",
      "\n",
      "Fold: 10  Epoch: 317  Training loss = 2.7219  Validation loss = 4.1942  \n",
      "\n",
      "Fold: 10  Epoch: 318  Training loss = 2.7216  Validation loss = 4.1933  \n",
      "\n",
      "Fold: 10  Epoch: 319  Training loss = 2.7211  Validation loss = 4.1896  \n",
      "\n",
      "Fold: 10  Epoch: 320  Training loss = 2.7208  Validation loss = 4.1902  \n",
      "\n",
      "Fold: 10  Epoch: 321  Training loss = 2.7202  Validation loss = 4.1857  \n",
      "\n",
      "Fold: 10  Epoch: 322  Training loss = 2.7196  Validation loss = 4.1812  \n",
      "\n",
      "Fold: 10  Epoch: 323  Training loss = 2.7192  Validation loss = 4.1791  \n",
      "\n",
      "Fold: 10  Epoch: 324  Training loss = 2.7189  Validation loss = 4.1799  \n",
      "\n",
      "Fold: 10  Epoch: 325  Training loss = 2.7184  Validation loss = 4.1761  \n",
      "\n",
      "Fold: 10  Epoch: 326  Training loss = 2.7182  Validation loss = 4.1777  \n",
      "\n",
      "Fold: 10  Epoch: 327  Training loss = 2.7178  Validation loss = 4.1748  \n",
      "\n",
      "Fold: 10  Epoch: 328  Training loss = 2.7174  Validation loss = 4.1720  \n",
      "\n",
      "Fold: 10  Epoch: 329  Training loss = 2.7171  Validation loss = 4.1719  \n",
      "\n",
      "Fold: 10  Epoch: 330  Training loss = 2.7166  Validation loss = 4.1685  \n",
      "\n",
      "Fold: 10  Epoch: 331  Training loss = 2.7162  Validation loss = 4.1676  \n",
      "\n",
      "Fold: 10  Epoch: 332  Training loss = 2.7157  Validation loss = 4.1657  \n",
      "\n",
      "Fold: 10  Epoch: 333  Training loss = 2.7150  Validation loss = 4.1597  \n",
      "\n",
      "Fold: 10  Epoch: 334  Training loss = 2.7148  Validation loss = 4.1592  \n",
      "\n",
      "Fold: 10  Epoch: 335  Training loss = 2.7145  Validation loss = 4.1587  \n",
      "\n",
      "Fold: 10  Epoch: 336  Training loss = 2.7141  Validation loss = 4.1563  \n",
      "\n",
      "Fold: 10  Epoch: 337  Training loss = 2.7140  Validation loss = 4.1582  \n",
      "\n",
      "Fold: 10  Epoch: 338  Training loss = 2.7135  Validation loss = 4.1559  \n",
      "\n",
      "Fold: 10  Epoch: 339  Training loss = 2.7133  Validation loss = 4.1561  \n",
      "\n",
      "Fold: 10  Epoch: 340  Training loss = 2.7127  Validation loss = 4.1517  \n",
      "\n",
      "Fold: 10  Epoch: 341  Training loss = 2.7124  Validation loss = 4.1509  \n",
      "\n",
      "Fold: 10  Epoch: 342  Training loss = 2.7121  Validation loss = 4.1505  \n",
      "\n",
      "Fold: 10  Epoch: 343  Training loss = 2.7117  Validation loss = 4.1482  \n",
      "\n",
      "Fold: 10  Epoch: 344  Training loss = 2.7112  Validation loss = 4.1443  \n",
      "\n",
      "Fold: 10  Epoch: 345  Training loss = 2.7108  Validation loss = 4.1420  \n",
      "\n",
      "Fold: 10  Epoch: 346  Training loss = 2.7105  Validation loss = 4.1404  \n",
      "\n",
      "Fold: 10  Epoch: 347  Training loss = 2.7104  Validation loss = 4.1432  \n",
      "\n",
      "Fold: 10  Epoch: 348  Training loss = 2.7102  Validation loss = 4.1431  \n",
      "\n",
      "Fold: 10  Epoch: 349  Training loss = 2.7101  Validation loss = 4.1443  \n",
      "\n",
      "Fold: 10  Epoch: 350  Training loss = 2.7094  Validation loss = 4.1407  \n",
      "\n",
      "Fold: 10  Epoch: 351  Training loss = 2.7091  Validation loss = 4.1400  \n",
      "\n",
      "Fold: 10  Epoch: 352  Training loss = 2.7086  Validation loss = 4.1363  \n",
      "\n",
      "Fold: 10  Epoch: 353  Training loss = 2.7079  Validation loss = 4.1313  \n",
      "\n",
      "Fold: 10  Epoch: 354  Training loss = 2.7075  Validation loss = 4.1291  \n",
      "\n",
      "Fold: 10  Epoch: 355  Training loss = 2.7069  Validation loss = 4.1233  \n",
      "\n",
      "Fold: 10  Epoch: 356  Training loss = 2.7067  Validation loss = 4.1237  \n",
      "\n",
      "Fold: 10  Epoch: 357  Training loss = 2.7063  Validation loss = 4.1215  \n",
      "\n",
      "Fold: 10  Epoch: 358  Training loss = 2.7061  Validation loss = 4.1231  \n",
      "\n",
      "Fold: 10  Epoch: 359  Training loss = 2.7057  Validation loss = 4.1238  \n",
      "\n",
      "Fold: 10  Epoch: 360  Training loss = 2.7055  Validation loss = 4.1226  \n",
      "\n",
      "Fold: 10  Epoch: 361  Training loss = 2.7050  Validation loss = 4.1199  \n",
      "\n",
      "Fold: 10  Epoch: 362  Training loss = 2.7048  Validation loss = 4.1192  \n",
      "\n",
      "Fold: 10  Epoch: 363  Training loss = 2.7043  Validation loss = 4.1149  \n",
      "\n",
      "Fold: 10  Epoch: 364  Training loss = 2.7039  Validation loss = 4.1129  \n",
      "\n",
      "Fold: 10  Epoch: 365  Training loss = 2.7036  Validation loss = 4.1125  \n",
      "\n",
      "Fold: 10  Epoch: 366  Training loss = 2.7031  Validation loss = 4.1075  \n",
      "\n",
      "Fold: 10  Epoch: 367  Training loss = 2.7028  Validation loss = 4.1074  \n",
      "\n",
      "Fold: 10  Epoch: 368  Training loss = 2.7025  Validation loss = 4.1055  \n",
      "\n",
      "Fold: 10  Epoch: 369  Training loss = 2.7021  Validation loss = 4.1040  \n",
      "\n",
      "Fold: 10  Epoch: 370  Training loss = 2.7017  Validation loss = 4.1035  \n",
      "\n",
      "Fold: 10  Epoch: 371  Training loss = 2.7012  Validation loss = 4.0994  \n",
      "\n",
      "Fold: 10  Epoch: 372  Training loss = 2.7009  Validation loss = 4.0983  \n",
      "\n",
      "Fold: 10  Epoch: 373  Training loss = 2.7005  Validation loss = 4.0951  \n",
      "\n",
      "Fold: 10  Epoch: 374  Training loss = 2.7003  Validation loss = 4.0962  \n",
      "\n",
      "Fold: 10  Epoch: 375  Training loss = 2.7000  Validation loss = 4.0941  \n",
      "\n",
      "Fold: 10  Epoch: 376  Training loss = 2.6996  Validation loss = 4.0914  \n",
      "\n",
      "Fold: 10  Epoch: 377  Training loss = 2.6991  Validation loss = 4.0875  \n",
      "\n",
      "Fold: 10  Epoch: 378  Training loss = 2.6985  Validation loss = 4.0813  \n",
      "\n",
      "Fold: 10  Epoch: 379  Training loss = 2.6978  Validation loss = 4.0709  \n",
      "\n",
      "Fold: 10  Epoch: 380  Training loss = 2.6973  Validation loss = 4.0654  \n",
      "\n",
      "Fold: 10  Epoch: 381  Training loss = 2.6970  Validation loss = 4.0649  \n",
      "\n",
      "Fold: 10  Epoch: 382  Training loss = 2.6967  Validation loss = 4.0649  \n",
      "\n",
      "Fold: 10  Epoch: 383  Training loss = 2.6965  Validation loss = 4.0644  \n",
      "\n",
      "Fold: 10  Epoch: 384  Training loss = 2.6962  Validation loss = 4.0633  \n",
      "\n",
      "Fold: 10  Epoch: 385  Training loss = 2.6957  Validation loss = 4.0605  \n",
      "\n",
      "Fold: 10  Epoch: 386  Training loss = 2.6952  Validation loss = 4.0546  \n",
      "\n",
      "Fold: 10  Epoch: 387  Training loss = 2.6949  Validation loss = 4.0543  \n",
      "\n",
      "Fold: 10  Epoch: 388  Training loss = 2.6945  Validation loss = 4.0522  \n",
      "\n",
      "Fold: 10  Epoch: 389  Training loss = 2.6942  Validation loss = 4.0502  \n",
      "\n",
      "Fold: 10  Epoch: 390  Training loss = 2.6939  Validation loss = 4.0471  \n",
      "\n",
      "Fold: 10  Epoch: 391  Training loss = 2.6934  Validation loss = 4.0428  \n",
      "\n",
      "Fold: 10  Epoch: 392  Training loss = 2.6932  Validation loss = 4.0423  \n",
      "\n",
      "Fold: 10  Epoch: 393  Training loss = 2.6926  Validation loss = 4.0359  \n",
      "\n",
      "Fold: 10  Epoch: 394  Training loss = 2.6921  Validation loss = 4.0306  \n",
      "\n",
      "Fold: 10  Epoch: 395  Training loss = 2.6916  Validation loss = 4.0237  \n",
      "\n",
      "Fold: 10  Epoch: 396  Training loss = 2.6912  Validation loss = 4.0207  \n",
      "\n",
      "Fold: 10  Epoch: 397  Training loss = 2.6909  Validation loss = 4.0233  \n",
      "\n",
      "Fold: 10  Epoch: 398  Training loss = 2.6907  Validation loss = 4.0213  \n",
      "\n",
      "Fold: 10  Epoch: 399  Training loss = 2.6899  Validation loss = 4.0121  \n",
      "\n",
      "Fold: 10  Epoch: 400  Training loss = 2.6896  Validation loss = 4.0093  \n",
      "\n",
      "Fold: 10  Epoch: 401  Training loss = 2.6892  Validation loss = 4.0061  \n",
      "\n",
      "Fold: 10  Epoch: 402  Training loss = 2.6887  Validation loss = 4.0003  \n",
      "\n",
      "Fold: 10  Epoch: 403  Training loss = 2.6881  Validation loss = 3.9949  \n",
      "\n",
      "Fold: 10  Epoch: 404  Training loss = 2.6878  Validation loss = 3.9918  \n",
      "\n",
      "Fold: 10  Epoch: 405  Training loss = 2.6873  Validation loss = 3.9869  \n",
      "\n",
      "Fold: 10  Epoch: 406  Training loss = 2.6869  Validation loss = 3.9842  \n",
      "\n",
      "Fold: 10  Epoch: 407  Training loss = 2.6868  Validation loss = 3.9876  \n",
      "\n",
      "Fold: 10  Epoch: 408  Training loss = 2.6864  Validation loss = 3.9859  \n",
      "\n",
      "Fold: 10  Epoch: 409  Training loss = 2.6859  Validation loss = 3.9818  \n",
      "\n",
      "Fold: 10  Epoch: 410  Training loss = 2.6856  Validation loss = 3.9815  \n",
      "\n",
      "Fold: 10  Epoch: 411  Training loss = 2.6853  Validation loss = 3.9798  \n",
      "\n",
      "Fold: 10  Epoch: 412  Training loss = 2.6847  Validation loss = 3.9751  \n",
      "\n",
      "Fold: 10  Epoch: 413  Training loss = 2.6843  Validation loss = 3.9747  \n",
      "\n",
      "Fold: 10  Epoch: 414  Training loss = 2.6841  Validation loss = 3.9751  \n",
      "\n",
      "Fold: 10  Epoch: 415  Training loss = 2.6839  Validation loss = 3.9754  \n",
      "\n",
      "Fold: 10  Epoch: 416  Training loss = 2.6835  Validation loss = 3.9735  \n",
      "\n",
      "Fold: 10  Epoch: 417  Training loss = 2.6831  Validation loss = 3.9692  \n",
      "\n",
      "Fold: 10  Epoch: 418  Training loss = 2.6827  Validation loss = 3.9701  \n",
      "\n",
      "Fold: 10  Epoch: 419  Training loss = 2.6823  Validation loss = 3.9673  \n",
      "\n",
      "Fold: 10  Epoch: 420  Training loss = 2.6822  Validation loss = 3.9718  \n",
      "\n",
      "Fold: 10  Epoch: 421  Training loss = 2.6819  Validation loss = 3.9694  \n",
      "\n",
      "Fold: 10  Epoch: 422  Training loss = 2.6815  Validation loss = 3.9700  \n",
      "\n",
      "Fold: 10  Epoch: 423  Training loss = 2.6812  Validation loss = 3.9700  \n",
      "\n",
      "Fold: 10  Epoch: 424  Training loss = 2.6808  Validation loss = 3.9668  \n",
      "\n",
      "Fold: 10  Epoch: 425  Training loss = 2.6804  Validation loss = 3.9660  \n",
      "\n",
      "Fold: 10  Epoch: 426  Training loss = 2.6801  Validation loss = 3.9641  \n",
      "\n",
      "Fold: 10  Epoch: 427  Training loss = 2.6799  Validation loss = 3.9658  \n",
      "\n",
      "Fold: 10  Epoch: 428  Training loss = 2.6796  Validation loss = 3.9673  \n",
      "\n",
      "Fold: 10  Epoch: 429  Training loss = 2.6792  Validation loss = 3.9648  \n",
      "\n",
      "Fold: 10  Epoch: 430  Training loss = 2.6789  Validation loss = 3.9641  \n",
      "\n",
      "Fold: 10  Epoch: 431  Training loss = 2.6788  Validation loss = 3.9651  \n",
      "\n",
      "Fold: 10  Epoch: 432  Training loss = 2.6782  Validation loss = 3.9606  \n",
      "\n",
      "Fold: 10  Epoch: 433  Training loss = 2.6778  Validation loss = 3.9595  \n",
      "\n",
      "Fold: 10  Epoch: 434  Training loss = 2.6774  Validation loss = 3.9577  \n",
      "\n",
      "Fold: 10  Epoch: 435  Training loss = 2.6767  Validation loss = 3.9534  \n",
      "\n",
      "Fold: 10  Epoch: 436  Training loss = 2.6762  Validation loss = 3.9504  \n",
      "\n",
      "Fold: 10  Epoch: 437  Training loss = 2.6759  Validation loss = 3.9498  \n",
      "\n",
      "Fold: 10  Epoch: 438  Training loss = 2.6754  Validation loss = 3.9472  \n",
      "\n",
      "Fold: 10  Epoch: 439  Training loss = 2.6751  Validation loss = 3.9480  \n",
      "\n",
      "Fold: 10  Epoch: 440  Training loss = 2.6746  Validation loss = 3.9440  \n",
      "\n",
      "Fold: 10  Epoch: 441  Training loss = 2.6741  Validation loss = 3.9428  \n",
      "\n",
      "Fold: 10  Epoch: 442  Training loss = 2.6737  Validation loss = 3.9419  \n",
      "\n",
      "Fold: 10  Epoch: 443  Training loss = 2.6734  Validation loss = 3.9423  \n",
      "\n",
      "Fold: 10  Epoch: 444  Training loss = 2.6731  Validation loss = 3.9437  \n",
      "\n",
      "Fold: 10  Epoch: 445  Training loss = 2.6729  Validation loss = 3.9449  \n",
      "\n",
      "Fold: 10  Epoch: 446  Training loss = 2.6726  Validation loss = 3.9447  \n",
      "\n",
      "Fold: 10  Epoch: 447  Training loss = 2.6722  Validation loss = 3.9444  \n",
      "\n",
      "Fold: 10  Epoch: 448  Training loss = 2.6716  Validation loss = 3.9413  \n",
      "\n",
      "Fold: 10  Epoch: 449  Training loss = 2.6713  Validation loss = 3.9406  \n",
      "\n",
      "Fold: 10  Epoch: 450  Training loss = 2.6708  Validation loss = 3.9390  \n",
      "\n",
      "Fold: 10  Epoch: 451  Training loss = 2.6705  Validation loss = 3.9390  \n",
      "\n",
      "Fold: 10  Epoch: 452  Training loss = 2.6700  Validation loss = 3.9390  \n",
      "\n",
      "Fold: 10  Epoch: 453  Training loss = 2.6694  Validation loss = 3.9361  \n",
      "\n",
      "Fold: 10  Epoch: 454  Training loss = 2.6690  Validation loss = 3.9336  \n",
      "\n",
      "Fold: 10  Epoch: 455  Training loss = 2.6686  Validation loss = 3.9317  \n",
      "\n",
      "Fold: 10  Epoch: 456  Training loss = 2.6681  Validation loss = 3.9286  \n",
      "\n",
      "Fold: 10  Epoch: 457  Training loss = 2.6676  Validation loss = 3.9249  \n",
      "\n",
      "Fold: 10  Epoch: 458  Training loss = 2.6674  Validation loss = 3.9247  \n",
      "\n",
      "Fold: 10  Epoch: 459  Training loss = 2.6670  Validation loss = 3.9243  \n",
      "\n",
      "Fold: 10  Epoch: 460  Training loss = 2.6666  Validation loss = 3.9244  \n",
      "\n",
      "Fold: 10  Epoch: 461  Training loss = 2.6661  Validation loss = 3.9209  \n",
      "\n",
      "Fold: 10  Epoch: 462  Training loss = 2.6658  Validation loss = 3.9205  \n",
      "\n",
      "Fold: 10  Epoch: 463  Training loss = 2.6656  Validation loss = 3.9247  \n",
      "\n",
      "Fold: 10  Epoch: 464  Training loss = 2.6651  Validation loss = 3.9229  \n",
      "\n",
      "Fold: 10  Epoch: 465  Training loss = 2.6645  Validation loss = 3.9196  \n",
      "\n",
      "Fold: 10  Epoch: 466  Training loss = 2.6642  Validation loss = 3.9213  \n",
      "\n",
      "Fold: 10  Epoch: 467  Training loss = 2.6638  Validation loss = 3.9217  \n",
      "\n",
      "Fold: 10  Epoch: 468  Training loss = 2.6636  Validation loss = 3.9225  \n",
      "\n",
      "Fold: 10  Epoch: 469  Training loss = 2.6632  Validation loss = 3.9234  \n",
      "\n",
      "Fold: 10  Epoch: 470  Training loss = 2.6627  Validation loss = 3.9225  \n",
      "\n",
      "Fold: 10  Epoch: 471  Training loss = 2.6624  Validation loss = 3.9216  \n",
      "\n",
      "Fold: 10  Epoch: 472  Training loss = 2.6622  Validation loss = 3.9225  \n",
      "\n",
      "Fold: 10  Epoch: 473  Training loss = 2.6619  Validation loss = 3.9268  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 465  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 2.7402  Validation loss = 1.3858  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 2.7356  Validation loss = 1.3857  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 2.7327  Validation loss = 1.3855  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 2.7313  Validation loss = 1.3848  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 2.7310  Validation loss = 1.3847  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 2.7298  Validation loss = 1.3844  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 2.7288  Validation loss = 1.3839  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 2.7265  Validation loss = 1.3838  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 2.7260  Validation loss = 1.3837  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 2.7249  Validation loss = 1.3832  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 2.7239  Validation loss = 1.3834  \n",
      "\n",
      "Fold: 11  Epoch: 12  Training loss = 2.7231  Validation loss = 1.3833  \n",
      "\n",
      "Fold: 11  Epoch: 13  Training loss = 2.7222  Validation loss = 1.3832  \n",
      "\n",
      "Fold: 11  Epoch: 14  Training loss = 2.7210  Validation loss = 1.3831  \n",
      "\n",
      "Fold: 11  Epoch: 15  Training loss = 2.7204  Validation loss = 1.3831  \n",
      "\n",
      "Fold: 11  Epoch: 16  Training loss = 2.7194  Validation loss = 1.3825  \n",
      "\n",
      "Fold: 11  Epoch: 17  Training loss = 2.7183  Validation loss = 1.3827  \n",
      "\n",
      "Fold: 11  Epoch: 18  Training loss = 2.7169  Validation loss = 1.3830  \n",
      "\n",
      "Fold: 11  Epoch: 19  Training loss = 2.7158  Validation loss = 1.3832  \n",
      "\n",
      "Fold: 11  Epoch: 20  Training loss = 2.7149  Validation loss = 1.3829  \n",
      "\n",
      "Fold: 11  Epoch: 21  Training loss = 2.7136  Validation loss = 1.3829  \n",
      "\n",
      "Fold: 11  Epoch: 22  Training loss = 2.7120  Validation loss = 1.3833  \n",
      "\n",
      "Fold: 11  Epoch: 23  Training loss = 2.7107  Validation loss = 1.3839  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 16  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 2.7045  Validation loss = 1.6708  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 2.7036  Validation loss = 1.6680  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 2.7021  Validation loss = 1.6612  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 2.7018  Validation loss = 1.6612  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 2.7012  Validation loss = 1.6601  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 2.6994  Validation loss = 1.6521  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 2.6981  Validation loss = 1.6473  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 2.6976  Validation loss = 1.6461  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 2.6963  Validation loss = 1.6410  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 2.6955  Validation loss = 1.6373  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 2.6949  Validation loss = 1.6357  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 2.6935  Validation loss = 1.6289  \n",
      "\n",
      "Fold: 12  Epoch: 13  Training loss = 2.6924  Validation loss = 1.6248  \n",
      "\n",
      "Fold: 12  Epoch: 14  Training loss = 2.6909  Validation loss = 1.6175  \n",
      "\n",
      "Fold: 12  Epoch: 15  Training loss = 2.6898  Validation loss = 1.6130  \n",
      "\n",
      "Fold: 12  Epoch: 16  Training loss = 2.6886  Validation loss = 1.6083  \n",
      "\n",
      "Fold: 12  Epoch: 17  Training loss = 2.6873  Validation loss = 1.6035  \n",
      "\n",
      "Fold: 12  Epoch: 18  Training loss = 2.6864  Validation loss = 1.5989  \n",
      "\n",
      "Fold: 12  Epoch: 19  Training loss = 2.6857  Validation loss = 1.5972  \n",
      "\n",
      "Fold: 12  Epoch: 20  Training loss = 2.6849  Validation loss = 1.5951  \n",
      "\n",
      "Fold: 12  Epoch: 21  Training loss = 2.6840  Validation loss = 1.5931  \n",
      "\n",
      "Fold: 12  Epoch: 22  Training loss = 2.6819  Validation loss = 1.5886  \n",
      "\n",
      "Fold: 12  Epoch: 23  Training loss = 2.6783  Validation loss = 1.5861  \n",
      "\n",
      "Fold: 12  Epoch: 24  Training loss = 2.6766  Validation loss = 1.5860  \n",
      "\n",
      "Fold: 12  Epoch: 25  Training loss = 2.6760  Validation loss = 1.5838  \n",
      "\n",
      "Fold: 12  Epoch: 26  Training loss = 2.6745  Validation loss = 1.5768  \n",
      "\n",
      "Fold: 12  Epoch: 27  Training loss = 2.6741  Validation loss = 1.5761  \n",
      "\n",
      "Fold: 12  Epoch: 28  Training loss = 2.6740  Validation loss = 1.5776  \n",
      "\n",
      "Fold: 12  Epoch: 29  Training loss = 2.6734  Validation loss = 1.5761  \n",
      "\n",
      "Fold: 12  Epoch: 30  Training loss = 2.6723  Validation loss = 1.5703  \n",
      "\n",
      "Fold: 12  Epoch: 31  Training loss = 2.6717  Validation loss = 1.5681  \n",
      "\n",
      "Fold: 12  Epoch: 32  Training loss = 2.6706  Validation loss = 1.5635  \n",
      "\n",
      "Fold: 12  Epoch: 33  Training loss = 2.6701  Validation loss = 1.5623  \n",
      "\n",
      "Fold: 12  Epoch: 34  Training loss = 2.6694  Validation loss = 1.5596  \n",
      "\n",
      "Fold: 12  Epoch: 35  Training loss = 2.6692  Validation loss = 1.5596  \n",
      "\n",
      "Fold: 12  Epoch: 36  Training loss = 2.6684  Validation loss = 1.5558  \n",
      "\n",
      "Fold: 12  Epoch: 37  Training loss = 2.6676  Validation loss = 1.5520  \n",
      "\n",
      "Fold: 12  Epoch: 38  Training loss = 2.6670  Validation loss = 1.5494  \n",
      "\n",
      "Fold: 12  Epoch: 39  Training loss = 2.6665  Validation loss = 1.5479  \n",
      "\n",
      "Fold: 12  Epoch: 40  Training loss = 2.6657  Validation loss = 1.5449  \n",
      "\n",
      "Fold: 12  Epoch: 41  Training loss = 2.6648  Validation loss = 1.5409  \n",
      "\n",
      "Fold: 12  Epoch: 42  Training loss = 2.6634  Validation loss = 1.5337  \n",
      "\n",
      "Fold: 12  Epoch: 43  Training loss = 2.6625  Validation loss = 1.5295  \n",
      "\n",
      "Fold: 12  Epoch: 44  Training loss = 2.6608  Validation loss = 1.5241  \n",
      "\n",
      "Fold: 12  Epoch: 45  Training loss = 2.6594  Validation loss = 1.5181  \n",
      "\n",
      "Fold: 12  Epoch: 46  Training loss = 2.6590  Validation loss = 1.5174  \n",
      "\n",
      "Fold: 12  Epoch: 47  Training loss = 2.6578  Validation loss = 1.5128  \n",
      "\n",
      "Fold: 12  Epoch: 48  Training loss = 2.6569  Validation loss = 1.5097  \n",
      "\n",
      "Fold: 12  Epoch: 49  Training loss = 2.6558  Validation loss = 1.5054  \n",
      "\n",
      "Fold: 12  Epoch: 50  Training loss = 2.6549  Validation loss = 1.5018  \n",
      "\n",
      "Fold: 12  Epoch: 51  Training loss = 2.6544  Validation loss = 1.5008  \n",
      "\n",
      "Fold: 12  Epoch: 52  Training loss = 2.6539  Validation loss = 1.4993  \n",
      "\n",
      "Fold: 12  Epoch: 53  Training loss = 2.6532  Validation loss = 1.4960  \n",
      "\n",
      "Fold: 12  Epoch: 54  Training loss = 2.6521  Validation loss = 1.4908  \n",
      "\n",
      "Fold: 12  Epoch: 55  Training loss = 2.6515  Validation loss = 1.4887  \n",
      "\n",
      "Fold: 12  Epoch: 56  Training loss = 2.6506  Validation loss = 1.4843  \n",
      "\n",
      "Fold: 12  Epoch: 57  Training loss = 2.6497  Validation loss = 1.4803  \n",
      "\n",
      "Fold: 12  Epoch: 58  Training loss = 2.6491  Validation loss = 1.4790  \n",
      "\n",
      "Fold: 12  Epoch: 59  Training loss = 2.6485  Validation loss = 1.4756  \n",
      "\n",
      "Fold: 12  Epoch: 60  Training loss = 2.6469  Validation loss = 1.4652  \n",
      "\n",
      "Fold: 12  Epoch: 61  Training loss = 2.6462  Validation loss = 1.4626  \n",
      "\n",
      "Fold: 12  Epoch: 62  Training loss = 2.6450  Validation loss = 1.4551  \n",
      "\n",
      "Fold: 12  Epoch: 63  Training loss = 2.6441  Validation loss = 1.4511  \n",
      "\n",
      "Fold: 12  Epoch: 64  Training loss = 2.6433  Validation loss = 1.4477  \n",
      "\n",
      "Fold: 12  Epoch: 65  Training loss = 2.6427  Validation loss = 1.4455  \n",
      "\n",
      "Fold: 12  Epoch: 66  Training loss = 2.6423  Validation loss = 1.4443  \n",
      "\n",
      "Fold: 12  Epoch: 67  Training loss = 2.6416  Validation loss = 1.4408  \n",
      "\n",
      "Fold: 12  Epoch: 68  Training loss = 2.6402  Validation loss = 1.4316  \n",
      "\n",
      "Fold: 12  Epoch: 69  Training loss = 2.6402  Validation loss = 1.4335  \n",
      "\n",
      "Fold: 12  Epoch: 70  Training loss = 2.6397  Validation loss = 1.4321  \n",
      "\n",
      "Fold: 12  Epoch: 71  Training loss = 2.6386  Validation loss = 1.4264  \n",
      "\n",
      "Fold: 12  Epoch: 72  Training loss = 2.6377  Validation loss = 1.4219  \n",
      "\n",
      "Fold: 12  Epoch: 73  Training loss = 2.6365  Validation loss = 1.4140  \n",
      "\n",
      "Fold: 12  Epoch: 74  Training loss = 2.6350  Validation loss = 1.4041  \n",
      "\n",
      "Fold: 12  Epoch: 75  Training loss = 2.6346  Validation loss = 1.4030  \n",
      "\n",
      "Fold: 12  Epoch: 76  Training loss = 2.6338  Validation loss = 1.4005  \n",
      "\n",
      "Fold: 12  Epoch: 77  Training loss = 2.6326  Validation loss = 1.3935  \n",
      "\n",
      "Fold: 12  Epoch: 78  Training loss = 2.6315  Validation loss = 1.3904  \n",
      "\n",
      "Fold: 12  Epoch: 79  Training loss = 2.6299  Validation loss = 1.3871  \n",
      "\n",
      "Fold: 12  Epoch: 80  Training loss = 2.6295  Validation loss = 1.3866  \n",
      "\n",
      "Fold: 12  Epoch: 81  Training loss = 2.6289  Validation loss = 1.3846  \n",
      "\n",
      "Fold: 12  Epoch: 82  Training loss = 2.6277  Validation loss = 1.3819  \n",
      "\n",
      "Fold: 12  Epoch: 83  Training loss = 2.6272  Validation loss = 1.3840  \n",
      "\n",
      "Fold: 12  Epoch: 84  Training loss = 2.6266  Validation loss = 1.3835  \n",
      "\n",
      "Fold: 12  Epoch: 85  Training loss = 2.6256  Validation loss = 1.3802  \n",
      "\n",
      "Fold: 12  Epoch: 86  Training loss = 2.6249  Validation loss = 1.3784  \n",
      "\n",
      "Fold: 12  Epoch: 87  Training loss = 2.6247  Validation loss = 1.3795  \n",
      "\n",
      "Fold: 12  Epoch: 88  Training loss = 2.6236  Validation loss = 1.3734  \n",
      "\n",
      "Fold: 12  Epoch: 89  Training loss = 2.6223  Validation loss = 1.3656  \n",
      "\n",
      "Fold: 12  Epoch: 90  Training loss = 2.6215  Validation loss = 1.3607  \n",
      "\n",
      "Fold: 12  Epoch: 91  Training loss = 2.6206  Validation loss = 1.3567  \n",
      "\n",
      "Fold: 12  Epoch: 92  Training loss = 2.6197  Validation loss = 1.3516  \n",
      "\n",
      "Fold: 12  Epoch: 93  Training loss = 2.6191  Validation loss = 1.3477  \n",
      "\n",
      "Fold: 12  Epoch: 94  Training loss = 2.6184  Validation loss = 1.3453  \n",
      "\n",
      "Fold: 12  Epoch: 95  Training loss = 2.6178  Validation loss = 1.3435  \n",
      "\n",
      "Fold: 12  Epoch: 96  Training loss = 2.6174  Validation loss = 1.3436  \n",
      "\n",
      "Fold: 12  Epoch: 97  Training loss = 2.6167  Validation loss = 1.3390  \n",
      "\n",
      "Fold: 12  Epoch: 98  Training loss = 2.6160  Validation loss = 1.3360  \n",
      "\n",
      "Fold: 12  Epoch: 99  Training loss = 2.6155  Validation loss = 1.3336  \n",
      "\n",
      "Fold: 12  Epoch: 100  Training loss = 2.6149  Validation loss = 1.3304  \n",
      "\n",
      "Fold: 12  Epoch: 101  Training loss = 2.6142  Validation loss = 1.3269  \n",
      "\n",
      "Fold: 12  Epoch: 102  Training loss = 2.6133  Validation loss = 1.3221  \n",
      "\n",
      "Fold: 12  Epoch: 103  Training loss = 2.6129  Validation loss = 1.3204  \n",
      "\n",
      "Fold: 12  Epoch: 104  Training loss = 2.6121  Validation loss = 1.3162  \n",
      "\n",
      "Fold: 12  Epoch: 105  Training loss = 2.6116  Validation loss = 1.3129  \n",
      "\n",
      "Fold: 12  Epoch: 106  Training loss = 2.6112  Validation loss = 1.3105  \n",
      "\n",
      "Fold: 12  Epoch: 107  Training loss = 2.6110  Validation loss = 1.3102  \n",
      "\n",
      "Fold: 12  Epoch: 108  Training loss = 2.6104  Validation loss = 1.3079  \n",
      "\n",
      "Fold: 12  Epoch: 109  Training loss = 2.6100  Validation loss = 1.3068  \n",
      "\n",
      "Fold: 12  Epoch: 110  Training loss = 2.6094  Validation loss = 1.3044  \n",
      "\n",
      "Fold: 12  Epoch: 111  Training loss = 2.6092  Validation loss = 1.3031  \n",
      "\n",
      "Fold: 12  Epoch: 112  Training loss = 2.6086  Validation loss = 1.3024  \n",
      "\n",
      "Fold: 12  Epoch: 113  Training loss = 2.6079  Validation loss = 1.2987  \n",
      "\n",
      "Fold: 12  Epoch: 114  Training loss = 2.6072  Validation loss = 1.2942  \n",
      "\n",
      "Fold: 12  Epoch: 115  Training loss = 2.6068  Validation loss = 1.2926  \n",
      "\n",
      "Fold: 12  Epoch: 116  Training loss = 2.6063  Validation loss = 1.2890  \n",
      "\n",
      "Fold: 12  Epoch: 117  Training loss = 2.6059  Validation loss = 1.2900  \n",
      "\n",
      "Fold: 12  Epoch: 118  Training loss = 2.6055  Validation loss = 1.2892  \n",
      "\n",
      "Fold: 12  Epoch: 119  Training loss = 2.6051  Validation loss = 1.2884  \n",
      "\n",
      "Fold: 12  Epoch: 120  Training loss = 2.6046  Validation loss = 1.2860  \n",
      "\n",
      "Fold: 12  Epoch: 121  Training loss = 2.6041  Validation loss = 1.2829  \n",
      "\n",
      "Fold: 12  Epoch: 122  Training loss = 2.6036  Validation loss = 1.2800  \n",
      "\n",
      "Fold: 12  Epoch: 123  Training loss = 2.6032  Validation loss = 1.2793  \n",
      "\n",
      "Fold: 12  Epoch: 124  Training loss = 2.6028  Validation loss = 1.2768  \n",
      "\n",
      "Fold: 12  Epoch: 125  Training loss = 2.6024  Validation loss = 1.2751  \n",
      "\n",
      "Fold: 12  Epoch: 126  Training loss = 2.6017  Validation loss = 1.2708  \n",
      "\n",
      "Fold: 12  Epoch: 127  Training loss = 2.6004  Validation loss = 1.2654  \n",
      "\n",
      "Fold: 12  Epoch: 128  Training loss = 2.5999  Validation loss = 1.2642  \n",
      "\n",
      "Fold: 12  Epoch: 129  Training loss = 2.5995  Validation loss = 1.2625  \n",
      "\n",
      "Fold: 12  Epoch: 130  Training loss = 2.5993  Validation loss = 1.2629  \n",
      "\n",
      "Fold: 12  Epoch: 131  Training loss = 2.5990  Validation loss = 1.2606  \n",
      "\n",
      "Fold: 12  Epoch: 132  Training loss = 2.5990  Validation loss = 1.2632  \n",
      "\n",
      "Fold: 12  Epoch: 133  Training loss = 2.5987  Validation loss = 1.2632  \n",
      "\n",
      "Fold: 12  Epoch: 134  Training loss = 2.5981  Validation loss = 1.2588  \n",
      "\n",
      "Fold: 12  Epoch: 135  Training loss = 2.5973  Validation loss = 1.2544  \n",
      "\n",
      "Fold: 12  Epoch: 136  Training loss = 2.5966  Validation loss = 1.2501  \n",
      "\n",
      "Fold: 12  Epoch: 137  Training loss = 2.5962  Validation loss = 1.2472  \n",
      "\n",
      "Fold: 12  Epoch: 138  Training loss = 2.5956  Validation loss = 1.2438  \n",
      "\n",
      "Fold: 12  Epoch: 139  Training loss = 2.5949  Validation loss = 1.2395  \n",
      "\n",
      "Fold: 12  Epoch: 140  Training loss = 2.5947  Validation loss = 1.2411  \n",
      "\n",
      "Fold: 12  Epoch: 141  Training loss = 2.5942  Validation loss = 1.2397  \n",
      "\n",
      "Fold: 12  Epoch: 142  Training loss = 2.5938  Validation loss = 1.2387  \n",
      "\n",
      "Fold: 12  Epoch: 143  Training loss = 2.5935  Validation loss = 1.2383  \n",
      "\n",
      "Fold: 12  Epoch: 144  Training loss = 2.5931  Validation loss = 1.2385  \n",
      "\n",
      "Fold: 12  Epoch: 145  Training loss = 2.5926  Validation loss = 1.2368  \n",
      "\n",
      "Fold: 12  Epoch: 146  Training loss = 2.5921  Validation loss = 1.2337  \n",
      "\n",
      "Fold: 12  Epoch: 147  Training loss = 2.5918  Validation loss = 1.2330  \n",
      "\n",
      "Fold: 12  Epoch: 148  Training loss = 2.5913  Validation loss = 1.2320  \n",
      "\n",
      "Fold: 12  Epoch: 149  Training loss = 2.5910  Validation loss = 1.2302  \n",
      "\n",
      "Fold: 12  Epoch: 150  Training loss = 2.5811  Validation loss = 1.2242  \n",
      "\n",
      "Fold: 12  Epoch: 151  Training loss = 2.5803  Validation loss = 1.2205  \n",
      "\n",
      "Fold: 12  Epoch: 152  Training loss = 2.5799  Validation loss = 1.2180  \n",
      "\n",
      "Fold: 12  Epoch: 153  Training loss = 2.5793  Validation loss = 1.2167  \n",
      "\n",
      "Fold: 12  Epoch: 154  Training loss = 2.5788  Validation loss = 1.2138  \n",
      "\n",
      "Fold: 12  Epoch: 155  Training loss = 2.5784  Validation loss = 1.2113  \n",
      "\n",
      "Fold: 12  Epoch: 156  Training loss = 2.5781  Validation loss = 1.2115  \n",
      "\n",
      "Fold: 12  Epoch: 157  Training loss = 2.5779  Validation loss = 1.2114  \n",
      "\n",
      "Fold: 12  Epoch: 158  Training loss = 2.5776  Validation loss = 1.2091  \n",
      "\n",
      "Fold: 12  Epoch: 159  Training loss = 2.5774  Validation loss = 1.2092  \n",
      "\n",
      "Fold: 12  Epoch: 160  Training loss = 2.5770  Validation loss = 1.2112  \n",
      "\n",
      "Fold: 12  Epoch: 161  Training loss = 2.5763  Validation loss = 1.2077  \n",
      "\n",
      "Fold: 12  Epoch: 162  Training loss = 2.5758  Validation loss = 1.2056  \n",
      "\n",
      "Fold: 12  Epoch: 163  Training loss = 2.5752  Validation loss = 1.2023  \n",
      "\n",
      "Fold: 12  Epoch: 164  Training loss = 2.5746  Validation loss = 1.1991  \n",
      "\n",
      "Fold: 12  Epoch: 165  Training loss = 2.5740  Validation loss = 1.1969  \n",
      "\n",
      "Fold: 12  Epoch: 166  Training loss = 2.5738  Validation loss = 1.1962  \n",
      "\n",
      "Fold: 12  Epoch: 167  Training loss = 2.5734  Validation loss = 1.1958  \n",
      "\n",
      "Fold: 12  Epoch: 168  Training loss = 2.5731  Validation loss = 1.1964  \n",
      "\n",
      "Fold: 12  Epoch: 169  Training loss = 2.5725  Validation loss = 1.1929  \n",
      "\n",
      "Fold: 12  Epoch: 170  Training loss = 2.5720  Validation loss = 1.1901  \n",
      "\n",
      "Fold: 12  Epoch: 171  Training loss = 2.5713  Validation loss = 1.1844  \n",
      "\n",
      "Fold: 12  Epoch: 172  Training loss = 2.5708  Validation loss = 1.1832  \n",
      "\n",
      "Fold: 12  Epoch: 173  Training loss = 2.5704  Validation loss = 1.1819  \n",
      "\n",
      "Fold: 12  Epoch: 174  Training loss = 2.5697  Validation loss = 1.1793  \n",
      "\n",
      "Fold: 12  Epoch: 175  Training loss = 2.5694  Validation loss = 1.1803  \n",
      "\n",
      "Fold: 12  Epoch: 176  Training loss = 2.5690  Validation loss = 1.1792  \n",
      "\n",
      "Fold: 12  Epoch: 177  Training loss = 2.5687  Validation loss = 1.1766  \n",
      "\n",
      "Fold: 12  Epoch: 178  Training loss = 2.5685  Validation loss = 1.1774  \n",
      "\n",
      "Fold: 12  Epoch: 179  Training loss = 2.5681  Validation loss = 1.1768  \n",
      "\n",
      "Fold: 12  Epoch: 180  Training loss = 2.5672  Validation loss = 1.1685  \n",
      "\n",
      "Fold: 12  Epoch: 181  Training loss = 2.5664  Validation loss = 1.1681  \n",
      "\n",
      "Fold: 12  Epoch: 182  Training loss = 2.5630  Validation loss = 1.1667  \n",
      "\n",
      "Fold: 12  Epoch: 183  Training loss = 2.5627  Validation loss = 1.1640  \n",
      "\n",
      "Fold: 12  Epoch: 184  Training loss = 2.5623  Validation loss = 1.1639  \n",
      "\n",
      "Fold: 12  Epoch: 185  Training loss = 2.5619  Validation loss = 1.1638  \n",
      "\n",
      "Fold: 12  Epoch: 186  Training loss = 2.5617  Validation loss = 1.1626  \n",
      "\n",
      "Fold: 12  Epoch: 187  Training loss = 2.5611  Validation loss = 1.1576  \n",
      "\n",
      "Fold: 12  Epoch: 188  Training loss = 2.5610  Validation loss = 1.1595  \n",
      "\n",
      "Fold: 12  Epoch: 189  Training loss = 2.5605  Validation loss = 1.1527  \n",
      "\n",
      "Fold: 12  Epoch: 190  Training loss = 2.5600  Validation loss = 1.1500  \n",
      "\n",
      "Fold: 12  Epoch: 191  Training loss = 2.5597  Validation loss = 1.1508  \n",
      "\n",
      "Fold: 12  Epoch: 192  Training loss = 2.5590  Validation loss = 1.1439  \n",
      "\n",
      "Fold: 12  Epoch: 193  Training loss = 2.5586  Validation loss = 1.1418  \n",
      "\n",
      "Fold: 12  Epoch: 194  Training loss = 2.5582  Validation loss = 1.1403  \n",
      "\n",
      "Fold: 12  Epoch: 195  Training loss = 2.5577  Validation loss = 1.1364  \n",
      "\n",
      "Fold: 12  Epoch: 196  Training loss = 2.5572  Validation loss = 1.1334  \n",
      "\n",
      "Fold: 12  Epoch: 197  Training loss = 2.5565  Validation loss = 1.1279  \n",
      "\n",
      "Fold: 12  Epoch: 198  Training loss = 2.5561  Validation loss = 1.1280  \n",
      "\n",
      "Fold: 12  Epoch: 199  Training loss = 2.5557  Validation loss = 1.1273  \n",
      "\n",
      "Fold: 12  Epoch: 200  Training loss = 2.5554  Validation loss = 1.1249  \n",
      "\n",
      "Fold: 12  Epoch: 201  Training loss = 2.5512  Validation loss = 1.1212  \n",
      "\n",
      "Fold: 12  Epoch: 202  Training loss = 2.5496  Validation loss = 1.1210  \n",
      "\n",
      "Fold: 12  Epoch: 203  Training loss = 2.5482  Validation loss = 1.1166  \n",
      "\n",
      "Fold: 12  Epoch: 204  Training loss = 2.5478  Validation loss = 1.1134  \n",
      "\n",
      "Fold: 12  Epoch: 205  Training loss = 2.5474  Validation loss = 1.1136  \n",
      "\n",
      "Fold: 12  Epoch: 206  Training loss = 2.5472  Validation loss = 1.1147  \n",
      "\n",
      "Fold: 12  Epoch: 207  Training loss = 2.5469  Validation loss = 1.1144  \n",
      "\n",
      "Fold: 12  Epoch: 208  Training loss = 2.5464  Validation loss = 1.1128  \n",
      "\n",
      "Fold: 12  Epoch: 209  Training loss = 2.5462  Validation loss = 1.1123  \n",
      "\n",
      "Fold: 12  Epoch: 210  Training loss = 2.5459  Validation loss = 1.1127  \n",
      "\n",
      "Fold: 12  Epoch: 211  Training loss = 2.5456  Validation loss = 1.1103  \n",
      "\n",
      "Fold: 12  Epoch: 212  Training loss = 2.5454  Validation loss = 1.1100  \n",
      "\n",
      "Fold: 12  Epoch: 213  Training loss = 2.5451  Validation loss = 1.1106  \n",
      "\n",
      "Fold: 12  Epoch: 214  Training loss = 2.5447  Validation loss = 1.1065  \n",
      "\n",
      "Fold: 12  Epoch: 215  Training loss = 2.5445  Validation loss = 1.1088  \n",
      "\n",
      "Fold: 12  Epoch: 216  Training loss = 2.5442  Validation loss = 1.1059  \n",
      "\n",
      "Fold: 12  Epoch: 217  Training loss = 2.5440  Validation loss = 1.1049  \n",
      "\n",
      "Fold: 12  Epoch: 218  Training loss = 2.5433  Validation loss = 1.1000  \n",
      "\n",
      "Fold: 12  Epoch: 219  Training loss = 2.5428  Validation loss = 1.0974  \n",
      "\n",
      "Fold: 12  Epoch: 220  Training loss = 2.5425  Validation loss = 1.0946  \n",
      "\n",
      "Fold: 12  Epoch: 221  Training loss = 2.5423  Validation loss = 1.0952  \n",
      "\n",
      "Fold: 12  Epoch: 222  Training loss = 2.5421  Validation loss = 1.0963  \n",
      "\n",
      "Fold: 12  Epoch: 223  Training loss = 2.5415  Validation loss = 1.0895  \n",
      "\n",
      "Fold: 12  Epoch: 224  Training loss = 2.5411  Validation loss = 1.0891  \n",
      "\n",
      "Fold: 12  Epoch: 225  Training loss = 2.5406  Validation loss = 1.0830  \n",
      "\n",
      "Fold: 12  Epoch: 226  Training loss = 2.5401  Validation loss = 1.0827  \n",
      "\n",
      "Fold: 12  Epoch: 227  Training loss = 2.5397  Validation loss = 1.0782  \n",
      "\n",
      "Fold: 12  Epoch: 228  Training loss = 2.5394  Validation loss = 1.0781  \n",
      "\n",
      "Fold: 12  Epoch: 229  Training loss = 2.5389  Validation loss = 1.0739  \n",
      "\n",
      "Fold: 12  Epoch: 230  Training loss = 2.5387  Validation loss = 1.0756  \n",
      "\n",
      "Fold: 12  Epoch: 231  Training loss = 2.5382  Validation loss = 1.0729  \n",
      "\n",
      "Fold: 12  Epoch: 232  Training loss = 2.5379  Validation loss = 1.0730  \n",
      "\n",
      "Fold: 12  Epoch: 233  Training loss = 2.5375  Validation loss = 1.0717  \n",
      "\n",
      "Fold: 12  Epoch: 234  Training loss = 2.5371  Validation loss = 1.0709  \n",
      "\n",
      "Fold: 12  Epoch: 235  Training loss = 2.5368  Validation loss = 1.0684  \n",
      "\n",
      "Fold: 12  Epoch: 236  Training loss = 2.5366  Validation loss = 1.0684  \n",
      "\n",
      "Fold: 12  Epoch: 237  Training loss = 2.5364  Validation loss = 1.0688  \n",
      "\n",
      "Fold: 12  Epoch: 238  Training loss = 2.5359  Validation loss = 1.0663  \n",
      "\n",
      "Fold: 12  Epoch: 239  Training loss = 2.5354  Validation loss = 1.0621  \n",
      "\n",
      "Fold: 12  Epoch: 240  Training loss = 2.5352  Validation loss = 1.0625  \n",
      "\n",
      "Fold: 12  Epoch: 241  Training loss = 2.5350  Validation loss = 1.0633  \n",
      "\n",
      "Fold: 12  Epoch: 242  Training loss = 2.5346  Validation loss = 1.0648  \n",
      "\n",
      "Fold: 12  Epoch: 243  Training loss = 2.5341  Validation loss = 1.0618  \n",
      "\n",
      "Fold: 12  Epoch: 244  Training loss = 2.5338  Validation loss = 1.0614  \n",
      "\n",
      "Fold: 12  Epoch: 245  Training loss = 2.5333  Validation loss = 1.0615  \n",
      "\n",
      "Fold: 12  Epoch: 246  Training loss = 2.5330  Validation loss = 1.0592  \n",
      "\n",
      "Fold: 12  Epoch: 247  Training loss = 2.5327  Validation loss = 1.0580  \n",
      "\n",
      "Fold: 12  Epoch: 248  Training loss = 2.5325  Validation loss = 1.0601  \n",
      "\n",
      "Fold: 12  Epoch: 249  Training loss = 2.5322  Validation loss = 1.0584  \n",
      "\n",
      "Fold: 12  Epoch: 250  Training loss = 2.5319  Validation loss = 1.0577  \n",
      "\n",
      "Fold: 12  Epoch: 251  Training loss = 2.5314  Validation loss = 1.0565  \n",
      "\n",
      "Fold: 12  Epoch: 252  Training loss = 2.5309  Validation loss = 1.0554  \n",
      "\n",
      "Fold: 12  Epoch: 253  Training loss = 2.5304  Validation loss = 1.0540  \n",
      "\n",
      "Fold: 12  Epoch: 254  Training loss = 2.5301  Validation loss = 1.0505  \n",
      "\n",
      "Fold: 12  Epoch: 255  Training loss = 2.5297  Validation loss = 1.0473  \n",
      "\n",
      "Fold: 12  Epoch: 256  Training loss = 2.5296  Validation loss = 1.0485  \n",
      "\n",
      "Fold: 12  Epoch: 257  Training loss = 2.5294  Validation loss = 1.0476  \n",
      "\n",
      "Fold: 12  Epoch: 258  Training loss = 2.5293  Validation loss = 1.0493  \n",
      "\n",
      "Fold: 12  Epoch: 259  Training loss = 2.5290  Validation loss = 1.0496  \n",
      "\n",
      "Fold: 12  Epoch: 260  Training loss = 2.5285  Validation loss = 1.0477  \n",
      "\n",
      "Fold: 12  Epoch: 261  Training loss = 2.5280  Validation loss = 1.0460  \n",
      "\n",
      "Fold: 12  Epoch: 262  Training loss = 2.5279  Validation loss = 1.0480  \n",
      "\n",
      "Fold: 12  Epoch: 263  Training loss = 2.5277  Validation loss = 1.0466  \n",
      "\n",
      "Fold: 12  Epoch: 264  Training loss = 2.5274  Validation loss = 1.0471  \n",
      "\n",
      "Fold: 12  Epoch: 265  Training loss = 2.5270  Validation loss = 1.0454  \n",
      "\n",
      "Fold: 12  Epoch: 266  Training loss = 2.5269  Validation loss = 1.0472  \n",
      "\n",
      "Fold: 12  Epoch: 267  Training loss = 2.5266  Validation loss = 1.0427  \n",
      "\n",
      "Fold: 12  Epoch: 268  Training loss = 2.5264  Validation loss = 1.0453  \n",
      "\n",
      "Fold: 12  Epoch: 269  Training loss = 2.5259  Validation loss = 1.0420  \n",
      "\n",
      "Fold: 12  Epoch: 270  Training loss = 2.5257  Validation loss = 1.0415  \n",
      "\n",
      "Fold: 12  Epoch: 271  Training loss = 2.5256  Validation loss = 1.0411  \n",
      "\n",
      "Fold: 12  Epoch: 272  Training loss = 2.5251  Validation loss = 1.0377  \n",
      "\n",
      "Fold: 12  Epoch: 273  Training loss = 2.5249  Validation loss = 1.0393  \n",
      "\n",
      "Fold: 12  Epoch: 274  Training loss = 2.5248  Validation loss = 1.0395  \n",
      "\n",
      "Fold: 12  Epoch: 275  Training loss = 2.5246  Validation loss = 1.0394  \n",
      "\n",
      "Fold: 12  Epoch: 276  Training loss = 2.5245  Validation loss = 1.0381  \n",
      "\n",
      "Fold: 12  Epoch: 277  Training loss = 2.5241  Validation loss = 1.0354  \n",
      "\n",
      "Fold: 12  Epoch: 278  Training loss = 2.5238  Validation loss = 1.0364  \n",
      "\n",
      "Fold: 12  Epoch: 279  Training loss = 2.5236  Validation loss = 1.0369  \n",
      "\n",
      "Fold: 12  Epoch: 280  Training loss = 2.5232  Validation loss = 1.0343  \n",
      "\n",
      "Fold: 12  Epoch: 281  Training loss = 2.5229  Validation loss = 1.0334  \n",
      "\n",
      "Fold: 12  Epoch: 282  Training loss = 2.5225  Validation loss = 1.0313  \n",
      "\n",
      "Fold: 12  Epoch: 283  Training loss = 2.5221  Validation loss = 1.0272  \n",
      "\n",
      "Fold: 12  Epoch: 284  Training loss = 2.5218  Validation loss = 1.0249  \n",
      "\n",
      "Fold: 12  Epoch: 285  Training loss = 2.5215  Validation loss = 1.0266  \n",
      "\n",
      "Fold: 12  Epoch: 286  Training loss = 2.5212  Validation loss = 1.0268  \n",
      "\n",
      "Fold: 12  Epoch: 287  Training loss = 2.5210  Validation loss = 1.0247  \n",
      "\n",
      "Fold: 12  Epoch: 288  Training loss = 2.5207  Validation loss = 1.0257  \n",
      "\n",
      "Fold: 12  Epoch: 289  Training loss = 2.5202  Validation loss = 1.0221  \n",
      "\n",
      "Fold: 12  Epoch: 290  Training loss = 2.5199  Validation loss = 1.0217  \n",
      "\n",
      "Fold: 12  Epoch: 291  Training loss = 2.5198  Validation loss = 1.0218  \n",
      "\n",
      "Fold: 12  Epoch: 292  Training loss = 2.5196  Validation loss = 1.0228  \n",
      "\n",
      "Fold: 12  Epoch: 293  Training loss = 2.5195  Validation loss = 1.0232  \n",
      "\n",
      "Fold: 12  Epoch: 294  Training loss = 2.5189  Validation loss = 1.0204  \n",
      "\n",
      "Fold: 12  Epoch: 295  Training loss = 2.5187  Validation loss = 1.0196  \n",
      "\n",
      "Fold: 12  Epoch: 296  Training loss = 2.5185  Validation loss = 1.0202  \n",
      "\n",
      "Fold: 12  Epoch: 297  Training loss = 2.5181  Validation loss = 1.0198  \n",
      "\n",
      "Fold: 12  Epoch: 298  Training loss = 2.5178  Validation loss = 1.0187  \n",
      "\n",
      "Fold: 12  Epoch: 299  Training loss = 2.5174  Validation loss = 1.0154  \n",
      "\n",
      "Fold: 12  Epoch: 300  Training loss = 2.5173  Validation loss = 1.0176  \n",
      "\n",
      "Fold: 12  Epoch: 301  Training loss = 2.5171  Validation loss = 1.0168  \n",
      "\n",
      "Fold: 12  Epoch: 302  Training loss = 2.5171  Validation loss = 1.0190  \n",
      "\n",
      "Fold: 12  Epoch: 303  Training loss = 2.5168  Validation loss = 1.0183  \n",
      "\n",
      "Fold: 12  Epoch: 304  Training loss = 2.5165  Validation loss = 1.0173  \n",
      "\n",
      "Fold: 12  Epoch: 305  Training loss = 2.5162  Validation loss = 1.0176  \n",
      "\n",
      "Fold: 12  Epoch: 306  Training loss = 2.5160  Validation loss = 1.0134  \n",
      "\n",
      "Fold: 12  Epoch: 307  Training loss = 2.5156  Validation loss = 1.0100  \n",
      "\n",
      "Fold: 12  Epoch: 308  Training loss = 2.5151  Validation loss = 1.0047  \n",
      "\n",
      "Fold: 12  Epoch: 309  Training loss = 2.5148  Validation loss = 1.0036  \n",
      "\n",
      "Fold: 12  Epoch: 310  Training loss = 2.5145  Validation loss = 1.0022  \n",
      "\n",
      "Fold: 12  Epoch: 311  Training loss = 2.5143  Validation loss = 1.0040  \n",
      "\n",
      "Fold: 12  Epoch: 312  Training loss = 2.5140  Validation loss = 1.0054  \n",
      "\n",
      "Fold: 12  Epoch: 313  Training loss = 2.5135  Validation loss = 1.0054  \n",
      "\n",
      "Fold: 12  Epoch: 314  Training loss = 2.5133  Validation loss = 1.0040  \n",
      "\n",
      "Fold: 12  Epoch: 315  Training loss = 2.5130  Validation loss = 1.0040  \n",
      "\n",
      "Fold: 12  Epoch: 316  Training loss = 2.5128  Validation loss = 1.0024  \n",
      "\n",
      "Fold: 12  Epoch: 317  Training loss = 2.5125  Validation loss = 1.0015  \n",
      "\n",
      "Fold: 12  Epoch: 318  Training loss = 2.5122  Validation loss = 1.0018  \n",
      "\n",
      "Fold: 12  Epoch: 319  Training loss = 2.5117  Validation loss = 0.9996  \n",
      "\n",
      "Fold: 12  Epoch: 320  Training loss = 2.5114  Validation loss = 1.0004  \n",
      "\n",
      "Fold: 12  Epoch: 321  Training loss = 2.5111  Validation loss = 0.9988  \n",
      "\n",
      "Fold: 12  Epoch: 322  Training loss = 2.5103  Validation loss = 0.9929  \n",
      "\n",
      "Fold: 12  Epoch: 323  Training loss = 2.5100  Validation loss = 0.9954  \n",
      "\n",
      "Fold: 12  Epoch: 324  Training loss = 2.5096  Validation loss = 0.9947  \n",
      "\n",
      "Fold: 12  Epoch: 325  Training loss = 2.5092  Validation loss = 0.9925  \n",
      "\n",
      "Fold: 12  Epoch: 326  Training loss = 2.5090  Validation loss = 0.9911  \n",
      "\n",
      "Fold: 12  Epoch: 327  Training loss = 2.5088  Validation loss = 0.9944  \n",
      "\n",
      "Fold: 12  Epoch: 328  Training loss = 2.5087  Validation loss = 0.9957  \n",
      "\n",
      "Fold: 12  Epoch: 329  Training loss = 2.5082  Validation loss = 0.9923  \n",
      "\n",
      "Fold: 12  Epoch: 330  Training loss = 2.5077  Validation loss = 0.9883  \n",
      "\n",
      "Fold: 12  Epoch: 331  Training loss = 2.5072  Validation loss = 0.9847  \n",
      "\n",
      "Fold: 12  Epoch: 332  Training loss = 2.5072  Validation loss = 0.9851  \n",
      "\n",
      "Fold: 12  Epoch: 333  Training loss = 2.5067  Validation loss = 0.9845  \n",
      "\n",
      "Fold: 12  Epoch: 334  Training loss = 2.5066  Validation loss = 0.9828  \n",
      "\n",
      "Fold: 12  Epoch: 335  Training loss = 2.5063  Validation loss = 0.9844  \n",
      "\n",
      "Fold: 12  Epoch: 336  Training loss = 2.5061  Validation loss = 0.9861  \n",
      "\n",
      "Fold: 12  Epoch: 337  Training loss = 2.5058  Validation loss = 0.9873  \n",
      "\n",
      "Fold: 12  Epoch: 338  Training loss = 2.5043  Validation loss = 0.9884  \n",
      "\n",
      "Fold: 12  Epoch: 339  Training loss = 2.5039  Validation loss = 0.9899  \n",
      "\n",
      "Fold: 12  Epoch: 340  Training loss = 2.5035  Validation loss = 0.9869  \n",
      "\n",
      "Fold: 12  Epoch: 341  Training loss = 2.5032  Validation loss = 0.9891  \n",
      "\n",
      "Fold: 12  Epoch: 342  Training loss = 2.5030  Validation loss = 0.9888  \n",
      "\n",
      "Fold: 12  Epoch: 343  Training loss = 2.5027  Validation loss = 0.9889  \n",
      "\n",
      "Fold: 12  Epoch: 344  Training loss = 2.5023  Validation loss = 0.9864  \n",
      "\n",
      "Fold: 12  Epoch: 345  Training loss = 2.5023  Validation loss = 0.9861  \n",
      "\n",
      "Fold: 12  Epoch: 346  Training loss = 2.5022  Validation loss = 0.9898  \n",
      "\n",
      "Fold: 12  Epoch: 347  Training loss = 2.5020  Validation loss = 0.9895  \n",
      "\n",
      "Fold: 12  Epoch: 348  Training loss = 2.5016  Validation loss = 0.9891  \n",
      "\n",
      "Fold: 12  Epoch: 349  Training loss = 2.5014  Validation loss = 0.9896  \n",
      "\n",
      "Fold: 12  Epoch: 350  Training loss = 2.5011  Validation loss = 0.9878  \n",
      "\n",
      "Fold: 12  Epoch: 351  Training loss = 2.5008  Validation loss = 0.9869  \n",
      "\n",
      "Fold: 12  Epoch: 352  Training loss = 2.5002  Validation loss = 0.9839  \n",
      "\n",
      "Fold: 12  Epoch: 353  Training loss = 2.5001  Validation loss = 0.9871  \n",
      "\n",
      "Fold: 12  Epoch: 354  Training loss = 2.4997  Validation loss = 0.9871  \n",
      "\n",
      "Fold: 12  Epoch: 355  Training loss = 2.4993  Validation loss = 0.9874  \n",
      "\n",
      "Fold: 12  Epoch: 356  Training loss = 2.4990  Validation loss = 0.9871  \n",
      "\n",
      "Fold: 12  Epoch: 357  Training loss = 2.4989  Validation loss = 0.9876  \n",
      "\n",
      "Fold: 12  Epoch: 358  Training loss = 2.4983  Validation loss = 0.9829  \n",
      "\n",
      "Fold: 12  Epoch: 359  Training loss = 2.4980  Validation loss = 0.9811  \n",
      "\n",
      "Fold: 12  Epoch: 360  Training loss = 2.4979  Validation loss = 0.9820  \n",
      "\n",
      "Fold: 12  Epoch: 361  Training loss = 2.4975  Validation loss = 0.9834  \n",
      "\n",
      "Fold: 12  Epoch: 362  Training loss = 2.4970  Validation loss = 0.9778  \n",
      "\n",
      "Fold: 12  Epoch: 363  Training loss = 2.4968  Validation loss = 0.9773  \n",
      "\n",
      "Fold: 12  Epoch: 364  Training loss = 2.4966  Validation loss = 0.9742  \n",
      "\n",
      "Fold: 12  Epoch: 365  Training loss = 2.4962  Validation loss = 0.9736  \n",
      "\n",
      "Fold: 12  Epoch: 366  Training loss = 2.4960  Validation loss = 0.9739  \n",
      "\n",
      "Fold: 12  Epoch: 367  Training loss = 2.4955  Validation loss = 0.9707  \n",
      "\n",
      "Fold: 12  Epoch: 368  Training loss = 2.4952  Validation loss = 0.9687  \n",
      "\n",
      "Fold: 12  Epoch: 369  Training loss = 2.4949  Validation loss = 0.9669  \n",
      "\n",
      "Fold: 12  Epoch: 370  Training loss = 2.4947  Validation loss = 0.9658  \n",
      "\n",
      "Fold: 12  Epoch: 371  Training loss = 2.4943  Validation loss = 0.9655  \n",
      "\n",
      "Fold: 12  Epoch: 372  Training loss = 2.4940  Validation loss = 0.9661  \n",
      "\n",
      "Fold: 12  Epoch: 373  Training loss = 2.4936  Validation loss = 0.9647  \n",
      "\n",
      "Fold: 12  Epoch: 374  Training loss = 2.4934  Validation loss = 0.9618  \n",
      "\n",
      "Fold: 12  Epoch: 375  Training loss = 2.4929  Validation loss = 0.9581  \n",
      "\n",
      "Fold: 12  Epoch: 376  Training loss = 2.4927  Validation loss = 0.9552  \n",
      "\n",
      "Fold: 12  Epoch: 377  Training loss = 2.4923  Validation loss = 0.9547  \n",
      "\n",
      "Fold: 12  Epoch: 378  Training loss = 2.4921  Validation loss = 0.9556  \n",
      "\n",
      "Fold: 12  Epoch: 379  Training loss = 2.4918  Validation loss = 0.9578  \n",
      "\n",
      "Fold: 12  Epoch: 380  Training loss = 2.4914  Validation loss = 0.9545  \n",
      "\n",
      "Fold: 12  Epoch: 381  Training loss = 2.4911  Validation loss = 0.9534  \n",
      "\n",
      "Fold: 12  Epoch: 382  Training loss = 2.4908  Validation loss = 0.9557  \n",
      "\n",
      "Fold: 12  Epoch: 383  Training loss = 2.4905  Validation loss = 0.9556  \n",
      "\n",
      "Fold: 12  Epoch: 384  Training loss = 2.4902  Validation loss = 0.9575  \n",
      "\n",
      "Fold: 12  Epoch: 385  Training loss = 2.4900  Validation loss = 0.9574  \n",
      "\n",
      "Fold: 12  Epoch: 386  Training loss = 2.4898  Validation loss = 0.9591  \n",
      "\n",
      "Fold: 12  Epoch: 387  Training loss = 2.4894  Validation loss = 0.9571  \n",
      "\n",
      "Fold: 12  Epoch: 388  Training loss = 2.4892  Validation loss = 0.9560  \n",
      "\n",
      "Fold: 12  Epoch: 389  Training loss = 2.4890  Validation loss = 0.9554  \n",
      "\n",
      "Fold: 12  Epoch: 390  Training loss = 2.4889  Validation loss = 0.9568  \n",
      "\n",
      "Fold: 12  Epoch: 391  Training loss = 2.4886  Validation loss = 0.9556  \n",
      "\n",
      "Fold: 12  Epoch: 392  Training loss = 2.4886  Validation loss = 0.9564  \n",
      "\n",
      "Fold: 12  Epoch: 393  Training loss = 2.4882  Validation loss = 0.9560  \n",
      "\n",
      "Fold: 12  Epoch: 394  Training loss = 2.4878  Validation loss = 0.9542  \n",
      "\n",
      "Fold: 12  Epoch: 395  Training loss = 2.4875  Validation loss = 0.9524  \n",
      "\n",
      "Fold: 12  Epoch: 396  Training loss = 2.4873  Validation loss = 0.9518  \n",
      "\n",
      "Fold: 12  Epoch: 397  Training loss = 2.4870  Validation loss = 0.9498  \n",
      "\n",
      "Fold: 12  Epoch: 398  Training loss = 2.4865  Validation loss = 0.9466  \n",
      "\n",
      "Fold: 12  Epoch: 399  Training loss = 2.4862  Validation loss = 0.9474  \n",
      "\n",
      "Fold: 12  Epoch: 400  Training loss = 2.4857  Validation loss = 0.9423  \n",
      "\n",
      "Fold: 12  Epoch: 401  Training loss = 2.4853  Validation loss = 0.9408  \n",
      "\n",
      "Fold: 12  Epoch: 402  Training loss = 2.4851  Validation loss = 0.9414  \n",
      "\n",
      "Fold: 12  Epoch: 403  Training loss = 2.4848  Validation loss = 0.9385  \n",
      "\n",
      "Fold: 12  Epoch: 404  Training loss = 2.4845  Validation loss = 0.9356  \n",
      "\n",
      "Fold: 12  Epoch: 405  Training loss = 2.4843  Validation loss = 0.9346  \n",
      "\n",
      "Fold: 12  Epoch: 406  Training loss = 2.4843  Validation loss = 0.9345  \n",
      "\n",
      "Fold: 12  Epoch: 407  Training loss = 2.4842  Validation loss = 0.9382  \n",
      "\n",
      "Fold: 12  Epoch: 408  Training loss = 2.4839  Validation loss = 0.9375  \n",
      "\n",
      "Fold: 12  Epoch: 409  Training loss = 2.4837  Validation loss = 0.9406  \n",
      "\n",
      "Fold: 12  Epoch: 410  Training loss = 2.4836  Validation loss = 0.9421  \n",
      "\n",
      "Fold: 12  Epoch: 411  Training loss = 2.4833  Validation loss = 0.9417  \n",
      "\n",
      "Fold: 12  Epoch: 412  Training loss = 2.4832  Validation loss = 0.9447  \n",
      "\n",
      "Fold: 12  Epoch: 413  Training loss = 2.4831  Validation loss = 0.9443  \n",
      "\n",
      "Fold: 12  Epoch: 414  Training loss = 2.4829  Validation loss = 0.9465  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 406  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 2.4324  Validation loss = 3.1873  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 2.4323  Validation loss = 3.1871  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 2.4316  Validation loss = 3.1812  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 2.4313  Validation loss = 3.1789  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 2.4312  Validation loss = 3.1792  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 2.4308  Validation loss = 3.1766  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 2.4299  Validation loss = 3.1679  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 2.4296  Validation loss = 3.1656  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 2.4295  Validation loss = 3.1668  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 2.4289  Validation loss = 3.1618  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 2.4283  Validation loss = 3.1549  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 2.4279  Validation loss = 3.1525  \n",
      "\n",
      "Fold: 13  Epoch: 13  Training loss = 2.4275  Validation loss = 3.1493  \n",
      "\n",
      "Fold: 13  Epoch: 14  Training loss = 2.4270  Validation loss = 3.1456  \n",
      "\n",
      "Fold: 13  Epoch: 15  Training loss = 2.4269  Validation loss = 3.1466  \n",
      "\n",
      "Fold: 13  Epoch: 16  Training loss = 2.4265  Validation loss = 3.1443  \n",
      "\n",
      "Fold: 13  Epoch: 17  Training loss = 2.4262  Validation loss = 3.1415  \n",
      "\n",
      "Fold: 13  Epoch: 18  Training loss = 2.4258  Validation loss = 3.1392  \n",
      "\n",
      "Fold: 13  Epoch: 19  Training loss = 2.4253  Validation loss = 3.1344  \n",
      "\n",
      "Fold: 13  Epoch: 20  Training loss = 2.4248  Validation loss = 3.1317  \n",
      "\n",
      "Fold: 13  Epoch: 21  Training loss = 2.4242  Validation loss = 3.1257  \n",
      "\n",
      "Fold: 13  Epoch: 22  Training loss = 2.4242  Validation loss = 3.1250  \n",
      "\n",
      "Fold: 13  Epoch: 23  Training loss = 2.4235  Validation loss = 3.1192  \n",
      "\n",
      "Fold: 13  Epoch: 24  Training loss = 2.4230  Validation loss = 3.1138  \n",
      "\n",
      "Fold: 13  Epoch: 25  Training loss = 2.4228  Validation loss = 3.1134  \n",
      "\n",
      "Fold: 13  Epoch: 26  Training loss = 2.4225  Validation loss = 3.1113  \n",
      "\n",
      "Fold: 13  Epoch: 27  Training loss = 2.4223  Validation loss = 3.1086  \n",
      "\n",
      "Fold: 13  Epoch: 28  Training loss = 2.4223  Validation loss = 3.1118  \n",
      "\n",
      "Fold: 13  Epoch: 29  Training loss = 2.4220  Validation loss = 3.1086  \n",
      "\n",
      "Fold: 13  Epoch: 30  Training loss = 2.4218  Validation loss = 3.1077  \n",
      "\n",
      "Fold: 13  Epoch: 31  Training loss = 2.4216  Validation loss = 3.1068  \n",
      "\n",
      "Fold: 13  Epoch: 32  Training loss = 2.4213  Validation loss = 3.1061  \n",
      "\n",
      "Fold: 13  Epoch: 33  Training loss = 2.4209  Validation loss = 3.1026  \n",
      "\n",
      "Fold: 13  Epoch: 34  Training loss = 2.4206  Validation loss = 3.1013  \n",
      "\n",
      "Fold: 13  Epoch: 35  Training loss = 2.4203  Validation loss = 3.1001  \n",
      "\n",
      "Fold: 13  Epoch: 36  Training loss = 2.4201  Validation loss = 3.0991  \n",
      "\n",
      "Fold: 13  Epoch: 37  Training loss = 2.4198  Validation loss = 3.0970  \n",
      "\n",
      "Fold: 13  Epoch: 38  Training loss = 2.4197  Validation loss = 3.0977  \n",
      "\n",
      "Fold: 13  Epoch: 39  Training loss = 2.4197  Validation loss = 3.0998  \n",
      "\n",
      "Fold: 13  Epoch: 40  Training loss = 2.4196  Validation loss = 3.0999  \n",
      "\n",
      "Fold: 13  Epoch: 41  Training loss = 2.4196  Validation loss = 3.0998  \n",
      "\n",
      "Fold: 13  Epoch: 42  Training loss = 2.4192  Validation loss = 3.0971  \n",
      "\n",
      "Fold: 13  Epoch: 43  Training loss = 2.4188  Validation loss = 3.0940  \n",
      "\n",
      "Fold: 13  Epoch: 44  Training loss = 2.4184  Validation loss = 3.0895  \n",
      "\n",
      "Fold: 13  Epoch: 45  Training loss = 2.4180  Validation loss = 3.0867  \n",
      "\n",
      "Fold: 13  Epoch: 46  Training loss = 2.4180  Validation loss = 3.0870  \n",
      "\n",
      "Fold: 13  Epoch: 47  Training loss = 2.4177  Validation loss = 3.0855  \n",
      "\n",
      "Fold: 13  Epoch: 48  Training loss = 2.4174  Validation loss = 3.0823  \n",
      "\n",
      "Fold: 13  Epoch: 49  Training loss = 2.4172  Validation loss = 3.0828  \n",
      "\n",
      "Fold: 13  Epoch: 50  Training loss = 2.4166  Validation loss = 3.0765  \n",
      "\n",
      "Fold: 13  Epoch: 51  Training loss = 2.4164  Validation loss = 3.0751  \n",
      "\n",
      "Fold: 13  Epoch: 52  Training loss = 2.4163  Validation loss = 3.0754  \n",
      "\n",
      "Fold: 13  Epoch: 53  Training loss = 2.4163  Validation loss = 3.0775  \n",
      "\n",
      "Fold: 13  Epoch: 54  Training loss = 2.4157  Validation loss = 3.0724  \n",
      "\n",
      "Fold: 13  Epoch: 55  Training loss = 2.4155  Validation loss = 3.0700  \n",
      "\n",
      "Fold: 13  Epoch: 56  Training loss = 2.4152  Validation loss = 3.0701  \n",
      "\n",
      "Fold: 13  Epoch: 57  Training loss = 2.4150  Validation loss = 3.0687  \n",
      "\n",
      "Fold: 13  Epoch: 58  Training loss = 2.4147  Validation loss = 3.0652  \n",
      "\n",
      "Fold: 13  Epoch: 59  Training loss = 2.4145  Validation loss = 3.0640  \n",
      "\n",
      "Fold: 13  Epoch: 60  Training loss = 2.4143  Validation loss = 3.0645  \n",
      "\n",
      "Fold: 13  Epoch: 61  Training loss = 2.4142  Validation loss = 3.0647  \n",
      "\n",
      "Fold: 13  Epoch: 62  Training loss = 2.4139  Validation loss = 3.0621  \n",
      "\n",
      "Fold: 13  Epoch: 63  Training loss = 2.4137  Validation loss = 3.0616  \n",
      "\n",
      "Fold: 13  Epoch: 64  Training loss = 2.4133  Validation loss = 3.0600  \n",
      "\n",
      "Fold: 13  Epoch: 65  Training loss = 2.4129  Validation loss = 3.0549  \n",
      "\n",
      "Fold: 13  Epoch: 66  Training loss = 2.4129  Validation loss = 3.0563  \n",
      "\n",
      "Fold: 13  Epoch: 67  Training loss = 2.4124  Validation loss = 3.0522  \n",
      "\n",
      "Fold: 13  Epoch: 68  Training loss = 2.4119  Validation loss = 3.0469  \n",
      "\n",
      "Fold: 13  Epoch: 69  Training loss = 2.4117  Validation loss = 3.0461  \n",
      "\n",
      "Fold: 13  Epoch: 70  Training loss = 2.4117  Validation loss = 3.0473  \n",
      "\n",
      "Fold: 13  Epoch: 71  Training loss = 2.4115  Validation loss = 3.0489  \n",
      "\n",
      "Fold: 13  Epoch: 72  Training loss = 2.4112  Validation loss = 3.0449  \n",
      "\n",
      "Fold: 13  Epoch: 73  Training loss = 2.4110  Validation loss = 3.0454  \n",
      "\n",
      "Fold: 13  Epoch: 74  Training loss = 2.4108  Validation loss = 3.0448  \n",
      "\n",
      "Fold: 13  Epoch: 75  Training loss = 2.4106  Validation loss = 3.0433  \n",
      "\n",
      "Fold: 13  Epoch: 76  Training loss = 2.4101  Validation loss = 3.0390  \n",
      "\n",
      "Fold: 13  Epoch: 77  Training loss = 2.4099  Validation loss = 3.0353  \n",
      "\n",
      "Fold: 13  Epoch: 78  Training loss = 2.4094  Validation loss = 3.0272  \n",
      "\n",
      "Fold: 13  Epoch: 79  Training loss = 2.4090  Validation loss = 3.0211  \n",
      "\n",
      "Fold: 13  Epoch: 80  Training loss = 2.4088  Validation loss = 3.0214  \n",
      "\n",
      "Fold: 13  Epoch: 81  Training loss = 2.4086  Validation loss = 3.0223  \n",
      "\n",
      "Fold: 13  Epoch: 82  Training loss = 2.4084  Validation loss = 3.0189  \n",
      "\n",
      "Fold: 13  Epoch: 83  Training loss = 2.4081  Validation loss = 3.0152  \n",
      "\n",
      "Fold: 13  Epoch: 84  Training loss = 2.4079  Validation loss = 3.0149  \n",
      "\n",
      "Fold: 13  Epoch: 85  Training loss = 2.4078  Validation loss = 3.0173  \n",
      "\n",
      "Fold: 13  Epoch: 86  Training loss = 2.4076  Validation loss = 3.0201  \n",
      "\n",
      "Fold: 13  Epoch: 87  Training loss = 2.4075  Validation loss = 3.0224  \n",
      "\n",
      "Fold: 13  Epoch: 88  Training loss = 2.4073  Validation loss = 3.0224  \n",
      "\n",
      "Fold: 13  Epoch: 89  Training loss = 2.4071  Validation loss = 3.0224  \n",
      "\n",
      "Fold: 13  Epoch: 90  Training loss = 2.4070  Validation loss = 3.0211  \n",
      "\n",
      "Fold: 13  Epoch: 91  Training loss = 2.4068  Validation loss = 3.0185  \n",
      "\n",
      "Fold: 13  Epoch: 92  Training loss = 2.4063  Validation loss = 3.0129  \n",
      "\n",
      "Fold: 13  Epoch: 93  Training loss = 2.4059  Validation loss = 3.0065  \n",
      "\n",
      "Fold: 13  Epoch: 94  Training loss = 2.4059  Validation loss = 3.0086  \n",
      "\n",
      "Fold: 13  Epoch: 95  Training loss = 2.4057  Validation loss = 3.0064  \n",
      "\n",
      "Fold: 13  Epoch: 96  Training loss = 2.4055  Validation loss = 3.0041  \n",
      "\n",
      "Fold: 13  Epoch: 97  Training loss = 2.4054  Validation loss = 3.0036  \n",
      "\n",
      "Fold: 13  Epoch: 98  Training loss = 2.4052  Validation loss = 3.0006  \n",
      "\n",
      "Fold: 13  Epoch: 99  Training loss = 2.4050  Validation loss = 2.9985  \n",
      "\n",
      "Fold: 13  Epoch: 100  Training loss = 2.4049  Validation loss = 2.9997  \n",
      "\n",
      "Fold: 13  Epoch: 101  Training loss = 2.4049  Validation loss = 3.0022  \n",
      "\n",
      "Fold: 13  Epoch: 102  Training loss = 2.4048  Validation loss = 3.0034  \n",
      "\n",
      "Fold: 13  Epoch: 103  Training loss = 2.4045  Validation loss = 2.9996  \n",
      "\n",
      "Fold: 13  Epoch: 104  Training loss = 2.4042  Validation loss = 2.9969  \n",
      "\n",
      "Fold: 13  Epoch: 105  Training loss = 2.4038  Validation loss = 2.9941  \n",
      "\n",
      "Fold: 13  Epoch: 106  Training loss = 2.4038  Validation loss = 2.9958  \n",
      "\n",
      "Fold: 13  Epoch: 107  Training loss = 2.4035  Validation loss = 2.9948  \n",
      "\n",
      "Fold: 13  Epoch: 108  Training loss = 2.4034  Validation loss = 2.9933  \n",
      "\n",
      "Fold: 13  Epoch: 109  Training loss = 2.4032  Validation loss = 2.9943  \n",
      "\n",
      "Fold: 13  Epoch: 110  Training loss = 2.4029  Validation loss = 2.9886  \n",
      "\n",
      "Fold: 13  Epoch: 111  Training loss = 2.4028  Validation loss = 2.9890  \n",
      "\n",
      "Fold: 13  Epoch: 112  Training loss = 2.4027  Validation loss = 2.9917  \n",
      "\n",
      "Fold: 13  Epoch: 113  Training loss = 2.4026  Validation loss = 2.9916  \n",
      "\n",
      "Fold: 13  Epoch: 114  Training loss = 2.4027  Validation loss = 2.9963  \n",
      "\n",
      "Fold: 13  Epoch: 115  Training loss = 2.4026  Validation loss = 3.0014  \n",
      "\n",
      "Fold: 13  Epoch: 116  Training loss = 2.4021  Validation loss = 2.9956  \n",
      "\n",
      "Fold: 13  Epoch: 117  Training loss = 2.4020  Validation loss = 2.9961  \n",
      "\n",
      "Fold: 13  Epoch: 118  Training loss = 2.4018  Validation loss = 2.9947  \n",
      "\n",
      "Fold: 13  Epoch: 119  Training loss = 2.4016  Validation loss = 2.9945  \n",
      "\n",
      "Fold: 13  Epoch: 120  Training loss = 2.4014  Validation loss = 2.9937  \n",
      "\n",
      "Fold: 13  Epoch: 121  Training loss = 2.4012  Validation loss = 2.9933  \n",
      "\n",
      "Fold: 13  Epoch: 122  Training loss = 2.4009  Validation loss = 2.9903  \n",
      "\n",
      "Fold: 13  Epoch: 123  Training loss = 2.4008  Validation loss = 2.9905  \n",
      "\n",
      "Fold: 13  Epoch: 124  Training loss = 2.4006  Validation loss = 2.9899  \n",
      "\n",
      "Fold: 13  Epoch: 125  Training loss = 2.4004  Validation loss = 2.9898  \n",
      "\n",
      "Fold: 13  Epoch: 126  Training loss = 2.4002  Validation loss = 2.9882  \n",
      "\n",
      "Fold: 13  Epoch: 127  Training loss = 2.3999  Validation loss = 2.9836  \n",
      "\n",
      "Fold: 13  Epoch: 128  Training loss = 2.3995  Validation loss = 2.9777  \n",
      "\n",
      "Fold: 13  Epoch: 129  Training loss = 2.3992  Validation loss = 2.9740  \n",
      "\n",
      "Fold: 13  Epoch: 130  Training loss = 2.3989  Validation loss = 2.9692  \n",
      "\n",
      "Fold: 13  Epoch: 131  Training loss = 2.3987  Validation loss = 2.9684  \n",
      "\n",
      "Fold: 13  Epoch: 132  Training loss = 2.3986  Validation loss = 2.9716  \n",
      "\n",
      "Fold: 13  Epoch: 133  Training loss = 2.3984  Validation loss = 2.9698  \n",
      "\n",
      "Fold: 13  Epoch: 134  Training loss = 2.3982  Validation loss = 2.9670  \n",
      "\n",
      "Fold: 13  Epoch: 135  Training loss = 2.3980  Validation loss = 2.9632  \n",
      "\n",
      "Fold: 13  Epoch: 136  Training loss = 2.3979  Validation loss = 2.9623  \n",
      "\n",
      "Fold: 13  Epoch: 137  Training loss = 2.3977  Validation loss = 2.9628  \n",
      "\n",
      "Fold: 13  Epoch: 138  Training loss = 2.3976  Validation loss = 2.9638  \n",
      "\n",
      "Fold: 13  Epoch: 139  Training loss = 2.3973  Validation loss = 2.9618  \n",
      "\n",
      "Fold: 13  Epoch: 140  Training loss = 2.3971  Validation loss = 2.9598  \n",
      "\n",
      "Fold: 13  Epoch: 141  Training loss = 2.3969  Validation loss = 2.9562  \n",
      "\n",
      "Fold: 13  Epoch: 142  Training loss = 2.3967  Validation loss = 2.9545  \n",
      "\n",
      "Fold: 13  Epoch: 143  Training loss = 2.3966  Validation loss = 2.9557  \n",
      "\n",
      "Fold: 13  Epoch: 144  Training loss = 2.3964  Validation loss = 2.9547  \n",
      "\n",
      "Fold: 13  Epoch: 145  Training loss = 2.3963  Validation loss = 2.9544  \n",
      "\n",
      "Fold: 13  Epoch: 146  Training loss = 2.3960  Validation loss = 2.9529  \n",
      "\n",
      "Fold: 13  Epoch: 147  Training loss = 2.3959  Validation loss = 2.9510  \n",
      "\n",
      "Fold: 13  Epoch: 148  Training loss = 2.3957  Validation loss = 2.9513  \n",
      "\n",
      "Fold: 13  Epoch: 149  Training loss = 2.3956  Validation loss = 2.9518  \n",
      "\n",
      "Fold: 13  Epoch: 150  Training loss = 2.3953  Validation loss = 2.9502  \n",
      "\n",
      "Fold: 13  Epoch: 151  Training loss = 2.3952  Validation loss = 2.9511  \n",
      "\n",
      "Fold: 13  Epoch: 152  Training loss = 2.3951  Validation loss = 2.9488  \n",
      "\n",
      "Fold: 13  Epoch: 153  Training loss = 2.3949  Validation loss = 2.9486  \n",
      "\n",
      "Fold: 13  Epoch: 154  Training loss = 2.3947  Validation loss = 2.9519  \n",
      "\n",
      "Fold: 13  Epoch: 155  Training loss = 2.3946  Validation loss = 2.9520  \n",
      "\n",
      "Fold: 13  Epoch: 156  Training loss = 2.3944  Validation loss = 2.9516  \n",
      "\n",
      "Fold: 13  Epoch: 157  Training loss = 2.3943  Validation loss = 2.9542  \n",
      "\n",
      "Fold: 13  Epoch: 158  Training loss = 2.3942  Validation loss = 2.9548  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 153  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 2.4698  Validation loss = 6.2030  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 2.4690  Validation loss = 6.1977  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 2.4690  Validation loss = 6.1984  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 2.4684  Validation loss = 6.1947  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 2.4679  Validation loss = 6.1911  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 2.4678  Validation loss = 6.1907  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 2.4670  Validation loss = 6.1850  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 2.4667  Validation loss = 6.1834  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 2.4662  Validation loss = 6.1798  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 2.4655  Validation loss = 6.1748  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 2.4653  Validation loss = 6.1734  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 2.4647  Validation loss = 6.1698  \n",
      "\n",
      "Fold: 14  Epoch: 13  Training loss = 2.4649  Validation loss = 6.1713  \n",
      "\n",
      "Fold: 14  Epoch: 14  Training loss = 2.4640  Validation loss = 6.1649  \n",
      "\n",
      "Fold: 14  Epoch: 15  Training loss = 2.4630  Validation loss = 6.1572  \n",
      "\n",
      "Fold: 14  Epoch: 16  Training loss = 2.4629  Validation loss = 6.1563  \n",
      "\n",
      "Fold: 14  Epoch: 17  Training loss = 2.4622  Validation loss = 6.1507  \n",
      "\n",
      "Fold: 14  Epoch: 18  Training loss = 2.4620  Validation loss = 6.1498  \n",
      "\n",
      "Fold: 14  Epoch: 19  Training loss = 2.4618  Validation loss = 6.1491  \n",
      "\n",
      "Fold: 14  Epoch: 20  Training loss = 2.4608  Validation loss = 6.1410  \n",
      "\n",
      "Fold: 14  Epoch: 21  Training loss = 2.4606  Validation loss = 6.1402  \n",
      "\n",
      "Fold: 14  Epoch: 22  Training loss = 2.4601  Validation loss = 6.1364  \n",
      "\n",
      "Fold: 14  Epoch: 23  Training loss = 2.4596  Validation loss = 6.1329  \n",
      "\n",
      "Fold: 14  Epoch: 24  Training loss = 2.4590  Validation loss = 6.1281  \n",
      "\n",
      "Fold: 14  Epoch: 25  Training loss = 2.4590  Validation loss = 6.1280  \n",
      "\n",
      "Fold: 14  Epoch: 26  Training loss = 2.4588  Validation loss = 6.1271  \n",
      "\n",
      "Fold: 14  Epoch: 27  Training loss = 2.4582  Validation loss = 6.1229  \n",
      "\n",
      "Fold: 14  Epoch: 28  Training loss = 2.4578  Validation loss = 6.1191  \n",
      "\n",
      "Fold: 14  Epoch: 29  Training loss = 2.4571  Validation loss = 6.1136  \n",
      "\n",
      "Fold: 14  Epoch: 30  Training loss = 2.4568  Validation loss = 6.1114  \n",
      "\n",
      "Fold: 14  Epoch: 31  Training loss = 2.4563  Validation loss = 6.1079  \n",
      "\n",
      "Fold: 14  Epoch: 32  Training loss = 2.4558  Validation loss = 6.1035  \n",
      "\n",
      "Fold: 14  Epoch: 33  Training loss = 2.4552  Validation loss = 6.0985  \n",
      "\n",
      "Fold: 14  Epoch: 34  Training loss = 2.4552  Validation loss = 6.0989  \n",
      "\n",
      "Fold: 14  Epoch: 35  Training loss = 2.4548  Validation loss = 6.0963  \n",
      "\n",
      "Fold: 14  Epoch: 36  Training loss = 2.4543  Validation loss = 6.0916  \n",
      "\n",
      "Fold: 14  Epoch: 37  Training loss = 2.4544  Validation loss = 6.0938  \n",
      "\n",
      "Fold: 14  Epoch: 38  Training loss = 2.4544  Validation loss = 6.0939  \n",
      "\n",
      "Fold: 14  Epoch: 39  Training loss = 2.4540  Validation loss = 6.0913  \n",
      "\n",
      "Fold: 14  Epoch: 40  Training loss = 2.4539  Validation loss = 6.0908  \n",
      "\n",
      "Fold: 14  Epoch: 41  Training loss = 2.4531  Validation loss = 6.0852  \n",
      "\n",
      "Fold: 14  Epoch: 42  Training loss = 2.4511  Validation loss = 6.0838  \n",
      "\n",
      "Fold: 14  Epoch: 43  Training loss = 2.4505  Validation loss = 6.0783  \n",
      "\n",
      "Fold: 14  Epoch: 44  Training loss = 2.4498  Validation loss = 6.0721  \n",
      "\n",
      "Fold: 14  Epoch: 45  Training loss = 2.4498  Validation loss = 6.0724  \n",
      "\n",
      "Fold: 14  Epoch: 46  Training loss = 2.4492  Validation loss = 6.0677  \n",
      "\n",
      "Fold: 14  Epoch: 47  Training loss = 2.4491  Validation loss = 6.0663  \n",
      "\n",
      "Fold: 14  Epoch: 48  Training loss = 2.4486  Validation loss = 6.0623  \n",
      "\n",
      "Fold: 14  Epoch: 49  Training loss = 2.4482  Validation loss = 6.0584  \n",
      "\n",
      "Fold: 14  Epoch: 50  Training loss = 2.4481  Validation loss = 6.0576  \n",
      "\n",
      "Fold: 14  Epoch: 51  Training loss = 2.4474  Validation loss = 6.0515  \n",
      "\n",
      "Fold: 14  Epoch: 52  Training loss = 2.4473  Validation loss = 6.0507  \n",
      "\n",
      "Fold: 14  Epoch: 53  Training loss = 2.4468  Validation loss = 6.0462  \n",
      "\n",
      "Fold: 14  Epoch: 54  Training loss = 2.4466  Validation loss = 6.0439  \n",
      "\n",
      "Fold: 14  Epoch: 55  Training loss = 2.4466  Validation loss = 6.0449  \n",
      "\n",
      "Fold: 14  Epoch: 56  Training loss = 2.4464  Validation loss = 6.0441  \n",
      "\n",
      "Fold: 14  Epoch: 57  Training loss = 2.4463  Validation loss = 6.0429  \n",
      "\n",
      "Fold: 14  Epoch: 58  Training loss = 2.4459  Validation loss = 6.0400  \n",
      "\n",
      "Fold: 14  Epoch: 59  Training loss = 2.4458  Validation loss = 6.0393  \n",
      "\n",
      "Fold: 14  Epoch: 60  Training loss = 2.4455  Validation loss = 6.0377  \n",
      "\n",
      "Fold: 14  Epoch: 61  Training loss = 2.4453  Validation loss = 6.0354  \n",
      "\n",
      "Fold: 14  Epoch: 62  Training loss = 2.4448  Validation loss = 6.0300  \n",
      "\n",
      "Fold: 14  Epoch: 63  Training loss = 2.4447  Validation loss = 6.0300  \n",
      "\n",
      "Fold: 14  Epoch: 64  Training loss = 2.4443  Validation loss = 6.0260  \n",
      "\n",
      "Fold: 14  Epoch: 65  Training loss = 2.4440  Validation loss = 6.0225  \n",
      "\n",
      "Fold: 14  Epoch: 66  Training loss = 2.4436  Validation loss = 6.0189  \n",
      "\n",
      "Fold: 14  Epoch: 67  Training loss = 2.4434  Validation loss = 6.0168  \n",
      "\n",
      "Fold: 14  Epoch: 68  Training loss = 2.4430  Validation loss = 6.0131  \n",
      "\n",
      "Fold: 14  Epoch: 69  Training loss = 2.4430  Validation loss = 6.0149  \n",
      "\n",
      "Fold: 14  Epoch: 70  Training loss = 2.4432  Validation loss = 6.0177  \n",
      "\n",
      "Fold: 14  Epoch: 71  Training loss = 2.4428  Validation loss = 6.0134  \n",
      "\n",
      "Fold: 14  Epoch: 72  Training loss = 2.4426  Validation loss = 6.0117  \n",
      "\n",
      "Fold: 14  Epoch: 73  Training loss = 2.4423  Validation loss = 6.0094  \n",
      "\n",
      "Fold: 14  Epoch: 74  Training loss = 2.4418  Validation loss = 6.0039  \n",
      "\n",
      "Fold: 14  Epoch: 75  Training loss = 2.4416  Validation loss = 6.0018  \n",
      "\n",
      "Fold: 14  Epoch: 76  Training loss = 2.4413  Validation loss = 5.9992  \n",
      "\n",
      "Fold: 14  Epoch: 77  Training loss = 2.4411  Validation loss = 5.9977  \n",
      "\n",
      "Fold: 14  Epoch: 78  Training loss = 2.4410  Validation loss = 5.9971  \n",
      "\n",
      "Fold: 14  Epoch: 79  Training loss = 2.4409  Validation loss = 5.9976  \n",
      "\n",
      "Fold: 14  Epoch: 80  Training loss = 2.4406  Validation loss = 5.9939  \n",
      "\n",
      "Fold: 14  Epoch: 81  Training loss = 2.4404  Validation loss = 5.9925  \n",
      "\n",
      "Fold: 14  Epoch: 82  Training loss = 2.4400  Validation loss = 5.9880  \n",
      "\n",
      "Fold: 14  Epoch: 83  Training loss = 2.4398  Validation loss = 5.9860  \n",
      "\n",
      "Fold: 14  Epoch: 84  Training loss = 2.4396  Validation loss = 5.9841  \n",
      "\n",
      "Fold: 14  Epoch: 85  Training loss = 2.4393  Validation loss = 5.9823  \n",
      "\n",
      "Fold: 14  Epoch: 86  Training loss = 2.4393  Validation loss = 5.9833  \n",
      "\n",
      "Fold: 14  Epoch: 87  Training loss = 2.4391  Validation loss = 5.9814  \n",
      "\n",
      "Fold: 14  Epoch: 88  Training loss = 2.4387  Validation loss = 5.9761  \n",
      "\n",
      "Fold: 14  Epoch: 89  Training loss = 2.4384  Validation loss = 5.9733  \n",
      "\n",
      "Fold: 14  Epoch: 90  Training loss = 2.4383  Validation loss = 5.9728  \n",
      "\n",
      "Fold: 14  Epoch: 91  Training loss = 2.4380  Validation loss = 5.9690  \n",
      "\n",
      "Fold: 14  Epoch: 92  Training loss = 2.4377  Validation loss = 5.9660  \n",
      "\n",
      "Fold: 14  Epoch: 93  Training loss = 2.4374  Validation loss = 5.9640  \n",
      "\n",
      "Fold: 14  Epoch: 94  Training loss = 2.4373  Validation loss = 5.9639  \n",
      "\n",
      "Fold: 14  Epoch: 95  Training loss = 2.4371  Validation loss = 5.9616  \n",
      "\n",
      "Fold: 14  Epoch: 96  Training loss = 2.4369  Validation loss = 5.9597  \n",
      "\n",
      "Fold: 14  Epoch: 97  Training loss = 2.4367  Validation loss = 5.9591  \n",
      "\n",
      "Fold: 14  Epoch: 98  Training loss = 2.4363  Validation loss = 5.9538  \n",
      "\n",
      "Fold: 14  Epoch: 99  Training loss = 2.4364  Validation loss = 5.9569  \n",
      "\n",
      "Fold: 14  Epoch: 100  Training loss = 2.4360  Validation loss = 5.9514  \n",
      "\n",
      "Fold: 14  Epoch: 101  Training loss = 2.4359  Validation loss = 5.9515  \n",
      "\n",
      "Fold: 14  Epoch: 102  Training loss = 2.4357  Validation loss = 5.9499  \n",
      "\n",
      "Fold: 14  Epoch: 103  Training loss = 2.4355  Validation loss = 5.9477  \n",
      "\n",
      "Fold: 14  Epoch: 104  Training loss = 2.4355  Validation loss = 5.9478  \n",
      "\n",
      "Fold: 14  Epoch: 105  Training loss = 2.4355  Validation loss = 5.9498  \n",
      "\n",
      "Fold: 14  Epoch: 106  Training loss = 2.4353  Validation loss = 5.9488  \n",
      "\n",
      "Fold: 14  Epoch: 107  Training loss = 2.4354  Validation loss = 5.9511  \n",
      "\n",
      "Fold: 14  Epoch: 108  Training loss = 2.4350  Validation loss = 5.9471  \n",
      "\n",
      "Fold: 14  Epoch: 109  Training loss = 2.4350  Validation loss = 5.9487  \n",
      "\n",
      "Fold: 14  Epoch: 110  Training loss = 2.4346  Validation loss = 5.9435  \n",
      "\n",
      "Fold: 14  Epoch: 111  Training loss = 2.4343  Validation loss = 5.9402  \n",
      "\n",
      "Fold: 14  Epoch: 112  Training loss = 2.4343  Validation loss = 5.9415  \n",
      "\n",
      "Fold: 14  Epoch: 113  Training loss = 2.4340  Validation loss = 5.9376  \n",
      "\n",
      "Fold: 14  Epoch: 114  Training loss = 2.4336  Validation loss = 5.9326  \n",
      "\n",
      "Fold: 14  Epoch: 115  Training loss = 2.4335  Validation loss = 5.9325  \n",
      "\n",
      "Fold: 14  Epoch: 116  Training loss = 2.4332  Validation loss = 5.9286  \n",
      "\n",
      "Fold: 14  Epoch: 117  Training loss = 2.4332  Validation loss = 5.9294  \n",
      "\n",
      "Fold: 14  Epoch: 118  Training loss = 2.4330  Validation loss = 5.9271  \n",
      "\n",
      "Fold: 14  Epoch: 119  Training loss = 2.4327  Validation loss = 5.9234  \n",
      "\n",
      "Fold: 14  Epoch: 120  Training loss = 2.4327  Validation loss = 5.9250  \n",
      "\n",
      "Fold: 14  Epoch: 121  Training loss = 2.4323  Validation loss = 5.9213  \n",
      "\n",
      "Fold: 14  Epoch: 122  Training loss = 2.4321  Validation loss = 5.9183  \n",
      "\n",
      "Fold: 14  Epoch: 123  Training loss = 2.4320  Validation loss = 5.9172  \n",
      "\n",
      "Fold: 14  Epoch: 124  Training loss = 2.4318  Validation loss = 5.9162  \n",
      "\n",
      "Fold: 14  Epoch: 125  Training loss = 2.4317  Validation loss = 5.9155  \n",
      "\n",
      "Fold: 14  Epoch: 126  Training loss = 2.4313  Validation loss = 5.9107  \n",
      "\n",
      "Fold: 14  Epoch: 127  Training loss = 2.4311  Validation loss = 5.9090  \n",
      "\n",
      "Fold: 14  Epoch: 128  Training loss = 2.4310  Validation loss = 5.9076  \n",
      "\n",
      "Fold: 14  Epoch: 129  Training loss = 2.4309  Validation loss = 5.9070  \n",
      "\n",
      "Fold: 14  Epoch: 130  Training loss = 2.4308  Validation loss = 5.9058  \n",
      "\n",
      "Fold: 14  Epoch: 131  Training loss = 2.4307  Validation loss = 5.9064  \n",
      "\n",
      "Fold: 14  Epoch: 132  Training loss = 2.4307  Validation loss = 5.9079  \n",
      "\n",
      "Fold: 14  Epoch: 133  Training loss = 2.4308  Validation loss = 5.9115  \n",
      "\n",
      "Fold: 14  Epoch: 134  Training loss = 2.4306  Validation loss = 5.9103  \n",
      "\n",
      "Fold: 14  Epoch: 135  Training loss = 2.4305  Validation loss = 5.9098  \n",
      "\n",
      "Fold: 14  Epoch: 136  Training loss = 2.4304  Validation loss = 5.9116  \n",
      "\n",
      "Fold: 14  Epoch: 137  Training loss = 2.4303  Validation loss = 5.9108  \n",
      "\n",
      "Fold: 14  Epoch: 138  Training loss = 2.4300  Validation loss = 5.9066  \n",
      "\n",
      "Fold: 14  Epoch: 139  Training loss = 2.4300  Validation loss = 5.9081  \n",
      "\n",
      "Fold: 14  Epoch: 140  Training loss = 2.4296  Validation loss = 5.9029  \n",
      "\n",
      "Fold: 14  Epoch: 141  Training loss = 2.4294  Validation loss = 5.8984  \n",
      "\n",
      "Fold: 14  Epoch: 142  Training loss = 2.4291  Validation loss = 5.8939  \n",
      "\n",
      "Fold: 14  Epoch: 143  Training loss = 2.4289  Validation loss = 5.8918  \n",
      "\n",
      "Fold: 14  Epoch: 144  Training loss = 2.4288  Validation loss = 5.8897  \n",
      "\n",
      "Fold: 14  Epoch: 145  Training loss = 2.4285  Validation loss = 5.8864  \n",
      "\n",
      "Fold: 14  Epoch: 146  Training loss = 2.4282  Validation loss = 5.8839  \n",
      "\n",
      "Fold: 14  Epoch: 147  Training loss = 2.4281  Validation loss = 5.8829  \n",
      "\n",
      "Fold: 14  Epoch: 148  Training loss = 2.4280  Validation loss = 5.8828  \n",
      "\n",
      "Fold: 14  Epoch: 149  Training loss = 2.4280  Validation loss = 5.8831  \n",
      "\n",
      "Fold: 14  Epoch: 150  Training loss = 2.4279  Validation loss = 5.8845  \n",
      "\n",
      "Fold: 14  Epoch: 151  Training loss = 2.4277  Validation loss = 5.8823  \n",
      "\n",
      "Fold: 14  Epoch: 152  Training loss = 2.4277  Validation loss = 5.8825  \n",
      "\n",
      "Fold: 14  Epoch: 153  Training loss = 2.4276  Validation loss = 5.8834  \n",
      "\n",
      "Fold: 14  Epoch: 154  Training loss = 2.4274  Validation loss = 5.8816  \n",
      "\n",
      "Fold: 14  Epoch: 155  Training loss = 2.4274  Validation loss = 5.8826  \n",
      "\n",
      "Fold: 14  Epoch: 156  Training loss = 2.4272  Validation loss = 5.8814  \n",
      "\n",
      "Fold: 14  Epoch: 157  Training loss = 2.4270  Validation loss = 5.8798  \n",
      "\n",
      "Fold: 14  Epoch: 158  Training loss = 2.4268  Validation loss = 5.8778  \n",
      "\n",
      "Fold: 14  Epoch: 159  Training loss = 2.4270  Validation loss = 5.8845  \n",
      "\n",
      "Fold: 14  Epoch: 160  Training loss = 2.4269  Validation loss = 5.8831  \n",
      "\n",
      "Fold: 14  Epoch: 161  Training loss = 2.4267  Validation loss = 5.8813  \n",
      "\n",
      "Fold: 14  Epoch: 162  Training loss = 2.4267  Validation loss = 5.8818  \n",
      "\n",
      "Fold: 14  Epoch: 163  Training loss = 2.4266  Validation loss = 5.8823  \n",
      "\n",
      "Fold: 14  Epoch: 164  Training loss = 2.4264  Validation loss = 5.8798  \n",
      "\n",
      "Fold: 14  Epoch: 165  Training loss = 2.4261  Validation loss = 5.8758  \n",
      "\n",
      "Fold: 14  Epoch: 166  Training loss = 2.4259  Validation loss = 5.8722  \n",
      "\n",
      "Fold: 14  Epoch: 167  Training loss = 2.4258  Validation loss = 5.8736  \n",
      "\n",
      "Fold: 14  Epoch: 168  Training loss = 2.4258  Validation loss = 5.8763  \n",
      "\n",
      "Fold: 14  Epoch: 169  Training loss = 2.4259  Validation loss = 5.8791  \n",
      "\n",
      "Fold: 14  Epoch: 170  Training loss = 2.4259  Validation loss = 5.8820  \n",
      "\n",
      "Fold: 14  Epoch: 171  Training loss = 2.4257  Validation loss = 5.8779  \n",
      "\n",
      "Fold: 14  Epoch: 172  Training loss = 2.4254  Validation loss = 5.8760  \n",
      "\n",
      "Fold: 14  Epoch: 173  Training loss = 2.4253  Validation loss = 5.8749  \n",
      "\n",
      "Fold: 14  Epoch: 174  Training loss = 2.4252  Validation loss = 5.8750  \n",
      "\n",
      "Fold: 14  Epoch: 175  Training loss = 2.4254  Validation loss = 5.8803  \n",
      "\n",
      "Fold: 14  Epoch: 176  Training loss = 2.4253  Validation loss = 5.8800  \n",
      "\n",
      "Fold: 14  Epoch: 177  Training loss = 2.4252  Validation loss = 5.8797  \n",
      "\n",
      "Fold: 14  Epoch: 178  Training loss = 2.4249  Validation loss = 5.8754  \n",
      "\n",
      "Fold: 14  Epoch: 179  Training loss = 2.4249  Validation loss = 5.8773  \n",
      "\n",
      "Fold: 14  Epoch: 180  Training loss = 2.4248  Validation loss = 5.8770  \n",
      "\n",
      "Fold: 14  Epoch: 181  Training loss = 2.4248  Validation loss = 5.8776  \n",
      "\n",
      "Fold: 14  Epoch: 182  Training loss = 2.4249  Validation loss = 5.8813  \n",
      "\n",
      "Fold: 14  Epoch: 183  Training loss = 2.4248  Validation loss = 5.8795  \n",
      "\n",
      "Fold: 14  Epoch: 184  Training loss = 2.4246  Validation loss = 5.8780  \n",
      "\n",
      "Fold: 14  Epoch: 185  Training loss = 2.4247  Validation loss = 5.8814  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 166  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 2.8035  Validation loss = 6.4055  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 2.8015  Validation loss = 6.3977  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 2.8011  Validation loss = 6.3959  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 2.8000  Validation loss = 6.3916  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 2.8000  Validation loss = 6.3915  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 2.7987  Validation loss = 6.3860  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 2.7979  Validation loss = 6.3823  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 2.7975  Validation loss = 6.3804  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 2.7952  Validation loss = 6.3697  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 2.7947  Validation loss = 6.3675  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 2.7941  Validation loss = 6.3648  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 2.7935  Validation loss = 6.3625  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 2.7930  Validation loss = 6.3605  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 2.7921  Validation loss = 6.3568  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 2.7912  Validation loss = 6.3530  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 2.7906  Validation loss = 6.3500  \n",
      "\n",
      "Fold: 15  Epoch: 17  Training loss = 2.7906  Validation loss = 6.3500  \n",
      "\n",
      "Fold: 15  Epoch: 18  Training loss = 2.7891  Validation loss = 6.3431  \n",
      "\n",
      "Fold: 15  Epoch: 19  Training loss = 2.7883  Validation loss = 6.3394  \n",
      "\n",
      "Fold: 15  Epoch: 20  Training loss = 2.7877  Validation loss = 6.3364  \n",
      "\n",
      "Fold: 15  Epoch: 21  Training loss = 2.7864  Validation loss = 6.3311  \n",
      "\n",
      "Fold: 15  Epoch: 22  Training loss = 2.7859  Validation loss = 6.3285  \n",
      "\n",
      "Fold: 15  Epoch: 23  Training loss = 2.7860  Validation loss = 6.3293  \n",
      "\n",
      "Fold: 15  Epoch: 24  Training loss = 2.7853  Validation loss = 6.3259  \n",
      "\n",
      "Fold: 15  Epoch: 25  Training loss = 2.7841  Validation loss = 6.3211  \n",
      "\n",
      "Fold: 15  Epoch: 26  Training loss = 2.7837  Validation loss = 6.3192  \n",
      "\n",
      "Fold: 15  Epoch: 27  Training loss = 2.7835  Validation loss = 6.3185  \n",
      "\n",
      "Fold: 15  Epoch: 28  Training loss = 2.7829  Validation loss = 6.3156  \n",
      "\n",
      "Fold: 15  Epoch: 29  Training loss = 2.7820  Validation loss = 6.3115  \n",
      "\n",
      "Fold: 15  Epoch: 30  Training loss = 2.7800  Validation loss = 6.3018  \n",
      "\n",
      "Fold: 15  Epoch: 31  Training loss = 2.7796  Validation loss = 6.3004  \n",
      "\n",
      "Fold: 15  Epoch: 32  Training loss = 2.7791  Validation loss = 6.2980  \n",
      "\n",
      "Fold: 15  Epoch: 33  Training loss = 2.7792  Validation loss = 6.2988  \n",
      "\n",
      "Fold: 15  Epoch: 34  Training loss = 2.7780  Validation loss = 6.2933  \n",
      "\n",
      "Fold: 15  Epoch: 35  Training loss = 2.7771  Validation loss = 6.2891  \n",
      "\n",
      "Fold: 15  Epoch: 36  Training loss = 2.7766  Validation loss = 6.2868  \n",
      "\n",
      "Fold: 15  Epoch: 37  Training loss = 2.7755  Validation loss = 6.2815  \n",
      "\n",
      "Fold: 15  Epoch: 38  Training loss = 2.7743  Validation loss = 6.2750  \n",
      "\n",
      "Fold: 15  Epoch: 39  Training loss = 2.7744  Validation loss = 6.2755  \n",
      "\n",
      "Fold: 15  Epoch: 40  Training loss = 2.7739  Validation loss = 6.2733  \n",
      "\n",
      "Fold: 15  Epoch: 41  Training loss = 2.7729  Validation loss = 6.2684  \n",
      "\n",
      "Fold: 15  Epoch: 42  Training loss = 2.7727  Validation loss = 6.2669  \n",
      "\n",
      "Fold: 15  Epoch: 43  Training loss = 2.7711  Validation loss = 6.2591  \n",
      "\n",
      "Fold: 15  Epoch: 44  Training loss = 2.7697  Validation loss = 6.2515  \n",
      "\n",
      "Fold: 15  Epoch: 45  Training loss = 2.7696  Validation loss = 6.2509  \n",
      "\n",
      "Fold: 15  Epoch: 46  Training loss = 2.7691  Validation loss = 6.2486  \n",
      "\n",
      "Fold: 15  Epoch: 47  Training loss = 2.7692  Validation loss = 6.2490  \n",
      "\n",
      "Fold: 15  Epoch: 48  Training loss = 2.7676  Validation loss = 6.2409  \n",
      "\n",
      "Fold: 15  Epoch: 49  Training loss = 2.7671  Validation loss = 6.2382  \n",
      "\n",
      "Fold: 15  Epoch: 50  Training loss = 2.7670  Validation loss = 6.2385  \n",
      "\n",
      "Fold: 15  Epoch: 51  Training loss = 2.7668  Validation loss = 6.2375  \n",
      "\n",
      "Fold: 15  Epoch: 52  Training loss = 2.7653  Validation loss = 6.2290  \n",
      "\n",
      "Fold: 15  Epoch: 53  Training loss = 2.7639  Validation loss = 6.2218  \n",
      "\n",
      "Fold: 15  Epoch: 54  Training loss = 2.7633  Validation loss = 6.2188  \n",
      "\n",
      "Fold: 15  Epoch: 55  Training loss = 2.7630  Validation loss = 6.2173  \n",
      "\n",
      "Fold: 15  Epoch: 56  Training loss = 2.7615  Validation loss = 6.2085  \n",
      "\n",
      "Fold: 15  Epoch: 57  Training loss = 2.7610  Validation loss = 6.2054  \n",
      "\n",
      "Fold: 15  Epoch: 58  Training loss = 2.7611  Validation loss = 6.2065  \n",
      "\n",
      "Fold: 15  Epoch: 59  Training loss = 2.7596  Validation loss = 6.2023  \n",
      "\n",
      "Fold: 15  Epoch: 60  Training loss = 2.7595  Validation loss = 6.2029  \n",
      "\n",
      "Fold: 15  Epoch: 61  Training loss = 2.7577  Validation loss = 6.1990  \n",
      "\n",
      "Fold: 15  Epoch: 62  Training loss = 2.7567  Validation loss = 6.1933  \n",
      "\n",
      "Fold: 15  Epoch: 63  Training loss = 2.7566  Validation loss = 6.1931  \n",
      "\n",
      "Fold: 15  Epoch: 64  Training loss = 2.7561  Validation loss = 6.1908  \n",
      "\n",
      "Fold: 15  Epoch: 65  Training loss = 2.7554  Validation loss = 6.1865  \n",
      "\n",
      "Fold: 15  Epoch: 66  Training loss = 2.7545  Validation loss = 6.1814  \n",
      "\n",
      "Fold: 15  Epoch: 67  Training loss = 2.7540  Validation loss = 6.1789  \n",
      "\n",
      "Fold: 15  Epoch: 68  Training loss = 2.7543  Validation loss = 6.1804  \n",
      "\n",
      "Fold: 15  Epoch: 69  Training loss = 2.7539  Validation loss = 6.1790  \n",
      "\n",
      "Fold: 15  Epoch: 70  Training loss = 2.7534  Validation loss = 6.1757  \n",
      "\n",
      "Fold: 15  Epoch: 71  Training loss = 2.7538  Validation loss = 6.1792  \n",
      "\n",
      "Fold: 15  Epoch: 72  Training loss = 2.7532  Validation loss = 6.1762  \n",
      "\n",
      "Fold: 15  Epoch: 73  Training loss = 2.7533  Validation loss = 6.1764  \n",
      "\n",
      "Fold: 15  Epoch: 74  Training loss = 2.7531  Validation loss = 6.1761  \n",
      "\n",
      "Fold: 15  Epoch: 75  Training loss = 2.7528  Validation loss = 6.1736  \n",
      "\n",
      "Fold: 15  Epoch: 76  Training loss = 2.7523  Validation loss = 6.1706  \n",
      "\n",
      "Fold: 15  Epoch: 77  Training loss = 2.7523  Validation loss = 6.1710  \n",
      "\n",
      "Fold: 15  Epoch: 78  Training loss = 2.7522  Validation loss = 6.1713  \n",
      "\n",
      "Fold: 15  Epoch: 79  Training loss = 2.7513  Validation loss = 6.1659  \n",
      "\n",
      "Fold: 15  Epoch: 80  Training loss = 2.7509  Validation loss = 6.1635  \n",
      "\n",
      "Fold: 15  Epoch: 81  Training loss = 2.7496  Validation loss = 6.1553  \n",
      "\n",
      "Fold: 15  Epoch: 82  Training loss = 2.7493  Validation loss = 6.1537  \n",
      "\n",
      "Fold: 15  Epoch: 83  Training loss = 2.7484  Validation loss = 6.1480  \n",
      "\n",
      "Fold: 15  Epoch: 84  Training loss = 2.7480  Validation loss = 6.1460  \n",
      "\n",
      "Fold: 15  Epoch: 85  Training loss = 2.7472  Validation loss = 6.1406  \n",
      "\n",
      "Fold: 15  Epoch: 86  Training loss = 2.7463  Validation loss = 6.1344  \n",
      "\n",
      "Fold: 15  Epoch: 87  Training loss = 2.7464  Validation loss = 6.1358  \n",
      "\n",
      "Fold: 15  Epoch: 88  Training loss = 2.7462  Validation loss = 6.1371  \n",
      "\n",
      "Fold: 15  Epoch: 89  Training loss = 2.7450  Validation loss = 6.1364  \n",
      "\n",
      "Fold: 15  Epoch: 90  Training loss = 2.7441  Validation loss = 6.1336  \n",
      "\n",
      "Fold: 15  Epoch: 91  Training loss = 2.7438  Validation loss = 6.1318  \n",
      "\n",
      "Fold: 15  Epoch: 92  Training loss = 2.7432  Validation loss = 6.1280  \n",
      "\n",
      "Fold: 15  Epoch: 93  Training loss = 2.7424  Validation loss = 6.1224  \n",
      "\n",
      "Fold: 15  Epoch: 94  Training loss = 2.7415  Validation loss = 6.1156  \n",
      "\n",
      "Fold: 15  Epoch: 95  Training loss = 2.7412  Validation loss = 6.1138  \n",
      "\n",
      "Fold: 15  Epoch: 96  Training loss = 2.7405  Validation loss = 6.1091  \n",
      "\n",
      "Fold: 15  Epoch: 97  Training loss = 2.7400  Validation loss = 6.1061  \n",
      "\n",
      "Fold: 15  Epoch: 98  Training loss = 2.7398  Validation loss = 6.1047  \n",
      "\n",
      "Fold: 15  Epoch: 99  Training loss = 2.7392  Validation loss = 6.1009  \n",
      "\n",
      "Fold: 15  Epoch: 100  Training loss = 2.7382  Validation loss = 6.0935  \n",
      "\n",
      "Fold: 15  Epoch: 101  Training loss = 2.7376  Validation loss = 6.0891  \n",
      "\n",
      "Fold: 15  Epoch: 102  Training loss = 2.7371  Validation loss = 6.0855  \n",
      "\n",
      "Fold: 15  Epoch: 103  Training loss = 2.7370  Validation loss = 6.0853  \n",
      "\n",
      "Fold: 15  Epoch: 104  Training loss = 2.7368  Validation loss = 6.0838  \n",
      "\n",
      "Fold: 15  Epoch: 105  Training loss = 2.7360  Validation loss = 6.0785  \n",
      "\n",
      "Fold: 15  Epoch: 106  Training loss = 2.7359  Validation loss = 6.0780  \n",
      "\n",
      "Fold: 15  Epoch: 107  Training loss = 2.7356  Validation loss = 6.0761  \n",
      "\n",
      "Fold: 15  Epoch: 108  Training loss = 2.7359  Validation loss = 6.0802  \n",
      "\n",
      "Fold: 15  Epoch: 109  Training loss = 2.7353  Validation loss = 6.0756  \n",
      "\n",
      "Fold: 15  Epoch: 110  Training loss = 2.7351  Validation loss = 6.0744  \n",
      "\n",
      "Fold: 15  Epoch: 111  Training loss = 2.7347  Validation loss = 6.0721  \n",
      "\n",
      "Fold: 15  Epoch: 112  Training loss = 2.7343  Validation loss = 6.0687  \n",
      "\n",
      "Fold: 15  Epoch: 113  Training loss = 2.7341  Validation loss = 6.0678  \n",
      "\n",
      "Fold: 15  Epoch: 114  Training loss = 2.7340  Validation loss = 6.0697  \n",
      "\n",
      "Fold: 15  Epoch: 115  Training loss = 2.7325  Validation loss = 6.0668  \n",
      "\n",
      "Fold: 15  Epoch: 116  Training loss = 2.7322  Validation loss = 6.0657  \n",
      "\n",
      "Fold: 15  Epoch: 117  Training loss = 2.7324  Validation loss = 6.0682  \n",
      "\n",
      "Fold: 15  Epoch: 118  Training loss = 2.7318  Validation loss = 6.0633  \n",
      "\n",
      "Fold: 15  Epoch: 119  Training loss = 2.7321  Validation loss = 6.0669  \n",
      "\n",
      "Fold: 15  Epoch: 120  Training loss = 2.7321  Validation loss = 6.0677  \n",
      "\n",
      "Fold: 15  Epoch: 121  Training loss = 2.7317  Validation loss = 6.0654  \n",
      "\n",
      "Fold: 15  Epoch: 122  Training loss = 2.7321  Validation loss = 6.0696  \n",
      "\n",
      "Fold: 15  Epoch: 123  Training loss = 2.7321  Validation loss = 6.0707  \n",
      "\n",
      "Fold: 15  Epoch: 124  Training loss = 2.7319  Validation loss = 6.0700  \n",
      "\n",
      "Fold: 15  Epoch: 125  Training loss = 2.7317  Validation loss = 6.0691  \n",
      "\n",
      "Fold: 15  Epoch: 126  Training loss = 2.7310  Validation loss = 6.0629  \n",
      "\n",
      "Fold: 15  Epoch: 127  Training loss = 2.7307  Validation loss = 6.0616  \n",
      "\n",
      "Fold: 15  Epoch: 128  Training loss = 2.7305  Validation loss = 6.0606  \n",
      "\n",
      "Fold: 15  Epoch: 129  Training loss = 2.7302  Validation loss = 6.0585  \n",
      "\n",
      "Fold: 15  Epoch: 130  Training loss = 2.7301  Validation loss = 6.0582  \n",
      "\n",
      "Fold: 15  Epoch: 131  Training loss = 2.7299  Validation loss = 6.0575  \n",
      "\n",
      "Fold: 15  Epoch: 132  Training loss = 2.7297  Validation loss = 6.0560  \n",
      "\n",
      "Fold: 15  Epoch: 133  Training loss = 2.7289  Validation loss = 6.0496  \n",
      "\n",
      "Fold: 15  Epoch: 134  Training loss = 2.7285  Validation loss = 6.0465  \n",
      "\n",
      "Fold: 15  Epoch: 135  Training loss = 2.7283  Validation loss = 6.0451  \n",
      "\n",
      "Fold: 15  Epoch: 136  Training loss = 2.7279  Validation loss = 6.0426  \n",
      "\n",
      "Fold: 15  Epoch: 137  Training loss = 2.7271  Validation loss = 6.0357  \n",
      "\n",
      "Fold: 15  Epoch: 138  Training loss = 2.7265  Validation loss = 6.0300  \n",
      "\n",
      "Fold: 15  Epoch: 139  Training loss = 2.7265  Validation loss = 6.0308  \n",
      "\n",
      "Fold: 15  Epoch: 140  Training loss = 2.7259  Validation loss = 6.0258  \n",
      "\n",
      "Fold: 15  Epoch: 141  Training loss = 2.7255  Validation loss = 6.0225  \n",
      "\n",
      "Fold: 15  Epoch: 142  Training loss = 2.7256  Validation loss = 6.0250  \n",
      "\n",
      "Fold: 15  Epoch: 143  Training loss = 2.7256  Validation loss = 6.0252  \n",
      "\n",
      "Fold: 15  Epoch: 144  Training loss = 2.7254  Validation loss = 6.0239  \n",
      "\n",
      "Fold: 15  Epoch: 145  Training loss = 2.7248  Validation loss = 6.0189  \n",
      "\n",
      "Fold: 15  Epoch: 146  Training loss = 2.7243  Validation loss = 6.0144  \n",
      "\n",
      "Fold: 15  Epoch: 147  Training loss = 2.7238  Validation loss = 6.0101  \n",
      "\n",
      "Fold: 15  Epoch: 148  Training loss = 2.7239  Validation loss = 6.0121  \n",
      "\n",
      "Fold: 15  Epoch: 149  Training loss = 2.7237  Validation loss = 6.0117  \n",
      "\n",
      "Fold: 15  Epoch: 150  Training loss = 2.7236  Validation loss = 6.0117  \n",
      "\n",
      "Fold: 15  Epoch: 151  Training loss = 2.7237  Validation loss = 6.0135  \n",
      "\n",
      "Fold: 15  Epoch: 152  Training loss = 2.7235  Validation loss = 6.0127  \n",
      "\n",
      "Fold: 15  Epoch: 153  Training loss = 2.7233  Validation loss = 6.0113  \n",
      "\n",
      "Fold: 15  Epoch: 154  Training loss = 2.7229  Validation loss = 6.0087  \n",
      "\n",
      "Fold: 15  Epoch: 155  Training loss = 2.7225  Validation loss = 6.0054  \n",
      "\n",
      "Fold: 15  Epoch: 156  Training loss = 2.7226  Validation loss = 6.0078  \n",
      "\n",
      "Fold: 15  Epoch: 157  Training loss = 2.7221  Validation loss = 6.0038  \n",
      "\n",
      "Fold: 15  Epoch: 158  Training loss = 2.7220  Validation loss = 6.0036  \n",
      "\n",
      "Fold: 15  Epoch: 159  Training loss = 2.7221  Validation loss = 6.0060  \n",
      "\n",
      "Fold: 15  Epoch: 160  Training loss = 2.7216  Validation loss = 6.0009  \n",
      "\n",
      "Fold: 15  Epoch: 161  Training loss = 2.7212  Validation loss = 5.9984  \n",
      "\n",
      "Fold: 15  Epoch: 162  Training loss = 2.7212  Validation loss = 5.9997  \n",
      "\n",
      "Fold: 15  Epoch: 163  Training loss = 2.7207  Validation loss = 5.9947  \n",
      "\n",
      "Fold: 15  Epoch: 164  Training loss = 2.7205  Validation loss = 5.9929  \n",
      "\n",
      "Fold: 15  Epoch: 165  Training loss = 2.7202  Validation loss = 5.9898  \n",
      "\n",
      "Fold: 15  Epoch: 166  Training loss = 2.7200  Validation loss = 5.9881  \n",
      "\n",
      "Fold: 15  Epoch: 167  Training loss = 2.7193  Validation loss = 5.9804  \n",
      "\n",
      "Fold: 15  Epoch: 168  Training loss = 2.7194  Validation loss = 5.9837  \n",
      "\n",
      "Fold: 15  Epoch: 169  Training loss = 2.7193  Validation loss = 5.9831  \n",
      "\n",
      "Fold: 15  Epoch: 170  Training loss = 2.7191  Validation loss = 5.9810  \n",
      "\n",
      "Fold: 15  Epoch: 171  Training loss = 2.7187  Validation loss = 5.9776  \n",
      "\n",
      "Fold: 15  Epoch: 172  Training loss = 2.7185  Validation loss = 5.9757  \n",
      "\n",
      "Fold: 15  Epoch: 173  Training loss = 2.7186  Validation loss = 5.9784  \n",
      "\n",
      "Fold: 15  Epoch: 174  Training loss = 2.7187  Validation loss = 5.9804  \n",
      "\n",
      "Fold: 15  Epoch: 175  Training loss = 2.7185  Validation loss = 5.9801  \n",
      "\n",
      "Fold: 15  Epoch: 176  Training loss = 2.7181  Validation loss = 5.9774  \n",
      "\n",
      "Fold: 15  Epoch: 177  Training loss = 2.7179  Validation loss = 5.9767  \n",
      "\n",
      "Fold: 15  Epoch: 178  Training loss = 2.7173  Validation loss = 5.9717  \n",
      "\n",
      "Fold: 15  Epoch: 179  Training loss = 2.7174  Validation loss = 5.9748  \n",
      "\n",
      "Fold: 15  Epoch: 180  Training loss = 2.7175  Validation loss = 5.9767  \n",
      "\n",
      "Fold: 15  Epoch: 181  Training loss = 2.7174  Validation loss = 5.9775  \n",
      "\n",
      "Fold: 15  Epoch: 182  Training loss = 2.7175  Validation loss = 5.9792  \n",
      "\n",
      "Fold: 15  Epoch: 183  Training loss = 2.7174  Validation loss = 5.9799  \n",
      "\n",
      "Fold: 15  Epoch: 184  Training loss = 2.7173  Validation loss = 5.9796  \n",
      "\n",
      "Fold: 15  Epoch: 185  Training loss = 2.7171  Validation loss = 5.9784  \n",
      "\n",
      "Fold: 15  Epoch: 186  Training loss = 2.7171  Validation loss = 5.9793  \n",
      "\n",
      "Fold: 15  Epoch: 187  Training loss = 2.7170  Validation loss = 5.9794  \n",
      "\n",
      "Fold: 15  Epoch: 188  Training loss = 2.7165  Validation loss = 5.9748  \n",
      "\n",
      "Fold: 15  Epoch: 189  Training loss = 2.7169  Validation loss = 5.9802  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 178  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 3.0527  Validation loss = 4.1529  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 3.0502  Validation loss = 4.1373  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 3.0483  Validation loss = 4.1349  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 3.0480  Validation loss = 4.1346  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 3.0473  Validation loss = 4.1330  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 3.0446  Validation loss = 4.1223  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 3.0440  Validation loss = 4.1214  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 3.0432  Validation loss = 4.1177  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 3.0426  Validation loss = 4.1147  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 3.0416  Validation loss = 4.1105  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 3.0409  Validation loss = 4.1124  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 3.0408  Validation loss = 4.1093  \n",
      "\n",
      "Fold: 16  Epoch: 13  Training loss = 3.0398  Validation loss = 4.1065  \n",
      "\n",
      "Fold: 16  Epoch: 14  Training loss = 3.0386  Validation loss = 4.1035  \n",
      "\n",
      "Fold: 16  Epoch: 15  Training loss = 3.0365  Validation loss = 4.0943  \n",
      "\n",
      "Fold: 16  Epoch: 16  Training loss = 3.0357  Validation loss = 4.0933  \n",
      "\n",
      "Fold: 16  Epoch: 17  Training loss = 3.0357  Validation loss = 4.0940  \n",
      "\n",
      "Fold: 16  Epoch: 18  Training loss = 3.0341  Validation loss = 4.0865  \n",
      "\n",
      "Fold: 16  Epoch: 19  Training loss = 3.0330  Validation loss = 4.0787  \n",
      "\n",
      "Fold: 16  Epoch: 20  Training loss = 3.0315  Validation loss = 4.0709  \n",
      "\n",
      "Fold: 16  Epoch: 21  Training loss = 3.0307  Validation loss = 4.0664  \n",
      "\n",
      "Fold: 16  Epoch: 22  Training loss = 3.0304  Validation loss = 4.0677  \n",
      "\n",
      "Fold: 16  Epoch: 23  Training loss = 3.0294  Validation loss = 4.0639  \n",
      "\n",
      "Fold: 16  Epoch: 24  Training loss = 3.0277  Validation loss = 4.0561  \n",
      "\n",
      "Fold: 16  Epoch: 25  Training loss = 3.0268  Validation loss = 4.0517  \n",
      "\n",
      "Fold: 16  Epoch: 26  Training loss = 3.0258  Validation loss = 4.0466  \n",
      "\n",
      "Fold: 16  Epoch: 27  Training loss = 3.0253  Validation loss = 4.0464  \n",
      "\n",
      "Fold: 16  Epoch: 28  Training loss = 3.0246  Validation loss = 4.0456  \n",
      "\n",
      "Fold: 16  Epoch: 29  Training loss = 3.0231  Validation loss = 4.0407  \n",
      "\n",
      "Fold: 16  Epoch: 30  Training loss = 3.0227  Validation loss = 4.0403  \n",
      "\n",
      "Fold: 16  Epoch: 31  Training loss = 3.0230  Validation loss = 4.0440  \n",
      "\n",
      "Fold: 16  Epoch: 32  Training loss = 3.0223  Validation loss = 4.0403  \n",
      "\n",
      "Fold: 16  Epoch: 33  Training loss = 3.0212  Validation loss = 4.0353  \n",
      "\n",
      "Fold: 16  Epoch: 34  Training loss = 3.0206  Validation loss = 4.0348  \n",
      "\n",
      "Fold: 16  Epoch: 35  Training loss = 3.0190  Validation loss = 4.0313  \n",
      "\n",
      "Fold: 16  Epoch: 36  Training loss = 3.0190  Validation loss = 4.0327  \n",
      "\n",
      "Fold: 16  Epoch: 37  Training loss = 3.0187  Validation loss = 4.0305  \n",
      "\n",
      "Fold: 16  Epoch: 38  Training loss = 3.0174  Validation loss = 4.0247  \n",
      "\n",
      "Fold: 16  Epoch: 39  Training loss = 3.0166  Validation loss = 4.0186  \n",
      "\n",
      "Fold: 16  Epoch: 40  Training loss = 3.0163  Validation loss = 4.0175  \n",
      "\n",
      "Fold: 16  Epoch: 41  Training loss = 3.0164  Validation loss = 4.0189  \n",
      "\n",
      "Fold: 16  Epoch: 42  Training loss = 3.0142  Validation loss = 4.0067  \n",
      "\n",
      "Fold: 16  Epoch: 43  Training loss = 3.0129  Validation loss = 3.9986  \n",
      "\n",
      "Fold: 16  Epoch: 44  Training loss = 3.0124  Validation loss = 3.9976  \n",
      "\n",
      "Fold: 16  Epoch: 45  Training loss = 3.0122  Validation loss = 3.9971  \n",
      "\n",
      "Fold: 16  Epoch: 46  Training loss = 3.0122  Validation loss = 4.0005  \n",
      "\n",
      "Fold: 16  Epoch: 47  Training loss = 3.0116  Validation loss = 3.9970  \n",
      "\n",
      "Fold: 16  Epoch: 48  Training loss = 3.0111  Validation loss = 3.9991  \n",
      "\n",
      "Fold: 16  Epoch: 49  Training loss = 3.0106  Validation loss = 3.9916  \n",
      "\n",
      "Fold: 16  Epoch: 50  Training loss = 3.0104  Validation loss = 3.9940  \n",
      "\n",
      "Fold: 16  Epoch: 51  Training loss = 3.0108  Validation loss = 3.9978  \n",
      "\n",
      "Fold: 16  Epoch: 52  Training loss = 3.0087  Validation loss = 3.9875  \n",
      "\n",
      "Fold: 16  Epoch: 53  Training loss = 3.0085  Validation loss = 3.9876  \n",
      "\n",
      "Fold: 16  Epoch: 54  Training loss = 3.0082  Validation loss = 3.9878  \n",
      "\n",
      "Fold: 16  Epoch: 55  Training loss = 3.0075  Validation loss = 3.9878  \n",
      "\n",
      "Fold: 16  Epoch: 56  Training loss = 3.0071  Validation loss = 3.9854  \n",
      "\n",
      "Fold: 16  Epoch: 57  Training loss = 3.0062  Validation loss = 3.9814  \n",
      "\n",
      "Fold: 16  Epoch: 58  Training loss = 3.0058  Validation loss = 3.9819  \n",
      "\n",
      "Fold: 16  Epoch: 59  Training loss = 3.0050  Validation loss = 3.9720  \n",
      "\n",
      "Fold: 16  Epoch: 60  Training loss = 3.0046  Validation loss = 3.9707  \n",
      "\n",
      "Fold: 16  Epoch: 61  Training loss = 3.0041  Validation loss = 3.9672  \n",
      "\n",
      "Fold: 16  Epoch: 62  Training loss = 3.0035  Validation loss = 3.9653  \n",
      "\n",
      "Fold: 16  Epoch: 63  Training loss = 3.0027  Validation loss = 3.9641  \n",
      "\n",
      "Fold: 16  Epoch: 64  Training loss = 3.0014  Validation loss = 3.9564  \n",
      "\n",
      "Fold: 16  Epoch: 65  Training loss = 3.0007  Validation loss = 3.9549  \n",
      "\n",
      "Fold: 16  Epoch: 66  Training loss = 2.9999  Validation loss = 3.9510  \n",
      "\n",
      "Fold: 16  Epoch: 67  Training loss = 2.9994  Validation loss = 3.9479  \n",
      "\n",
      "Fold: 16  Epoch: 68  Training loss = 2.9991  Validation loss = 3.9481  \n",
      "\n",
      "Fold: 16  Epoch: 69  Training loss = 2.9986  Validation loss = 3.9417  \n",
      "\n",
      "Fold: 16  Epoch: 70  Training loss = 2.9986  Validation loss = 3.9413  \n",
      "\n",
      "Fold: 16  Epoch: 71  Training loss = 2.9981  Validation loss = 3.9423  \n",
      "\n",
      "Fold: 16  Epoch: 72  Training loss = 2.9976  Validation loss = 3.9416  \n",
      "\n",
      "Fold: 16  Epoch: 73  Training loss = 2.9968  Validation loss = 3.9361  \n",
      "\n",
      "Fold: 16  Epoch: 74  Training loss = 2.9962  Validation loss = 3.9319  \n",
      "\n",
      "Fold: 16  Epoch: 75  Training loss = 2.9958  Validation loss = 3.9273  \n",
      "\n",
      "Fold: 16  Epoch: 76  Training loss = 2.9959  Validation loss = 3.9274  \n",
      "\n",
      "Fold: 16  Epoch: 77  Training loss = 2.9957  Validation loss = 3.9286  \n",
      "\n",
      "Fold: 16  Epoch: 78  Training loss = 2.9950  Validation loss = 3.9283  \n",
      "\n",
      "Fold: 16  Epoch: 79  Training loss = 2.9947  Validation loss = 3.9235  \n",
      "\n",
      "Fold: 16  Epoch: 80  Training loss = 2.9943  Validation loss = 3.9208  \n",
      "\n",
      "Fold: 16  Epoch: 81  Training loss = 2.9930  Validation loss = 3.9188  \n",
      "\n",
      "Fold: 16  Epoch: 82  Training loss = 2.9927  Validation loss = 3.9187  \n",
      "\n",
      "Fold: 16  Epoch: 83  Training loss = 2.9918  Validation loss = 3.9206  \n",
      "\n",
      "Fold: 16  Epoch: 84  Training loss = 2.9914  Validation loss = 3.9169  \n",
      "\n",
      "Fold: 16  Epoch: 85  Training loss = 2.9910  Validation loss = 3.9121  \n",
      "\n",
      "Fold: 16  Epoch: 86  Training loss = 2.9912  Validation loss = 3.9148  \n",
      "\n",
      "Fold: 16  Epoch: 87  Training loss = 2.9913  Validation loss = 3.9162  \n",
      "\n",
      "Fold: 16  Epoch: 88  Training loss = 2.9908  Validation loss = 3.9137  \n",
      "\n",
      "Fold: 16  Epoch: 89  Training loss = 2.9899  Validation loss = 3.9094  \n",
      "\n",
      "Fold: 16  Epoch: 90  Training loss = 2.9900  Validation loss = 3.9117  \n",
      "\n",
      "Fold: 16  Epoch: 91  Training loss = 2.9898  Validation loss = 3.9111  \n",
      "\n",
      "Fold: 16  Epoch: 92  Training loss = 2.9890  Validation loss = 3.9078  \n",
      "\n",
      "Fold: 16  Epoch: 93  Training loss = 2.9889  Validation loss = 3.9101  \n",
      "\n",
      "Fold: 16  Epoch: 94  Training loss = 2.9886  Validation loss = 3.9110  \n",
      "\n",
      "Fold: 16  Epoch: 95  Training loss = 2.9884  Validation loss = 3.9085  \n",
      "\n",
      "Fold: 16  Epoch: 96  Training loss = 2.9878  Validation loss = 3.9077  \n",
      "\n",
      "Fold: 16  Epoch: 97  Training loss = 2.9870  Validation loss = 3.9009  \n",
      "\n",
      "Fold: 16  Epoch: 98  Training loss = 2.9871  Validation loss = 3.9052  \n",
      "\n",
      "Fold: 16  Epoch: 99  Training loss = 2.9866  Validation loss = 3.9045  \n",
      "\n",
      "Fold: 16  Epoch: 100  Training loss = 2.9862  Validation loss = 3.9018  \n",
      "\n",
      "Fold: 16  Epoch: 101  Training loss = 2.9856  Validation loss = 3.8956  \n",
      "\n",
      "Fold: 16  Epoch: 102  Training loss = 2.9853  Validation loss = 3.8932  \n",
      "\n",
      "Fold: 16  Epoch: 103  Training loss = 2.9845  Validation loss = 3.8881  \n",
      "\n",
      "Fold: 16  Epoch: 104  Training loss = 2.9844  Validation loss = 3.8875  \n",
      "\n",
      "Fold: 16  Epoch: 105  Training loss = 2.9847  Validation loss = 3.8925  \n",
      "\n",
      "Fold: 16  Epoch: 106  Training loss = 2.9843  Validation loss = 3.8930  \n",
      "\n",
      "Fold: 16  Epoch: 107  Training loss = 2.9838  Validation loss = 3.8882  \n",
      "\n",
      "Fold: 16  Epoch: 108  Training loss = 2.9835  Validation loss = 3.8911  \n",
      "\n",
      "Fold: 16  Epoch: 109  Training loss = 2.9833  Validation loss = 3.8883  \n",
      "\n",
      "Fold: 16  Epoch: 110  Training loss = 2.9828  Validation loss = 3.8862  \n",
      "\n",
      "Fold: 16  Epoch: 111  Training loss = 2.9823  Validation loss = 3.8849  \n",
      "\n",
      "Fold: 16  Epoch: 112  Training loss = 2.9820  Validation loss = 3.8828  \n",
      "\n",
      "Fold: 16  Epoch: 113  Training loss = 2.9816  Validation loss = 3.8841  \n",
      "\n",
      "Fold: 16  Epoch: 114  Training loss = 2.9811  Validation loss = 3.8835  \n",
      "\n",
      "Fold: 16  Epoch: 115  Training loss = 2.9807  Validation loss = 3.8821  \n",
      "\n",
      "Fold: 16  Epoch: 116  Training loss = 2.9799  Validation loss = 3.8785  \n",
      "\n",
      "Fold: 16  Epoch: 117  Training loss = 2.9795  Validation loss = 3.8775  \n",
      "\n",
      "Fold: 16  Epoch: 118  Training loss = 2.9794  Validation loss = 3.8767  \n",
      "\n",
      "Fold: 16  Epoch: 119  Training loss = 2.9791  Validation loss = 3.8771  \n",
      "\n",
      "Fold: 16  Epoch: 120  Training loss = 2.9787  Validation loss = 3.8747  \n",
      "\n",
      "Fold: 16  Epoch: 121  Training loss = 2.9789  Validation loss = 3.8792  \n",
      "\n",
      "Fold: 16  Epoch: 122  Training loss = 2.9789  Validation loss = 3.8803  \n",
      "\n",
      "Fold: 16  Epoch: 123  Training loss = 2.9788  Validation loss = 3.8807  \n",
      "\n",
      "Fold: 16  Epoch: 124  Training loss = 2.9786  Validation loss = 3.8815  \n",
      "\n",
      "Fold: 16  Epoch: 125  Training loss = 2.9789  Validation loss = 3.8878  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 120  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 3.1038  Validation loss = 3.7283  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 3.1028  Validation loss = 3.7321  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 3.1007  Validation loss = 3.7404  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 3.0981  Validation loss = 3.7504  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 3.0970  Validation loss = 3.7550  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 3.0957  Validation loss = 3.7594  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 3.0950  Validation loss = 3.7622  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 3.0941  Validation loss = 3.7652  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 3.0929  Validation loss = 3.7698  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 3.0913  Validation loss = 3.7760  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 3.0897  Validation loss = 3.7828  \n",
      "\n",
      "Fold: 17  Epoch: 12  Training loss = 3.0886  Validation loss = 3.7873  \n",
      "\n",
      "Fold: 17  Epoch: 13  Training loss = 3.0879  Validation loss = 3.7900  \n",
      "\n",
      "Fold: 17  Epoch: 14  Training loss = 3.0867  Validation loss = 3.7951  \n",
      "\n",
      "Fold: 17  Epoch: 15  Training loss = 3.0868  Validation loss = 3.7925  \n",
      "\n",
      "Fold: 17  Epoch: 16  Training loss = 3.0854  Validation loss = 3.7982  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 1  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 3.2210  Validation loss = 2.2694  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 3.2201  Validation loss = 2.2696  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 3.2191  Validation loss = 2.2696  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 3.2184  Validation loss = 2.2713  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 3.2183  Validation loss = 2.2695  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 3.2180  Validation loss = 2.2675  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 3.2171  Validation loss = 2.2717  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 3.2163  Validation loss = 2.2712  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 3.2160  Validation loss = 2.2694  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 3.2152  Validation loss = 2.2687  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 3.2141  Validation loss = 2.2711  \n",
      "\n",
      "Fold: 18  Epoch: 12  Training loss = 3.2135  Validation loss = 2.2685  \n",
      "\n",
      "Fold: 18  Epoch: 13  Training loss = 3.2128  Validation loss = 2.2702  \n",
      "\n",
      "Fold: 18  Epoch: 14  Training loss = 3.2123  Validation loss = 2.2717  \n",
      "\n",
      "Fold: 18  Epoch: 15  Training loss = 3.2116  Validation loss = 2.2731  \n",
      "\n",
      "Fold: 18  Epoch: 16  Training loss = 3.2113  Validation loss = 2.2704  \n",
      "\n",
      "Fold: 18  Epoch: 17  Training loss = 3.2111  Validation loss = 2.2689  \n",
      "\n",
      "Fold: 18  Epoch: 18  Training loss = 3.2105  Validation loss = 2.2685  \n",
      "\n",
      "Fold: 18  Epoch: 19  Training loss = 3.2101  Validation loss = 2.2693  \n",
      "\n",
      "Fold: 18  Epoch: 20  Training loss = 3.2097  Validation loss = 2.2688  \n",
      "\n",
      "Fold: 18  Epoch: 21  Training loss = 3.2089  Validation loss = 2.2691  \n",
      "\n",
      "Fold: 18  Epoch: 22  Training loss = 3.2078  Validation loss = 2.2712  \n",
      "\n",
      "Fold: 18  Epoch: 23  Training loss = 3.2079  Validation loss = 2.2710  \n",
      "\n",
      "Fold: 18  Epoch: 24  Training loss = 3.2064  Validation loss = 2.2739  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 6  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 3.2049  Validation loss = 0.7996  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 3.2038  Validation loss = 0.7966  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 3.2026  Validation loss = 0.7964  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 3.2023  Validation loss = 0.7983  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 3.2026  Validation loss = 0.8008  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 3.2020  Validation loss = 0.7987  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 3.2010  Validation loss = 0.7988  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 3.2002  Validation loss = 0.7990  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 3.2002  Validation loss = 0.8001  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 3.2001  Validation loss = 0.8023  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 3.1993  Validation loss = 0.8013  \n",
      "\n",
      "Fold: 19  Epoch: 12  Training loss = 3.1987  Validation loss = 0.8013  \n",
      "\n",
      "Fold: 19  Epoch: 13  Training loss = 3.1985  Validation loss = 0.8015  \n",
      "\n",
      "Fold: 19  Epoch: 14  Training loss = 3.1982  Validation loss = 0.8035  \n",
      "\n",
      "Fold: 19  Epoch: 15  Training loss = 3.1969  Validation loss = 0.8030  \n",
      "\n",
      "Fold: 19  Epoch: 16  Training loss = 3.1964  Validation loss = 0.8010  \n",
      "\n",
      "Fold: 19  Epoch: 17  Training loss = 3.1946  Validation loss = 0.7994  \n",
      "\n",
      "Fold: 19  Epoch: 18  Training loss = 3.1939  Validation loss = 0.7993  \n",
      "\n",
      "Fold: 19  Epoch: 19  Training loss = 3.1926  Validation loss = 0.7978  \n",
      "\n",
      "Fold: 19  Epoch: 20  Training loss = 3.1913  Validation loss = 0.7959  \n",
      "\n",
      "Fold: 19  Epoch: 21  Training loss = 3.1903  Validation loss = 0.7945  \n",
      "\n",
      "Fold: 19  Epoch: 22  Training loss = 3.1899  Validation loss = 0.7938  \n",
      "\n",
      "Fold: 19  Epoch: 23  Training loss = 3.1890  Validation loss = 0.7934  \n",
      "\n",
      "Fold: 19  Epoch: 24  Training loss = 3.1869  Validation loss = 0.7928  \n",
      "\n",
      "Fold: 19  Epoch: 25  Training loss = 3.1865  Validation loss = 0.7884  \n",
      "\n",
      "Fold: 19  Epoch: 26  Training loss = 3.1857  Validation loss = 0.7896  \n",
      "\n",
      "Fold: 19  Epoch: 27  Training loss = 3.1855  Validation loss = 0.7938  \n",
      "\n",
      "Fold: 19  Epoch: 28  Training loss = 3.1849  Validation loss = 0.7921  \n",
      "\n",
      "Fold: 19  Epoch: 29  Training loss = 3.1851  Validation loss = 0.7937  \n",
      "\n",
      "Fold: 19  Epoch: 30  Training loss = 3.1847  Validation loss = 0.7953  \n",
      "\n",
      "Fold: 19  Epoch: 31  Training loss = 3.1854  Validation loss = 0.7947  \n",
      "\n",
      "Fold: 19  Epoch: 32  Training loss = 3.1864  Validation loss = 0.7924  \n",
      "\n",
      "Fold: 19  Epoch: 33  Training loss = 3.1864  Validation loss = 0.7936  \n",
      "\n",
      "Fold: 19  Epoch: 34  Training loss = 3.1851  Validation loss = 0.7896  \n",
      "\n",
      "Fold: 19  Epoch: 35  Training loss = 3.1845  Validation loss = 0.7874  \n",
      "\n",
      "Fold: 19  Epoch: 36  Training loss = 3.1842  Validation loss = 0.7894  \n",
      "\n",
      "Fold: 19  Epoch: 37  Training loss = 3.1813  Validation loss = 0.7864  \n",
      "\n",
      "Fold: 19  Epoch: 38  Training loss = 3.1805  Validation loss = 0.7823  \n",
      "\n",
      "Fold: 19  Epoch: 39  Training loss = 3.1800  Validation loss = 0.7807  \n",
      "\n",
      "Fold: 19  Epoch: 40  Training loss = 3.1795  Validation loss = 0.7795  \n",
      "\n",
      "Fold: 19  Epoch: 41  Training loss = 3.1792  Validation loss = 0.7799  \n",
      "\n",
      "Fold: 19  Epoch: 42  Training loss = 3.1791  Validation loss = 0.7811  \n",
      "\n",
      "Fold: 19  Epoch: 43  Training loss = 3.1788  Validation loss = 0.7799  \n",
      "\n",
      "Fold: 19  Epoch: 44  Training loss = 3.1781  Validation loss = 0.7778  \n",
      "\n",
      "Fold: 19  Epoch: 45  Training loss = 3.1775  Validation loss = 0.7785  \n",
      "\n",
      "Fold: 19  Epoch: 46  Training loss = 3.1770  Validation loss = 0.7756  \n",
      "\n",
      "Fold: 19  Epoch: 47  Training loss = 3.1765  Validation loss = 0.7771  \n",
      "\n",
      "Fold: 19  Epoch: 48  Training loss = 3.1762  Validation loss = 0.7787  \n",
      "\n",
      "Fold: 19  Epoch: 49  Training loss = 3.1757  Validation loss = 0.7787  \n",
      "\n",
      "Fold: 19  Epoch: 50  Training loss = 3.1757  Validation loss = 0.7846  \n",
      "\n",
      "Fold: 19  Epoch: 51  Training loss = 3.1750  Validation loss = 0.7832  \n",
      "\n",
      "Fold: 19  Epoch: 52  Training loss = 3.1747  Validation loss = 0.7841  \n",
      "\n",
      "Fold: 19  Epoch: 53  Training loss = 3.1744  Validation loss = 0.7852  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 46  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 3.1628  Validation loss = 2.0307  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 3.1619  Validation loss = 2.0337  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 3.1610  Validation loss = 2.0408  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 3.1610  Validation loss = 2.0360  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 3.1605  Validation loss = 2.0413  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 3.1594  Validation loss = 2.0519  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 3.1589  Validation loss = 2.0567  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 3.1584  Validation loss = 2.0556  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 3.1577  Validation loss = 2.0617  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 3.1573  Validation loss = 2.0588  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 3.1562  Validation loss = 2.0654  \n",
      "\n",
      "Fold: 20  Epoch: 12  Training loss = 3.1557  Validation loss = 2.0627  \n",
      "\n",
      "Fold: 20  Epoch: 13  Training loss = 3.1551  Validation loss = 2.0647  \n",
      "\n",
      "Fold: 20  Epoch: 14  Training loss = 3.1546  Validation loss = 2.0651  \n",
      "\n",
      "Fold: 20  Epoch: 15  Training loss = 3.1538  Validation loss = 2.0750  \n",
      "\n",
      "Fold: 20  Epoch: 16  Training loss = 3.1532  Validation loss = 2.0804  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 1  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 3.1879  Validation loss = 2.6073  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 3.1876  Validation loss = 2.6044  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 3.1870  Validation loss = 2.5996  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 3.1867  Validation loss = 2.5974  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 3.1862  Validation loss = 2.5974  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 3.1857  Validation loss = 2.5951  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 3.1851  Validation loss = 2.5968  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 3.1847  Validation loss = 2.5981  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 3.1841  Validation loss = 2.6000  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 3.1836  Validation loss = 2.6006  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 3.1832  Validation loss = 2.5978  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 3.1829  Validation loss = 2.5959  \n",
      "\n",
      "Fold: 21  Epoch: 13  Training loss = 3.1826  Validation loss = 2.5949  \n",
      "\n",
      "Fold: 21  Epoch: 14  Training loss = 3.1823  Validation loss = 2.5947  \n",
      "\n",
      "Fold: 21  Epoch: 15  Training loss = 3.1819  Validation loss = 2.5960  \n",
      "\n",
      "Fold: 21  Epoch: 16  Training loss = 3.1810  Validation loss = 2.5996  \n",
      "\n",
      "Fold: 21  Epoch: 17  Training loss = 3.1804  Validation loss = 2.5996  \n",
      "\n",
      "Fold: 21  Epoch: 18  Training loss = 3.1798  Validation loss = 2.5998  \n",
      "\n",
      "Fold: 21  Epoch: 19  Training loss = 3.1793  Validation loss = 2.6012  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 14  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 3.2180  Validation loss = 1.4569  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 3.2174  Validation loss = 1.4561  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 3.2168  Validation loss = 1.4558  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 3.2158  Validation loss = 1.4590  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 3.2151  Validation loss = 1.4588  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 3.2145  Validation loss = 1.4597  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 3.2140  Validation loss = 1.4600  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 3.2131  Validation loss = 1.4600  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 3.2124  Validation loss = 1.4587  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 3.2114  Validation loss = 1.4606  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 3.2106  Validation loss = 1.4600  \n",
      "\n",
      "Fold: 22  Epoch: 12  Training loss = 3.2100  Validation loss = 1.4621  \n",
      "\n",
      "Fold: 22  Epoch: 13  Training loss = 3.2095  Validation loss = 1.4640  \n",
      "\n",
      "Fold: 22  Epoch: 14  Training loss = 3.2091  Validation loss = 1.4651  \n",
      "\n",
      "Fold: 22  Epoch: 15  Training loss = 3.2082  Validation loss = 1.4651  \n",
      "\n",
      "Fold: 22  Epoch: 16  Training loss = 3.2077  Validation loss = 1.4640  \n",
      "\n",
      "Fold: 22  Epoch: 17  Training loss = 3.2067  Validation loss = 1.4633  \n",
      "\n",
      "Fold: 22  Epoch: 18  Training loss = 3.2058  Validation loss = 1.4640  \n",
      "\n",
      "Fold: 22  Epoch: 19  Training loss = 3.2049  Validation loss = 1.4646  \n",
      "\n",
      "Fold: 22  Epoch: 20  Training loss = 3.2041  Validation loss = 1.4647  \n",
      "\n",
      "Fold: 22  Epoch: 21  Training loss = 3.2035  Validation loss = 1.4637  \n",
      "\n",
      "Fold: 22  Epoch: 22  Training loss = 3.2025  Validation loss = 1.4632  \n",
      "\n",
      "Fold: 22  Epoch: 23  Training loss = 3.2018  Validation loss = 1.4627  \n",
      "\n",
      "Fold: 22  Epoch: 24  Training loss = 3.2013  Validation loss = 1.4653  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 3  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 3.1314  Validation loss = 1.2904  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 3.1306  Validation loss = 1.3169  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 3.1299  Validation loss = 1.2873  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 3.1284  Validation loss = 1.3125  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 3.1280  Validation loss = 1.3029  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 3.1270  Validation loss = 1.3205  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 3.1261  Validation loss = 1.3243  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 3.1253  Validation loss = 1.3196  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 3.1244  Validation loss = 1.3277  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 3.1237  Validation loss = 1.3105  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 3.1226  Validation loss = 1.3173  \n",
      "\n",
      "Fold: 23  Epoch: 12  Training loss = 3.1219  Validation loss = 1.3218  \n",
      "\n",
      "Fold: 23  Epoch: 13  Training loss = 3.1206  Validation loss = 1.3267  \n",
      "\n",
      "Fold: 23  Epoch: 14  Training loss = 3.1202  Validation loss = 1.3047  \n",
      "\n",
      "Fold: 23  Epoch: 15  Training loss = 3.1196  Validation loss = 1.3028  \n",
      "\n",
      "Fold: 23  Epoch: 16  Training loss = 3.1201  Validation loss = 1.2691  \n",
      "\n",
      "Fold: 23  Epoch: 17  Training loss = 3.1192  Validation loss = 1.2703  \n",
      "\n",
      "Fold: 23  Epoch: 18  Training loss = 3.1183  Validation loss = 1.2803  \n",
      "\n",
      "Fold: 23  Epoch: 19  Training loss = 3.1174  Validation loss = 1.2839  \n",
      "\n",
      "Fold: 23  Epoch: 20  Training loss = 3.1159  Validation loss = 1.2924  \n",
      "\n",
      "Fold: 23  Epoch: 21  Training loss = 3.1156  Validation loss = 1.2959  \n",
      "\n",
      "Fold: 23  Epoch: 22  Training loss = 3.1146  Validation loss = 1.3004  \n",
      "\n",
      "Fold: 23  Epoch: 23  Training loss = 3.1139  Validation loss = 1.2906  \n",
      "\n",
      "Fold: 23  Epoch: 24  Training loss = 3.1131  Validation loss = 1.2914  \n",
      "\n",
      "Fold: 23  Epoch: 25  Training loss = 3.1126  Validation loss = 1.2938  \n",
      "\n",
      "Fold: 23  Epoch: 26  Training loss = 3.1120  Validation loss = 1.2852  \n",
      "\n",
      "Fold: 23  Epoch: 27  Training loss = 3.1117  Validation loss = 1.2773  \n",
      "\n",
      "Fold: 23  Epoch: 28  Training loss = 3.1111  Validation loss = 1.2974  \n",
      "\n",
      "Fold: 23  Epoch: 29  Training loss = 3.1103  Validation loss = 1.2851  \n",
      "\n",
      "Fold: 23  Epoch: 30  Training loss = 3.1095  Validation loss = 1.2708  \n",
      "\n",
      "Fold: 23  Epoch: 31  Training loss = 3.1086  Validation loss = 1.2741  \n",
      "\n",
      "Fold: 23  Epoch: 32  Training loss = 3.1078  Validation loss = 1.2750  \n",
      "\n",
      "Fold: 23  Epoch: 33  Training loss = 3.1070  Validation loss = 1.2965  \n",
      "\n",
      "Fold: 23  Epoch: 34  Training loss = 3.1064  Validation loss = 1.2873  \n",
      "\n",
      "Fold: 23  Epoch: 35  Training loss = 3.1060  Validation loss = 1.2835  \n",
      "\n",
      "Fold: 23  Epoch: 36  Training loss = 3.1052  Validation loss = 1.2970  \n",
      "\n",
      "Fold: 23  Epoch: 37  Training loss = 3.1050  Validation loss = 1.2733  \n",
      "\n",
      "Fold: 23  Epoch: 38  Training loss = 3.1044  Validation loss = 1.2599  \n",
      "\n",
      "Fold: 23  Epoch: 39  Training loss = 3.1040  Validation loss = 1.2506  \n",
      "\n",
      "Fold: 23  Epoch: 40  Training loss = 3.1032  Validation loss = 1.2507  \n",
      "\n",
      "Fold: 23  Epoch: 41  Training loss = 3.1026  Validation loss = 1.2481  \n",
      "\n",
      "Fold: 23  Epoch: 42  Training loss = 3.1020  Validation loss = 1.2390  \n",
      "\n",
      "Fold: 23  Epoch: 43  Training loss = 3.1018  Validation loss = 1.2244  \n",
      "\n",
      "Fold: 23  Epoch: 44  Training loss = 3.1011  Validation loss = 1.2247  \n",
      "\n",
      "Fold: 23  Epoch: 45  Training loss = 3.1005  Validation loss = 1.2297  \n",
      "\n",
      "Fold: 23  Epoch: 46  Training loss = 3.0999  Validation loss = 1.2260  \n",
      "\n",
      "Fold: 23  Epoch: 47  Training loss = 3.0996  Validation loss = 1.2108  \n",
      "\n",
      "Fold: 23  Epoch: 48  Training loss = 3.0992  Validation loss = 1.2145  \n",
      "\n",
      "Fold: 23  Epoch: 49  Training loss = 3.0986  Validation loss = 1.2103  \n",
      "\n",
      "Fold: 23  Epoch: 50  Training loss = 3.0983  Validation loss = 1.2001  \n",
      "\n",
      "Fold: 23  Epoch: 51  Training loss = 3.0979  Validation loss = 1.1964  \n",
      "\n",
      "Fold: 23  Epoch: 52  Training loss = 3.0972  Validation loss = 1.1986  \n",
      "\n",
      "Fold: 23  Epoch: 53  Training loss = 3.0965  Validation loss = 1.1948  \n",
      "\n",
      "Fold: 23  Epoch: 54  Training loss = 3.0957  Validation loss = 1.2036  \n",
      "\n",
      "Fold: 23  Epoch: 55  Training loss = 3.0946  Validation loss = 1.2136  \n",
      "\n",
      "Fold: 23  Epoch: 56  Training loss = 3.0934  Validation loss = 1.2259  \n",
      "\n",
      "Fold: 23  Epoch: 57  Training loss = 3.0931  Validation loss = 1.2187  \n",
      "\n",
      "Fold: 23  Epoch: 58  Training loss = 3.0926  Validation loss = 1.2152  \n",
      "\n",
      "Fold: 23  Epoch: 59  Training loss = 3.0924  Validation loss = 1.2052  \n",
      "\n",
      "Fold: 23  Epoch: 60  Training loss = 3.0917  Validation loss = 1.1968  \n",
      "\n",
      "Fold: 23  Epoch: 61  Training loss = 3.0915  Validation loss = 1.1864  \n",
      "\n",
      "Fold: 23  Epoch: 62  Training loss = 3.0910  Validation loss = 1.1818  \n",
      "\n",
      "Fold: 23  Epoch: 63  Training loss = 3.0899  Validation loss = 1.1864  \n",
      "\n",
      "Fold: 23  Epoch: 64  Training loss = 3.0889  Validation loss = 1.1908  \n",
      "\n",
      "Fold: 23  Epoch: 65  Training loss = 3.0879  Validation loss = 1.1990  \n",
      "\n",
      "Fold: 23  Epoch: 66  Training loss = 3.0871  Validation loss = 1.2280  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 62  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 2.9411  Validation loss = 2.0049  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 2.9403  Validation loss = 2.0047  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 2.9389  Validation loss = 2.0002  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 2.9364  Validation loss = 1.9953  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 2.9349  Validation loss = 1.9930  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 2.9334  Validation loss = 1.9928  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 2.9329  Validation loss = 1.9948  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 2.9322  Validation loss = 1.9942  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 2.9320  Validation loss = 1.9972  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 2.9314  Validation loss = 1.9989  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 2.9302  Validation loss = 1.9962  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 2.9288  Validation loss = 1.9966  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 2.9274  Validation loss = 1.9964  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 2.9263  Validation loss = 1.9966  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 2.9252  Validation loss = 1.9982  \n",
      "\n",
      "Fold: 24  Epoch: 16  Training loss = 2.9248  Validation loss = 2.0012  \n",
      "\n",
      "Fold: 24  Epoch: 17  Training loss = 2.9244  Validation loss = 2.0047  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 6  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 2.9079  Validation loss = 1.8616  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 2.9045  Validation loss = 1.8241  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 2.9031  Validation loss = 1.8109  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 2.9022  Validation loss = 1.8134  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 2.9010  Validation loss = 1.8090  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 2.8999  Validation loss = 1.8120  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 2.8987  Validation loss = 1.7978  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 2.8979  Validation loss = 1.7915  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 2.8971  Validation loss = 1.8113  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 2.8958  Validation loss = 1.8097  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 2.8955  Validation loss = 1.8057  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 2.8940  Validation loss = 1.7988  \n",
      "\n",
      "Fold: 25  Epoch: 13  Training loss = 2.8940  Validation loss = 1.8133  \n",
      "\n",
      "Fold: 25  Epoch: 14  Training loss = 2.8931  Validation loss = 1.8080  \n",
      "\n",
      "Fold: 25  Epoch: 15  Training loss = 2.8918  Validation loss = 1.7948  \n",
      "\n",
      "Fold: 25  Epoch: 16  Training loss = 2.8907  Validation loss = 1.7761  \n",
      "\n",
      "Fold: 25  Epoch: 17  Training loss = 2.8893  Validation loss = 1.8021  \n",
      "\n",
      "Fold: 25  Epoch: 18  Training loss = 2.8884  Validation loss = 1.7884  \n",
      "\n",
      "Fold: 25  Epoch: 19  Training loss = 2.8865  Validation loss = 1.7801  \n",
      "\n",
      "Fold: 25  Epoch: 20  Training loss = 2.8863  Validation loss = 1.8056  \n",
      "\n",
      "Fold: 25  Epoch: 21  Training loss = 2.8849  Validation loss = 1.7742  \n",
      "\n",
      "Fold: 25  Epoch: 22  Training loss = 2.8835  Validation loss = 1.7786  \n",
      "\n",
      "Fold: 25  Epoch: 23  Training loss = 2.8825  Validation loss = 1.7738  \n",
      "\n",
      "Fold: 25  Epoch: 24  Training loss = 2.8826  Validation loss = 1.8067  \n",
      "\n",
      "Fold: 25  Epoch: 25  Training loss = 2.8816  Validation loss = 1.8037  \n",
      "\n",
      "Fold: 25  Epoch: 26  Training loss = 2.8807  Validation loss = 1.8154  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 23  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 2.6543  Validation loss = 2.8122  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 2.6511  Validation loss = 2.8403  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 2.6498  Validation loss = 2.8406  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 2.6484  Validation loss = 2.8510  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 2.6471  Validation loss = 2.8696  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 2.6463  Validation loss = 2.8615  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 2.6457  Validation loss = 2.8541  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 2.6444  Validation loss = 2.8640  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 2.6434  Validation loss = 2.8572  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 2.6426  Validation loss = 2.8583  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 2.6416  Validation loss = 2.8704  \n",
      "\n",
      "Fold: 26  Epoch: 12  Training loss = 2.6408  Validation loss = 2.8788  \n",
      "\n",
      "Fold: 26  Epoch: 13  Training loss = 2.6400  Validation loss = 2.8852  \n",
      "\n",
      "Fold: 26  Epoch: 14  Training loss = 2.6389  Validation loss = 2.8873  \n",
      "\n",
      "Fold: 26  Epoch: 15  Training loss = 2.6379  Validation loss = 2.8762  \n",
      "\n",
      "Fold: 26  Epoch: 16  Training loss = 2.6366  Validation loss = 2.8898  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 1  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 2.6483  Validation loss = 2.0290  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 2.6470  Validation loss = 2.0161  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 2.6463  Validation loss = 2.0185  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 2.6443  Validation loss = 1.9868  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 2.6435  Validation loss = 1.9986  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 2.6414  Validation loss = 1.9666  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 2.6404  Validation loss = 1.9620  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 2.6386  Validation loss = 1.9235  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 2.6373  Validation loss = 1.8573  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 2.6362  Validation loss = 1.8106  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 2.6353  Validation loss = 1.7796  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 2.6344  Validation loss = 1.7750  \n",
      "\n",
      "Fold: 27  Epoch: 13  Training loss = 2.6336  Validation loss = 1.8151  \n",
      "\n",
      "Fold: 27  Epoch: 14  Training loss = 2.6331  Validation loss = 1.8062  \n",
      "\n",
      "Fold: 27  Epoch: 15  Training loss = 2.6328  Validation loss = 1.8316  \n",
      "\n",
      "Fold: 27  Epoch: 16  Training loss = 2.6321  Validation loss = 1.8356  \n",
      "\n",
      "Fold: 27  Epoch: 17  Training loss = 2.6304  Validation loss = 1.7500  \n",
      "\n",
      "Fold: 27  Epoch: 18  Training loss = 2.6297  Validation loss = 1.7487  \n",
      "\n",
      "Fold: 27  Epoch: 19  Training loss = 2.6294  Validation loss = 1.8139  \n",
      "\n",
      "Fold: 27  Epoch: 20  Training loss = 2.6293  Validation loss = 1.8270  \n",
      "\n",
      "Fold: 27  Epoch: 21  Training loss = 2.6278  Validation loss = 1.7680  \n",
      "\n",
      "Fold: 27  Epoch: 22  Training loss = 2.6276  Validation loss = 1.8279  \n",
      "\n",
      "Fold: 27  Epoch: 23  Training loss = 2.6265  Validation loss = 1.8096  \n",
      "\n",
      "Fold: 27  Epoch: 24  Training loss = 2.6258  Validation loss = 1.7113  \n",
      "\n",
      "Fold: 27  Epoch: 25  Training loss = 2.6247  Validation loss = 1.7243  \n",
      "\n",
      "Fold: 27  Epoch: 26  Training loss = 2.6233  Validation loss = 1.7074  \n",
      "\n",
      "Fold: 27  Epoch: 27  Training loss = 2.6224  Validation loss = 1.7088  \n",
      "\n",
      "Fold: 27  Epoch: 28  Training loss = 2.6216  Validation loss = 1.6252  \n",
      "\n",
      "Fold: 27  Epoch: 29  Training loss = 2.6203  Validation loss = 1.6001  \n",
      "\n",
      "Fold: 27  Epoch: 30  Training loss = 2.6198  Validation loss = 1.5725  \n",
      "\n",
      "Fold: 27  Epoch: 31  Training loss = 2.6186  Validation loss = 1.6080  \n",
      "\n",
      "Fold: 27  Epoch: 32  Training loss = 2.6183  Validation loss = 1.6883  \n",
      "\n",
      "Fold: 27  Epoch: 33  Training loss = 2.6176  Validation loss = 1.7129  \n",
      "\n",
      "Fold: 27  Epoch: 34  Training loss = 2.6167  Validation loss = 1.6559  \n",
      "\n",
      "Fold: 27  Epoch: 35  Training loss = 2.6159  Validation loss = 1.5635  \n",
      "\n",
      "Fold: 27  Epoch: 36  Training loss = 2.6150  Validation loss = 1.5445  \n",
      "\n",
      "Fold: 27  Epoch: 37  Training loss = 2.6141  Validation loss = 1.5792  \n",
      "\n",
      "Fold: 27  Epoch: 38  Training loss = 2.6133  Validation loss = 1.5855  \n",
      "\n",
      "Fold: 27  Epoch: 39  Training loss = 2.6129  Validation loss = 1.5701  \n",
      "\n",
      "Fold: 27  Epoch: 40  Training loss = 2.6124  Validation loss = 1.5702  \n",
      "\n",
      "Fold: 27  Epoch: 41  Training loss = 2.6115  Validation loss = 1.5842  \n",
      "\n",
      "Fold: 27  Epoch: 42  Training loss = 2.6108  Validation loss = 1.5776  \n",
      "\n",
      "Fold: 27  Epoch: 43  Training loss = 2.6103  Validation loss = 1.5294  \n",
      "\n",
      "Fold: 27  Epoch: 44  Training loss = 2.6093  Validation loss = 1.5631  \n",
      "\n",
      "Fold: 27  Epoch: 45  Training loss = 2.6084  Validation loss = 1.5694  \n",
      "\n",
      "Fold: 27  Epoch: 46  Training loss = 2.6079  Validation loss = 1.5521  \n",
      "\n",
      "Fold: 27  Epoch: 47  Training loss = 2.6070  Validation loss = 1.5986  \n",
      "\n",
      "Fold: 27  Epoch: 48  Training loss = 2.6064  Validation loss = 1.6538  \n",
      "\n",
      "Fold: 27  Epoch: 49  Training loss = 2.6054  Validation loss = 1.6170  \n",
      "\n",
      "Fold: 27  Epoch: 50  Training loss = 2.6045  Validation loss = 1.6115  \n",
      "\n",
      "Fold: 27  Epoch: 51  Training loss = 2.6040  Validation loss = 1.6748  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 43  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 2.4939  Validation loss = 1.6989  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 2.4916  Validation loss = 1.6947  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 2.4897  Validation loss = 1.6855  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 2.4880  Validation loss = 1.6905  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 2.4882  Validation loss = 1.6976  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 2.4830  Validation loss = 1.6814  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 2.4834  Validation loss = 1.6920  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 2.4827  Validation loss = 1.6940  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 2.4800  Validation loss = 1.6864  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 2.4796  Validation loss = 1.6861  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 2.4761  Validation loss = 1.6830  \n",
      "\n",
      "Fold: 28  Epoch: 12  Training loss = 2.4750  Validation loss = 1.6907  \n",
      "\n",
      "Fold: 28  Epoch: 13  Training loss = 2.4731  Validation loss = 1.6738  \n",
      "\n",
      "Fold: 28  Epoch: 14  Training loss = 2.4724  Validation loss = 1.6769  \n",
      "\n",
      "Fold: 28  Epoch: 15  Training loss = 2.4731  Validation loss = 1.6918  \n",
      "\n",
      "Fold: 28  Epoch: 16  Training loss = 2.4691  Validation loss = 1.6877  \n",
      "\n",
      "Fold: 28  Epoch: 17  Training loss = 2.4680  Validation loss = 1.6815  \n",
      "\n",
      "Fold: 28  Epoch: 18  Training loss = 2.4682  Validation loss = 1.6904  \n",
      "\n",
      "Fold: 28  Epoch: 19  Training loss = 2.4676  Validation loss = 1.6952  \n",
      "\n",
      "Fold: 28  Epoch: 20  Training loss = 2.4651  Validation loss = 1.6889  \n",
      "\n",
      "Fold: 28  Epoch: 21  Training loss = 2.4671  Validation loss = 1.7051  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 13  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 2.4665  Validation loss = 2.2551  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 2.4641  Validation loss = 2.2541  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 2.4628  Validation loss = 2.2496  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 2.4628  Validation loss = 2.2505  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 2.4600  Validation loss = 2.2399  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 2.4591  Validation loss = 2.2475  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 2.4570  Validation loss = 2.2394  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 2.4560  Validation loss = 2.2471  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 2.4576  Validation loss = 2.2545  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 2.4550  Validation loss = 2.2536  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 2.4539  Validation loss = 2.2529  \n",
      "\n",
      "Fold: 29  Epoch: 12  Training loss = 2.4505  Validation loss = 2.2462  \n",
      "\n",
      "Fold: 29  Epoch: 13  Training loss = 2.4485  Validation loss = 2.2427  \n",
      "\n",
      "Fold: 29  Epoch: 14  Training loss = 2.4470  Validation loss = 2.2349  \n",
      "\n",
      "Fold: 29  Epoch: 15  Training loss = 2.4463  Validation loss = 2.2250  \n",
      "\n",
      "Fold: 29  Epoch: 16  Training loss = 2.4447  Validation loss = 2.2369  \n",
      "\n",
      "Fold: 29  Epoch: 17  Training loss = 2.4424  Validation loss = 2.2327  \n",
      "\n",
      "Fold: 29  Epoch: 18  Training loss = 2.4424  Validation loss = 2.2406  \n",
      "\n",
      "Fold: 29  Epoch: 19  Training loss = 2.4390  Validation loss = 2.2315  \n",
      "\n",
      "Fold: 29  Epoch: 20  Training loss = 2.4373  Validation loss = 2.2317  \n",
      "\n",
      "Fold: 29  Epoch: 21  Training loss = 2.4353  Validation loss = 2.2258  \n",
      "\n",
      "Fold: 29  Epoch: 22  Training loss = 2.4352  Validation loss = 2.2209  \n",
      "\n",
      "Fold: 29  Epoch: 23  Training loss = 2.4332  Validation loss = 2.2234  \n",
      "\n",
      "Fold: 29  Epoch: 24  Training loss = 2.4322  Validation loss = 2.2291  \n",
      "\n",
      "Fold: 29  Epoch: 25  Training loss = 2.4307  Validation loss = 2.2281  \n",
      "\n",
      "Fold: 29  Epoch: 26  Training loss = 2.4305  Validation loss = 2.2323  \n",
      "\n",
      "Fold: 29  Epoch: 27  Training loss = 2.4286  Validation loss = 2.2263  \n",
      "\n",
      "Fold: 29  Epoch: 28  Training loss = 2.4284  Validation loss = 2.2354  \n",
      "\n",
      "Fold: 29  Epoch: 29  Training loss = 2.4291  Validation loss = 2.2461  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 22  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 2.4653  Validation loss = 1.4822  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 2.4644  Validation loss = 1.4823  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 2.4636  Validation loss = 1.4820  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 2.4625  Validation loss = 1.4813  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 2.4614  Validation loss = 1.4794  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 2.4600  Validation loss = 1.4795  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 2.4586  Validation loss = 1.4735  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 2.4576  Validation loss = 1.4720  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 2.4562  Validation loss = 1.4723  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 2.4549  Validation loss = 1.4742  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 2.4541  Validation loss = 1.4715  \n",
      "\n",
      "Fold: 30  Epoch: 12  Training loss = 2.4525  Validation loss = 1.4672  \n",
      "\n",
      "Fold: 30  Epoch: 13  Training loss = 2.4516  Validation loss = 1.4635  \n",
      "\n",
      "Fold: 30  Epoch: 14  Training loss = 2.4508  Validation loss = 1.4618  \n",
      "\n",
      "Fold: 30  Epoch: 15  Training loss = 2.4492  Validation loss = 1.4598  \n",
      "\n",
      "Fold: 30  Epoch: 16  Training loss = 2.4485  Validation loss = 1.4596  \n",
      "\n",
      "Fold: 30  Epoch: 17  Training loss = 2.4477  Validation loss = 1.4598  \n",
      "\n",
      "Fold: 30  Epoch: 18  Training loss = 2.4471  Validation loss = 1.4619  \n",
      "\n",
      "Fold: 30  Epoch: 19  Training loss = 2.4458  Validation loss = 1.4589  \n",
      "\n",
      "Fold: 30  Epoch: 20  Training loss = 2.4448  Validation loss = 1.4602  \n",
      "\n",
      "Fold: 30  Epoch: 21  Training loss = 2.4434  Validation loss = 1.4573  \n",
      "\n",
      "Fold: 30  Epoch: 22  Training loss = 2.4420  Validation loss = 1.4549  \n",
      "\n",
      "Fold: 30  Epoch: 23  Training loss = 2.4416  Validation loss = 1.4576  \n",
      "\n",
      "Fold: 30  Epoch: 24  Training loss = 2.4407  Validation loss = 1.4566  \n",
      "\n",
      "Fold: 30  Epoch: 25  Training loss = 2.4426  Validation loss = 1.4557  \n",
      "\n",
      "Fold: 30  Epoch: 26  Training loss = 2.4385  Validation loss = 1.4521  \n",
      "\n",
      "Fold: 30  Epoch: 27  Training loss = 2.4373  Validation loss = 1.4520  \n",
      "\n",
      "Fold: 30  Epoch: 28  Training loss = 2.4361  Validation loss = 1.4520  \n",
      "\n",
      "Fold: 30  Epoch: 29  Training loss = 2.4348  Validation loss = 1.4503  \n",
      "\n",
      "Fold: 30  Epoch: 30  Training loss = 2.4340  Validation loss = 1.4509  \n",
      "\n",
      "Fold: 30  Epoch: 31  Training loss = 2.4329  Validation loss = 1.4478  \n",
      "\n",
      "Fold: 30  Epoch: 32  Training loss = 2.4327  Validation loss = 1.4536  \n",
      "\n",
      "Fold: 30  Epoch: 33  Training loss = 2.4314  Validation loss = 1.4561  \n",
      "\n",
      "Fold: 30  Epoch: 34  Training loss = 2.4304  Validation loss = 1.4500  \n",
      "\n",
      "Fold: 30  Epoch: 35  Training loss = 2.4296  Validation loss = 1.4528  \n",
      "\n",
      "Fold: 30  Epoch: 36  Training loss = 2.4295  Validation loss = 1.4509  \n",
      "\n",
      "Fold: 30  Epoch: 37  Training loss = 2.4283  Validation loss = 1.4547  \n",
      "\n",
      "Fold: 30  Epoch: 38  Training loss = 2.4269  Validation loss = 1.4513  \n",
      "\n",
      "Fold: 30  Epoch: 39  Training loss = 2.4266  Validation loss = 1.4533  \n",
      "\n",
      "Fold: 30  Epoch: 40  Training loss = 2.4250  Validation loss = 1.4543  \n",
      "\n",
      "Fold: 30  Epoch: 41  Training loss = 2.4242  Validation loss = 1.4560  \n",
      "\n",
      "Fold: 30  Epoch: 42  Training loss = 2.4233  Validation loss = 1.4529  \n",
      "\n",
      "Fold: 30  Epoch: 43  Training loss = 2.4218  Validation loss = 1.4518  \n",
      "\n",
      "Fold: 30  Epoch: 44  Training loss = 2.4209  Validation loss = 1.4496  \n",
      "\n",
      "Fold: 30  Epoch: 45  Training loss = 2.4194  Validation loss = 1.4504  \n",
      "\n",
      "Fold: 30  Epoch: 46  Training loss = 2.4188  Validation loss = 1.4444  \n",
      "\n",
      "Fold: 30  Epoch: 47  Training loss = 2.4177  Validation loss = 1.4453  \n",
      "\n",
      "Fold: 30  Epoch: 48  Training loss = 2.4157  Validation loss = 1.4443  \n",
      "\n",
      "Fold: 30  Epoch: 49  Training loss = 2.4147  Validation loss = 1.4409  \n",
      "\n",
      "Fold: 30  Epoch: 50  Training loss = 2.4136  Validation loss = 1.4415  \n",
      "\n",
      "Fold: 30  Epoch: 51  Training loss = 2.4128  Validation loss = 1.4447  \n",
      "\n",
      "Fold: 30  Epoch: 52  Training loss = 2.4114  Validation loss = 1.4422  \n",
      "\n",
      "Fold: 30  Epoch: 53  Training loss = 2.4115  Validation loss = 1.4447  \n",
      "\n",
      "Fold: 30  Epoch: 54  Training loss = 2.4086  Validation loss = 1.4412  \n",
      "\n",
      "Fold: 30  Epoch: 55  Training loss = 2.4072  Validation loss = 1.4392  \n",
      "\n",
      "Fold: 30  Epoch: 56  Training loss = 2.4057  Validation loss = 1.4365  \n",
      "\n",
      "Fold: 30  Epoch: 57  Training loss = 2.4046  Validation loss = 1.4347  \n",
      "\n",
      "Fold: 30  Epoch: 58  Training loss = 2.4040  Validation loss = 1.4301  \n",
      "\n",
      "Fold: 30  Epoch: 59  Training loss = 2.4018  Validation loss = 1.4334  \n",
      "\n",
      "Fold: 30  Epoch: 60  Training loss = 2.4018  Validation loss = 1.4346  \n",
      "\n",
      "Fold: 30  Epoch: 61  Training loss = 2.4007  Validation loss = 1.4334  \n",
      "\n",
      "Fold: 30  Epoch: 62  Training loss = 2.3998  Validation loss = 1.4336  \n",
      "\n",
      "Fold: 30  Epoch: 63  Training loss = 2.3989  Validation loss = 1.4328  \n",
      "\n",
      "Fold: 30  Epoch: 64  Training loss = 2.3990  Validation loss = 1.4335  \n",
      "\n",
      "Fold: 30  Epoch: 65  Training loss = 2.3971  Validation loss = 1.4278  \n",
      "\n",
      "Fold: 30  Epoch: 66  Training loss = 2.3963  Validation loss = 1.4287  \n",
      "\n",
      "Fold: 30  Epoch: 67  Training loss = 2.3971  Validation loss = 1.4273  \n",
      "\n",
      "Fold: 30  Epoch: 68  Training loss = 2.3963  Validation loss = 1.4265  \n",
      "\n",
      "Fold: 30  Epoch: 69  Training loss = 2.3950  Validation loss = 1.4266  \n",
      "\n",
      "Fold: 30  Epoch: 70  Training loss = 2.3939  Validation loss = 1.4258  \n",
      "\n",
      "Fold: 30  Epoch: 71  Training loss = 2.3924  Validation loss = 1.4283  \n",
      "\n",
      "Fold: 30  Epoch: 72  Training loss = 2.3914  Validation loss = 1.4223  \n",
      "\n",
      "Fold: 30  Epoch: 73  Training loss = 2.3907  Validation loss = 1.4216  \n",
      "\n",
      "Fold: 30  Epoch: 74  Training loss = 2.3895  Validation loss = 1.4206  \n",
      "\n",
      "Fold: 30  Epoch: 75  Training loss = 2.3907  Validation loss = 1.4166  \n",
      "\n",
      "Fold: 30  Epoch: 76  Training loss = 2.3891  Validation loss = 1.4197  \n",
      "\n",
      "Fold: 30  Epoch: 77  Training loss = 2.3880  Validation loss = 1.4186  \n",
      "\n",
      "Fold: 30  Epoch: 78  Training loss = 2.3866  Validation loss = 1.4254  \n",
      "\n",
      "Fold: 30  Epoch: 79  Training loss = 2.3857  Validation loss = 1.4243  \n",
      "\n",
      "Fold: 30  Epoch: 80  Training loss = 2.3845  Validation loss = 1.4227  \n",
      "\n",
      "Fold: 30  Epoch: 81  Training loss = 2.3837  Validation loss = 1.4225  \n",
      "\n",
      "Fold: 30  Epoch: 82  Training loss = 2.3845  Validation loss = 1.4190  \n",
      "\n",
      "Fold: 30  Epoch: 83  Training loss = 2.3839  Validation loss = 1.4151  \n",
      "\n",
      "Fold: 30  Epoch: 84  Training loss = 2.3825  Validation loss = 1.4156  \n",
      "\n",
      "Fold: 30  Epoch: 85  Training loss = 2.3812  Validation loss = 1.4131  \n",
      "\n",
      "Fold: 30  Epoch: 86  Training loss = 2.3791  Validation loss = 1.4195  \n",
      "\n",
      "Fold: 30  Epoch: 87  Training loss = 2.3785  Validation loss = 1.4186  \n",
      "\n",
      "Fold: 30  Epoch: 88  Training loss = 2.3774  Validation loss = 1.4185  \n",
      "\n",
      "Fold: 30  Epoch: 89  Training loss = 2.3771  Validation loss = 1.4176  \n",
      "\n",
      "Fold: 30  Epoch: 90  Training loss = 2.3758  Validation loss = 1.4167  \n",
      "\n",
      "Fold: 30  Epoch: 91  Training loss = 2.3748  Validation loss = 1.4161  \n",
      "\n",
      "Fold: 30  Epoch: 92  Training loss = 2.3742  Validation loss = 1.4179  \n",
      "\n",
      "Fold: 30  Epoch: 93  Training loss = 2.3733  Validation loss = 1.4177  \n",
      "\n",
      "Fold: 30  Epoch: 94  Training loss = 2.3718  Validation loss = 1.4151  \n",
      "\n",
      "Fold: 30  Epoch: 95  Training loss = 2.3710  Validation loss = 1.4138  \n",
      "\n",
      "Fold: 30  Epoch: 96  Training loss = 2.3704  Validation loss = 1.4128  \n",
      "\n",
      "Fold: 30  Epoch: 97  Training loss = 2.3695  Validation loss = 1.4141  \n",
      "\n",
      "Fold: 30  Epoch: 98  Training loss = 2.3688  Validation loss = 1.4146  \n",
      "\n",
      "Fold: 30  Epoch: 99  Training loss = 2.3680  Validation loss = 1.4106  \n",
      "\n",
      "Fold: 30  Epoch: 100  Training loss = 2.3679  Validation loss = 1.4098  \n",
      "\n",
      "Fold: 30  Epoch: 101  Training loss = 2.3663  Validation loss = 1.4120  \n",
      "\n",
      "Fold: 30  Epoch: 102  Training loss = 2.3651  Validation loss = 1.4104  \n",
      "\n",
      "Fold: 30  Epoch: 103  Training loss = 2.3652  Validation loss = 1.4154  \n",
      "\n",
      "Fold: 30  Epoch: 104  Training loss = 2.3631  Validation loss = 1.4118  \n",
      "\n",
      "Fold: 30  Epoch: 105  Training loss = 2.3627  Validation loss = 1.4137  \n",
      "\n",
      "Fold: 30  Epoch: 106  Training loss = 2.3614  Validation loss = 1.4123  \n",
      "\n",
      "Fold: 30  Epoch: 107  Training loss = 2.3605  Validation loss = 1.4125  \n",
      "\n",
      "Fold: 30  Epoch: 108  Training loss = 2.3596  Validation loss = 1.4108  \n",
      "\n",
      "Fold: 30  Epoch: 109  Training loss = 2.3588  Validation loss = 1.4077  \n",
      "\n",
      "Fold: 30  Epoch: 110  Training loss = 2.3584  Validation loss = 1.4143  \n",
      "\n",
      "Fold: 30  Epoch: 111  Training loss = 2.3605  Validation loss = 1.4179  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 109  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 2.2347  Validation loss = 1.4877  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 2.2332  Validation loss = 1.4836  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 2.2310  Validation loss = 1.4592  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 2.2306  Validation loss = 1.4696  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 2.2298  Validation loss = 1.4541  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 2.2286  Validation loss = 1.4565  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 2.2275  Validation loss = 1.4523  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 2.2266  Validation loss = 1.4440  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 2.2261  Validation loss = 1.4458  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 2.2252  Validation loss = 1.4376  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 2.2251  Validation loss = 1.4167  \n",
      "\n",
      "Fold: 31  Epoch: 12  Training loss = 2.2236  Validation loss = 1.4190  \n",
      "\n",
      "Fold: 31  Epoch: 13  Training loss = 2.2229  Validation loss = 1.4403  \n",
      "\n",
      "Fold: 31  Epoch: 14  Training loss = 2.2222  Validation loss = 1.4037  \n",
      "\n",
      "Fold: 31  Epoch: 15  Training loss = 2.2210  Validation loss = 1.4424  \n",
      "\n",
      "Fold: 31  Epoch: 16  Training loss = 2.2202  Validation loss = 1.4091  \n",
      "\n",
      "Fold: 31  Epoch: 17  Training loss = 2.2198  Validation loss = 1.4051  \n",
      "\n",
      "Fold: 31  Epoch: 18  Training loss = 2.2198  Validation loss = 1.3842  \n",
      "\n",
      "Fold: 31  Epoch: 19  Training loss = 2.2173  Validation loss = 1.3985  \n",
      "\n",
      "Fold: 31  Epoch: 20  Training loss = 2.2169  Validation loss = 1.3971  \n",
      "\n",
      "Fold: 31  Epoch: 21  Training loss = 2.2155  Validation loss = 1.3997  \n",
      "\n",
      "Fold: 31  Epoch: 22  Training loss = 2.2146  Validation loss = 1.3926  \n",
      "\n",
      "Fold: 31  Epoch: 23  Training loss = 2.2136  Validation loss = 1.3919  \n",
      "\n",
      "Fold: 31  Epoch: 24  Training loss = 2.2127  Validation loss = 1.3973  \n",
      "\n",
      "Fold: 31  Epoch: 25  Training loss = 2.2137  Validation loss = 1.4311  \n",
      "\n",
      "Fold: 31  Epoch: 26  Training loss = 2.2159  Validation loss = 1.4619  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 18  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 1.9051  Validation loss = 3.1363  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 1.9044  Validation loss = 3.1354  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 1.9008  Validation loss = 3.1090  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 1.9017  Validation loss = 3.1254  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 1.8977  Validation loss = 3.0722  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 1.8960  Validation loss = 3.0711  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 1.8951  Validation loss = 3.0510  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 1.8941  Validation loss = 3.0525  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 1.8946  Validation loss = 3.0970  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 1.8929  Validation loss = 3.0909  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 1.8910  Validation loss = 3.0744  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 1.8919  Validation loss = 3.1010  \n",
      "\n",
      "Fold: 32  Epoch: 13  Training loss = 1.8902  Validation loss = 3.0938  \n",
      "\n",
      "Fold: 32  Epoch: 14  Training loss = 1.8882  Validation loss = 3.0733  \n",
      "\n",
      "Fold: 32  Epoch: 15  Training loss = 1.8863  Validation loss = 3.0445  \n",
      "\n",
      "Fold: 32  Epoch: 16  Training loss = 1.8859  Validation loss = 3.0520  \n",
      "\n",
      "Fold: 32  Epoch: 17  Training loss = 1.8856  Validation loss = 3.0737  \n",
      "\n",
      "Fold: 32  Epoch: 18  Training loss = 1.8829  Validation loss = 3.0351  \n",
      "\n",
      "Fold: 32  Epoch: 19  Training loss = 1.8837  Validation loss = 3.0702  \n",
      "\n",
      "Fold: 32  Epoch: 20  Training loss = 1.8842  Validation loss = 3.0819  \n",
      "\n",
      "Fold: 32  Epoch: 21  Training loss = 1.8840  Validation loss = 3.0845  \n",
      "\n",
      "Fold: 32  Epoch: 22  Training loss = 1.8811  Validation loss = 3.0626  \n",
      "\n",
      "Fold: 32  Epoch: 23  Training loss = 1.8787  Validation loss = 3.0423  \n",
      "\n",
      "Fold: 32  Epoch: 24  Training loss = 1.8758  Validation loss = 3.0271  \n",
      "\n",
      "Fold: 32  Epoch: 25  Training loss = 1.8746  Validation loss = 3.0081  \n",
      "\n",
      "Fold: 32  Epoch: 26  Training loss = 1.8751  Validation loss = 2.9676  \n",
      "\n",
      "Fold: 32  Epoch: 27  Training loss = 1.8737  Validation loss = 2.9690  \n",
      "\n",
      "Fold: 32  Epoch: 28  Training loss = 1.8720  Validation loss = 2.9712  \n",
      "\n",
      "Fold: 32  Epoch: 29  Training loss = 1.8696  Validation loss = 3.0022  \n",
      "\n",
      "Fold: 32  Epoch: 30  Training loss = 1.8691  Validation loss = 3.0252  \n",
      "\n",
      "Fold: 32  Epoch: 31  Training loss = 1.8672  Validation loss = 3.0141  \n",
      "\n",
      "Fold: 32  Epoch: 32  Training loss = 1.8659  Validation loss = 2.9968  \n",
      "\n",
      "Fold: 32  Epoch: 33  Training loss = 1.8659  Validation loss = 2.9565  \n",
      "\n",
      "Fold: 32  Epoch: 34  Training loss = 1.8655  Validation loss = 2.9400  \n",
      "\n",
      "Fold: 32  Epoch: 35  Training loss = 1.8645  Validation loss = 2.9282  \n",
      "\n",
      "Fold: 32  Epoch: 36  Training loss = 1.8608  Validation loss = 2.9426  \n",
      "\n",
      "Fold: 32  Epoch: 37  Training loss = 1.8591  Validation loss = 2.9606  \n",
      "\n",
      "Fold: 32  Epoch: 38  Training loss = 1.8587  Validation loss = 2.9404  \n",
      "\n",
      "Fold: 32  Epoch: 39  Training loss = 1.8574  Validation loss = 2.9433  \n",
      "\n",
      "Fold: 32  Epoch: 40  Training loss = 1.8573  Validation loss = 2.9814  \n",
      "\n",
      "Fold: 32  Epoch: 41  Training loss = 1.8583  Validation loss = 3.0093  \n",
      "\n",
      "Fold: 32  Epoch: 42  Training loss = 1.8585  Validation loss = 3.0216  \n",
      "\n",
      "Fold: 32  Epoch: 43  Training loss = 1.8608  Validation loss = 3.0497  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 35  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    # Load model with lowest validation error for each fold\n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 147\n",
      "Average validation error: 2.77642\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 1.8540  Test loss = 3.0221  \n",
      "\n",
      "Epoch: 2  Training loss = 1.8523  Test loss = 3.0172  \n",
      "\n",
      "Epoch: 3  Training loss = 1.8507  Test loss = 3.0126  \n",
      "\n",
      "Epoch: 4  Training loss = 1.8492  Test loss = 3.0081  \n",
      "\n",
      "Epoch: 5  Training loss = 1.8477  Test loss = 3.0037  \n",
      "\n",
      "Epoch: 6  Training loss = 1.8464  Test loss = 2.9995  \n",
      "\n",
      "Epoch: 7  Training loss = 1.8451  Test loss = 2.9953  \n",
      "\n",
      "Epoch: 8  Training loss = 1.8438  Test loss = 2.9914  \n",
      "\n",
      "Epoch: 9  Training loss = 1.8426  Test loss = 2.9875  \n",
      "\n",
      "Epoch: 10  Training loss = 1.8415  Test loss = 2.9837  \n",
      "\n",
      "Epoch: 11  Training loss = 1.8404  Test loss = 2.9801  \n",
      "\n",
      "Epoch: 12  Training loss = 1.8393  Test loss = 2.9765  \n",
      "\n",
      "Epoch: 13  Training loss = 1.8383  Test loss = 2.9730  \n",
      "\n",
      "Epoch: 14  Training loss = 1.8373  Test loss = 2.9696  \n",
      "\n",
      "Epoch: 15  Training loss = 1.8364  Test loss = 2.9663  \n",
      "\n",
      "Epoch: 16  Training loss = 1.8354  Test loss = 2.9631  \n",
      "\n",
      "Epoch: 17  Training loss = 1.8346  Test loss = 2.9599  \n",
      "\n",
      "Epoch: 18  Training loss = 1.8337  Test loss = 2.9569  \n",
      "\n",
      "Epoch: 19  Training loss = 1.8328  Test loss = 2.9538  \n",
      "\n",
      "Epoch: 20  Training loss = 1.8320  Test loss = 2.9509  \n",
      "\n",
      "Epoch: 21  Training loss = 1.8312  Test loss = 2.9480  \n",
      "\n",
      "Epoch: 22  Training loss = 1.8304  Test loss = 2.9452  \n",
      "\n",
      "Epoch: 23  Training loss = 1.8297  Test loss = 2.9424  \n",
      "\n",
      "Epoch: 24  Training loss = 1.8289  Test loss = 2.9397  \n",
      "\n",
      "Epoch: 25  Training loss = 1.8282  Test loss = 2.9370  \n",
      "\n",
      "Epoch: 26  Training loss = 1.8275  Test loss = 2.9344  \n",
      "\n",
      "Epoch: 27  Training loss = 1.8268  Test loss = 2.9318  \n",
      "\n",
      "Epoch: 28  Training loss = 1.8261  Test loss = 2.9293  \n",
      "\n",
      "Epoch: 29  Training loss = 1.8254  Test loss = 2.9269  \n",
      "\n",
      "Epoch: 30  Training loss = 1.8248  Test loss = 2.9244  \n",
      "\n",
      "Epoch: 31  Training loss = 1.8241  Test loss = 2.9221  \n",
      "\n",
      "Epoch: 32  Training loss = 1.8235  Test loss = 2.9197  \n",
      "\n",
      "Epoch: 33  Training loss = 1.8229  Test loss = 2.9174  \n",
      "\n",
      "Epoch: 34  Training loss = 1.8222  Test loss = 2.9152  \n",
      "\n",
      "Epoch: 35  Training loss = 1.8216  Test loss = 2.9129  \n",
      "\n",
      "Epoch: 36  Training loss = 1.8210  Test loss = 2.9108  \n",
      "\n",
      "Epoch: 37  Training loss = 1.8205  Test loss = 2.9086  \n",
      "\n",
      "Epoch: 38  Training loss = 1.8199  Test loss = 2.9065  \n",
      "\n",
      "Epoch: 39  Training loss = 1.8193  Test loss = 2.9044  \n",
      "\n",
      "Epoch: 40  Training loss = 1.8187  Test loss = 2.9024  \n",
      "\n",
      "Epoch: 41  Training loss = 1.8182  Test loss = 2.9004  \n",
      "\n",
      "Epoch: 42  Training loss = 1.8177  Test loss = 2.8985  \n",
      "\n",
      "Epoch: 43  Training loss = 1.8171  Test loss = 2.8965  \n",
      "\n",
      "Epoch: 44  Training loss = 1.8166  Test loss = 2.8946  \n",
      "\n",
      "Epoch: 45  Training loss = 1.8161  Test loss = 2.8928  \n",
      "\n",
      "Epoch: 46  Training loss = 1.8156  Test loss = 2.8909  \n",
      "\n",
      "Epoch: 47  Training loss = 1.8151  Test loss = 2.8891  \n",
      "\n",
      "Epoch: 48  Training loss = 1.8146  Test loss = 2.8874  \n",
      "\n",
      "Epoch: 49  Training loss = 1.8141  Test loss = 2.8856  \n",
      "\n",
      "Epoch: 50  Training loss = 1.8136  Test loss = 2.8839  \n",
      "\n",
      "Epoch: 51  Training loss = 1.8131  Test loss = 2.8822  \n",
      "\n",
      "Epoch: 52  Training loss = 1.8126  Test loss = 2.8806  \n",
      "\n",
      "Epoch: 53  Training loss = 1.8122  Test loss = 2.8790  \n",
      "\n",
      "Epoch: 54  Training loss = 1.8117  Test loss = 2.8774  \n",
      "\n",
      "Epoch: 55  Training loss = 1.8112  Test loss = 2.8758  \n",
      "\n",
      "Epoch: 56  Training loss = 1.8108  Test loss = 2.8742  \n",
      "\n",
      "Epoch: 57  Training loss = 1.8103  Test loss = 2.8727  \n",
      "\n",
      "Epoch: 58  Training loss = 1.8099  Test loss = 2.8712  \n",
      "\n",
      "Epoch: 59  Training loss = 1.8095  Test loss = 2.8697  \n",
      "\n",
      "Epoch: 60  Training loss = 1.8090  Test loss = 2.8683  \n",
      "\n",
      "Epoch: 61  Training loss = 1.8086  Test loss = 2.8669  \n",
      "\n",
      "Epoch: 62  Training loss = 1.8082  Test loss = 2.8654  \n",
      "\n",
      "Epoch: 63  Training loss = 1.8078  Test loss = 2.8641  \n",
      "\n",
      "Epoch: 64  Training loss = 1.8073  Test loss = 2.8627  \n",
      "\n",
      "Epoch: 65  Training loss = 1.8069  Test loss = 2.8613  \n",
      "\n",
      "Epoch: 66  Training loss = 1.8065  Test loss = 2.8600  \n",
      "\n",
      "Epoch: 67  Training loss = 1.8061  Test loss = 2.8587  \n",
      "\n",
      "Epoch: 68  Training loss = 1.8057  Test loss = 2.8574  \n",
      "\n",
      "Epoch: 69  Training loss = 1.8053  Test loss = 2.8562  \n",
      "\n",
      "Epoch: 70  Training loss = 1.8049  Test loss = 2.8549  \n",
      "\n",
      "Epoch: 71  Training loss = 1.8045  Test loss = 2.8537  \n",
      "\n",
      "Epoch: 72  Training loss = 1.8041  Test loss = 2.8524  \n",
      "\n",
      "Epoch: 73  Training loss = 1.8037  Test loss = 2.8512  \n",
      "\n",
      "Epoch: 74  Training loss = 1.8034  Test loss = 2.8500  \n",
      "\n",
      "Epoch: 75  Training loss = 1.8030  Test loss = 2.8489  \n",
      "\n",
      "Epoch: 76  Training loss = 1.8026  Test loss = 2.8477  \n",
      "\n",
      "Epoch: 77  Training loss = 1.8022  Test loss = 2.8466  \n",
      "\n",
      "Epoch: 78  Training loss = 1.8019  Test loss = 2.8454  \n",
      "\n",
      "Epoch: 79  Training loss = 1.8015  Test loss = 2.8443  \n",
      "\n",
      "Epoch: 80  Training loss = 1.8011  Test loss = 2.8432  \n",
      "\n",
      "Epoch: 81  Training loss = 1.8008  Test loss = 2.8421  \n",
      "\n",
      "Epoch: 82  Training loss = 1.8004  Test loss = 2.8411  \n",
      "\n",
      "Epoch: 83  Training loss = 1.8000  Test loss = 2.8400  \n",
      "\n",
      "Epoch: 84  Training loss = 1.7997  Test loss = 2.8389  \n",
      "\n",
      "Epoch: 85  Training loss = 1.7993  Test loss = 2.8379  \n",
      "\n",
      "Epoch: 86  Training loss = 1.7990  Test loss = 2.8369  \n",
      "\n",
      "Epoch: 87  Training loss = 1.7986  Test loss = 2.8358  \n",
      "\n",
      "Epoch: 88  Training loss = 1.7982  Test loss = 2.8348  \n",
      "\n",
      "Epoch: 89  Training loss = 1.7979  Test loss = 2.8338  \n",
      "\n",
      "Epoch: 90  Training loss = 1.7976  Test loss = 2.8329  \n",
      "\n",
      "Epoch: 91  Training loss = 1.7972  Test loss = 2.8319  \n",
      "\n",
      "Epoch: 92  Training loss = 1.7969  Test loss = 2.8309  \n",
      "\n",
      "Epoch: 93  Training loss = 1.7965  Test loss = 2.8300  \n",
      "\n",
      "Epoch: 94  Training loss = 1.7962  Test loss = 2.8290  \n",
      "\n",
      "Epoch: 95  Training loss = 1.7958  Test loss = 2.8281  \n",
      "\n",
      "Epoch: 96  Training loss = 1.7955  Test loss = 2.8271  \n",
      "\n",
      "Epoch: 97  Training loss = 1.7952  Test loss = 2.8262  \n",
      "\n",
      "Epoch: 98  Training loss = 1.7948  Test loss = 2.8253  \n",
      "\n",
      "Epoch: 99  Training loss = 1.7945  Test loss = 2.8244  \n",
      "\n",
      "Epoch: 100  Training loss = 1.7942  Test loss = 2.8235  \n",
      "\n",
      "Epoch: 101  Training loss = 1.7938  Test loss = 2.8226  \n",
      "\n",
      "Epoch: 102  Training loss = 1.7935  Test loss = 2.8217  \n",
      "\n",
      "Epoch: 103  Training loss = 1.7932  Test loss = 2.8208  \n",
      "\n",
      "Epoch: 104  Training loss = 1.7928  Test loss = 2.8200  \n",
      "\n",
      "Epoch: 105  Training loss = 1.7925  Test loss = 2.8191  \n",
      "\n",
      "Epoch: 106  Training loss = 1.7922  Test loss = 2.8183  \n",
      "\n",
      "Epoch: 107  Training loss = 1.7919  Test loss = 2.8174  \n",
      "\n",
      "Epoch: 108  Training loss = 1.7915  Test loss = 2.8166  \n",
      "\n",
      "Epoch: 109  Training loss = 1.7912  Test loss = 2.8157  \n",
      "\n",
      "Epoch: 110  Training loss = 1.7909  Test loss = 2.8149  \n",
      "\n",
      "Epoch: 111  Training loss = 1.7906  Test loss = 2.8141  \n",
      "\n",
      "Epoch: 112  Training loss = 1.7903  Test loss = 2.8133  \n",
      "\n",
      "Epoch: 113  Training loss = 1.7899  Test loss = 2.8124  \n",
      "\n",
      "Epoch: 114  Training loss = 1.7896  Test loss = 2.8116  \n",
      "\n",
      "Epoch: 115  Training loss = 1.7893  Test loss = 2.8108  \n",
      "\n",
      "Epoch: 116  Training loss = 1.7890  Test loss = 2.8100  \n",
      "\n",
      "Epoch: 117  Training loss = 1.7887  Test loss = 2.8092  \n",
      "\n",
      "Epoch: 118  Training loss = 1.7884  Test loss = 2.8085  \n",
      "\n",
      "Epoch: 119  Training loss = 1.7881  Test loss = 2.8077  \n",
      "\n",
      "Epoch: 120  Training loss = 1.7877  Test loss = 2.8069  \n",
      "\n",
      "Epoch: 121  Training loss = 1.7874  Test loss = 2.8061  \n",
      "\n",
      "Epoch: 122  Training loss = 1.7871  Test loss = 2.8054  \n",
      "\n",
      "Epoch: 123  Training loss = 1.7868  Test loss = 2.8046  \n",
      "\n",
      "Epoch: 124  Training loss = 1.7865  Test loss = 2.8038  \n",
      "\n",
      "Epoch: 125  Training loss = 1.7862  Test loss = 2.8031  \n",
      "\n",
      "Epoch: 126  Training loss = 1.7859  Test loss = 2.8023  \n",
      "\n",
      "Epoch: 127  Training loss = 1.7856  Test loss = 2.8016  \n",
      "\n",
      "Epoch: 128  Training loss = 1.7853  Test loss = 2.8008  \n",
      "\n",
      "Epoch: 129  Training loss = 1.7850  Test loss = 2.8001  \n",
      "\n",
      "Epoch: 130  Training loss = 1.7847  Test loss = 2.7994  \n",
      "\n",
      "Epoch: 131  Training loss = 1.7844  Test loss = 2.7986  \n",
      "\n",
      "Epoch: 132  Training loss = 1.7841  Test loss = 2.7979  \n",
      "\n",
      "Epoch: 133  Training loss = 1.7838  Test loss = 2.7972  \n",
      "\n",
      "Epoch: 134  Training loss = 1.7835  Test loss = 2.7965  \n",
      "\n",
      "Epoch: 135  Training loss = 1.7832  Test loss = 2.7957  \n",
      "\n",
      "Epoch: 136  Training loss = 1.7829  Test loss = 2.7950  \n",
      "\n",
      "Epoch: 137  Training loss = 1.7826  Test loss = 2.7943  \n",
      "\n",
      "Epoch: 138  Training loss = 1.7823  Test loss = 2.7936  \n",
      "\n",
      "Epoch: 139  Training loss = 1.7820  Test loss = 2.7929  \n",
      "\n",
      "Epoch: 140  Training loss = 1.7817  Test loss = 2.7922  \n",
      "\n",
      "Epoch: 141  Training loss = 1.7814  Test loss = 2.7915  \n",
      "\n",
      "Epoch: 142  Training loss = 1.7811  Test loss = 2.7908  \n",
      "\n",
      "Epoch: 143  Training loss = 1.7808  Test loss = 2.7901  \n",
      "\n",
      "Epoch: 144  Training loss = 1.7805  Test loss = 2.7894  \n",
      "\n",
      "Epoch: 145  Training loss = 1.7802  Test loss = 2.7887  \n",
      "\n",
      "Epoch: 146  Training loss = 1.7799  Test loss = 2.7881  \n",
      "\n",
      "Epoch: 147  Training loss = 1.7796  Test loss = 2.7874  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXl4VOXZ/z8n+0IWEnYSIGwJYZdFFjdQREXADRXX6mtb\nQattbfu6tVprq/352letrUu19VURVAoqqGwCUsRSQSTIFtawhATIQvZ1zu+PZ57JmZlzZkkm28zz\nuS6uIZMzZ85MZr7ne+7nXjRd11EoFApF8BDW3gegUCgUisCihF2hUCiCDCXsCoVCEWQoYVcoFIog\nQwm7QqFQBBlK2BUKhSLIUMKuUCgUQYYSdoVCoQgylLArFApFkBHRHk/arVs3fcCAAe3x1AqFQtFp\n2b59+1ld17t7265dhH3AgAFs27atPZ5aoVAoOi2apuX5sp0KxSgUCkWQoYRdoVAoggwl7AqFQhFk\nKGFXKBSKIEMJu0KhUAQZStgVCoUiyFDCrlAoFEFG5xL2tWvh2Web9dCTJ08G+GAUCoWiY9L5hP3X\nv4bCQr8etnr1atLT0zl69GjrHJci5Nm6dStlZWXtfRgKBdDZhP2uu6ChAd5916+HbdmyBV3XKSgo\naKUDUwQrixcv5oknnvC4zbFjx5g0aRJDhgzh9ddfp7GxsY2OTqEwp3MJ+7BhMGkS/P3voOs+P2zX\nrl0AVFVVtdaRKYKUJUuW8Le//c3jNvn5+QDExMTw4x//mLFjx7J27dq2ODyFwpTOJewAd98Ne/bA\nN9/4/JCcnBwAqqurW+uoFEFKUVERxcXF6B6MRHFxMSBOAkuXLqWiooLLL7+cefPmYbPZ2upQFQoH\nnU/Yb7oJYmOFa/eBiooKDh06BCjHrvCfoqIiamtrPX52pLCnpqZy/fXXs3fvXu677z6WLl3KwYMH\n2+pQFQoHnU/YExNh3jxYvBh8EOrdu3c7/q8cu8JfioqKgCbxNkP+LiUlBYDo6GhmzpwJQHl5eSsf\noULhTucTdhCLqGVlsHy5101lGAZcHPvLL8Mll7TCwSmCBV3XHaLti7B37drVcV9iYiKAypRRtAud\nU9gvuggGDvQpHLNr1y4iIkTbeSfH/re/wb/+5dcirCK0OHfunCPDRTp3M4qLi0lOTiY8PNxxnxT2\nc+fOte5BKhQmdE5hDwsTrn39ejhyxOOmOTk5jB49GjA49mPHICcHbDaoqGjto1V0Uoxi7smxFxUV\nOcIwkqSkJEA5dkX70KmEvaGhgZKSEvHDnXeCpsFbb1lur+s6OTk5nHfeeYSHhzc59pUrmzYqLW29\nA1Z0anwV9uLiYjdhV6EYRXvSqYR94cKFDBs2TPyQng4zZghht0gpy8/Pp6SkhNGjRxMbG9vk2JWw\nhxwlJSXU1tb69RijsHsLxbgKe0JCAqCEXdE+dCphT01Ndc4pvvtuEVZZv950e7lwOnLkSOLi4oRj\nr6wU2w8fLjZSwh4SnH/++fzud7/z6zH+OPbU1FSn+6Kjo4mOjlbCrmgXOpWwp6SkUF9fT2Vlpbhj\n7lxIToZFi0y3lxWnI0eObHLs69ZBbS3cfrvYSC1uBT21tbUcOHCAw4cP+/U4KeyxsbF+h2JAhGOU\nsCvag04n7GBwUjExMG4c7N1run1OTg7p6el07dq1ybGvXCly4a++WmykHHvQI0v+S/38WxcVFREW\nFkZGRoZlKMZms1FSUmIp7CorRtEedEphd3JPAweChRPLyclh5MiRgHBdNVVVQtivuAJ69BAbKWEP\nek6cOAE0T9i7du1K9+7dLR17aWkpuq6bCntSUpJy7Ip2oVMJu4xjOn3JMjLgzBm3tMW6ujr27dvH\nqFGjAIiLi6NvYSEUFAi3bk9HU8Ie/LRE2FNTU0lJSbEUdteqUyMqFKNoLzqVsJs69owMcevSa33/\n/v3U19c7hD02NpbxBQUiRfLKKyEqCuLilLCHAIEQdqtQjLFPjCtK2BXtRacUdqcvmRR2l0IlY0YM\nCMc+uagIJk+Gbt3ERklJSthDACnsjhoIH5HC7paNZUA5dkVHpFMKu6ljdxH2Xbt2ERkZSWZmJgB9\ndJ3s6mqYPbtpo+RkJewhgBT2mpoaampqfH6c0bHX1dWZdnj0Juxq8VTRHnQqYY+JiSEuLs5Z2Lt3\nFyEVE8eenZ1NZGQkAOefPSt+IbNhQAi7+uIFPVLYwb/eLUZhlz+74otj99TLXaFoDTqVsAPuC1ma\nZpoZs2vXLkcYBuC8/HyOhYU1FSaBcuwhwokTJ4iNjQV8j7PX1NRQWVnpCMWAeZGSFHtjZ0dJUlIS\n9fX1fle8KhQtJSDCrmlasqZpSzVN26dp2l5N0yYHYr9mmC5kZWQ4Ofbi4mJOnDjhWDiluprMEyf4\nVNPEiUCihD3oqa+v59SpUwy3n9B9FXb5GTM6djNhLy4uJjEx0dFB1IjqF6NoLwLl2F8EVum6ngWM\nBswrhgKAXMhyQgq7/ZJXVpw6hH3TJqIaGvi4sdH5slgJe9BTUFCAruuMGDECaJmwW4VizDJiQAm7\nov1osbBrmpYEXAS8CaDrep2u662mlqY5xRkZIo/d/sUzthIA4NtvAfganBfPpLCrGGjQIuPrUth9\nzYwxCrunUIxVOwFQwq5oPwLh2DOAM8A/NE3boWnaG5qmxQdgv6ZYCjs4wjE5OTmkpqbSu3dv7HdQ\nlpJCGS7DNpKToaHBpxF7is7JyZMnAQLi2Jsr7CozRtHWBELYI4DzgFd0XR8LVAIPu26kadqPNE3b\npmnatjNnzjT7yWSM3Smk4iLsu3btYtSoUWgynr5rFyVpaYDLeLzkZHHb0cMxhYVw+nR7H0WnRDp2\nefXWHGGPjY0lJibGNBRjNmRDohy7or0IhLCfAE7our7V/vNShNA7oev667quj9d1fXz37t2b/WSp\nqanOHR6hSdgPH8ZmszlnxNTWwr59lA8YALg4dtlWoKM7qjvvFAO8FX4jM2J69+5NdHR0s4Rd3vrr\n2NUUJUV70WJh13W9ADiuaVqm/a5LgT0t3a8VppfFCQmimvTIEQoKCqisrCQrK0v8bt8+aGykatAg\noJM69qNHYfPmjn+cHZATJ06QlpaGpmkkJyf7JexxcXHExMQA5iFA2dlRLZ4qOhqByor5CbBI07Qc\nYAzwhwDt1w3LDAV7ZkxBQQEAffr0EffbWwvU2itQ3WLs0CqCqet64ApTzp4VU6IsBooorJHCDvgt\n7EbBTk1NdfvMlZWVYbPZVChG0eEIiLDruv6dPcwyStf1a3Rd968phx9YLmTZhf3UqVMATQunu3ZB\nVBT64MFA2zn266+/nnvuuaflO2psBPla165t+f5CDFdh9ycrxijsZo7dU9UpiClKUVFRStgVbU6n\nqzy1TD3LyIC8PArtQxV69eol7s/JgexsYu3uqa0c+9atW9m2bVvLd1RS0pSOqYTdL2w2GydPngyI\nY2+OsIPqF6NoHzqdsHt07PX1VOTmAtCzZ09xf04OjBrlKCl3cuyt1JO9traW/Px8px4lzUb2uJky\nBQ4dshwqonDn9OnTNDQ0OIS9a9euLQ7FGMNrMjTjTdiVY1e0NZ1O2GVPDrcY+8CBANgOHqRr165E\nR0cLUTx1CuzDrMHFscfEiH8BFvZjx44B4uRj1hHQL6Swz58vbpVr9xl5Yu3bty/Qcsfumo3lqRe7\nRE1RUrQHnU7YY2NjzYcL21MeI44fd46vg7Vjh1ZpK3DUMPSjxa5d5vxPnQrp6UrY/UC+966hGG+L\n2jLbpZvs24/5laKvoRgl7Iq2ptMJO1jkFPfrB5pG3OnTTfF1KexWjh1EOCbAMdAjhoZkx48fb9nO\npGPv3h1mzIAvvhALqgqvmAl7XV2d157spaWl2Gw2t1AMOF8pys+gWWdHiRJ2RXvQKYXdtK1AVBSk\npdG1pMR54bRbN+jVq/M6dinsqalw+eXiWAOxKBsCnDhxgsjISGRBXLJ9sdxbZoxrcRJYO/aEhARH\nz38z1OKpoj3otMJuVt6tZ2TQo6rK2bGPHAmaRmRkJOHh4e6OvZWEXebRB0TY4+MhNhYuvVS0HV6z\nJgBHGfycOHGCvn37EhYmPuZS2L3F2f0Rdk9hGFCOXdE+dEphtyrvbkhLo7/NJmLsNht8/z3YW/dq\nmkZsbGybOfasrCy6desWmFCMbMHQrRuMHavi7D5iTHWElgm7WSjGU58YiRJ2RXvQKYXdNBQDlHXr\nRh+gT0qKSAusqnIIO4iB1m3l2DMyMkhLSwuMYzcs4nH55fD111Be3rL9hgDG4iRoioU3R9jlY10d\nu6eMGBBZMXV1dWqKUiiyaVNT8kMb02mF3a3DI3CmSxfCgAGa5mglgGE8nkfHHqDy/5qaGk6dOsWA\nAQNIT09vuWM/c8ZZ2GfMEK2GN25s2X6bg83W9s/ZTHRddxP2ljh2s2wsX0MxYG8r8MUX8Oqr/r0Q\nReekvl58V2++uV3mPXRaYXfr8AjkR0cD0Ke2VsTXNc1pxqmlY6+rAz+m13siLy8PgAEDBrSOY586\nVcTb2zoc8+67Iud/1ix4++0O3xGzuLiYmpqaZgt7eHi4ozujxLVfjN/C/vOfwy9/qQa7hALHjgld\nWb8e/vnPNn/6TinsVm0FZJJht/Jy4dgHDwZ7miN4cOwQMKGSGTFS2FtcpOQq7NHRcPHFbb+A+t13\nQpB27xZthHv0gLlz4csv2/Y4fMQ11RGa2uj6khWTkpLS1M/fjjEEqOu6X8Jeu327+ExWVIiiOUVw\nIyvEExLgoYfafJhPpxR2q7YCh6urqQHiT58Wjt0QXwcLxx7gtgJGYU9PTwdakBlTWyti6UZhB3GJ\nt38/tDTM4w+FhZCWJoaZ/PvfcN998NVX8JOftN0x+IGZsEdHRxMbG+vVsZ89e9Y0dm4U9rKyMhob\nG30W9viPP266c/9+n16DohMjhf2VV4R7/+Mf2/TpO7Wwu6Y85hcUcDIiAm3PHjh40Cm+Dl4cewCF\nPTIykj59+jhEpdnCLl+fq7Bffrm4bctwTGGhcOmaBuefD3/6E9x2m/gAd8DQgpmwg29tBVzbCUiM\noRhf2gmAEHYN6L52bdPnUQl78HP4sKituflm0Q7kj390THhrCzq1sLs69oKCAgrj4kRcS9d9c+yt\nIOz9+/cnLCzM4dibvYBqrDo1Mny4CDHt3t2CI/WTwkKQjdUkAwdCZWW7rfx74sSJE4SHhzfVNNjx\npRGYlbAbHbsv7QRAhH+mAnFnzsCvfiXWR5SwBz+HD8OAARAeDs89BxERYo2ljeiUwm4VYy8oKKC0\na9emhVAXYY+NjW11YT9y5AgD7GP4ZPOpZjt2Keyujl3ToE8fsA9qbhOshB1ar+Pk0qXNzv45ceIE\nvXv3Jjw83On+ljh2Kewyvi7v80RiYiK3AvVRUXDNNTB0qHdh//BDlc7a2Tl8uOn70bcvPP44fPRR\nm62NdUphN8spBiHsVVJ84uObZqHaaatQjBT22NjYlhUpSSfsKuwgPixtJeyNjeJYrIS9NS4xdR1u\nvx2mTYN773UIXX19vft2LtlR4J7DLmlpKKa+vp6KigrfhT0mhnlA7rBh0KULZGZ6Fva9e+HGG+GF\nFzzuV9HBMQo7wM9+JpI5HnhAZMu0Mp1S2GVOsTHG3tjYKPpv28MfjBgBYc4vr7VDMdXV1RQWFjqE\nHWhZyqOVY4e2FfbiYpHD7irs8nW2hmMvLhZXXqNHw+uvw8iRfPvcc3Tr1o3PP/9crKE88YT4svTq\n5fZeeBJ2T1kxVVVV1NTUWDp2cWjFPgt79IYNpALfDBki7sjMFDNsrQqWvv1W3BoXWxWdi5ISoSdG\nYY+OFifr/fuFc29lOqWwg3v16ZkzZ7DZbITLL5DLwilYOPaYGLHIEYB0R5nDnmG4UgiIsJuJR9++\nkJ/fNguXhYXi1kXYf/744xRFRWE7eDDwz2mfhMXjj8PmzTSEh3Per37FX8rK6HfzzTBkCPzud9C/\nv0gl++tfHQ/VdZ3jx487QmFGvDl2s+IkiVHYfRmyAaC99x5nNY3tcn+ZmeIkeeiQ+QN27BC327e3\nbdaTInBIo2MUdhA1IFu2wLx5rX4InVbYXfvFyCHWMSNGiEveCy5we4x07E4Vq5oWsLYCxlRHSYuq\nT8+eha5dxcKLK337Ctdn0gwt4FgI+6ZNm9hXV0dea+SyS2Hv0wfbpEnMGzyYF8PDuQXQy8o4/fOf\nizSy9ethzhx47TWwX42VlZVRWVnpMRRj1ZPdk7Ab+8UUFxfTpUsXoqKirF9DWRl88gmfJyRQKsNF\n9qHqluGY774T2UcAn3xivW9Fy8nPb/qcBRIZmnQVdoDJk4XmtDKdVthdHbscYt1t4EDhdG6/3e0x\nsnWvWz/upKSACLvsw+4aiml2kZJrcZIR6UbbIhwjhV0KDvKpT3JE09COHOHrr78O7HPK19WnDy+8\n8AIfrVlDxIsvcjYvj3GRkTxdXy/y6gEefFCc4N57z3Fc4J7qCGJ9prGx0a1qWeKrY/elOInly6Gm\nhi969mxqBDZ0qLg1E3ZdF459zhxxAlDhmNZD10Xa8B13BH7f0rG7rPG1JZ1a2I0xdunYe/XqJRx4\nmPtLsxy2EUDHHhUV5ZRi16IipY4m7AbH3tDQQGFhId0nTiQd+MEttwS2i6HdSW3Pz+fhhx/mmmuu\nYeHChfTo1495N97I//3f/1FRUSG2vfhikQH10ktg7xED5sLura1AQIV90SLIyOBIr15N701iIvTu\nbS7sJ06ItYUxY0RV78aNHb51Q6dl0yaRLtwaKcOHD4v5CfbitPagUwu7WSjGNW/ZSHOGbXz++ec+\nD0ow5rBLWlSkdOaMew67pK2FPTJShIXsFBQUoOs60VlZhAN6Xh73339/4J4zPx9bSgo33XEHvXr1\n4s0333SU+C9cuJCysjIWLVokttU0kW2QkwNfftkxhL2gQDT9uuUWEpOSnD9DVpkxMr4+dqwQ9vp6\n+Pxz6+dQNB/ZjK2gwDSrqkW4ZsS0A51W2GWMXcZKCwoKSEpKcoi3Gf469vLycmbNmsXvf/97n47J\nmOooaTXH3ru3ELS2EnZZdWpHhjuisrIAePzWW3nnnXdYvHhxYJ4zP58Tus6RI0dYtGiRk4hOnjyZ\nMWPG8Ne//rUpVn7LLcIlvfii472Ww06MeJui5EnYY2JiiIuLc8TYPVadLlsmFklvucW9J7tVLvt3\n34n3eNQoUd3bo0ebZFCEHIWFojGX/K5aLWQ3FyXszSclJYW6ujpHrPTUqVMe3Tr479jPnTuHrut8\n+umnPh2T7MNuRGZm+L2AquuehT0yUnzx20LYT592WziVwp44ZgwAt06ezKRJk1iwYIEjO6gl6CdO\nsKe0lHvvvZcLL7zQ6XeapnHfffeRk5PDV199Je6MjYUf/xg+/pjSHTvo2bOn6cKmL47d06KovFL0\nOmRj9WoRYx02zF3YMzNFyMV14XvHDpHt06WLqFicPVs49jbIew4p/vEPcTUkDZunrK7Tp8Wax7Fj\nvu27oQHy8pSwNxfXtgIFBQVehd2jYzcJt5Tbi2L27NnjNMfUjMrKSk6fPu3m2GNjY0lNTfXfsVdW\niqwXK2GHtstlN6k6zbfHwHuMGQNRUYTn5bFo0SJsNhu33norDQ0NLXrKxhMnOKnrjB8/3vT38+fP\nJykpib8a0hxt995Lo6bR96OPmD59uunjfBF2T05c9ovxGIqR7Vovvxw0jaSkJHdhB3fX/t13Ir4u\nmTtXZNa0R+/9YKWxUWRQTZsGV10l7vPk2DdsgBUr4Nlnfdv/iRNC3JWwNw8zYe/du7fHx3h07NXV\nbkUjjsU54LPPPvO4b2MfdlealfLoqThJ0o7CfvLkSSIjI+nWo4e4pD18mIEDB/Laa6/x1Vdf8cQT\nTzT/+RobCT9zhnxgiKxLcCE+Pp677rqLpUuXUlhYSHl5Odf95Cd8YLOxICqKt15+2fRxLRX2lJQU\n8vLyaGhosBb2f/9btOedORMQbQVqa2ubpiiZCXtpqShcGju26b7LLhM9gdojO+bZZ4WzDTbWrBHv\n84IF4nufmurZse/dK27/8Q/feiJZ5bC3MZ1W2F37xfgSivHo2MHNtZcb+nV4C8eY5bBLmlWk1FGE\nXdfF5ahJqmPv3r3FQvHAgY7c3fnz53P33XfzzDPPsG7duuY95+nTaDYbJ7EWdoAFCxZQX1/Pk08+\nydSpU1mxYgXhP/0pcXV1RC1ZYvqYQAj7QbsQWAr7mjUilGK/apCtex2fpwEDRCjNKOzffSdujY49\nNlacHD7+uO07aL70kigA64CdO1vEK68IkzJ3rvh50CDPjn3vXpEOXVMDf/mL9/0rYW8Zxta9FRUV\nVFRUtCzGDm5xdvlFvOCCC1i/fr37CcGAJ2FPT09vPWEvKgrY9CdTSktFaMHEsTsqOzMynNoKvPTS\nS2RlZXHbbbdRKFMl/cEe5imJjaWHywnFyNChQ5kxYwavvvoqx48fZ9WqVdz4pz/BhAmiDYEJERER\ndOnSpUWhGPn5sRT21ath0iRHr38p7I7MmIgI0QrBTNiNjh2EAJ08KSpR24qaGjEM5MgR2Lev7Z63\ntTl2DD79FO65R1Sbg/g7eHPsF1wg4uwvv+x9YMbhw+Lva5KR1ZZ0emEvLi52iEezHbvFsA0Zirnp\nppuoqalhw4YNlvs+evQoMTExpseQlpZGUVGRf0VK8rLPKt0RmlIeW6N6TmJRdZqfn98k7AMHiv4Y\n9kyT+Ph4PvjgA86dO8cdd9yBzd9ZqfarkMh+/dymGLny1FNPMWvWLLZu3cqMGTNEVsl118HOnZaX\nzp76xfji2CWm2509K0RY9szHZTyexDXlcccO0fPGtR/PrFmiJqMtwzHGhUIvIchOxd/+Jq5AfvjD\npvsGDxYFjWa9exobITcXhg0TIw2LiryHpw4fFm0uzKrF25CgEHaZw96iGDtYOvZZs2YRFxfnMRwj\nc9jNhEjmU5/0J2ziq2OH1hX206fFrYljd6QTmnR5HDFiBC+++CJr1qzhueee8+857a8nQcaiPTBp\n0iRWrlzJUFnRCXDJJeLWYtHRql9MY2MjpaWlPgu7qWNft06Ihz2+Dh6E/eBBIR4gHLurWwfx97/g\ngrYVdpkoEBkpHG4wUF8Pb7whTpT9+zfdP2iQSEs1S444ckQI/rBhYtbwpEliwIz8m5nRAVIdoRML\nu3FqvGwn0OIYu4Wwd+/enUsvvZTPPvvMsseIsQ+7K80auHH2rIjTugxUdqItipRMHHt5eTnl5eXO\njh3c2vf+8Ic/ZN68eTz22GPs3LnT56dsPH6cRqDHiBHNO+Zx40TKoMUVlpWwl5SUoOu611CMxFTY\n16wRhVyGbB45a9Utl72+vqnT4549zvF1I3PnilGPbdUUTIrcvHnwr38FR/Xrxx+LYqR773W+f/Bg\ncWsWZ5cLp8OGiSvBX/5SCPeyZdbPo4S95ci2Ar5UnYIPjt3lAyxDMXFxccyaNYujR4+yV/6xXTAr\nTpI0q/pU5rB7CkW0k7DLKw+nGDu4te/VNI3XXnuNmJgYXnzxRZ+fsjw3lwJgkA+O3ZTISLjwQr8d\nu6fiJIlRzLsaKnEB4dTXrBHZLIYBH5aOHUQ4ZvdukSJn5thBFCuBEPe24OhREUr44Q/FcbXlCMbW\nYsMGUeJ/xRXO9w8aJG7N4uxGYQdxgh08WExEMjN4586JcE079oiRdHphl6GY8PBwunkKWwCRkZGE\nh4f75di7dOlCWFgYV9lzXs3CMRUVFZw9e7Z1hN0TSUkiHc5K2IuLLV2rzxQWihivQexkDrtD2JOS\nRGthk77sXbt25bbbbmPx4sVug1GsqDtyxGOqo09ccon4YtpP+q7H1FJhj4uLIyYmxvmXe/aIv4Uh\nvg4mi6fgLOyylYCVY5fCYmEqAs7Ro9CvnwgBJScHR5w9NxeyspxOuIDI9urSxdqxy95TIB770EPw\nzTei14wrnro6tjGdWthlW4GCggJ69uzp1KPFDE3TzHuyx8UJh2KyeJqQkACIcMrIkSNN89nN+rAb\nkUVKfodivAm7lxF5+h//iD5jhqOdbbMoLBTHYfhCSMfuVLJvSHl0ZeHChdTU1PDWW2/59JRafn7L\nhX3aNHFr4tqtHPtZ+7qGL6EY023k2DMLYXdy7N26iZPh/v0ivt6lS5N7dCUlRQjQnj2WxxVQjh4V\nKZkREWKt4LPPRBy6M5Ob29RZ04imiffdyrHLk6rkzjtFQoPZulEwCrumaeGapu3QNG1loPbpDRmK\n8SWHXWI6RcmiJ7t07JJZs2axefNmt6ZgnlIdJX6nPJ45413YwWMu+8kVK9AaG2loSYm/RXGSeGrD\nIIuBAy0nKY0aNYoLLriAV155xacMmZiSEoqiojz3YvHG2LHi0tuDsLsei6lj37AB/v53x4/SsZvG\n11evFq6wXz+nu2NiYoiIiHDvfikzY3bsEJOiPBmTYcPazrEfOdLUR2XWLPEZkJOdOiNVVSLTx0zY\nQYRXXB27rpsLe2ws3H+/WFTOyXH+XQfJYYfAOvYHgTb65AmMoRhfhd3UsYOlsEvHDkLYGxoaWOMy\nkNasD7sraWlp/jt2T6mOEith13WS7B+0ku+/9/15XbFoJ5CUlER8fHzTnRkZwulZZAwsXLiQgwcP\nei9aqq0loaaGuu7dvaY6eiQiQsTZTUJRycnJ6LruVIAGkJOTQ0xMjHNXyOefF19kezqcpbDX1MCX\nXzplw0g0TXPvFwNC2PftE6mZVvF1iRT21i4Yqq4W4Sv5Wb7iCmF8OnN2jHTjVms2gwYJUTZ+dk+d\nEu0cXIUdxOehSxd45hnn+w8fFjriuvbSDgRE2DVNSwNmAW8EYn++IoX91KlTXlMdJaaOHUyHbVRU\nVDg59kmTJtG1a1dHOObgwYPcf//9/Pd//zc9e/b0WEzjV/WpzSYWYXx17GYj8k6cIMEuRmUtuYS3\naADmNnZu4ECR5WFx9XDdddfRo0cPp94upthj4mGBKPCYNk1cgrukg1pVn27ZsoWJEyc6NwA7elSI\n3datAERHRxMfH+8u7Js3C3F3CcNI3PrFgBCaggLRfsAqvi7Jzhafz+YUfPmDzGGXwt69O0yc2Lnj\n7LJewJN/qOC2AAAgAElEQVRjr68XfV4krgunRlJSYOFC+OAD8fmSdJCMGAicY38B+BXQpoG41NRU\n6urq/ArF+OPY686d4+/ffQfvvAOIqsWZM2eycuVKrrvuOoYOHcrrr7/OjTfeyMaNGz06zPT0dN+L\nlEpLhbj7KuxmI/LkghxQfeCA9/1YYRGKcWuJa5HyKImOjuaee+5hxYoVHPPQKa/W/vg4mYbWEizi\n7GbCXl1dzbfffsuUKVOaNtT1ptS/9esdd48cOZIRrqmYq1eLasaLLzY9FEvHLvHFsUPrx9nl6zWu\nF82aJRYMZU1DZ0OKr9WajfysGePsnoQd4Oc/F39vY3OwYBJ2TdOuBk7ruu6x5lnTtB9pmrZN07Rt\nZ3xppuMDRtfUohg7mHZ4HFtYyKCyMnF2tn/gZ8+ezdmzZ9m4cSOPPPIIeXl5/OMf/yDL3pfcCr+K\nlHwpTpJYpDw2fPMNNqASaPS15agrFRUiPumLY7dIeTTyox/9CIDXXnvNcpvT9hNSSnNz2I2MHi3+\nri7hGJmmaBT2bdu20dDQ4CzsRUVNQxgMwv7111/z5JNPOj/XmjUii8QYnjKQmJjoPrBFCntEBAwf\n7vm1tFVmjBR2Y1hx1ixxkuusQz/27xcl/hZ/G8eitTHOvndv07QrM3r2FOmg77wj2vQ2Nor3LliE\nHZgKzNE07SiwBJiuadq7rhvpuv66ruvjdV0f392X2LEPNEfY/XHs00tKqIyKEjHG//ovsNm4+eab\nWbVqFcePH+f3v/+9zyEgvwZuBEDYq7/+mv3AYSDCXsDlNyY57DabjVOnTrkLe79+YvHPg7D379+f\nq6++mjfeeKOp06ELpXZH2nvcuOYds5HwcLjoIjdhN3PsW7ZsAcQQDwfy6iMrS3RstJq0c/KkWEiz\nCMOAhWMfNEi8Z9nZEB3t+bX06QMJCW0j7JGRzoI2dqz4ubPG2XNzrePrIEQ/OtrdscvCJCt++Uvx\n++eeE+G+urrgEXZd1x/RdT1N1/UBwM3Ael3Xb2vxkfmAUdhbHGN3FfbaWi6rqeH7IUPEAtr69fDa\na4SFhTFz5kznhUMfkI7dpwVUeUXTAmGPyMlhB3ASiHUN0/iKyRDr06dP09jY6C7skZFC3D0IO4hF\n1NOnT7PMonqv6tAhaoEMiz7sfjNtmnBihvfdbIrSli1bGDp0qHMthHSv//VfIga7ebP5c3z4obi9\n5hrLwzAV9uhoIZoW4RsnNE2cANpC2Pv1c8731jS48kpxVVJf37rPH2h0XTh2q/g6iJNrRoa7Y7cK\nw0jS08Uw7DfeALsxCBphb0+MaWkBceyVlY4Prm3tWpKA3JEjRTe4yy9vKin2hbo6py9BWloakZGR\nXvu6A/45drMReUVFxJ45ww6gLCGBRENfeb/wUHVqNnbOUy67ZMaMGQwaNMhyEdV24gQFYWEkByqz\nwCTO7urYdV1ny5YtzmEYaBL2224TJy5DOMaJxYuFQHtwhabCDqLQ5fnnfXklQmjaIsZult01a5YI\nVUoB6yycPSsMm7cqZmOXx9JSsajtTdgBHn5YfM9/9SvxczAKu67rG3VdvzqQ+/SE0bH3dO2KZ0Fs\nbKy1YwdHnL3x/fc5B5wZNUoI5xtvCBdz992+FWvMng033eT0vI8//jjvv/8+79gXYy2Rwm4IWVn1\nqDEdkWdvAbsnKgotPZ0Ul5OMz/jSTsCIS/teM8LCwrj33nvZvHkz+03mfkadPcs5P6+GPDJypMhi\nMIRjZMGQFPZDhw5x9uxZc2FPSRHVh5MmmQv7oUPwn//A/PkeD8M0KwZEcVxkpG+vZdgwITgWLYcD\ngpWwz5ghPv8uqb5uvPsuXHutx01sNhsLFy7k27bIjfeWESORfdll/jr4JuyDB4vv+bFjwvm71DC0\nF53asUth79Kli1Naoifi4uKsHTuIL019PeGffsoKIE46x/R0+N//FbnK3hrul5WJCfWffOKUSfDY\nY49x0UUXsXDhQg54ylQ5e1YUQsTFUVFRwYwZM0hOTmbixInccccdPPPMM3z00UdNJyjXXHb7AmRp\nRgZ6376EAXpzOkDKYzeEYtzaCRgZOFCcDLxMfb/lllvQNI0lJsMwEsvLqfE0S9RfwsJEqMMg7OHh\n4SQmJjqEXcbX3YTdWKhz6aWiHa9ru9/33xe3hpO4GYmJidTU1FDXkvmlrb2A6prDbiQhQTRXMyul\nN/Lmm2IAt4eQY15eHq+88krgBp97QmbE+OLYKyvF51f2oPdF2AEefVTc9uvn+0m6lenUwh4bG0tM\nTIzP8XX5GMs8dhDC/uWXhJWU8E9wPmHcdZeINf73f3tuvPXll2KVvLERli513B0eHs67775LZGQk\n8+fPt/6S29sJVFdXM2fOHDZs2MB1111HUlISGzZs4NFHH+Xaa69taodrIuynIiNJHTqUCPuXtLw5\nYlBYKByr4cN68uRJwsLCzK+QvKQ8Svr06cPFF1/M4sWLna5Eqqqq6NHQYJ2J0FymTRNO1NCa1dhW\nYMuWLSQlJTHM9YtsdK/Tpws39+WXztssXiyyYbw4NdO2Av6SnS1uW0vYXXPYXbn4YnF1YtWioqYG\nvv5a/F8OGTch1y62+9piiEdurkhLNLbqNcPY5XHvXrH+4WszrxEjRIbMlVe27FgDSKcWdhBxdl/j\n69C0eOoW2jCGYv75TxpjYlgNTpWnaJpw7dXVnlt3rlsnHHdmpvjiG0hPT+fNN99k+/btPPbYY+aP\nP3sWW2oq8+bNY+PGjbz11lv84x//YO3atRw/fpyysjIyMzPZtm2b2N5F2PUdO9huszFo0CDi7Jeg\npc2pPrXIYe/Vqxfhrs2UwGdhBzFCb//+/XwnJwcBh3NySAKiAx2nNOnPbmwEtmXLFiZPnuzca0jm\nsEuRO/98ETYxhmO+/178u/lmr4cQEGEfMEAITmvF2eXfzZOw19WJDCEzvv66aWCFh1h8mwr7/v0w\neDBniov55JNPrIesG7s87t0rQjdmn3ErXn8dvBXftSGdXtiHDx/OGG9VewZk694a13FyUtiLimD5\ncorPP59qcA/xZGaKfytWWD/JunUize6220Qmhctl6bXXXsuCBQv4n//5H1avXu32cP3MGXadOsWn\nn37KK6+8wm23OScZJSQkMG7cuCZRNI7Iq6qC/fvZ1tjIwIEDSbbnR1cZK+R8xULYTcMw4FMuu+T6\n668nIiLC6XL8pP1EleTrJbCvDB8urjykm6RpitK5c+f4/vvv3cMwZ86IE7h8TVFRokXBF180bbN4\nsfjyz5vn9RACIuzh4eKz1wLHruu6Q1jdMMthNzJ1qjA3rlctko0bRehr3DiPjl2urRw5csQy7TVg\n2Jt/Pffcc8ydO5dRo0axcuVKd2PXv794f6WwB/oz2MZ0emH//PPPeeGFF3ze3uuwjc8+g8JCjtt7\nYDs5dsmcOeJDbPYlPXVKOKpLL21ycjIOa+D5559n+PDh3H777fziF7/g1Vdf5YsvvuDo0aOc3rOH\n3YWFPPfcc/z4xz82fR2jR4/m+PHjohWucZJSTg6azcYOYODAgfTKyqISqDebEOONwkK3IdZOI/Fc\nkR0L//Mfr7tOTU1l5syZLFmyxNGM66y9qVIPP07UPhEWJhyYIZ1NhmK2bt2Kruvm8XVwFrnp08Xf\ntqBAOPolS8Tf2UMrCUlAhB1a3AzstddeIzMzk2+++cb9l2Y57EaSk0XrAyth37ABzjtPhCR27hQF\nbibIE0tjYyOHPA2SbimNjUKoMzPZs2cPvXv3pqGhgdmzZzN9+nS2G+fIRkWJcNru3eJvbxf2qqoq\nnnzySSq9rBt1NDq9sIeFhXlt12vE67CNDz6A6GgO2ytJTYV99myRZWLith2O7rLLRNxuwgS3cIw8\njvfff59+/frxl7/8hQULFnDZZZeRkZFBdHk5gyZO5Be/+IXl65BXKTt37nTOZbcvnEph79O3LyeA\nsOYsnvrr2DVNuNfly81Pei7Mnz+f48ePOxYvK+1f+IC0E3DFpfukFPYtW7YQFhbGxIkTnbc3c6/T\np4vbDRvEyevwYZ/CMGAxRak5ZGeLY/Nnfq6d+vp6nrWXwL/99tvuG5jlsLty8cUiFOPqtKuqxP3T\npgln39hoeYLPzc0l076Y2arhmLw8EToaOpR9+/ZxwQUXsHv3bl5++WW+//57xo8fzzPGRl6DB4ur\nbZtNFKUhjONvf/tbPvnkk9Y7zlag0wu7v1g69i5dhLOrroaZMym1d3ozzbaZPFk4U7NwzLp1YijF\n6NHi5/nzRctTk8vf4cOHs23bNiorKzl+/Djr16/nb3/9K8nARPtgDytG2/dvJuxVsbEcR/SHj4qK\n4nRkJNEyhdJXamqEOBuEvbq6mpKSEvMcdsmdd4r30LBobMXcuXOJjY11hGMa5OKdp/03l4EDxeKg\nPe3TKOyjRo1yP4FLYTcuuo0dKwzA+vXCrUdFeU3tk5gO22gOw4Y1Fd34yZIlS8jLy6Nfv34sWbKE\netcU2KNHvS8YXnyx+Gy4Ov4tW8R7O22aSA3VNNNwTHV1NceOHWP27NlAKwu7/T2qHTCAI0eOkJWV\nRWRkJPfddx8HDx7kwgsv5I03DH0LBw0C2fHT7tjlSEfTK5wOTMgJu6Vjlz3ZAa6/3tHS1dSxR0SI\ngo3PPhOjwyS6Lhz79OlNvbVvvFHs2yS1TxIWFkZaWhrTpk3jHrtQaF4u73v27EnPnj1FnN1F2I8k\nJ9O3b1/HhJ/ShAQS/RUUkyHWHlMdJZMmiWZL//d/Xp+iS5cuzJ49mw8//JCGhgbCCwupiYgQPToC\nzcCBwkXa1zuSk5MpKyvj3//+t3sYBoTIpaaKND9JeLhYiF23ToTXrrqq6TPjhZaGYhwZVM1MebTZ\nbDz77LOMGDGCl156ibNnz7q1n7bMYTdywQXi1jUcs3GjeH/k1KXhw02F/dChQ+i6zrhx4+jbt69p\nLUPAsJupwxER2Gw2p35OSUlJXHHFFRw+fLjpZCuvFDXNkfeeYw8POhIVOgkhJ+yWjh3EBzIiAmbP\ndsw7tcyPnz1bLFgaFuTIzRWtPy+7rOm+vn3FQurixb710vaj6nTMmDHCUcgReXl5sGsXOWFhDDRk\nllSnpNC1psbzdHVX/C1OkmiacO2bNvmcHXPmzBk+/vhjkqqqqPRRKP1Gvh/2cIxsBFZeXm4u7EeO\nmLvX6dOFAJ465bUoyUhLhP1///d/6dGjB4cPHxYnzbAwv4V9xYoV7Nmzh4cffpgrr7yS1NRUFi1a\n1LRBdbX4m3sT9m7dRHrfl1+ybds2UlJSRJuMDRvEAG95Ipw6VXw3XIr5ZHx96NChZGVl+e3YLQv1\nzNi/H7p2ZY/dpLg26pPhTCneDmHPyBBZbTQ59m+//ZZGf74/7UzICbulYwfxoZ41C7p2pby83DH5\nxpSZM8VCkzEcI4dIXHqp87bz5zcNVPCGH8I+evRo9uzZQ119vTiBfPEF1NayubKSQYYxa429ehEJ\n/rVd9bedgJHbbxcCbxbHdeHKK68kKSmJ3/3ud/QFGn2sIPYbF2FPNpxALB27mcjJv218PFzte5F1\nbGws4eHhfgv7F198wS9+8QvOnTvHu+++K9IdBw3yTdhLS2HIEPQVK3jmmWfIyMjgpptuIioqiptu\nuomPPvqoadiInLLlTdhBhGO2bGHLl19SUlLCto0bRTxdtm8AmDJFhPJ273Z6qBT2IUOGOITdV7G2\n2WzMnDnTMqHADXtGzD77VcFQl+pTKew7ZItr+Z2xXxWdO3eOo0ePkpWVRWVlpeUg+45IyAm7R8f+\n0UdgdzGu05PcSEwUl+VGYf/iC/HFcM3Dvv56cSXgIRzjwI8GYGPGjKGurk64nr59HfnN60tLnRx7\nmL14ptafDITmOnYQC3DTpwth9/KljY6O5tprr2Xnzp30ASJbqyS7Tx8RE3cR9l69erlPvtJ1IXRm\nIjdsmLh/3jxxleQjllOUPJCXl8dNN91EVlYWkyZN4r333hMimJ3tWy77ihVw8CDFTz/N1q1b+eUv\nf+kwKrfeeivV1dUsX75cbOst1dHIxRdDZSW19qvVqrVrRUjSKOxTp4pbl3BMbm4uvXv3JiEhgczM\nTMrKyij0cXjIK6+8wtq1a1m7dq1P2zuEfd8+0tLS3K6+e/fuTffu3ZvShgcOFOEke4rwrl27ALjn\nnnuAzhWOCTlh9+jYExIcPZtdpyeZMnu2cOIHDogwx/r1Igzj2uqzWzfRa2PJEu/hGD8dOzgvoNpi\nY8kFJ2GPtQ8Y8GtEnklnx/z8fOLj4x1hBY/ceacQUauOiAbm20MafYB4bz09mkt4uBAtF2GfMmWK\n+4CUwkKxQGgmcpom3Km3thImWPaLMaG6uprrrruO+vp6li9fzt13391U0DVsmPjMeev/Yxft5G++\nIbt7d+666y7HryZPnkxGRoa4CgD/hP2ii8R+7Veg8du2CeMixRyESPbs6VaotH//fodzlqERX8Ix\nx44d4+GHHyYyMpIjR444QqWWVFaK9ZTMTPbv3286L0HTNMaOHdsk7HFxItPNno0mwzA33HADCQkJ\nnWoBNeSE3aNjN+DVsYMQdhDOaPt2UbXqGoaRzJ8vXKAxJm+GFFQfBjkPHTqUmJgYpwXUc/36YQOn\nUEyivRS9wp94ZmGhONHZT4TQlOro0yzS664TmUY+LKJOnz6dId26EQtE+SIszcXQpMwo7G7ItQGr\nDJHu3f1y6xLTYRsm6LruaJL1zjvvMHToUK6//noiIyN57733hLA3NLgPYDZSVQWrVlF+3nmE6zr/\nO3WqYzEdhKjddtttfPHFF2JR3FsOu5GePSEzk0H2K7iBR4+K8XnG5m2aJsIxJo7dX2HXdZ0FCxZg\ns9n44x//CMAeb1cs9k6N+pAh7Nu3z3IQzpgxY9i9e3fT4vSllzqa7+Xk5NC1a1f69evHuHHjlLB3\nZDw6dgM+OfYBA0T3wBUrmvLXZa6zK9dcI0TyXbcZJM58/jmMGiXCBl6IiIhgxIgRTo79uN3pGx17\nj+xsaoE6X1sOg+WsU6/xdUl8PNxwg6gL8PJeR0RE8If77xc/tEaqo8SQyz58+HAeffRRbr/9dvft\n/HGvfuBrKObVV1/lrbfe4je/+Q1z5swBRMO7K6+8ksWLF2OTDa1cYr5PP/00N954Iz//+c/55L77\noLqaP4SHsy8sjOkm6yu33norNptNNGM7ckSkdvpYE9JwwQWMq66mW0QE2dXV2Mx6yk+dKt5v+xzb\n4uJizp496xD2vn37EhcX5zUzZsmSJXz22Wc8/fTTjjTJ3S6xezfs+zyTkkJ5eblHYXeEM13YuXMn\no0aNQtM0JkyYwM6dO1vWxK0NCTlhD6hjB+Ha//Uvkbc9erR1FWJCgnCxixeLy3wz9u8Xl/l33OH9\nee2MGTOG7777Dt3utPbExBAfH49xSlXf9HROgvOwXm/4W5xkxp13irzgjz7yuukNcnJRawt7SQmU\nlBAREcHvf/978wHkZjnsAcAXYc/Pz+fBBx/kqquu4oknnnD63S233MLJkyfZUlws7jAI+5dffsmv\nf/1rNm/ezKuvvkrpW29RBPzPN99w8sILidiyxa21RWZmJhMmTBDhGF9SHQ0UDB1KEvDcgAFEACfN\nuifKqyF7OEZ2NJXCHhYWRmZmpkfHfvbsWR544AEmTJjAAw88QEZGBjExMXzvLaxoX6TdYxdiT8IO\nOPUsArFQu2vXLke4c8KECdTV1Tni7h2dkBN2Xx27X8Le2CiKkKzCMJIf/EBkKnz8sfnv331XOKZb\nbvH+vHZGjx5NUVERhQMGwKhRrEO4dWO4JDExkYKwMKL8zYoxCLuu657bCZhx0UVCLHwIxyArY1tb\n2MF7GubRo+JyPJB94cGpVbAVy5cvp76+nueff96tonr27NnEx8fzzkcfiTbS9nBEY2MjDz74IOnp\n6Rw8eJDK0lJuS0oibM4cVq1bxwWyOZVJa4vbbruNHTt20HDwoF/C/r29tfLN+fnUAt8ZwjwOzjtP\nZPHYwzEyIybTcBLwlvL40EMPUVpayhtvvEF4eDjh4eFkZ2d7F/b9+6FfP/bas30yLdr2Dh06lNjY\nWDdhP3ToEFVVVQ5hH2+f6NVZwjEhJ+yRkZGEh4d7dew+hWJAxBal6zPmr5sxbZr4Qr71lvvvbDYx\nGPeyy/xqWys/eN+eOQM7d/L1qVNO8XUQ8dSS+Hi6+DOgwUXYi4qKqKur80/Yw8LE1cfatR77cwNt\nK+zeQlJWOewtZOTIkRw6dIhjHoaLL1++nKysLFOHGRcXx7XXXsuHH34owjF2Yf/b3/7Gzp07ef75\n54mLi0P78kvCzp2j6913c+mllxKdnS1yzE1aW9x00010CQsjorjYL2HPKS7mEBBTVcVWYJdZvD86\nWrTUsDv23NxcwsPDyTC8t1lZWeTl5Zl+H9etW8fbb7/Nww8/zKhRoxz3jxgxwjfHbs+IiY+Pt/zc\nhoeHM3LkSDdhlwun8nkHDBhAampqp8mMCTlh1zTNejyeAZ8de1iYaAoWHS26/3kiPFyEJ9asce/n\nvnmzWFw1i/l6QH7wvvvuO3Rd5/Dhw07xdUlF1650raryrUiqvl4UXzUnh92Vu+4SGRO//a3n7fLz\nRZsGM+cXKHztPulnWMJXbrIP43jfxDmDOHlu3LiR6667znIf8+fPp6SkhNyePWHHDspWruSxxx7j\n4osv5oYbbhAbLVsmFneNw7UtWlv07NmTW+zZLD/50584//zzueaaa1iwYIGjh48ZBw4cYKv9b/Vt\nYqL1YuaUKSKxoLqa3NxcR5sLSWZmJrqumw6e+dOf/kRaWppbe+sRI0aQn5/vNLPWCdlywR7mycrK\n8rjg7whnGr4bOTk5hIWFMdye+ijj7Mqxd2AsB1rb0XWd8vJyn6cy8cwzIs7uy/Z33tnkzo288464\n9Pex94gkKSmJjIwMdu7cSUFBATU1NabCXt+zJ9G6DjI+6wmZS2+IP/ucw+7KgAHws5+JyTpbt5pv\n09AgsoXsA79bjaQkkW3kSdhtNusc9hYyaNAgJk6caDk5aOXKlTQ2NnKth8/AjBkzSE1N5ZnaWsjI\noPr226kqKeGll14S4mWziVDflVc6ZTRx000iU8XkuR+3t4Xuef75JCcnc+jQId566y0eeeQRy+PI\nzc3lUHo6ACezsqyFfepUYRS2bXPKiJFYZcacOXOGNWvWcOuttzpl8wAOsbVcQD1zRmSo2R27VXxd\nMmbMGEpKSpwGze/cudMRppFMmDCB3bt3ezWFHYGQFHZvjr22tpbGxkbfHDuInPMJE3zbdvBg4ezf\neqvJPdfUiEn311/frLiudByyBaprKAZAs4umzUMYwIFJcZJPfWKsePxxEWK57z7ztgZPPy3c5C9/\n6f++/cXbwO2CAtERsJXSLufPn8+OHTtMM0GWL19Oeno648aNs3x8ZGQkN954I0s/+4x9Dz5Iz9JS\nlo4b1xSq2LpVtDtwdf0eWluk2/8mj7/xBqtXr2bXrl388Ic/ZPv27ZZl9AcOHODo5MmwahW2qVPZ\nu3ev+bb2RXHbV1+ZCvuQIUPQNM3t/fjwww9pbGzkFpP1phEjRgBYh2PstRM1/ftz7Ngxy/i6xK0C\nFeHYZZhTMn78eBobG93CNh2RkBR2b47dYwOwQPCDH4hLRTmJZsUK4TD8DMNIRo8ezYEDBxwr9maO\nPdou9ue8pYmBuHQGR2m1rut88MEHJCUl+TWG0EFCAvzP/4j9vvmm8+82bYLf/U7E4l0GirQKLu17\n3fA2RaiF3HjjjWia5ubaKysrWb16Nddcc43XOoFbbrmFqqoqpj71FB9GRXFVTk5ThsyyZSIffdYs\n9wfOny8+d0Zhys8Xj4mKEkO77YwfP57KykrThc2KigpOnTrF4KwsmDmT7OHDqampIU+2JTDSvTsM\nGULtxo1UVVW5CXtcXBz9+vVze57FixczfPhwRo4c6bbL9PR0EhISzB37mTPCQGRns9/+erw5dpnS\nKAVbthJwFfYJdvPWGcIxISns3hy71wZgLUWWo8tF1HfeEY7WWJLtB2PGjEHXdT7++GM0TaO/SZpe\ngv3DXeZLKfr69eJLbn/MBx98wNq1a3n66aeJbO6w3ptvFo7xkUdE/B7E7a23CrF9+eXm7ddfBg4U\nMXSrhk4y1bEVFk9BrFFccsklbvNeV61aRU1NjccwjGTKlCn069eP4uJiyn/7W7T4ePjxj0UYZvly\nUUshZ/gaueGGptYWpaViCPPgwaJT45NPOuWwexIxGQ8fYq9ozrYXwFmGY4YMocF+Nekq7OCeGZOX\nl8fmzZsdQ89d0TSN4cOHuzt2XYe77xYprYsXs9d+kvYm7PHx8QwdOtQh7LIpmHHBFkQLgr59+yph\n76i0u2NPSBBfsiVLRI/wzz8XAufPjEUD0lmsX7+etLQ0oqOj3bZJHT6cRqDGXpFnia4LYZ8+HTSN\nsrIyfvaznzFu3DgWLFjQrOMDRHz35ZfFlcmvfy2e5557RNhnyRLn9ritycCBIqZvldPfSjnsRubP\nn09ubq7Tpf/y5ctJTU3lQm8L8Ij874ceeoiZM2dyxy9+Ia6G/vUvsZZx6JB7GEaSmioWVN94Q7wP\nzzwjtt23T5xwDWRmZlqW0bvmo8sh4JbCnpZGhL1IyUrY9+/f7zjRLbH3VJrvoXvmiBEj2LVrl3MD\nsb/+FVauhP/3/2DUKPbt24emaY4TkCdkOBOahN3VsYO4kukMmTEhKezeHHurCzuIcExZmRD0hoZm\nh2EA+vfvT1JSEvX19abxdYC+/ftzCrB5Szvct0+Irb2C9je/+Q0FBQW88sor5gOs/WHkSLj/fnj1\nVXH70Ufw7LNiRmZb4S3l8ehRsbZgXHgMMLI9gAzH1NXVsXLlSubMmWPdTdSFBx54gFWrVont77pL\nNOZ66SVxAp071/qBd90lFtAnThTrGu++6960DnHysCqjl/nog+1tbpOTk+nTp4+1sKenE1tRQUps\nrFG5wOkAABj7SURBVOkaTWZmJpWVlY4F+vfee8/Ry8aKESNGUFRUxGlZm/H99/DQQ2LR+Cc/AURf\nGlnQ5I0xY8Zw9OhRSktL2blzJykpKabHOmHCBPbv39/ygSmtTEgKuzfH3uqhGBBfxAEDxELPmDFC\n9JqJpmkOd2EWXwfo0aMHJ4EIb5301q8Xt9OmsWPHDv785z9z7733Oi7NW8yTT4q461//Kr6EP/1p\nYPbrK96E/ciRVouvS1JSUpzmvW7cuJFz5875FIYxRdPgtddEnHzKFLeKYSduuEGk2q5aJSZCecCq\njP7AgQOOdgCS7Oxsj8IOMKV/f9MxlsbMmO+//56cnBzTRVMjTpkx1dVi/SA5WYQ37eGbffv2eV04\nlRhHTebk5Dji7q7I74HTvNQOSEgKe4dw7GFhIvURWuTWJd6EPTw8nKLYWOK8pTtu2AD9+2Pr358F\nCxbQrVs3/vCHP7T4+BwkJwvHPnWq+BL6Ma82IKSliTizJ8feSvF1I/Pnz+fEiRN89dVXLFu2jPj4\neGbMmNH8HWZmCrF+7TXv2/pYi2BVRm+W3ZKdnc3evXvNe6vbM7LGW5xwpLDv37+fxYsXExYWxrx5\n8zwem1NmzK9+JRz7W285UnRtNptlV0czpLBv377dqZWAKzJjqaOHY0JW2NvdsQMsWCAujX/wgxbv\nSn4wrUIxAOVJSSR7andqswlhnz6dv73xBlu3buX55593GkoREK69VlypeBn/1ypERIj4uZmwNzaK\nNY9WduwAc+bMITY2lkWLFvHxxx9z1VVX+RQy8Mi0aY5e4oHAagH1wIEDbnHr7OxsKioqOGGydlFv\nz04Zbragi+iJn5CQwN69e3nvvfe47LLL6Oll4ErPnj1JTU3l+NatYu3mvvvgiiscvz9+/DjV1dU+\nC3uvXr3o2bMny5Yto6qqym3hVJKamsrAgQM7/AJqSAp7XFxc+zt2EJfMf/+7qLhsITNmzGDixIlc\nIGdSmlDXvTvxjY0itm9GTg4UF1M5cSKPPPIIl1xyCbfeemuLj63DYZXyeOqUKKZpA2Hv0qULc+bM\n4c0336SgoKD5YZhWpH///qSmpjqJWHFxMUVFRaaOHcwXUI/Y+8YPtOhYqmkaWVlZfPjhhxw9etRr\nGEY+ZsSIEXSTKcP33uv0e5kX76uwgzBHX9n72lg5dqBTVKCGpLB7c+xtJuwBJD09na1bt5LmoXpT\nl4tBVhkhGzYAkJOaSklJCQ8//LBvvdc7G1bC3so57K7Mnz+fhoYGIiMjueqqq9rkOf3BrIzeNdVR\n4knY9x8/ThHQ18PM0KysLE6fPu2YqOULw4cPZ9SxY+gDBrhdqcj0SV9j7NB01WtsJWDG2LFjycvL\n69ALqCEp7HLx1GrWYkVFBREREU49LYKBCHvsuNKlX4iD9eth6FDO2kMCqT4M++iUDBwoJlW5XrnI\n96WNhP2KK64gOTmZyy67jCSLMEV7I8voKysrAfdUR0lqaio9evQwFfbc3FyOg+hVZIEU4NmzZ/s2\noQsYM3QolzQ0UHHJJW5Ty/bt20dycrJ5W2ar/dmFPTMz02NYTL72g95Sh9uRkBR22f+hxqIvumwA\nFmxuVY6dM60+bWgQhSrTprXdGkN7Yda+12aDP/9ZFOzIafWtTHR0NBs2bOA1XxY824kJEyZgs9kc\nOfe5ubmEhYWZLtJbZcbk5uZyOjKSKHsuuxnSIfsShpFMqa4mFthnkqcuF079+Q6PtWcJeQrDQFOa\npxL2DoZM07KKs/vcsreTkWLPJKgyc+zffiuGYkyf3ilDUX5hlvL40Uewcyf85jfNLhRrDmPGjCHd\nng7YEXFdQD1w4AADBgwwvZqVwm68ErbZbGzfvp3KlBSPg16uvvpqPv74Y+Z6ysF3YdDevZQB/zLJ\nrPKl+ZcrgwcPZsyYMV7DYjJBQQl7B0M6dqs4u88tezsZfTIyKAQazXp6yPz1Sy4JPWG32UR+/dCh\nIh9a4aBXr16kpaU5hD03N9eyknPYsGGUlpZSYHDmv/3tb9m+fTt9Jk4ULSQszFRERARz5swxzXM3\nxWYjZu1avoyJIcelz0xZWRn5+fl+C3t4eDg7duwwH5doIC4ujr59+5q2Gu4ohKSwexuPF7TC3qcP\nJ4GYvDz3vuzr18OIEdCjhyMUEx/gCUIdhuRk6Nq1SdiXLYNdu4Rb97HyM5SQZfSyb7pZWwBwX0Bd\nunQpTz31FHfddRcTr79ebOQ6h6C5fPstnDrF3kGD3HrGrFq1CvBv4dRfBg8e7Ldj13Wdf/7zn5Yd\nMwNJSAq7t/F4wRqKiY2NZX1sLP2PHoUHHhBOFUSb2s2bHW0EysvLiYuLa3kLgY6MzIyx2cQQkKws\n0ahM4caECRM4cOAAubm5lJeXWzp2o7B/99133HnnnUyePJlXXnkFrV8/sZG3lha+smIFhIVRfuGF\n7NmzB5vNhs1m46mnnuLmm28mOzubac1squcLQ4YM8UvYz549y9y5c7nhhhv48MMPW+24JC0Wdk3T\n0jVN26Bp2h5N03ZrmvZgIA6sNQlVxw7wzuDBLBs0SBR13HGHyNveulWUZRuEPVhfvwMp7EuXiqrF\nJ55o09h6Z0LG2WVvGyth79mzJ127dmXjxo3MnTuXlJQUli1bJprSyTTcQAn7J5/A5MkMGD+e6upq\nvvnmG8cA8FtvvZX//Oc/rZppNHjwYAoLC70OJwfRnG/06NGsXr2aF154wTFJqzUJhGNvAB7SdT0b\nmATcp2ladgD222p4c+x+TU/qZPTq3Zs/pqTAH/4AixaJKtDPPhPpYhddBATvFYsTcuDGk09CdrZo\npawwRQ5yXrRoEWDeoRFE3nt2djbLli3j9OnTfPTRR/SSPd6lsHtYQPWZ48dFT/k5cxytBS655BJH\nhtHbb7/d6mFEmRlzyGzWq536+noeffRRLrvsMhISEti6dSsPPvhgm2TbtTigqOv6KeCU/f/lmqbt\nBfoCPjT+bh+8OfaKioqgdayJiYliBNgjj4g488KF8OmnosNi165ACDn2+noxoOL995Vb90DXrl0d\nMeXIyEj6ybCKCcOHD+err77i73//u/MkqNhY0TY4EI595UpxO3s22fY21b1792bp0qWcd955Ld+/\nD8irloMHDzrSJI3ous5VV13FunXruOeee3jhhRfadM0qoCtFmqYNAMYCbsMtNU37EfAjwOMHoy3w\nxbEHq7AlJiY2XT7ee69YSLz9dqeJO8H8+h3IRl8jRoiOhwqPTJgwgYMHDzJo0CCPrYUfffRRrr76\nambPnu3+y/T0wDj2FStg0CDIyiLBPvmoT58+Phc2BQKZ8miVGZOfn8+6dev49a9/zVNPPdVmxyUJ\n2OKppmldgH8CP9V13S3wpOv667quj9d1fXz37t0D9bTNwpNjr6+vp7a2NmhDEU7CDmLB8NgxMZfU\nTkgI++jRYlbts8+2fYfJTogMx3gbWtG/f39zUQcRjmmpY6+sFBlcs2c7qk2zsrLaVNRBZIz17t3b\ncgFVdn+8wtCYrC0JiGPXNC0SIeqLdF1fFoh9tiaeHLtM9QtWYUtMTKSiogKbzdaUM+wyx7SiosJj\nl8igoEcPOH3arRRdYY5cQLWKr/tEejrYm2w1m7VrobZWCHs74ykzZvv27YSFhTnaFLQ1gciK0YA3\ngb26rv+p5YfU+nhy7LI4J5gdu67rjt4fZoSEYwcl6n4wbtw4RowYwaWXXtr8naSni3mkHj57Xlmx\nQsxz9WGEYGszePBgy1DM9u3bGTZsmNMwkrYkENegU4Hbgemapn1n/9fxWtUZCGXHLl+XpzStkBF2\nhc/ExcWxa9currzyyubvpKWZMboOa9bAjBnQ3KHqAWTw4MEUFBQ4NEOi6zrbt293XjxuY1os7Lqu\nb9Z1XdN1fZSu62Ps/z4LxMG1FpGRkYSHh3t07MEqbDIWaSXsuq6HRrqjou2RPXGaK+x5eeKxl1wS\nsENqCcbMGCP5+fkUFhZ2bmHvjGiaZjkeL9g7G3oT9qqqKnRdD9oTm6IdaWmR0qZN4tZeb9HeWHV5\nlPNQlbC3A1YDrUPdsQf761e0Iy0NxWzaJGotAjj+ryVYdXls74VTCGFht3LsobB4CtbCHuxXLIp2\nJCYGundvmWO/8MIOk56akJBAr1693BZQt23bxrBhw9q1iV7HeIfaASvHHuyLp8qxK9qV5uaynzoF\nBw50mDCMxLXLY0dYOIUQFnZvjj1YhU0Ju6JdaW71aQeLr0tchb0jLJxCCAu7J8euaVq75Z+2NlKw\npYC7ooRd0ao017Fv2gTx8WDSl6U9GTJkCPn5+Y66kI6wcAohLOyeHHuXLl2Cbt6pJDIyktjYWBVj\nV7QP6elQWgouud9e2bQJpk7tcINQXLs8doSFUwhhYfeUFRPsopaQkKBCMYr2oTm57EVFomf+xRe3\nzjG1ANeUx+3bt5OVldXu08dCVtg95bEHu6i5NQIzoIRd0ao0J+Vx82Zx28Hi69Ak7DIzpiMsnEKI\nC7uVYw92UfMk7CoUo2hVpGP3J86+aRNER4O9EVlHIjExkR49enDw4EHy8/MpKChQwt6exMXFmTbC\nCoVQjDfHHhMT47HntkLRbPr2Fbf+CPuXX8KkSULcOyAyM0a26pUtjtuTkBX2jIwMSkpKKCwsdLpf\nhWKC/4pF0Y5ER4uWyb6GYsrKYMeODhmGkQwZMoQDBw50mIVTCGFhnzJlCgBbtmxxuj/UHbtqAKZo\ndfxJedyyBWy2Di3sgwcP5uTJk2zevLlDLJxCCAv7eeedR1RUlJuwh4pj95THHuyvX9HO+FOktGmT\nSHGcPLl1j6kFyAXUjRs3doj4OoSwsEdHRzN+/HhTxx7swqZCMYp2JT3dd8e+aZMYtN4BXLAVsn2v\nzWZTwt4RmDJlCtu2baO2thYQf5jKysqgD0UkJCRQV1fneN1GlLArWp20NDh3DiyuGh1UV8N//tMh\n89eNGMdIKmHvAEyZMoW6ujq+/fZbAEeWTLALm6d+MSrGrmh1XIuUbDZYtQoWLoQnn4S33xa56ytX\nQn19h46vAyQnJ9OtWzc0TesQC6cQoGHWnRXjAurkyZODvmWvxCjs3bt3d/qdcuyKVkcWKe3bBxs3\nwosvwv79ItxSVSVG4Ek0TbQS6OBkZmZSUlLSYbQjpIW9Z8+eDBo0iC1btvDQQw8FfcteiSfHroRd\n0epIx3799ULEx4+Hd9+FefPEz8eOweHD4l9qKiQnt+/x+sCf//xn6uvr2/swHIS0sINw7WvXrkXX\n9ZApp7cSdjXvVNEmpKWJLJe0NPjpT8X/jU33hgwR/zoRYztY10kl7FOm8M4773D06NGQDMUYqamp\nobGxMehPbIp2JjJS5KcrWo2QXjwF5zh7qIViXHPZQ+WKRaEIdkJe2IcPH05CQgJbtmwJeceuGoAp\nFMFByAt7eHg4kyZNCknH7irsyrErFMFByAs7iHBMTk4Op06dAoJf2GJjYwkLC1PCrlAEKUrYEcJu\ns9lYt24dQIdo4tOaaJpm2lZACbtCERwoYQfOP/98NE3j66+/JjY2NiR6kZsJu4qxKxTBgRJ2ICkp\niREjRtDQ0BAyblU5doUieFHCbkemPYaKW1XCrlAEL0rY7Uy196MIFVFToRiFInhRwm4nFB27WYFS\nVFQUUVFR7XRUCoUiEChhtzNw4EB69OgR0o5dNQBTKIKD4E//8BFN03j55ZdJSUlp70NpE6xCMaFy\nxaJQBDNK2A3MmzevvQ+hzUhISKCiooLGxkbCw8MB5dgVimAhIKEYTdOu0DRtv6ZpBzVNezgQ+1S0\nLrKtgFwwBSXsCkWw0GJh1zQtHPgLcCWQDczXNC27pftVtC5m/WKUsCsUwUEgHPtE4KCu64d1Xa8D\nlgBzA7BfRStiJuwqxq5QBAeBEPa+wHHDzyfs9yk6MMqxKxTBS5ulO2qa9iNN07ZpmrbtzJkzbfW0\nCgvMhm0oYVcogoNACPtJIN3wc5r9Pid0XX9d1/Xxuq6P7969ewCeVtESVChGoQheAiHs3wBDNE3L\n0DQtCrgZ+CQA+1W0Iq7CXltbS319vXLsCkUQ0OI8dl3XGzRNux9YDYQDf9d1fXeLj0zRqkgBl8Ku\nGoApFMFDQAqUdF3/DPgsEPtStA2uwq4agCkUwYPqFROiREREEBcXpxy7QhGEKGEPYYz9YpSwKxTB\ngxL2EEYJu0IRnChhD2GMwq5i7ApF8KCEPYQxDttQjl2hCB6UsIcwKhSjUAQnSthDGBWKUSiCEyXs\nIUxCQoKTY4+IiCA6Orqdj0qhULQUJewhjHTsuq47GoBpmtbeh6VQKFqIEvYQJjExkYaGBmpqalQD\nMIUiiFDCHsIYG4Gplr0KRfCghD2EUcKuUAQnSthDGOOwDSXsCkXwoIQ9hDE6dhVjVyiCByXsIYwK\nxSgUwYkS9hBGCbtCEZwoYQ9hjMM2VChGoQgelLCHMNKxFxUVUVtbqxy7QhEkKGEPYWJiYoiIiODk\nyZOAagCmUAQLSthDGE3TSExMJD8/H1ANwBSKYEEJe4hjFHbl2BWK4EAJe4iTmJioQjEKRZChhD3E\nSUxMpLCwEFDCrlAEC0rYQ5zExER0XQdUjF2hCBaUsIc4RpeuHLtCERwoYQ9xZC47KGFXKIIFJewh\njlHYVShGoQgOlLCHOFLYw8LCiI2NbeejUSgUgUAJe4gjhV3NO1Uoggcl7CGOFHYVhlEoggcl7CGO\n0bErFIrgQAl7iKOEXaEIPpSwhzhK2BWK4EMJe4gjBV3F2BWK4EEJe4ijHLtCEXy0SNg1TXtO07R9\nmqblaJq2XNO05EAdmKJtUMKuUAQfLXXsa4ERuq6PAnKBR1p+SIq2RIZgVChGoQgeIlryYF3X1xh+\n/DdwQ8sOR9HWhIeH8/zzz3PZZZe196EoFIoAocmWrS3ekaatAN7Xdf1di9//CPgRQL9+/cbl5eUF\n5HkVCoUiVNA0bbuu6+O9befVsWuatg7oZfKrx3Rd/9i+zWNAA7DIaj+6rr8OvA4wfvz4wJxNFAqF\nQuGGV2HXdd3jNbqmaT8ArgYu1QNl/xUKhULRbFoUY9c07QrgV8DFuq5X/f/27ifEqjKM4/j3h2V/\nTFJTRBppjCRxkaOLUpIoo5gkWrUoWrhw6cKgjRIELdtUQlFE/zZRkf2TWVRmrjXNsUaHSSNBRRuD\nJGgRWU+L805chmbm6lw85zn8PnC4533vhfndmXeeufPcc+7pTSQzM5uN2R4V8wowH9graVjS6z3I\nZGZmszDbo2Lu6FUQMzPrDZ95ambWMi7sZmYt48JuZtYyPTtB6bK+qHQBuNIzlBYDv/YwztWWOX/m\n7JA7f+bs4Py9cltELJnpQbUU9tmQdKibM6+aKnP+zNkhd/7M2cH5rza3YszMWsaF3cysZTIW9jfq\nDjBLmfNnzg6582fODs5/VaXrsZuZ2fQyvmI3M7NppCrskgYljUk6KWlH3XlmIultSeOSRjrmFkna\nK+lEuV1YZ8apSFouab+k45KOSdpe5hufX9L1kg5KOlqyP1/mV0g6UNbPh5Lm1p11OpLmSDoiaaiM\nU+SXdErSD+Xzow6VucavmwmSFkjaXS77OSppQ6b8kKiwS5oDvAo8AqwGnpS0ut5UM3oXGJw0twPY\nFxErgX1l3ESXgGciYjWwHthWvt8Z8v8JbIqINcAAMChpPfAC8FL5jKPfgK01ZuzGdmC0Y5wp/wMR\nMdBxiGCGdTNhF/BFRKwC1lD9DDLlh4hIsQEbgC87xjuBnXXn6iJ3PzDSMR4DlpX9ZcBY3Rm7fB6f\nAw9lyw/cCHwH3EN1gsk1/7eemrYBfVQFZBMwBChLfuAUsHjSXIp1A9wM/Ex5/zFb/oktzSt24Fbg\ndMf4TJnLZmlEnCv754GldYbphqR+YC1wgCT5SxtjGBinuuj6T8DFiLhUHtL09fMy1bUO/injW8iT\nP4CvJB0ul8SEJOsGWAFcAN4pbbA3Jc0jT34gUSumjaL689/ow5Ik3QR8DDwdEb933tfk/BHxd0QM\nUL3yvRtYVXOkrkl6FBiPiMN1Z7lCGyNiHVXbdJuk+zrvbPK6ofoo83XAaxGxFviDSW2XhucHchX2\ns8DyjnFfmcvmF0nLAMrteM15piTpWqqi/l5EfFKm0+QHiIiLwH6q1sUCSRPXIGjy+rkXeEzSKeAD\nqnbMLpLkj4iz5XYc+JTqD2uWdXMGOBMRB8p4N1Whz5IfyFXYvwVWliMD5gJPAHtqznQl9gBbyv4W\nqt5140gS8BYwGhEvdtzV+PySlkhaUPZvoHpvYJSqwD9eHtbI7AARsTMi+iKin2qdfxMRT5Egv6R5\nkuZP7AMPAyMkWDcAEXEeOC3pzjL1IHCcJPn/U3eT/zLf2NgM/EjVL3227jxd5H0fOAf8RfVKYCtV\nr3QfcAL4GlhUd84psm+k+nfze2C4bJsz5AfuAo6U7CPAc2X+duAgcBL4CLiu7qxdPJf7gaEs+UvG\no2U7NvF7mmHddDyHAeBQWT+fAQsz5Y8In3lqZtY2mVoxZmbWBRd2M7OWcWE3M2sZF3Yzs5ZxYTcz\naxkXdjOzlnFhNzNrGRd2M7OW+RexEAlqaTwMQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc1f7e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcfea668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 2.78737279345 \n",
      "Fixed scheme MAE:  1.98418606264\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 1.7796  Test loss = 3.7965  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 1.8399  Test loss = 2.6616  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 1.8327  Test loss = 0.2101  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 1.8285  Test loss = 0.6558  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 1.6086  Test loss = 0.3786  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 1.4948  Test loss = 1.0855  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 1.4535  Test loss = 1.5330  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 1.4631  Test loss = 2.3427  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 1.4221  Test loss = 2.5522  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 1.4523  Test loss = 0.6479  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 1.4292  Test loss = 0.7559  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 1.4304  Test loss = 0.0120  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 1.3706  Test loss = 1.0576  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 1.3767  Test loss = 1.0446  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 1.3765  Test loss = 2.5129  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 1.4092  Test loss = 4.4739  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 1.4418  Test loss = 2.6246  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 1.4780  Test loss = 0.1262  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 1.4767  Test loss = 0.1972  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 1.3722  Test loss = 0.9894  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 1.3213  Test loss = 1.8103  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 1.3319  Test loss = 3.3918  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 1.3942  Test loss = 0.3424  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 1.3910  Test loss = 1.0683  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 1.3634  Test loss = 0.2020  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 1.3626  Test loss = 0.4894  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 1.3633  Test loss = 0.6145  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 1.3553  Test loss = 1.4247  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 1.3099  Test loss = 0.4905  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 1.3054  Test loss = 0.1134  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 1.2988  Test loss = 3.6735  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 1.3263  Test loss = 1.2803  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 1.3048  Test loss = 0.1737  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 1.3022  Test loss = 1.1120  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 1.2596  Test loss = 1.3989  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 1.2709  Test loss = 4.3830  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 1.2911  Test loss = 1.4420  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 1.2417  Test loss = 1.3474  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 1.2525  Test loss = 0.4367  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 1.2492  Test loss = 2.4580  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 1.2600  Test loss = 1.9871  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 1.2835  Test loss = 2.1262  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 1.3103  Test loss = 3.8877  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 1.3948  Test loss = 11.9001  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 2.0211  Test loss = 5.3105  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.1256  Test loss = 0.6122  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.1270  Test loss = 0.1705  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.1268  Test loss = 0.1053  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 2.0142  Test loss = 2.2716  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 2.0305  Test loss = 3.2568  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 2.0682  Test loss = 1.2952  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 2.0731  Test loss = 1.2056  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 2.0307  Test loss = 0.7558  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 2.0323  Test loss = 1.6954  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 2.0432  Test loss = 0.1164  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 2.0425  Test loss = 0.1584  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 2.0127  Test loss = 0.7447  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 2.0140  Test loss = 0.6641  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 2.0153  Test loss = 1.0330  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 2.0186  Test loss = 0.3183  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 2.0003  Test loss = 1.4094  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 2.0065  Test loss = 2.8297  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 2.0365  Test loss = 0.4830  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 2.0339  Test loss = 0.8583  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 2.0185  Test loss = 0.0200  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 2.0181  Test loss = 0.1968  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 2.0066  Test loss = 0.2176  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 2.0019  Test loss = 3.0072  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 2.0230  Test loss = 3.9174  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 2.0771  Test loss = 0.4495  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 2.0709  Test loss = 0.8810  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 2.0731  Test loss = 2.4903  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 2.0702  Test loss = 1.9567  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 2.0842  Test loss = 0.3802  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 2.0791  Test loss = 0.1503  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 2.0755  Test loss = 1.4537  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 2.0449  Test loss = 1.4834  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZERiD4IgKBmR2IMgCEpGJPYgCIKS8QebA2pdJuGRHAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc843400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.31051006087 \n",
      "Updating scheme MAE:  1.54688748202\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_test_fixed.csv\", yhat_test_fixed)\n",
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_update.csv\", yhat_update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
