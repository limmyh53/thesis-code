{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"1Q/32_units/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 500\n",
    "learning_rate = 1e-3\n",
    "batch_size = 5\n",
    "early_stop_iters = 10\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 32 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag1',\n",
    "                                       'inflation.lag2',\n",
    "                                       'inflation.lag3',\n",
    "                                       'inflation.lag4']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag1',\n",
    "                                   'unemp.lag2',\n",
    "                                   'unemp.lag3',\n",
    "                                   'unemp.lag4']])\n",
    "train_4lag_oil = np.array(train[['oil.lag1',\n",
    "                                 'oil.lag2',\n",
    "                                 'oil.lag3',\n",
    "                                 'oil.lag4']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag1',\n",
    "                                     'inflation.lag2',\n",
    "                                     'inflation.lag3',\n",
    "                                     'inflation.lag4']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag1',\n",
    "                                 'unemp.lag2',\n",
    "                                 'unemp.lag3',\n",
    "                                 'unemp.lag4']])\n",
    "test_4lag_oil = np.array(test[['oil.lag1',\n",
    "                               'oil.lag2',\n",
    "                               'oil.lag3',\n",
    "                               'oil.lag4']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 32 \n",
      "Learning rate = 0.001 \n",
      "Epochs = 500 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 10 \n",
      "Learning rate = 0.001\n",
      "Fold: 1  Epoch: 1  Training loss = 3.3506  Validation loss = 3.6283  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 3.1102  Validation loss = 3.1739  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 3.0019  Validation loss = 2.9507  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 2.9297  Validation loss = 2.7780  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 2.8197  Validation loss = 2.4623  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 2.7783  Validation loss = 2.3253  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 2.7396  Validation loss = 2.1857  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 2.7249  Validation loss = 2.1196  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 2.7012  Validation loss = 1.9943  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 2.6825  Validation loss = 1.8735  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 2.6731  Validation loss = 1.7964  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 2.6627  Validation loss = 1.6455  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 2.6596  Validation loss = 1.6553  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 2.6515  Validation loss = 1.5860  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 2.6491  Validation loss = 1.5393  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 2.6447  Validation loss = 1.4802  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 2.6401  Validation loss = 1.5144  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 2.6356  Validation loss = 1.5003  \n",
      "\n",
      "Fold: 1  Epoch: 19  Training loss = 2.6340  Validation loss = 1.4695  \n",
      "\n",
      "Fold: 1  Epoch: 20  Training loss = 2.6310  Validation loss = 1.5186  \n",
      "\n",
      "Fold: 1  Epoch: 21  Training loss = 2.6280  Validation loss = 1.5327  \n",
      "\n",
      "Fold: 1  Epoch: 22  Training loss = 2.6223  Validation loss = 1.5816  \n",
      "\n",
      "Fold: 1  Epoch: 23  Training loss = 2.6260  Validation loss = 1.1983  \n",
      "\n",
      "Fold: 1  Epoch: 24  Training loss = 2.5964  Validation loss = 1.3306  \n",
      "\n",
      "Fold: 1  Epoch: 25  Training loss = 2.5899  Validation loss = 1.4127  \n",
      "\n",
      "Fold: 1  Epoch: 26  Training loss = 2.5825  Validation loss = 1.4008  \n",
      "\n",
      "Fold: 1  Epoch: 27  Training loss = 2.5790  Validation loss = 1.3024  \n",
      "\n",
      "Fold: 1  Epoch: 28  Training loss = 2.5712  Validation loss = 1.2795  \n",
      "\n",
      "Fold: 1  Epoch: 29  Training loss = 2.5565  Validation loss = 1.2981  \n",
      "\n",
      "Fold: 1  Epoch: 30  Training loss = 2.5459  Validation loss = 1.2760  \n",
      "\n",
      "Fold: 1  Epoch: 31  Training loss = 2.5480  Validation loss = 1.4138  \n",
      "\n",
      "Fold: 1  Epoch: 32  Training loss = 2.5626  Validation loss = 1.5575  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 23  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.4015  Validation loss = 1.9188  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.4068  Validation loss = 1.8546  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.3833  Validation loss = 1.9124  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.3777  Validation loss = 1.9003  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.3689  Validation loss = 1.8748  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.3603  Validation loss = 1.9354  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.3482  Validation loss = 1.9254  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.3383  Validation loss = 1.9350  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.3354  Validation loss = 1.9608  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.3482  Validation loss = 2.0771  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.3350  Validation loss = 1.9870  \n",
      "\n",
      "Fold: 2  Epoch: 12  Training loss = 2.3404  Validation loss = 2.0156  \n",
      "\n",
      "Fold: 2  Epoch: 13  Training loss = 2.3261  Validation loss = 2.0206  \n",
      "\n",
      "Fold: 2  Epoch: 14  Training loss = 2.3087  Validation loss = 2.0244  \n",
      "\n",
      "Fold: 2  Epoch: 15  Training loss = 2.3029  Validation loss = 1.9851  \n",
      "\n",
      "Fold: 2  Epoch: 16  Training loss = 2.2909  Validation loss = 1.9445  \n",
      "\n",
      "Fold: 2  Epoch: 17  Training loss = 2.2858  Validation loss = 1.9686  \n",
      "\n",
      "Fold: 2  Epoch: 18  Training loss = 2.2829  Validation loss = 1.8824  \n",
      "\n",
      "Fold: 2  Epoch: 19  Training loss = 2.2746  Validation loss = 1.9640  \n",
      "\n",
      "Fold: 2  Epoch: 20  Training loss = 2.2658  Validation loss = 1.9287  \n",
      "\n",
      "Fold: 2  Epoch: 21  Training loss = 2.2589  Validation loss = 1.9433  \n",
      "\n",
      "Fold: 2  Epoch: 22  Training loss = 2.2520  Validation loss = 1.9771  \n",
      "\n",
      "Fold: 2  Epoch: 23  Training loss = 2.2432  Validation loss = 1.9277  \n",
      "\n",
      "Fold: 2  Epoch: 24  Training loss = 2.2379  Validation loss = 1.9057  \n",
      "\n",
      "Fold: 2  Epoch: 25  Training loss = 2.2311  Validation loss = 1.9340  \n",
      "\n",
      "Fold: 2  Epoch: 26  Training loss = 2.2276  Validation loss = 1.9258  \n",
      "\n",
      "Fold: 2  Epoch: 27  Training loss = 2.2227  Validation loss = 1.9114  \n",
      "\n",
      "Fold: 2  Epoch: 28  Training loss = 2.2156  Validation loss = 1.8857  \n",
      "\n",
      "Fold: 2  Epoch: 29  Training loss = 2.1964  Validation loss = 1.9383  \n",
      "\n",
      "Fold: 2  Epoch: 30  Training loss = 2.1959  Validation loss = 1.9276  \n",
      "\n",
      "Fold: 2  Epoch: 31  Training loss = 2.2004  Validation loss = 2.1009  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 2  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.3112  Validation loss = 1.8248  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.3019  Validation loss = 1.7491  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.2904  Validation loss = 1.8222  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.2921  Validation loss = 1.8680  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.2922  Validation loss = 1.7506  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.2884  Validation loss = 1.7409  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.2845  Validation loss = 1.7674  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.2831  Validation loss = 1.7570  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.2813  Validation loss = 1.7892  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.2960  Validation loss = 1.6900  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.2756  Validation loss = 1.7521  \n",
      "\n",
      "Fold: 3  Epoch: 12  Training loss = 1.2830  Validation loss = 1.7965  \n",
      "\n",
      "Fold: 3  Epoch: 13  Training loss = 1.2713  Validation loss = 1.7559  \n",
      "\n",
      "Fold: 3  Epoch: 14  Training loss = 1.2936  Validation loss = 1.8126  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 10  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.2627  Validation loss = 2.9453  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.2566  Validation loss = 2.8764  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.2448  Validation loss = 2.8694  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.2403  Validation loss = 2.8173  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.2460  Validation loss = 2.7995  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.2381  Validation loss = 2.7717  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.2257  Validation loss = 2.7444  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.2285  Validation loss = 2.7572  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.2258  Validation loss = 2.7312  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 1.2374  Validation loss = 2.7098  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.2339  Validation loss = 2.7378  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 1.2183  Validation loss = 2.7353  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.2219  Validation loss = 2.7253  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 1.2139  Validation loss = 2.7259  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.2305  Validation loss = 2.7660  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 1.2197  Validation loss = 2.7573  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 1.2070  Validation loss = 2.6829  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 1.2057  Validation loss = 2.6794  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 1.2082  Validation loss = 2.6785  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 1.2097  Validation loss = 2.6605  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 1.2159  Validation loss = 2.7105  \n",
      "\n",
      "Fold: 4  Epoch: 22  Training loss = 1.2158  Validation loss = 2.6318  \n",
      "\n",
      "Fold: 4  Epoch: 23  Training loss = 1.1982  Validation loss = 2.6671  \n",
      "\n",
      "Fold: 4  Epoch: 24  Training loss = 1.1914  Validation loss = 2.6382  \n",
      "\n",
      "Fold: 4  Epoch: 25  Training loss = 1.1944  Validation loss = 2.6025  \n",
      "\n",
      "Fold: 4  Epoch: 26  Training loss = 1.1940  Validation loss = 2.5905  \n",
      "\n",
      "Fold: 4  Epoch: 27  Training loss = 1.1917  Validation loss = 2.6153  \n",
      "\n",
      "Fold: 4  Epoch: 28  Training loss = 1.1871  Validation loss = 2.6062  \n",
      "\n",
      "Fold: 4  Epoch: 29  Training loss = 1.1938  Validation loss = 2.6276  \n",
      "\n",
      "Fold: 4  Epoch: 30  Training loss = 1.1955  Validation loss = 2.6562  \n",
      "\n",
      "Fold: 4  Epoch: 31  Training loss = 1.1893  Validation loss = 2.6322  \n",
      "\n",
      "Fold: 4  Epoch: 32  Training loss = 1.1863  Validation loss = 2.6043  \n",
      "\n",
      "Fold: 4  Epoch: 33  Training loss = 1.2017  Validation loss = 2.5478  \n",
      "\n",
      "Fold: 4  Epoch: 34  Training loss = 1.1817  Validation loss = 2.5917  \n",
      "\n",
      "Fold: 4  Epoch: 35  Training loss = 1.1789  Validation loss = 2.5957  \n",
      "\n",
      "Fold: 4  Epoch: 36  Training loss = 1.1822  Validation loss = 2.6022  \n",
      "\n",
      "Fold: 4  Epoch: 37  Training loss = 1.1785  Validation loss = 2.6081  \n",
      "\n",
      "Fold: 4  Epoch: 38  Training loss = 1.2060  Validation loss = 2.6593  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 33  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.2283  Validation loss = 2.4451  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.2161  Validation loss = 2.4040  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.2212  Validation loss = 2.3664  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.2083  Validation loss = 2.3193  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.2091  Validation loss = 2.2658  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 1.2034  Validation loss = 2.2382  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 1.1940  Validation loss = 2.2407  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.2083  Validation loss = 2.1844  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 1.1911  Validation loss = 2.1928  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.1858  Validation loss = 2.1672  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 1.1783  Validation loss = 2.0908  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 1.1761  Validation loss = 2.0892  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 1.1975  Validation loss = 2.0730  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.1804  Validation loss = 2.0311  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 1.2050  Validation loss = 1.9698  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 1.1865  Validation loss = 2.0154  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 1.1797  Validation loss = 2.0158  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 1.1885  Validation loss = 2.0123  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 1.2080  Validation loss = 2.0420  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 1.2175  Validation loss = 2.0232  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 1.1889  Validation loss = 1.9367  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 1.1580  Validation loss = 1.9911  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 1.1851  Validation loss = 1.9002  \n",
      "\n",
      "Fold: 5  Epoch: 24  Training loss = 1.1588  Validation loss = 1.9063  \n",
      "\n",
      "Fold: 5  Epoch: 25  Training loss = 1.1844  Validation loss = 1.8839  \n",
      "\n",
      "Fold: 5  Epoch: 26  Training loss = 1.1496  Validation loss = 1.8947  \n",
      "\n",
      "Fold: 5  Epoch: 27  Training loss = 1.1535  Validation loss = 1.9128  \n",
      "\n",
      "Fold: 5  Epoch: 28  Training loss = 1.1521  Validation loss = 1.9247  \n",
      "\n",
      "Fold: 5  Epoch: 29  Training loss = 1.1448  Validation loss = 1.8942  \n",
      "\n",
      "Fold: 5  Epoch: 30  Training loss = 1.1625  Validation loss = 1.8931  \n",
      "\n",
      "Fold: 5  Epoch: 31  Training loss = 1.1401  Validation loss = 1.8494  \n",
      "\n",
      "Fold: 5  Epoch: 32  Training loss = 1.1484  Validation loss = 1.8731  \n",
      "\n",
      "Fold: 5  Epoch: 33  Training loss = 1.1481  Validation loss = 1.8093  \n",
      "\n",
      "Fold: 5  Epoch: 34  Training loss = 1.1404  Validation loss = 1.8340  \n",
      "\n",
      "Fold: 5  Epoch: 35  Training loss = 1.1370  Validation loss = 1.8240  \n",
      "\n",
      "Fold: 5  Epoch: 36  Training loss = 1.1392  Validation loss = 1.8166  \n",
      "\n",
      "Fold: 5  Epoch: 37  Training loss = 1.1656  Validation loss = 1.8122  \n",
      "\n",
      "Fold: 5  Epoch: 38  Training loss = 1.1520  Validation loss = 1.7615  \n",
      "\n",
      "Fold: 5  Epoch: 39  Training loss = 1.1591  Validation loss = 1.6653  \n",
      "\n",
      "Fold: 5  Epoch: 40  Training loss = 1.1372  Validation loss = 1.7413  \n",
      "\n",
      "Fold: 5  Epoch: 41  Training loss = 1.1376  Validation loss = 1.8083  \n",
      "\n",
      "Fold: 5  Epoch: 42  Training loss = 1.1384  Validation loss = 1.8028  \n",
      "\n",
      "Fold: 5  Epoch: 43  Training loss = 1.1234  Validation loss = 1.7738  \n",
      "\n",
      "Fold: 5  Epoch: 44  Training loss = 1.1224  Validation loss = 1.7454  \n",
      "\n",
      "Fold: 5  Epoch: 45  Training loss = 1.1314  Validation loss = 1.7116  \n",
      "\n",
      "Fold: 5  Epoch: 46  Training loss = 1.1289  Validation loss = 1.7683  \n",
      "\n",
      "Fold: 5  Epoch: 47  Training loss = 1.1779  Validation loss = 1.7340  \n",
      "\n",
      "Fold: 5  Epoch: 48  Training loss = 1.1405  Validation loss = 1.6496  \n",
      "\n",
      "Fold: 5  Epoch: 49  Training loss = 1.1244  Validation loss = 1.7235  \n",
      "\n",
      "Fold: 5  Epoch: 50  Training loss = 1.1438  Validation loss = 1.7502  \n",
      "\n",
      "Fold: 5  Epoch: 51  Training loss = 1.1310  Validation loss = 1.7028  \n",
      "\n",
      "Fold: 5  Epoch: 52  Training loss = 1.1136  Validation loss = 1.6966  \n",
      "\n",
      "Fold: 5  Epoch: 53  Training loss = 1.1195  Validation loss = 1.7520  \n",
      "\n",
      "Fold: 5  Epoch: 54  Training loss = 1.1075  Validation loss = 1.7128  \n",
      "\n",
      "Fold: 5  Epoch: 55  Training loss = 1.1263  Validation loss = 1.7447  \n",
      "\n",
      "Fold: 5  Epoch: 56  Training loss = 1.1289  Validation loss = 1.6446  \n",
      "\n",
      "Fold: 5  Epoch: 57  Training loss = 1.1103  Validation loss = 1.6586  \n",
      "\n",
      "Fold: 5  Epoch: 58  Training loss = 1.1084  Validation loss = 1.6618  \n",
      "\n",
      "Fold: 5  Epoch: 59  Training loss = 1.1080  Validation loss = 1.7004  \n",
      "\n",
      "Fold: 5  Epoch: 60  Training loss = 1.1085  Validation loss = 1.6759  \n",
      "\n",
      "Fold: 5  Epoch: 61  Training loss = 1.1165  Validation loss = 1.6607  \n",
      "\n",
      "Fold: 5  Epoch: 62  Training loss = 1.1274  Validation loss = 1.6492  \n",
      "\n",
      "Fold: 5  Epoch: 63  Training loss = 1.1048  Validation loss = 1.6589  \n",
      "\n",
      "Fold: 5  Epoch: 64  Training loss = 1.1122  Validation loss = 1.5843  \n",
      "\n",
      "Fold: 5  Epoch: 65  Training loss = 1.0984  Validation loss = 1.6142  \n",
      "\n",
      "Fold: 5  Epoch: 66  Training loss = 1.0956  Validation loss = 1.6183  \n",
      "\n",
      "Fold: 5  Epoch: 67  Training loss = 1.0979  Validation loss = 1.6009  \n",
      "\n",
      "Fold: 5  Epoch: 68  Training loss = 1.1055  Validation loss = 1.5511  \n",
      "\n",
      "Fold: 5  Epoch: 69  Training loss = 1.1007  Validation loss = 1.5159  \n",
      "\n",
      "Fold: 5  Epoch: 70  Training loss = 1.0931  Validation loss = 1.5255  \n",
      "\n",
      "Fold: 5  Epoch: 71  Training loss = 1.0946  Validation loss = 1.5383  \n",
      "\n",
      "Fold: 5  Epoch: 72  Training loss = 1.0839  Validation loss = 1.5507  \n",
      "\n",
      "Fold: 5  Epoch: 73  Training loss = 1.0864  Validation loss = 1.5827  \n",
      "\n",
      "Fold: 5  Epoch: 74  Training loss = 1.0803  Validation loss = 1.5466  \n",
      "\n",
      "Fold: 5  Epoch: 75  Training loss = 1.0989  Validation loss = 1.5150  \n",
      "\n",
      "Fold: 5  Epoch: 76  Training loss = 1.0831  Validation loss = 1.6344  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 75  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 1.1456  Validation loss = 0.8317  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 1.1335  Validation loss = 0.6845  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 1.1269  Validation loss = 0.7795  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 1.1243  Validation loss = 0.8757  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 1.1252  Validation loss = 0.9669  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 1.1209  Validation loss = 0.8718  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 1.1134  Validation loss = 0.7771  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 1.1235  Validation loss = 0.9827  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 1.1077  Validation loss = 0.8074  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 1.1186  Validation loss = 0.9115  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 1.1140  Validation loss = 0.9336  \n",
      "\n",
      "Fold: 6  Epoch: 12  Training loss = 1.0986  Validation loss = 0.8315  \n",
      "\n",
      "Fold: 6  Epoch: 13  Training loss = 1.1001  Validation loss = 0.9432  \n",
      "\n",
      "Fold: 6  Epoch: 14  Training loss = 1.1030  Validation loss = 0.9606  \n",
      "\n",
      "Fold: 6  Epoch: 15  Training loss = 1.0987  Validation loss = 0.9741  \n",
      "\n",
      "Fold: 6  Epoch: 16  Training loss = 1.0926  Validation loss = 0.7197  \n",
      "\n",
      "Fold: 6  Epoch: 17  Training loss = 1.0837  Validation loss = 0.8188  \n",
      "\n",
      "Fold: 6  Epoch: 18  Training loss = 1.0838  Validation loss = 0.9541  \n",
      "\n",
      "Fold: 6  Epoch: 19  Training loss = 1.0835  Validation loss = 0.8593  \n",
      "\n",
      "Fold: 6  Epoch: 20  Training loss = 1.1121  Validation loss = 1.2024  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 2  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 1.0986  Validation loss = 0.6680  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 1.0487  Validation loss = 1.1447  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 1.0599  Validation loss = 0.8227  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.0407  Validation loss = 1.3802  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 1.0399  Validation loss = 1.3116  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 1.0489  Validation loss = 1.7286  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 1.0391  Validation loss = 1.1261  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 1.0338  Validation loss = 1.4085  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 1.1210  Validation loss = 0.5938  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 1.0356  Validation loss = 0.8245  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 1.0369  Validation loss = 0.8297  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 1.0254  Validation loss = 1.0016  \n",
      "\n",
      "Fold: 7  Epoch: 13  Training loss = 1.0447  Validation loss = 0.6728  \n",
      "\n",
      "Fold: 7  Epoch: 14  Training loss = 1.0271  Validation loss = 0.8505  \n",
      "\n",
      "Fold: 7  Epoch: 15  Training loss = 1.0407  Validation loss = 1.7329  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 9  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 0.9589  Validation loss = 4.4932  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 1.0108  Validation loss = 4.6828  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 0.9659  Validation loss = 4.3537  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 0.9674  Validation loss = 4.2785  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 0.9717  Validation loss = 4.3369  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 0.9665  Validation loss = 4.2102  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 0.9687  Validation loss = 4.1699  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 0.9706  Validation loss = 4.2463  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 1.1076  Validation loss = 4.8471  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 0.9519  Validation loss = 4.3938  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 0.9838  Validation loss = 4.5118  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 0.9667  Validation loss = 4.4492  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 0.9442  Validation loss = 4.4388  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 0.9560  Validation loss = 4.3561  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 0.9702  Validation loss = 4.3724  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 0.9532  Validation loss = 4.2127  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 1.0249  Validation loss = 4.1251  \n",
      "\n",
      "Fold: 8  Epoch: 18  Training loss = 0.9722  Validation loss = 4.1930  \n",
      "\n",
      "Fold: 8  Epoch: 19  Training loss = 0.9679  Validation loss = 4.1940  \n",
      "\n",
      "Fold: 8  Epoch: 20  Training loss = 0.9403  Validation loss = 4.3107  \n",
      "\n",
      "Fold: 8  Epoch: 21  Training loss = 0.9453  Validation loss = 4.3552  \n",
      "\n",
      "Fold: 8  Epoch: 22  Training loss = 0.9528  Validation loss = 4.4222  \n",
      "\n",
      "Fold: 8  Epoch: 23  Training loss = 0.9556  Validation loss = 4.5091  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 17  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 1.3802  Validation loss = 7.4313  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 1.3922  Validation loss = 7.5235  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 1.3710  Validation loss = 7.5161  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 1.3577  Validation loss = 7.4455  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 1.3553  Validation loss = 7.4993  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 1.3652  Validation loss = 7.5587  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 1.4352  Validation loss = 7.9195  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 1.4067  Validation loss = 7.7437  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 1.3394  Validation loss = 7.5382  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 1.3368  Validation loss = 7.6027  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 1.4113  Validation loss = 7.7497  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 1.3222  Validation loss = 7.4275  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 1.3163  Validation loss = 7.4375  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 1.3389  Validation loss = 7.6628  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 1.3527  Validation loss = 7.3466  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 1.3078  Validation loss = 7.5222  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 1.3531  Validation loss = 7.7146  \n",
      "\n",
      "Fold: 9  Epoch: 18  Training loss = 1.3901  Validation loss = 7.6114  \n",
      "\n",
      "Fold: 9  Epoch: 19  Training loss = 1.3032  Validation loss = 7.7554  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 15  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 2.2137  Validation loss = 3.0212  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 1.9295  Validation loss = 2.1693  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 1.8439  Validation loss = 2.2274  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 1.7896  Validation loss = 2.1984  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 1.7694  Validation loss = 2.1868  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 1.7305  Validation loss = 2.0938  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 1.7585  Validation loss = 2.1133  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 1.7447  Validation loss = 1.9647  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 1.7211  Validation loss = 2.0355  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 1.7033  Validation loss = 2.0616  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 1.7512  Validation loss = 1.9580  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 1.6606  Validation loss = 1.8835  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 1.6435  Validation loss = 1.9489  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 1.6360  Validation loss = 1.8902  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 1.6523  Validation loss = 2.2189  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 12  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 1.6450  Validation loss = 1.4453  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 1.6400  Validation loss = 1.4642  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 1.6155  Validation loss = 1.4646  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 1.6123  Validation loss = 1.9223  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 1.6112  Validation loss = 1.5485  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 1.6383  Validation loss = 1.5793  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 1.5828  Validation loss = 2.0786  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 1.5932  Validation loss = 1.5171  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 1.5810  Validation loss = 1.5437  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 1.5857  Validation loss = 2.0767  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 1.5814  Validation loss = 1.6886  \n",
      "\n",
      "Fold: 11  Epoch: 12  Training loss = 1.5881  Validation loss = 1.7101  \n",
      "\n",
      "Fold: 11  Epoch: 13  Training loss = 1.5805  Validation loss = 1.3370  \n",
      "\n",
      "Fold: 11  Epoch: 14  Training loss = 1.5639  Validation loss = 1.3616  \n",
      "\n",
      "Fold: 11  Epoch: 15  Training loss = 1.5591  Validation loss = 1.4751  \n",
      "\n",
      "Fold: 11  Epoch: 16  Training loss = 1.5356  Validation loss = 1.4904  \n",
      "\n",
      "Fold: 11  Epoch: 17  Training loss = 1.5143  Validation loss = 1.5688  \n",
      "\n",
      "Fold: 11  Epoch: 18  Training loss = 1.5226  Validation loss = 1.2999  \n",
      "\n",
      "Fold: 11  Epoch: 19  Training loss = 1.5272  Validation loss = 1.3233  \n",
      "\n",
      "Fold: 11  Epoch: 20  Training loss = 1.5310  Validation loss = 1.3912  \n",
      "\n",
      "Fold: 11  Epoch: 21  Training loss = 1.5197  Validation loss = 1.3130  \n",
      "\n",
      "Fold: 11  Epoch: 22  Training loss = 1.5236  Validation loss = 1.3188  \n",
      "\n",
      "Fold: 11  Epoch: 23  Training loss = 1.4695  Validation loss = 1.5884  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 18  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 1.5014  Validation loss = 2.6451  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 1.4932  Validation loss = 1.3116  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 1.4912  Validation loss = 2.2181  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 1.4672  Validation loss = 1.6396  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 1.4668  Validation loss = 1.6146  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 1.4516  Validation loss = 1.3945  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 1.4528  Validation loss = 1.7134  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 1.4947  Validation loss = 1.2899  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 1.4267  Validation loss = 1.2985  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 1.4126  Validation loss = 1.4069  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 1.4036  Validation loss = 1.6331  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 1.3939  Validation loss = 1.6451  \n",
      "\n",
      "Fold: 12  Epoch: 13  Training loss = 1.3968  Validation loss = 1.5130  \n",
      "\n",
      "Fold: 12  Epoch: 14  Training loss = 1.4107  Validation loss = 1.4934  \n",
      "\n",
      "Fold: 12  Epoch: 15  Training loss = 1.3922  Validation loss = 1.4007  \n",
      "\n",
      "Fold: 12  Epoch: 16  Training loss = 1.3916  Validation loss = 1.4873  \n",
      "\n",
      "Fold: 12  Epoch: 17  Training loss = 1.3687  Validation loss = 1.3271  \n",
      "\n",
      "Fold: 12  Epoch: 18  Training loss = 1.3790  Validation loss = 1.2105  \n",
      "\n",
      "Fold: 12  Epoch: 19  Training loss = 1.3506  Validation loss = 1.5058  \n",
      "\n",
      "Fold: 12  Epoch: 20  Training loss = 1.3499  Validation loss = 1.5860  \n",
      "\n",
      "Fold: 12  Epoch: 21  Training loss = 1.3381  Validation loss = 1.4057  \n",
      "\n",
      "Fold: 12  Epoch: 22  Training loss = 1.3271  Validation loss = 1.3570  \n",
      "\n",
      "Fold: 12  Epoch: 23  Training loss = 1.3732  Validation loss = 1.3727  \n",
      "\n",
      "Fold: 12  Epoch: 24  Training loss = 1.3656  Validation loss = 1.7321  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 18  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 1.3274  Validation loss = 3.7174  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 1.3124  Validation loss = 3.7409  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 1.3266  Validation loss = 3.0895  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 1.3039  Validation loss = 3.5965  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 1.3322  Validation loss = 2.8445  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 1.4250  Validation loss = 4.0739  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 1.3725  Validation loss = 3.7385  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 1.3407  Validation loss = 2.8397  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 1.3539  Validation loss = 3.5721  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 1.3186  Validation loss = 2.9106  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 1.2961  Validation loss = 3.2230  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 1.3067  Validation loss = 3.0817  \n",
      "\n",
      "Fold: 13  Epoch: 13  Training loss = 1.3086  Validation loss = 2.6920  \n",
      "\n",
      "Fold: 13  Epoch: 14  Training loss = 1.2863  Validation loss = 3.1924  \n",
      "\n",
      "Fold: 13  Epoch: 15  Training loss = 1.2498  Validation loss = 3.1168  \n",
      "\n",
      "Fold: 13  Epoch: 16  Training loss = 1.2560  Validation loss = 3.1916  \n",
      "\n",
      "Fold: 13  Epoch: 17  Training loss = 1.2685  Validation loss = 3.5597  \n",
      "\n",
      "Fold: 13  Epoch: 18  Training loss = 1.3252  Validation loss = 2.7409  \n",
      "\n",
      "Fold: 13  Epoch: 19  Training loss = 1.2733  Validation loss = 3.3899  \n",
      "\n",
      "Fold: 13  Epoch: 20  Training loss = 1.2529  Validation loss = 3.4190  \n",
      "\n",
      "Fold: 13  Epoch: 21  Training loss = 1.2291  Validation loss = 3.1548  \n",
      "\n",
      "Fold: 13  Epoch: 22  Training loss = 1.2246  Validation loss = 3.5981  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 13  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 1.4235  Validation loss = 4.9501  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 1.4486  Validation loss = 4.3764  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 1.4381  Validation loss = 4.4814  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 1.4077  Validation loss = 4.6621  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 1.4077  Validation loss = 4.4001  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 1.4159  Validation loss = 4.5168  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 1.4343  Validation loss = 4.2915  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 1.4129  Validation loss = 4.7985  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 1.4668  Validation loss = 4.9266  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 1.3588  Validation loss = 4.5913  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 1.3597  Validation loss = 4.6221  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 1.3894  Validation loss = 4.6045  \n",
      "\n",
      "Fold: 14  Epoch: 13  Training loss = 1.3749  Validation loss = 4.3455  \n",
      "\n",
      "Fold: 14  Epoch: 14  Training loss = 1.3672  Validation loss = 4.6816  \n",
      "\n",
      "Fold: 14  Epoch: 15  Training loss = 1.3795  Validation loss = 4.5609  \n",
      "\n",
      "Fold: 14  Epoch: 16  Training loss = 1.3270  Validation loss = 4.7774  \n",
      "\n",
      "Fold: 14  Epoch: 17  Training loss = 1.3405  Validation loss = 4.4367  \n",
      "\n",
      "Fold: 14  Epoch: 18  Training loss = 1.4247  Validation loss = 4.5171  \n",
      "\n",
      "Fold: 14  Epoch: 19  Training loss = 1.3371  Validation loss = 4.6616  \n",
      "\n",
      "Fold: 14  Epoch: 20  Training loss = 1.3325  Validation loss = 4.8090  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 7  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 1.6930  Validation loss = 3.6888  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 1.6322  Validation loss = 3.7834  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 1.6498  Validation loss = 3.6783  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 1.5873  Validation loss = 3.7555  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 1.5853  Validation loss = 3.7149  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 1.5552  Validation loss = 3.8025  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 1.6034  Validation loss = 3.8234  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 1.5535  Validation loss = 3.8178  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 1.5344  Validation loss = 3.7046  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 1.5442  Validation loss = 3.7154  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 1.6354  Validation loss = 4.0396  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 3  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 1.9299  Validation loss = 4.1384  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 1.7869  Validation loss = 4.5642  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 1.7966  Validation loss = 4.1610  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 1.8369  Validation loss = 3.8309  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 1.7989  Validation loss = 4.3982  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 1.7382  Validation loss = 5.1745  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 1.7327  Validation loss = 4.5765  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 1.6800  Validation loss = 4.4385  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 1.7216  Validation loss = 4.2089  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 1.6718  Validation loss = 4.1481  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 1.8922  Validation loss = 4.1682  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 1.8200  Validation loss = 5.4841  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 4  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 1.9454  Validation loss = 2.7546  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 1.8532  Validation loss = 2.8207  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 1.8595  Validation loss = 3.1101  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 1.7987  Validation loss = 3.2228  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 1.7775  Validation loss = 3.0886  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 1.8134  Validation loss = 3.2598  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 2.0005  Validation loss = 3.7093  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 1.7780  Validation loss = 3.4934  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 1.8607  Validation loss = 3.1396  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 1.7313  Validation loss = 3.2775  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 1.7462  Validation loss = 3.3718  \n",
      "\n",
      "Fold: 17  Epoch: 12  Training loss = 1.7439  Validation loss = 3.1974  \n",
      "\n",
      "Fold: 17  Epoch: 13  Training loss = 1.7794  Validation loss = 3.0292  \n",
      "\n",
      "Fold: 17  Epoch: 14  Training loss = 1.9630  Validation loss = 3.3951  \n",
      "\n",
      "Fold: 17  Epoch: 15  Training loss = 1.6953  Validation loss = 3.2159  \n",
      "\n",
      "Fold: 17  Epoch: 16  Training loss = 1.6598  Validation loss = 3.2845  \n",
      "\n",
      "Fold: 17  Epoch: 17  Training loss = 1.7661  Validation loss = 3.4394  \n",
      "\n",
      "Fold: 17  Epoch: 18  Training loss = 1.8006  Validation loss = 3.5001  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 1  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 1.8347  Validation loss = 0.9902  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 1.7791  Validation loss = 0.9982  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 2.0142  Validation loss = 0.9823  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 1.8933  Validation loss = 1.0240  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 1.8663  Validation loss = 1.1692  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 1.8770  Validation loss = 1.0420  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 1.8511  Validation loss = 1.2956  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 1.8593  Validation loss = 1.2352  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 1.7781  Validation loss = 1.2287  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 1.8103  Validation loss = 1.2883  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 1.7840  Validation loss = 1.3424  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 3  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 1.6891  Validation loss = 1.6801  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 1.7407  Validation loss = 1.7858  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 1.7325  Validation loss = 1.7642  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 1.7122  Validation loss = 1.7072  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 1.6711  Validation loss = 1.6419  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 1.7003  Validation loss = 1.6720  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 1.6531  Validation loss = 1.9715  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 1.6630  Validation loss = 1.8038  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 1.6775  Validation loss = 1.8576  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 1.7939  Validation loss = 1.8842  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 1.6675  Validation loss = 1.9218  \n",
      "\n",
      "Fold: 19  Epoch: 12  Training loss = 1.5952  Validation loss = 1.9225  \n",
      "\n",
      "Fold: 19  Epoch: 13  Training loss = 1.5950  Validation loss = 1.8665  \n",
      "\n",
      "Fold: 19  Epoch: 14  Training loss = 1.5928  Validation loss = 2.0181  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 5  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 1.6502  Validation loss = 1.2217  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 1.6430  Validation loss = 1.3441  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 1.6210  Validation loss = 1.2585  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 1.6018  Validation loss = 1.0317  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 1.6388  Validation loss = 1.0908  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 1.6004  Validation loss = 1.3667  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 1.6830  Validation loss = 1.2426  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 1.7702  Validation loss = 1.0684  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 1.5892  Validation loss = 1.1346  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 1.6102  Validation loss = 1.4097  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 1.5418  Validation loss = 1.2244  \n",
      "\n",
      "Fold: 20  Epoch: 12  Training loss = 1.5384  Validation loss = 1.1645  \n",
      "\n",
      "Fold: 20  Epoch: 13  Training loss = 1.5651  Validation loss = 1.2295  \n",
      "\n",
      "Fold: 20  Epoch: 14  Training loss = 1.5644  Validation loss = 1.2679  \n",
      "\n",
      "Fold: 20  Epoch: 15  Training loss = 1.5959  Validation loss = 1.2702  \n",
      "\n",
      "Fold: 20  Epoch: 16  Training loss = 1.5490  Validation loss = 1.3273  \n",
      "\n",
      "Fold: 20  Epoch: 17  Training loss = 1.7143  Validation loss = 1.0353  \n",
      "\n",
      "Fold: 20  Epoch: 18  Training loss = 1.7072  Validation loss = 1.3244  \n",
      "\n",
      "Fold: 20  Epoch: 19  Training loss = 1.6334  Validation loss = 1.0274  \n",
      "\n",
      "Fold: 20  Epoch: 20  Training loss = 2.2312  Validation loss = 0.9983  \n",
      "\n",
      "Fold: 20  Epoch: 21  Training loss = 1.6210  Validation loss = 1.2509  \n",
      "\n",
      "Fold: 20  Epoch: 22  Training loss = 1.7070  Validation loss = 0.9843  \n",
      "\n",
      "Fold: 20  Epoch: 23  Training loss = 1.6094  Validation loss = 1.4651  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 22  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 1.8039  Validation loss = 4.6067  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 1.5772  Validation loss = 4.4716  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 1.6283  Validation loss = 4.3529  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 1.5149  Validation loss = 4.2690  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 1.5160  Validation loss = 4.2050  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 1.5559  Validation loss = 4.3000  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 1.5743  Validation loss = 4.6686  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 1.5270  Validation loss = 4.3889  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 1.5419  Validation loss = 4.3382  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 1.5426  Validation loss = 4.4460  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 1.5178  Validation loss = 4.2967  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 1.4764  Validation loss = 3.9032  \n",
      "\n",
      "Fold: 21  Epoch: 13  Training loss = 1.5035  Validation loss = 3.8373  \n",
      "\n",
      "Fold: 21  Epoch: 14  Training loss = 1.5726  Validation loss = 3.6099  \n",
      "\n",
      "Fold: 21  Epoch: 15  Training loss = 1.4658  Validation loss = 4.1338  \n",
      "\n",
      "Fold: 21  Epoch: 16  Training loss = 1.5418  Validation loss = 4.4085  \n",
      "\n",
      "Fold: 21  Epoch: 17  Training loss = 1.5134  Validation loss = 4.6484  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 14  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 1.8835  Validation loss = 3.6996  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 1.7903  Validation loss = 3.1518  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 1.7440  Validation loss = 4.0802  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 1.6953  Validation loss = 4.0723  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 1.7165  Validation loss = 4.4061  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 1.6611  Validation loss = 3.9622  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 1.6529  Validation loss = 3.6394  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 1.7159  Validation loss = 4.6473  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 1.5929  Validation loss = 4.4657  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 1.6010  Validation loss = 5.1945  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 1.6526  Validation loss = 5.3840  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 2  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 1.9243  Validation loss = 1.8941  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 1.6930  Validation loss = 2.0417  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 1.6762  Validation loss = 1.9713  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 1.6438  Validation loss = 1.5944  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 1.7043  Validation loss = 2.1076  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 1.7465  Validation loss = 2.1891  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 1.7810  Validation loss = 1.8122  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 1.9926  Validation loss = 2.9847  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 1.9520  Validation loss = 2.0259  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 1.9833  Validation loss = 2.4698  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 1.9235  Validation loss = 2.6071  \n",
      "\n",
      "Fold: 23  Epoch: 12  Training loss = 1.7887  Validation loss = 2.6697  \n",
      "\n",
      "Fold: 23  Epoch: 13  Training loss = 1.7605  Validation loss = 2.5650  \n",
      "\n",
      "Fold: 23  Epoch: 14  Training loss = 1.7628  Validation loss = 1.8445  \n",
      "\n",
      "Fold: 23  Epoch: 15  Training loss = 1.8576  Validation loss = 2.8627  \n",
      "\n",
      "Fold: 23  Epoch: 16  Training loss = 1.7671  Validation loss = 1.8577  \n",
      "\n",
      "Fold: 23  Epoch: 17  Training loss = 1.6908  Validation loss = 1.9621  \n",
      "\n",
      "Fold: 23  Epoch: 18  Training loss = 1.6769  Validation loss = 2.1898  \n",
      "\n",
      "Fold: 23  Epoch: 19  Training loss = 1.6922  Validation loss = 1.6190  \n",
      "\n",
      "Fold: 23  Epoch: 20  Training loss = 1.8025  Validation loss = 1.6721  \n",
      "\n",
      "Fold: 23  Epoch: 21  Training loss = 1.6870  Validation loss = 2.1559  \n",
      "\n",
      "Fold: 23  Epoch: 22  Training loss = 1.7784  Validation loss = 2.9369  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 4  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 2.0710  Validation loss = 1.5791  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 1.7277  Validation loss = 1.6233  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 1.8874  Validation loss = 1.8200  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 1.7733  Validation loss = 2.3922  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 1.7007  Validation loss = 2.0030  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 1.6566  Validation loss = 1.7566  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 2.0814  Validation loss = 2.0860  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 1.6017  Validation loss = 1.5587  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 1.5632  Validation loss = 1.6139  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 1.5168  Validation loss = 1.5333  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 1.6739  Validation loss = 1.5888  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 1.8898  Validation loss = 2.1266  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 2.3136  Validation loss = 1.6022  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 2.3041  Validation loss = 1.5322  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 2.0820  Validation loss = 1.4214  \n",
      "\n",
      "Fold: 24  Epoch: 16  Training loss = 1.8216  Validation loss = 1.6666  \n",
      "\n",
      "Fold: 24  Epoch: 17  Training loss = 1.7761  Validation loss = 1.5484  \n",
      "\n",
      "Fold: 24  Epoch: 18  Training loss = 2.2472  Validation loss = 1.5048  \n",
      "\n",
      "Fold: 24  Epoch: 19  Training loss = 2.2747  Validation loss = 1.2644  \n",
      "\n",
      "Fold: 24  Epoch: 20  Training loss = 2.2132  Validation loss = 1.3526  \n",
      "\n",
      "Fold: 24  Epoch: 21  Training loss = 1.9486  Validation loss = 1.3915  \n",
      "\n",
      "Fold: 24  Epoch: 22  Training loss = 1.9267  Validation loss = 1.6463  \n",
      "\n",
      "Fold: 24  Epoch: 23  Training loss = 1.9315  Validation loss = 1.6886  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 19  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 1.8754  Validation loss = 1.9299  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 2.0521  Validation loss = 1.9656  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 1.9475  Validation loss = 2.0570  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 2.1783  Validation loss = 2.0900  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 2.1072  Validation loss = 1.8998  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 2.1295  Validation loss = 2.1451  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 2.0733  Validation loss = 2.0254  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 1.9920  Validation loss = 2.1992  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 1.9498  Validation loss = 2.4458  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 1.8190  Validation loss = 2.4316  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 1.8088  Validation loss = 2.2422  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 1.7796  Validation loss = 2.2309  \n",
      "\n",
      "Fold: 25  Epoch: 13  Training loss = 1.7317  Validation loss = 2.3954  \n",
      "\n",
      "Fold: 25  Epoch: 14  Training loss = 1.8324  Validation loss = 2.3636  \n",
      "\n",
      "Fold: 25  Epoch: 15  Training loss = 1.7538  Validation loss = 2.4178  \n",
      "\n",
      "Fold: 25  Epoch: 16  Training loss = 1.6636  Validation loss = 2.3247  \n",
      "\n",
      "Fold: 25  Epoch: 17  Training loss = 1.8541  Validation loss = 2.4229  \n",
      "\n",
      "Fold: 25  Epoch: 18  Training loss = 1.8208  Validation loss = 2.5884  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 5  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 1.9125  Validation loss = 1.2228  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 1.9163  Validation loss = 1.4001  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 1.7455  Validation loss = 1.9342  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 1.7835  Validation loss = 1.9985  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 1.8133  Validation loss = 1.5280  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 1.7176  Validation loss = 1.9102  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 1.7969  Validation loss = 1.4337  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 1.7113  Validation loss = 1.8661  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 1.7332  Validation loss = 1.3637  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 1.7235  Validation loss = 1.7182  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 1.7016  Validation loss = 2.0101  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 1  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 1.7562  Validation loss = 1.0092  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 1.6994  Validation loss = 0.8270  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 1.6862  Validation loss = 1.1162  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 1.6820  Validation loss = 0.9654  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 1.6816  Validation loss = 0.9804  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 1.6161  Validation loss = 0.9498  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 1.5828  Validation loss = 0.8439  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 1.7821  Validation loss = 0.5182  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 1.6224  Validation loss = 0.9032  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 1.5093  Validation loss = 0.9812  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 1.6393  Validation loss = 0.2800  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 1.6673  Validation loss = 0.3865  \n",
      "\n",
      "Fold: 27  Epoch: 13  Training loss = 1.5652  Validation loss = 0.3140  \n",
      "\n",
      "Fold: 27  Epoch: 14  Training loss = 1.5249  Validation loss = 0.7877  \n",
      "\n",
      "Fold: 27  Epoch: 15  Training loss = 1.5451  Validation loss = 0.5638  \n",
      "\n",
      "Fold: 27  Epoch: 16  Training loss = 1.5661  Validation loss = 0.8351  \n",
      "\n",
      "Fold: 27  Epoch: 17  Training loss = 1.5285  Validation loss = 0.6556  \n",
      "\n",
      "Fold: 27  Epoch: 18  Training loss = 1.5216  Validation loss = 0.6112  \n",
      "\n",
      "Fold: 27  Epoch: 19  Training loss = 1.5398  Validation loss = 0.6637  \n",
      "\n",
      "Fold: 27  Epoch: 20  Training loss = 1.5051  Validation loss = 0.5641  \n",
      "\n",
      "Fold: 27  Epoch: 21  Training loss = 1.5335  Validation loss = 0.5968  \n",
      "\n",
      "Fold: 27  Epoch: 22  Training loss = 1.6725  Validation loss = 0.2475  \n",
      "\n",
      "Fold: 27  Epoch: 23  Training loss = 1.5105  Validation loss = 0.7332  \n",
      "\n",
      "Fold: 27  Epoch: 24  Training loss = 1.5286  Validation loss = 0.3831  \n",
      "\n",
      "Fold: 27  Epoch: 25  Training loss = 1.5234  Validation loss = 0.6453  \n",
      "\n",
      "Fold: 27  Epoch: 26  Training loss = 1.4850  Validation loss = 0.7411  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 22  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 1.4392  Validation loss = 1.4074  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 1.4320  Validation loss = 1.1560  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 1.4159  Validation loss = 1.2660  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 1.4139  Validation loss = 1.4260  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 1.3899  Validation loss = 1.3633  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 1.4257  Validation loss = 1.0422  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 1.3921  Validation loss = 0.9947  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 1.3965  Validation loss = 1.2719  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 1.4018  Validation loss = 0.8791  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 1.4594  Validation loss = 1.1691  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 1.4245  Validation loss = 0.8749  \n",
      "\n",
      "Fold: 28  Epoch: 12  Training loss = 1.4588  Validation loss = 0.8793  \n",
      "\n",
      "Fold: 28  Epoch: 13  Training loss = 1.3677  Validation loss = 1.0695  \n",
      "\n",
      "Fold: 28  Epoch: 14  Training loss = 1.3323  Validation loss = 1.2749  \n",
      "\n",
      "Fold: 28  Epoch: 15  Training loss = 1.3687  Validation loss = 1.3767  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 11  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 1.3632  Validation loss = 1.1072  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 1.3968  Validation loss = 1.2998  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 1.3300  Validation loss = 1.2617  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 1.3286  Validation loss = 1.4416  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 1.5004  Validation loss = 1.5004  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 1.3516  Validation loss = 1.0107  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 1.3355  Validation loss = 1.3389  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 1.4087  Validation loss = 1.4824  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 1.5050  Validation loss = 1.4706  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 1.3208  Validation loss = 1.4048  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 1.3192  Validation loss = 1.3369  \n",
      "\n",
      "Fold: 29  Epoch: 12  Training loss = 1.3019  Validation loss = 0.9844  \n",
      "\n",
      "Fold: 29  Epoch: 13  Training loss = 1.3165  Validation loss = 1.4402  \n",
      "\n",
      "Fold: 29  Epoch: 14  Training loss = 1.3361  Validation loss = 1.3938  \n",
      "\n",
      "Fold: 29  Epoch: 15  Training loss = 1.3257  Validation loss = 0.9314  \n",
      "\n",
      "Fold: 29  Epoch: 16  Training loss = 1.3290  Validation loss = 1.3033  \n",
      "\n",
      "Fold: 29  Epoch: 17  Training loss = 1.2956  Validation loss = 1.1885  \n",
      "\n",
      "Fold: 29  Epoch: 18  Training loss = 1.3386  Validation loss = 1.1144  \n",
      "\n",
      "Fold: 29  Epoch: 19  Training loss = 1.3045  Validation loss = 0.9013  \n",
      "\n",
      "Fold: 29  Epoch: 20  Training loss = 1.2460  Validation loss = 1.1538  \n",
      "\n",
      "Fold: 29  Epoch: 21  Training loss = 1.2781  Validation loss = 1.1392  \n",
      "\n",
      "Fold: 29  Epoch: 22  Training loss = 1.2910  Validation loss = 1.5354  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 19  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 1.3023  Validation loss = 1.6838  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 1.3556  Validation loss = 2.2847  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 1.2946  Validation loss = 1.5117  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 1.3418  Validation loss = 1.8277  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 1.2579  Validation loss = 2.3148  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 1.2795  Validation loss = 2.4094  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 1.2845  Validation loss = 2.0749  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 1.2193  Validation loss = 2.1356  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 1.2327  Validation loss = 2.1567  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 1.2406  Validation loss = 2.1644  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 1.2167  Validation loss = 2.1689  \n",
      "\n",
      "Fold: 30  Epoch: 12  Training loss = 1.2122  Validation loss = 1.8218  \n",
      "\n",
      "Fold: 30  Epoch: 13  Training loss = 1.2638  Validation loss = 1.8949  \n",
      "\n",
      "Fold: 30  Epoch: 14  Training loss = 1.2352  Validation loss = 2.1709  \n",
      "\n",
      "Fold: 30  Epoch: 15  Training loss = 1.2490  Validation loss = 1.7817  \n",
      "\n",
      "Fold: 30  Epoch: 16  Training loss = 1.1953  Validation loss = 1.9760  \n",
      "\n",
      "Fold: 30  Epoch: 17  Training loss = 1.2177  Validation loss = 1.8848  \n",
      "\n",
      "Fold: 30  Epoch: 18  Training loss = 1.1803  Validation loss = 1.9735  \n",
      "\n",
      "Fold: 30  Epoch: 19  Training loss = 1.2106  Validation loss = 2.4147  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 3  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 1.2809  Validation loss = 1.8808  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 1.2887  Validation loss = 1.8593  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 1.2232  Validation loss = 1.8828  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 1.2420  Validation loss = 1.7093  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 1.2725  Validation loss = 1.6662  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 1.3229  Validation loss = 1.7822  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 1.1960  Validation loss = 1.8220  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 1.2938  Validation loss = 1.6872  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 1.1960  Validation loss = 1.6890  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 1.2208  Validation loss = 1.7259  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 1.1889  Validation loss = 1.7387  \n",
      "\n",
      "Fold: 31  Epoch: 12  Training loss = 1.2164  Validation loss = 1.9055  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 5  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 1.1726  Validation loss = 2.4518  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 1.1802  Validation loss = 3.1784  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 1.1283  Validation loss = 3.1763  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 1.3354  Validation loss = 2.9410  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 1.1647  Validation loss = 3.2547  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 1.1067  Validation loss = 2.8435  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 1.1056  Validation loss = 2.7409  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 1.0791  Validation loss = 2.7542  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 1.0871  Validation loss = 2.4297  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 1.0926  Validation loss = 2.9039  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 1.0483  Validation loss = 2.7962  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 1.0528  Validation loss = 2.6957  \n",
      "\n",
      "Fold: 32  Epoch: 13  Training loss = 1.0201  Validation loss = 2.4702  \n",
      "\n",
      "Fold: 32  Epoch: 14  Training loss = 1.0237  Validation loss = 2.4896  \n",
      "\n",
      "Fold: 32  Epoch: 15  Training loss = 1.0142  Validation loss = 2.2575  \n",
      "\n",
      "Fold: 32  Epoch: 16  Training loss = 1.0484  Validation loss = 2.5561  \n",
      "\n",
      "Fold: 32  Epoch: 17  Training loss = 1.0102  Validation loss = 2.6360  \n",
      "\n",
      "Fold: 32  Epoch: 18  Training loss = 1.0356  Validation loss = 2.7233  \n",
      "\n",
      "Fold: 32  Epoch: 19  Training loss = 1.0119  Validation loss = 2.3507  \n",
      "\n",
      "Fold: 32  Epoch: 20  Training loss = 1.1156  Validation loss = 3.0683  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 15  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    # Load model with lowest validation error for each fold\n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 12\n",
      "Average validation error: 2.72058\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 1.2336  Test loss = 3.2812  \n",
      "\n",
      "Epoch: 2  Training loss = 1.1661  Test loss = 3.1662  \n",
      "\n",
      "Epoch: 3  Training loss = 1.1234  Test loss = 3.0838  \n",
      "\n",
      "Epoch: 4  Training loss = 1.1015  Test loss = 3.0378  \n",
      "\n",
      "Epoch: 5  Training loss = 1.0871  Test loss = 3.0070  \n",
      "\n",
      "Epoch: 6  Training loss = 1.0767  Test loss = 2.9853  \n",
      "\n",
      "Epoch: 7  Training loss = 1.0687  Test loss = 2.9696  \n",
      "\n",
      "Epoch: 8  Training loss = 1.0623  Test loss = 2.9579  \n",
      "\n",
      "Epoch: 9  Training loss = 1.0569  Test loss = 2.9492  \n",
      "\n",
      "Epoch: 10  Training loss = 1.0522  Test loss = 2.9424  \n",
      "\n",
      "Epoch: 11  Training loss = 1.0479  Test loss = 2.9373  \n",
      "\n",
      "Epoch: 12  Training loss = 1.0441  Test loss = 2.9333  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n64XuPYT8Rs4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc2005c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcac9a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 2.93326367843 \n",
      "Fixed scheme MAE:  1.97722840585\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 1.0441  Test loss = 2.8780  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 1.0910  Test loss = 3.1699  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 1.1574  Test loss = 0.5817  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 1.1595  Test loss = 1.3572  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 0.9490  Test loss = 1.3337  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 0.9579  Test loss = 0.0118  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 0.9568  Test loss = 0.0269  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 0.9557  Test loss = 0.4730  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 0.8459  Test loss = 0.9870  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 0.8546  Test loss = 1.3151  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 0.8627  Test loss = 1.9359  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 0.8955  Test loss = 1.9999  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 0.7579  Test loss = 0.9496  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 0.7667  Test loss = 1.8911  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 0.8017  Test loss = 4.3409  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 0.9636  Test loss = 4.8486  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 0.9493  Test loss = 0.4189  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 0.9484  Test loss = 1.1785  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 0.9592  Test loss = 1.4019  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 0.9503  Test loss = 1.6408  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 0.7545  Test loss = 2.4965  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 0.8151  Test loss = 3.8892  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 0.9420  Test loss = 0.8021  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 0.9458  Test loss = 1.7811  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 0.8916  Test loss = 0.3779  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 0.8919  Test loss = 0.3262  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 0.8901  Test loss = 0.8180  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 0.8923  Test loss = 2.2945  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 0.8730  Test loss = 1.1799  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 0.8845  Test loss = 0.9977  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 0.8930  Test loss = 3.0948  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 0.9662  Test loss = 0.1187  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 0.8984  Test loss = 0.0724  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 0.8983  Test loss = 0.3292  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 0.8820  Test loss = 0.5968  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 0.8752  Test loss = 6.1463  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 1.0718  Test loss = 0.6676  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 1.0631  Test loss = 0.5775  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 1.0551  Test loss = 1.1664  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 1.0647  Test loss = 1.1679  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 0.8267  Test loss = 1.9977  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 0.8611  Test loss = 1.9145  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 0.8932  Test loss = 3.0933  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 0.9718  Test loss = 12.9645  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 1.9075  Test loss = 7.2766  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.1101  Test loss = 2.6287  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.1351  Test loss = 3.2878  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.1736  Test loss = 1.6497  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 1.6501  Test loss = 3.9410  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 1.7151  Test loss = 2.9462  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 1.7527  Test loss = 0.6364  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 1.7544  Test loss = 1.2188  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 1.5620  Test loss = 3.5118  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 1.6212  Test loss = 2.9682  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 1.6614  Test loss = 0.0140  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 1.6519  Test loss = 0.8699  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 1.4919  Test loss = 0.2137  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 1.4919  Test loss = 1.8725  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 1.5050  Test loss = 0.3295  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 1.5048  Test loss = 0.1840  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.4521  Test loss = 0.6166  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.4515  Test loss = 2.5437  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 1.4822  Test loss = 0.0351  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 1.4822  Test loss = 0.5555  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 1.4215  Test loss = 0.3963  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 1.4217  Test loss = 1.2703  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.4304  Test loss = 1.9263  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 1.4499  Test loss = 2.3144  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 1.4335  Test loss = 4.5217  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 1.5394  Test loss = 0.1264  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 1.5393  Test loss = 1.9883  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 1.5588  Test loss = 2.9675  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 1.5099  Test loss = 2.6980  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 1.5391  Test loss = 2.0178  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 1.5587  Test loss = 1.3647  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 1.5612  Test loss = 0.0114  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 1.4243  Test loss = 1.4804  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd8a9940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.66804266854 \n",
      "Updating scheme MAE:  1.84408607374\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_test_fixed.csv\", yhat_test_fixed)\n",
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_update.csv\", yhat_update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
