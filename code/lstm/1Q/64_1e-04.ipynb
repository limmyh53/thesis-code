{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"1Q/64_units/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 500\n",
    "learning_rate = 1e-4\n",
    "batch_size = 5\n",
    "early_stop_iters = 10\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 64 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag1',\n",
    "                                       'inflation.lag2',\n",
    "                                       'inflation.lag3',\n",
    "                                       'inflation.lag4']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag1',\n",
    "                                   'unemp.lag2',\n",
    "                                   'unemp.lag3',\n",
    "                                   'unemp.lag4']])\n",
    "train_4lag_oil = np.array(train[['oil.lag1',\n",
    "                                 'oil.lag2',\n",
    "                                 'oil.lag3',\n",
    "                                 'oil.lag4']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag1',\n",
    "                                     'inflation.lag2',\n",
    "                                     'inflation.lag3',\n",
    "                                     'inflation.lag4']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag1',\n",
    "                                 'unemp.lag2',\n",
    "                                 'unemp.lag3',\n",
    "                                 'unemp.lag4']])\n",
    "test_4lag_oil = np.array(test[['oil.lag1',\n",
    "                               'oil.lag2',\n",
    "                               'oil.lag3',\n",
    "                               'oil.lag4']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 64 \n",
      "Learning rate = 0.0001 \n",
      "Epochs = 500 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 10 \n",
      "Learning rate = 0.0001\n",
      "Fold: 1  Epoch: 1  Training loss = 3.2261  Validation loss = 3.4172  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 3.2045  Validation loss = 3.3769  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 3.1832  Validation loss = 3.3372  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 3.1621  Validation loss = 3.2976  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 3.1456  Validation loss = 3.2657  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 3.1279  Validation loss = 3.2312  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 3.1146  Validation loss = 3.2047  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 3.0941  Validation loss = 3.1631  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 3.0738  Validation loss = 3.1208  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 3.0505  Validation loss = 3.0722  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 3.0388  Validation loss = 3.0474  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 3.0204  Validation loss = 3.0075  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 2.9979  Validation loss = 2.9567  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 2.9872  Validation loss = 2.9323  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 2.9692  Validation loss = 2.8904  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 2.9616  Validation loss = 2.8727  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 2.9520  Validation loss = 2.8496  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 2.9436  Validation loss = 2.8292  \n",
      "\n",
      "Fold: 1  Epoch: 19  Training loss = 2.9278  Validation loss = 2.7906  \n",
      "\n",
      "Fold: 1  Epoch: 20  Training loss = 2.9156  Validation loss = 2.7593  \n",
      "\n",
      "Fold: 1  Epoch: 21  Training loss = 2.9044  Validation loss = 2.7299  \n",
      "\n",
      "Fold: 1  Epoch: 22  Training loss = 2.8988  Validation loss = 2.7155  \n",
      "\n",
      "Fold: 1  Epoch: 23  Training loss = 2.8816  Validation loss = 2.6697  \n",
      "\n",
      "Fold: 1  Epoch: 24  Training loss = 2.8683  Validation loss = 2.6332  \n",
      "\n",
      "Fold: 1  Epoch: 25  Training loss = 2.8598  Validation loss = 2.6094  \n",
      "\n",
      "Fold: 1  Epoch: 26  Training loss = 2.8494  Validation loss = 2.5796  \n",
      "\n",
      "Fold: 1  Epoch: 27  Training loss = 2.8456  Validation loss = 2.5684  \n",
      "\n",
      "Fold: 1  Epoch: 28  Training loss = 2.8372  Validation loss = 2.5440  \n",
      "\n",
      "Fold: 1  Epoch: 29  Training loss = 2.8294  Validation loss = 2.5205  \n",
      "\n",
      "Fold: 1  Epoch: 30  Training loss = 2.8215  Validation loss = 2.4969  \n",
      "\n",
      "Fold: 1  Epoch: 31  Training loss = 2.8108  Validation loss = 2.4635  \n",
      "\n",
      "Fold: 1  Epoch: 32  Training loss = 2.8034  Validation loss = 2.4401  \n",
      "\n",
      "Fold: 1  Epoch: 33  Training loss = 2.7911  Validation loss = 2.4000  \n",
      "\n",
      "Fold: 1  Epoch: 34  Training loss = 2.7871  Validation loss = 2.3868  \n",
      "\n",
      "Fold: 1  Epoch: 35  Training loss = 2.7815  Validation loss = 2.3677  \n",
      "\n",
      "Fold: 1  Epoch: 36  Training loss = 2.7794  Validation loss = 2.3605  \n",
      "\n",
      "Fold: 1  Epoch: 37  Training loss = 2.7755  Validation loss = 2.3467  \n",
      "\n",
      "Fold: 1  Epoch: 38  Training loss = 2.7704  Validation loss = 2.3285  \n",
      "\n",
      "Fold: 1  Epoch: 39  Training loss = 2.7683  Validation loss = 2.3209  \n",
      "\n",
      "Fold: 1  Epoch: 40  Training loss = 2.7672  Validation loss = 2.3171  \n",
      "\n",
      "Fold: 1  Epoch: 41  Training loss = 2.7583  Validation loss = 2.2847  \n",
      "\n",
      "Fold: 1  Epoch: 42  Training loss = 2.7556  Validation loss = 2.2747  \n",
      "\n",
      "Fold: 1  Epoch: 43  Training loss = 2.7512  Validation loss = 2.2587  \n",
      "\n",
      "Fold: 1  Epoch: 44  Training loss = 2.7495  Validation loss = 2.2521  \n",
      "\n",
      "Fold: 1  Epoch: 45  Training loss = 2.7468  Validation loss = 2.2420  \n",
      "\n",
      "Fold: 1  Epoch: 46  Training loss = 2.7396  Validation loss = 2.2142  \n",
      "\n",
      "Fold: 1  Epoch: 47  Training loss = 2.7353  Validation loss = 2.1969  \n",
      "\n",
      "Fold: 1  Epoch: 48  Training loss = 2.7311  Validation loss = 2.1793  \n",
      "\n",
      "Fold: 1  Epoch: 49  Training loss = 2.7290  Validation loss = 2.1703  \n",
      "\n",
      "Fold: 1  Epoch: 50  Training loss = 2.7250  Validation loss = 2.1528  \n",
      "\n",
      "Fold: 1  Epoch: 51  Training loss = 2.7229  Validation loss = 2.1441  \n",
      "\n",
      "Fold: 1  Epoch: 52  Training loss = 2.7208  Validation loss = 2.1349  \n",
      "\n",
      "Fold: 1  Epoch: 53  Training loss = 2.7171  Validation loss = 2.1182  \n",
      "\n",
      "Fold: 1  Epoch: 54  Training loss = 2.7157  Validation loss = 2.1119  \n",
      "\n",
      "Fold: 1  Epoch: 55  Training loss = 2.7124  Validation loss = 2.0960  \n",
      "\n",
      "Fold: 1  Epoch: 56  Training loss = 2.7103  Validation loss = 2.0863  \n",
      "\n",
      "Fold: 1  Epoch: 57  Training loss = 2.7087  Validation loss = 2.0790  \n",
      "\n",
      "Fold: 1  Epoch: 58  Training loss = 2.7055  Validation loss = 2.0627  \n",
      "\n",
      "Fold: 1  Epoch: 59  Training loss = 2.7032  Validation loss = 2.0516  \n",
      "\n",
      "Fold: 1  Epoch: 60  Training loss = 2.7017  Validation loss = 2.0439  \n",
      "\n",
      "Fold: 1  Epoch: 61  Training loss = 2.7019  Validation loss = 2.0459  \n",
      "\n",
      "Fold: 1  Epoch: 62  Training loss = 2.6973  Validation loss = 2.0207  \n",
      "\n",
      "Fold: 1  Epoch: 63  Training loss = 2.6962  Validation loss = 2.0157  \n",
      "\n",
      "Fold: 1  Epoch: 64  Training loss = 2.6934  Validation loss = 2.0003  \n",
      "\n",
      "Fold: 1  Epoch: 65  Training loss = 2.6943  Validation loss = 2.0060  \n",
      "\n",
      "Fold: 1  Epoch: 66  Training loss = 2.6907  Validation loss = 1.9860  \n",
      "\n",
      "Fold: 1  Epoch: 67  Training loss = 2.6869  Validation loss = 1.9625  \n",
      "\n",
      "Fold: 1  Epoch: 68  Training loss = 2.6851  Validation loss = 1.9515  \n",
      "\n",
      "Fold: 1  Epoch: 69  Training loss = 2.6849  Validation loss = 1.9515  \n",
      "\n",
      "Fold: 1  Epoch: 70  Training loss = 2.6834  Validation loss = 1.9440  \n",
      "\n",
      "Fold: 1  Epoch: 71  Training loss = 2.6826  Validation loss = 1.9400  \n",
      "\n",
      "Fold: 1  Epoch: 72  Training loss = 2.6802  Validation loss = 1.9240  \n",
      "\n",
      "Fold: 1  Epoch: 73  Training loss = 2.6797  Validation loss = 1.9215  \n",
      "\n",
      "Fold: 1  Epoch: 74  Training loss = 2.6772  Validation loss = 1.9065  \n",
      "\n",
      "Fold: 1  Epoch: 75  Training loss = 2.6775  Validation loss = 1.9087  \n",
      "\n",
      "Fold: 1  Epoch: 76  Training loss = 2.6779  Validation loss = 1.9131  \n",
      "\n",
      "Fold: 1  Epoch: 77  Training loss = 2.6736  Validation loss = 1.8843  \n",
      "\n",
      "Fold: 1  Epoch: 78  Training loss = 2.6721  Validation loss = 1.8746  \n",
      "\n",
      "Fold: 1  Epoch: 79  Training loss = 2.6714  Validation loss = 1.8706  \n",
      "\n",
      "Fold: 1  Epoch: 80  Training loss = 2.6699  Validation loss = 1.8592  \n",
      "\n",
      "Fold: 1  Epoch: 81  Training loss = 2.6696  Validation loss = 1.8574  \n",
      "\n",
      "Fold: 1  Epoch: 82  Training loss = 2.6687  Validation loss = 1.8524  \n",
      "\n",
      "Fold: 1  Epoch: 83  Training loss = 2.6672  Validation loss = 1.8399  \n",
      "\n",
      "Fold: 1  Epoch: 84  Training loss = 2.6672  Validation loss = 1.8417  \n",
      "\n",
      "Fold: 1  Epoch: 85  Training loss = 2.6667  Validation loss = 1.8401  \n",
      "\n",
      "Fold: 1  Epoch: 86  Training loss = 2.6659  Validation loss = 1.8338  \n",
      "\n",
      "Fold: 1  Epoch: 87  Training loss = 2.6656  Validation loss = 1.8338  \n",
      "\n",
      "Fold: 1  Epoch: 88  Training loss = 2.6650  Validation loss = 1.8301  \n",
      "\n",
      "Fold: 1  Epoch: 89  Training loss = 2.6634  Validation loss = 1.8171  \n",
      "\n",
      "Fold: 1  Epoch: 90  Training loss = 2.6618  Validation loss = 1.8053  \n",
      "\n",
      "Fold: 1  Epoch: 91  Training loss = 2.6607  Validation loss = 1.7966  \n",
      "\n",
      "Fold: 1  Epoch: 92  Training loss = 2.6591  Validation loss = 1.7855  \n",
      "\n",
      "Fold: 1  Epoch: 93  Training loss = 2.6577  Validation loss = 1.7731  \n",
      "\n",
      "Fold: 1  Epoch: 94  Training loss = 2.6563  Validation loss = 1.7592  \n",
      "\n",
      "Fold: 1  Epoch: 95  Training loss = 2.6543  Validation loss = 1.7384  \n",
      "\n",
      "Fold: 1  Epoch: 96  Training loss = 2.6530  Validation loss = 1.7233  \n",
      "\n",
      "Fold: 1  Epoch: 97  Training loss = 2.6520  Validation loss = 1.7153  \n",
      "\n",
      "Fold: 1  Epoch: 98  Training loss = 2.6511  Validation loss = 1.7083  \n",
      "\n",
      "Fold: 1  Epoch: 99  Training loss = 2.6501  Validation loss = 1.6970  \n",
      "\n",
      "Fold: 1  Epoch: 100  Training loss = 2.6494  Validation loss = 1.6969  \n",
      "\n",
      "Fold: 1  Epoch: 101  Training loss = 2.6493  Validation loss = 1.6967  \n",
      "\n",
      "Fold: 1  Epoch: 102  Training loss = 2.6487  Validation loss = 1.6932  \n",
      "\n",
      "Fold: 1  Epoch: 103  Training loss = 2.6481  Validation loss = 1.6880  \n",
      "\n",
      "Fold: 1  Epoch: 104  Training loss = 2.6471  Validation loss = 1.6771  \n",
      "\n",
      "Fold: 1  Epoch: 105  Training loss = 2.6472  Validation loss = 1.6818  \n",
      "\n",
      "Fold: 1  Epoch: 106  Training loss = 2.6460  Validation loss = 1.6689  \n",
      "\n",
      "Fold: 1  Epoch: 107  Training loss = 2.6456  Validation loss = 1.6644  \n",
      "\n",
      "Fold: 1  Epoch: 108  Training loss = 2.6450  Validation loss = 1.6580  \n",
      "\n",
      "Fold: 1  Epoch: 109  Training loss = 2.6444  Validation loss = 1.6551  \n",
      "\n",
      "Fold: 1  Epoch: 110  Training loss = 2.6447  Validation loss = 1.6653  \n",
      "\n",
      "Fold: 1  Epoch: 111  Training loss = 2.6433  Validation loss = 1.6477  \n",
      "\n",
      "Fold: 1  Epoch: 112  Training loss = 2.6429  Validation loss = 1.6431  \n",
      "\n",
      "Fold: 1  Epoch: 113  Training loss = 2.6428  Validation loss = 1.6498  \n",
      "\n",
      "Fold: 1  Epoch: 114  Training loss = 2.6411  Validation loss = 1.6247  \n",
      "\n",
      "Fold: 1  Epoch: 115  Training loss = 2.6407  Validation loss = 1.6205  \n",
      "\n",
      "Fold: 1  Epoch: 116  Training loss = 2.6406  Validation loss = 1.6270  \n",
      "\n",
      "Fold: 1  Epoch: 117  Training loss = 2.6402  Validation loss = 1.6217  \n",
      "\n",
      "Fold: 1  Epoch: 118  Training loss = 2.6394  Validation loss = 1.6089  \n",
      "\n",
      "Fold: 1  Epoch: 119  Training loss = 2.6387  Validation loss = 1.5947  \n",
      "\n",
      "Fold: 1  Epoch: 120  Training loss = 2.6384  Validation loss = 1.5946  \n",
      "\n",
      "Fold: 1  Epoch: 121  Training loss = 2.6378  Validation loss = 1.5890  \n",
      "\n",
      "Fold: 1  Epoch: 122  Training loss = 2.6377  Validation loss = 1.5935  \n",
      "\n",
      "Fold: 1  Epoch: 123  Training loss = 2.6372  Validation loss = 1.5887  \n",
      "\n",
      "Fold: 1  Epoch: 124  Training loss = 2.6371  Validation loss = 1.5860  \n",
      "\n",
      "Fold: 1  Epoch: 125  Training loss = 2.6361  Validation loss = 1.5722  \n",
      "\n",
      "Fold: 1  Epoch: 126  Training loss = 2.6359  Validation loss = 1.5683  \n",
      "\n",
      "Fold: 1  Epoch: 127  Training loss = 2.6355  Validation loss = 1.5667  \n",
      "\n",
      "Fold: 1  Epoch: 128  Training loss = 2.6354  Validation loss = 1.5706  \n",
      "\n",
      "Fold: 1  Epoch: 129  Training loss = 2.6350  Validation loss = 1.5725  \n",
      "\n",
      "Fold: 1  Epoch: 130  Training loss = 2.6350  Validation loss = 1.5778  \n",
      "\n",
      "Fold: 1  Epoch: 131  Training loss = 2.6347  Validation loss = 1.5799  \n",
      "\n",
      "Fold: 1  Epoch: 132  Training loss = 2.6342  Validation loss = 1.5743  \n",
      "\n",
      "Fold: 1  Epoch: 133  Training loss = 2.6340  Validation loss = 1.5728  \n",
      "\n",
      "Fold: 1  Epoch: 134  Training loss = 2.6337  Validation loss = 1.5770  \n",
      "\n",
      "Fold: 1  Epoch: 135  Training loss = 2.6337  Validation loss = 1.5863  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 127  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.5262  Validation loss = 1.8090  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.5260  Validation loss = 1.8080  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.5253  Validation loss = 1.8065  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.5247  Validation loss = 1.8056  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.5242  Validation loss = 1.8062  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.5237  Validation loss = 1.8102  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.5221  Validation loss = 1.7984  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.5220  Validation loss = 1.8017  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.5209  Validation loss = 1.7943  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.5199  Validation loss = 1.7860  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.5195  Validation loss = 1.7812  \n",
      "\n",
      "Fold: 2  Epoch: 12  Training loss = 2.5192  Validation loss = 1.7820  \n",
      "\n",
      "Fold: 2  Epoch: 13  Training loss = 2.5186  Validation loss = 1.7802  \n",
      "\n",
      "Fold: 2  Epoch: 14  Training loss = 2.5184  Validation loss = 1.7811  \n",
      "\n",
      "Fold: 2  Epoch: 15  Training loss = 2.5181  Validation loss = 1.7805  \n",
      "\n",
      "Fold: 2  Epoch: 16  Training loss = 2.5180  Validation loss = 1.7878  \n",
      "\n",
      "Fold: 2  Epoch: 17  Training loss = 2.5179  Validation loss = 1.7914  \n",
      "\n",
      "Fold: 2  Epoch: 18  Training loss = 2.5172  Validation loss = 1.7917  \n",
      "\n",
      "Fold: 2  Epoch: 19  Training loss = 2.5167  Validation loss = 1.7945  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 13  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.5097  Validation loss = 2.6464  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.5076  Validation loss = 2.6511  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.5061  Validation loss = 2.6526  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.5055  Validation loss = 2.6515  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.5053  Validation loss = 2.6468  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.5035  Validation loss = 2.6525  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.5012  Validation loss = 2.6575  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.5008  Validation loss = 2.6592  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.4991  Validation loss = 2.6646  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.4975  Validation loss = 2.6705  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.4962  Validation loss = 2.6715  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 1  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.5420  Validation loss = 3.8521  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.5415  Validation loss = 3.8531  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.5408  Validation loss = 3.8557  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.5396  Validation loss = 3.8557  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.5385  Validation loss = 3.8573  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.5367  Validation loss = 3.8472  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.5359  Validation loss = 3.8454  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.5350  Validation loss = 3.8493  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.5339  Validation loss = 3.8468  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 1.5330  Validation loss = 3.8463  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.5322  Validation loss = 3.8418  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 1.5311  Validation loss = 3.8403  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.5302  Validation loss = 3.8440  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 1.5294  Validation loss = 3.8472  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.5283  Validation loss = 3.8420  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 1.5271  Validation loss = 3.8373  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 1.5265  Validation loss = 3.8375  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 1.5256  Validation loss = 3.8377  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 1.5250  Validation loss = 3.8401  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 1.5243  Validation loss = 3.8362  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 1.5237  Validation loss = 3.8342  \n",
      "\n",
      "Fold: 4  Epoch: 22  Training loss = 1.5227  Validation loss = 3.8367  \n",
      "\n",
      "Fold: 4  Epoch: 23  Training loss = 1.5215  Validation loss = 3.8280  \n",
      "\n",
      "Fold: 4  Epoch: 24  Training loss = 1.5204  Validation loss = 3.8212  \n",
      "\n",
      "Fold: 4  Epoch: 25  Training loss = 1.5192  Validation loss = 3.8167  \n",
      "\n",
      "Fold: 4  Epoch: 26  Training loss = 1.5184  Validation loss = 3.8151  \n",
      "\n",
      "Fold: 4  Epoch: 27  Training loss = 1.5171  Validation loss = 3.8059  \n",
      "\n",
      "Fold: 4  Epoch: 28  Training loss = 1.5165  Validation loss = 3.8050  \n",
      "\n",
      "Fold: 4  Epoch: 29  Training loss = 1.5153  Validation loss = 3.8011  \n",
      "\n",
      "Fold: 4  Epoch: 30  Training loss = 1.5143  Validation loss = 3.7952  \n",
      "\n",
      "Fold: 4  Epoch: 31  Training loss = 1.5134  Validation loss = 3.7975  \n",
      "\n",
      "Fold: 4  Epoch: 32  Training loss = 1.5125  Validation loss = 3.7961  \n",
      "\n",
      "Fold: 4  Epoch: 33  Training loss = 1.5116  Validation loss = 3.7931  \n",
      "\n",
      "Fold: 4  Epoch: 34  Training loss = 1.5106  Validation loss = 3.7884  \n",
      "\n",
      "Fold: 4  Epoch: 35  Training loss = 1.5097  Validation loss = 3.7740  \n",
      "\n",
      "Fold: 4  Epoch: 36  Training loss = 1.5089  Validation loss = 3.7790  \n",
      "\n",
      "Fold: 4  Epoch: 37  Training loss = 1.5082  Validation loss = 3.7767  \n",
      "\n",
      "Fold: 4  Epoch: 38  Training loss = 1.5074  Validation loss = 3.7683  \n",
      "\n",
      "Fold: 4  Epoch: 39  Training loss = 1.5065  Validation loss = 3.7736  \n",
      "\n",
      "Fold: 4  Epoch: 40  Training loss = 1.5054  Validation loss = 3.7696  \n",
      "\n",
      "Fold: 4  Epoch: 41  Training loss = 1.5049  Validation loss = 3.7699  \n",
      "\n",
      "Fold: 4  Epoch: 42  Training loss = 1.5039  Validation loss = 3.7659  \n",
      "\n",
      "Fold: 4  Epoch: 43  Training loss = 1.5027  Validation loss = 3.7612  \n",
      "\n",
      "Fold: 4  Epoch: 44  Training loss = 1.5016  Validation loss = 3.7509  \n",
      "\n",
      "Fold: 4  Epoch: 45  Training loss = 1.5006  Validation loss = 3.7474  \n",
      "\n",
      "Fold: 4  Epoch: 46  Training loss = 1.5001  Validation loss = 3.7493  \n",
      "\n",
      "Fold: 4  Epoch: 47  Training loss = 1.4993  Validation loss = 3.7499  \n",
      "\n",
      "Fold: 4  Epoch: 48  Training loss = 1.4986  Validation loss = 3.7425  \n",
      "\n",
      "Fold: 4  Epoch: 49  Training loss = 1.4978  Validation loss = 3.7373  \n",
      "\n",
      "Fold: 4  Epoch: 50  Training loss = 1.4974  Validation loss = 3.7334  \n",
      "\n",
      "Fold: 4  Epoch: 51  Training loss = 1.4967  Validation loss = 3.7310  \n",
      "\n",
      "Fold: 4  Epoch: 52  Training loss = 1.4963  Validation loss = 3.7435  \n",
      "\n",
      "Fold: 4  Epoch: 53  Training loss = 1.4950  Validation loss = 3.7323  \n",
      "\n",
      "Fold: 4  Epoch: 54  Training loss = 1.4938  Validation loss = 3.7239  \n",
      "\n",
      "Fold: 4  Epoch: 55  Training loss = 1.4926  Validation loss = 3.7130  \n",
      "\n",
      "Fold: 4  Epoch: 56  Training loss = 1.4916  Validation loss = 3.7059  \n",
      "\n",
      "Fold: 4  Epoch: 57  Training loss = 1.4909  Validation loss = 3.6986  \n",
      "\n",
      "Fold: 4  Epoch: 58  Training loss = 1.4898  Validation loss = 3.6886  \n",
      "\n",
      "Fold: 4  Epoch: 59  Training loss = 1.4889  Validation loss = 3.6899  \n",
      "\n",
      "Fold: 4  Epoch: 60  Training loss = 1.4880  Validation loss = 3.6878  \n",
      "\n",
      "Fold: 4  Epoch: 61  Training loss = 1.4873  Validation loss = 3.6854  \n",
      "\n",
      "Fold: 4  Epoch: 62  Training loss = 1.4864  Validation loss = 3.6884  \n",
      "\n",
      "Fold: 4  Epoch: 63  Training loss = 1.4851  Validation loss = 3.6745  \n",
      "\n",
      "Fold: 4  Epoch: 64  Training loss = 1.4842  Validation loss = 3.6701  \n",
      "\n",
      "Fold: 4  Epoch: 65  Training loss = 1.4832  Validation loss = 3.6628  \n",
      "\n",
      "Fold: 4  Epoch: 66  Training loss = 1.4825  Validation loss = 3.6572  \n",
      "\n",
      "Fold: 4  Epoch: 67  Training loss = 1.4812  Validation loss = 3.6511  \n",
      "\n",
      "Fold: 4  Epoch: 68  Training loss = 1.4802  Validation loss = 3.6524  \n",
      "\n",
      "Fold: 4  Epoch: 69  Training loss = 1.4798  Validation loss = 3.6569  \n",
      "\n",
      "Fold: 4  Epoch: 70  Training loss = 1.4794  Validation loss = 3.6554  \n",
      "\n",
      "Fold: 4  Epoch: 71  Training loss = 1.4784  Validation loss = 3.6520  \n",
      "\n",
      "Fold: 4  Epoch: 72  Training loss = 1.4775  Validation loss = 3.6523  \n",
      "\n",
      "Fold: 4  Epoch: 73  Training loss = 1.4770  Validation loss = 3.6457  \n",
      "\n",
      "Fold: 4  Epoch: 74  Training loss = 1.4761  Validation loss = 3.6391  \n",
      "\n",
      "Fold: 4  Epoch: 75  Training loss = 1.4757  Validation loss = 3.6314  \n",
      "\n",
      "Fold: 4  Epoch: 76  Training loss = 1.4752  Validation loss = 3.6342  \n",
      "\n",
      "Fold: 4  Epoch: 77  Training loss = 1.4743  Validation loss = 3.6286  \n",
      "\n",
      "Fold: 4  Epoch: 78  Training loss = 1.4734  Validation loss = 3.6189  \n",
      "\n",
      "Fold: 4  Epoch: 79  Training loss = 1.4731  Validation loss = 3.6245  \n",
      "\n",
      "Fold: 4  Epoch: 80  Training loss = 1.4723  Validation loss = 3.6205  \n",
      "\n",
      "Fold: 4  Epoch: 81  Training loss = 1.4715  Validation loss = 3.6244  \n",
      "\n",
      "Fold: 4  Epoch: 82  Training loss = 1.4710  Validation loss = 3.6183  \n",
      "\n",
      "Fold: 4  Epoch: 83  Training loss = 1.4699  Validation loss = 3.6148  \n",
      "\n",
      "Fold: 4  Epoch: 84  Training loss = 1.4691  Validation loss = 3.6152  \n",
      "\n",
      "Fold: 4  Epoch: 85  Training loss = 1.4680  Validation loss = 3.6079  \n",
      "\n",
      "Fold: 4  Epoch: 86  Training loss = 1.4675  Validation loss = 3.6087  \n",
      "\n",
      "Fold: 4  Epoch: 87  Training loss = 1.4665  Validation loss = 3.6084  \n",
      "\n",
      "Fold: 4  Epoch: 88  Training loss = 1.4658  Validation loss = 3.6017  \n",
      "\n",
      "Fold: 4  Epoch: 89  Training loss = 1.4647  Validation loss = 3.5918  \n",
      "\n",
      "Fold: 4  Epoch: 90  Training loss = 1.4637  Validation loss = 3.5844  \n",
      "\n",
      "Fold: 4  Epoch: 91  Training loss = 1.4629  Validation loss = 3.5837  \n",
      "\n",
      "Fold: 4  Epoch: 92  Training loss = 1.4621  Validation loss = 3.5789  \n",
      "\n",
      "Fold: 4  Epoch: 93  Training loss = 1.4608  Validation loss = 3.5741  \n",
      "\n",
      "Fold: 4  Epoch: 94  Training loss = 1.4600  Validation loss = 3.5652  \n",
      "\n",
      "Fold: 4  Epoch: 95  Training loss = 1.4595  Validation loss = 3.5592  \n",
      "\n",
      "Fold: 4  Epoch: 96  Training loss = 1.4589  Validation loss = 3.5575  \n",
      "\n",
      "Fold: 4  Epoch: 97  Training loss = 1.4581  Validation loss = 3.5523  \n",
      "\n",
      "Fold: 4  Epoch: 98  Training loss = 1.4573  Validation loss = 3.5514  \n",
      "\n",
      "Fold: 4  Epoch: 99  Training loss = 1.4565  Validation loss = 3.5487  \n",
      "\n",
      "Fold: 4  Epoch: 100  Training loss = 1.4558  Validation loss = 3.5443  \n",
      "\n",
      "Fold: 4  Epoch: 101  Training loss = 1.4552  Validation loss = 3.5470  \n",
      "\n",
      "Fold: 4  Epoch: 102  Training loss = 1.4543  Validation loss = 3.5418  \n",
      "\n",
      "Fold: 4  Epoch: 103  Training loss = 1.4536  Validation loss = 3.5420  \n",
      "\n",
      "Fold: 4  Epoch: 104  Training loss = 1.4532  Validation loss = 3.5424  \n",
      "\n",
      "Fold: 4  Epoch: 105  Training loss = 1.4528  Validation loss = 3.5509  \n",
      "\n",
      "Fold: 4  Epoch: 106  Training loss = 1.4521  Validation loss = 3.5449  \n",
      "\n",
      "Fold: 4  Epoch: 107  Training loss = 1.4509  Validation loss = 3.5313  \n",
      "\n",
      "Fold: 4  Epoch: 108  Training loss = 1.4502  Validation loss = 3.5317  \n",
      "\n",
      "Fold: 4  Epoch: 109  Training loss = 1.4494  Validation loss = 3.5283  \n",
      "\n",
      "Fold: 4  Epoch: 110  Training loss = 1.4488  Validation loss = 3.5247  \n",
      "\n",
      "Fold: 4  Epoch: 111  Training loss = 1.4484  Validation loss = 3.5209  \n",
      "\n",
      "Fold: 4  Epoch: 112  Training loss = 1.4475  Validation loss = 3.5238  \n",
      "\n",
      "Fold: 4  Epoch: 113  Training loss = 1.4468  Validation loss = 3.5140  \n",
      "\n",
      "Fold: 4  Epoch: 114  Training loss = 1.4460  Validation loss = 3.5107  \n",
      "\n",
      "Fold: 4  Epoch: 115  Training loss = 1.4453  Validation loss = 3.5068  \n",
      "\n",
      "Fold: 4  Epoch: 116  Training loss = 1.4448  Validation loss = 3.5105  \n",
      "\n",
      "Fold: 4  Epoch: 117  Training loss = 1.4441  Validation loss = 3.5063  \n",
      "\n",
      "Fold: 4  Epoch: 118  Training loss = 1.4433  Validation loss = 3.5045  \n",
      "\n",
      "Fold: 4  Epoch: 119  Training loss = 1.4424  Validation loss = 3.4985  \n",
      "\n",
      "Fold: 4  Epoch: 120  Training loss = 1.4414  Validation loss = 3.4993  \n",
      "\n",
      "Fold: 4  Epoch: 121  Training loss = 1.4406  Validation loss = 3.4964  \n",
      "\n",
      "Fold: 4  Epoch: 122  Training loss = 1.4404  Validation loss = 3.5009  \n",
      "\n",
      "Fold: 4  Epoch: 123  Training loss = 1.4395  Validation loss = 3.4980  \n",
      "\n",
      "Fold: 4  Epoch: 124  Training loss = 1.4390  Validation loss = 3.4934  \n",
      "\n",
      "Fold: 4  Epoch: 125  Training loss = 1.4385  Validation loss = 3.4888  \n",
      "\n",
      "Fold: 4  Epoch: 126  Training loss = 1.4375  Validation loss = 3.4821  \n",
      "\n",
      "Fold: 4  Epoch: 127  Training loss = 1.4368  Validation loss = 3.4816  \n",
      "\n",
      "Fold: 4  Epoch: 128  Training loss = 1.4360  Validation loss = 3.4820  \n",
      "\n",
      "Fold: 4  Epoch: 129  Training loss = 1.4353  Validation loss = 3.4795  \n",
      "\n",
      "Fold: 4  Epoch: 130  Training loss = 1.4345  Validation loss = 3.4720  \n",
      "\n",
      "Fold: 4  Epoch: 131  Training loss = 1.4338  Validation loss = 3.4619  \n",
      "\n",
      "Fold: 4  Epoch: 132  Training loss = 1.4332  Validation loss = 3.4650  \n",
      "\n",
      "Fold: 4  Epoch: 133  Training loss = 1.4326  Validation loss = 3.4643  \n",
      "\n",
      "Fold: 4  Epoch: 134  Training loss = 1.4317  Validation loss = 3.4598  \n",
      "\n",
      "Fold: 4  Epoch: 135  Training loss = 1.4309  Validation loss = 3.4553  \n",
      "\n",
      "Fold: 4  Epoch: 136  Training loss = 1.4306  Validation loss = 3.4557  \n",
      "\n",
      "Fold: 4  Epoch: 137  Training loss = 1.4295  Validation loss = 3.4474  \n",
      "\n",
      "Fold: 4  Epoch: 138  Training loss = 1.4287  Validation loss = 3.4344  \n",
      "\n",
      "Fold: 4  Epoch: 139  Training loss = 1.4279  Validation loss = 3.4214  \n",
      "\n",
      "Fold: 4  Epoch: 140  Training loss = 1.4271  Validation loss = 3.4247  \n",
      "\n",
      "Fold: 4  Epoch: 141  Training loss = 1.4263  Validation loss = 3.4188  \n",
      "\n",
      "Fold: 4  Epoch: 142  Training loss = 1.4256  Validation loss = 3.4150  \n",
      "\n",
      "Fold: 4  Epoch: 143  Training loss = 1.4250  Validation loss = 3.4227  \n",
      "\n",
      "Fold: 4  Epoch: 144  Training loss = 1.4243  Validation loss = 3.4197  \n",
      "\n",
      "Fold: 4  Epoch: 145  Training loss = 1.4239  Validation loss = 3.4156  \n",
      "\n",
      "Fold: 4  Epoch: 146  Training loss = 1.4234  Validation loss = 3.4180  \n",
      "\n",
      "Fold: 4  Epoch: 147  Training loss = 1.4226  Validation loss = 3.4098  \n",
      "\n",
      "Fold: 4  Epoch: 148  Training loss = 1.4218  Validation loss = 3.4078  \n",
      "\n",
      "Fold: 4  Epoch: 149  Training loss = 1.4212  Validation loss = 3.4102  \n",
      "\n",
      "Fold: 4  Epoch: 150  Training loss = 1.4204  Validation loss = 3.4050  \n",
      "\n",
      "Fold: 4  Epoch: 151  Training loss = 1.4199  Validation loss = 3.4045  \n",
      "\n",
      "Fold: 4  Epoch: 152  Training loss = 1.4189  Validation loss = 3.3973  \n",
      "\n",
      "Fold: 4  Epoch: 153  Training loss = 1.4185  Validation loss = 3.3995  \n",
      "\n",
      "Fold: 4  Epoch: 154  Training loss = 1.4175  Validation loss = 3.3930  \n",
      "\n",
      "Fold: 4  Epoch: 155  Training loss = 1.4169  Validation loss = 3.3930  \n",
      "\n",
      "Fold: 4  Epoch: 156  Training loss = 1.4165  Validation loss = 3.3949  \n",
      "\n",
      "Fold: 4  Epoch: 157  Training loss = 1.4158  Validation loss = 3.3946  \n",
      "\n",
      "Fold: 4  Epoch: 158  Training loss = 1.4151  Validation loss = 3.3869  \n",
      "\n",
      "Fold: 4  Epoch: 159  Training loss = 1.4143  Validation loss = 3.3804  \n",
      "\n",
      "Fold: 4  Epoch: 160  Training loss = 1.4135  Validation loss = 3.3806  \n",
      "\n",
      "Fold: 4  Epoch: 161  Training loss = 1.4127  Validation loss = 3.3689  \n",
      "\n",
      "Fold: 4  Epoch: 162  Training loss = 1.4121  Validation loss = 3.3724  \n",
      "\n",
      "Fold: 4  Epoch: 163  Training loss = 1.4111  Validation loss = 3.3630  \n",
      "\n",
      "Fold: 4  Epoch: 164  Training loss = 1.4105  Validation loss = 3.3587  \n",
      "\n",
      "Fold: 4  Epoch: 165  Training loss = 1.4098  Validation loss = 3.3462  \n",
      "\n",
      "Fold: 4  Epoch: 166  Training loss = 1.4092  Validation loss = 3.3422  \n",
      "\n",
      "Fold: 4  Epoch: 167  Training loss = 1.4085  Validation loss = 3.3347  \n",
      "\n",
      "Fold: 4  Epoch: 168  Training loss = 1.4076  Validation loss = 3.3355  \n",
      "\n",
      "Fold: 4  Epoch: 169  Training loss = 1.4071  Validation loss = 3.3262  \n",
      "\n",
      "Fold: 4  Epoch: 170  Training loss = 1.4062  Validation loss = 3.3221  \n",
      "\n",
      "Fold: 4  Epoch: 171  Training loss = 1.4055  Validation loss = 3.3251  \n",
      "\n",
      "Fold: 4  Epoch: 172  Training loss = 1.4046  Validation loss = 3.3145  \n",
      "\n",
      "Fold: 4  Epoch: 173  Training loss = 1.4040  Validation loss = 3.3123  \n",
      "\n",
      "Fold: 4  Epoch: 174  Training loss = 1.4038  Validation loss = 3.3158  \n",
      "\n",
      "Fold: 4  Epoch: 175  Training loss = 1.4031  Validation loss = 3.3136  \n",
      "\n",
      "Fold: 4  Epoch: 176  Training loss = 1.4025  Validation loss = 3.3146  \n",
      "\n",
      "Fold: 4  Epoch: 177  Training loss = 1.4017  Validation loss = 3.3076  \n",
      "\n",
      "Fold: 4  Epoch: 178  Training loss = 1.4009  Validation loss = 3.3049  \n",
      "\n",
      "Fold: 4  Epoch: 179  Training loss = 1.4007  Validation loss = 3.3172  \n",
      "\n",
      "Fold: 4  Epoch: 180  Training loss = 1.4000  Validation loss = 3.3079  \n",
      "\n",
      "Fold: 4  Epoch: 181  Training loss = 1.3995  Validation loss = 3.2974  \n",
      "\n",
      "Fold: 4  Epoch: 182  Training loss = 1.3987  Validation loss = 3.2959  \n",
      "\n",
      "Fold: 4  Epoch: 183  Training loss = 1.3980  Validation loss = 3.2983  \n",
      "\n",
      "Fold: 4  Epoch: 184  Training loss = 1.3974  Validation loss = 3.2906  \n",
      "\n",
      "Fold: 4  Epoch: 185  Training loss = 1.3966  Validation loss = 3.2830  \n",
      "\n",
      "Fold: 4  Epoch: 186  Training loss = 1.3961  Validation loss = 3.2846  \n",
      "\n",
      "Fold: 4  Epoch: 187  Training loss = 1.3954  Validation loss = 3.2816  \n",
      "\n",
      "Fold: 4  Epoch: 188  Training loss = 1.3945  Validation loss = 3.2797  \n",
      "\n",
      "Fold: 4  Epoch: 189  Training loss = 1.3939  Validation loss = 3.2797  \n",
      "\n",
      "Fold: 4  Epoch: 190  Training loss = 1.3932  Validation loss = 3.2773  \n",
      "\n",
      "Fold: 4  Epoch: 191  Training loss = 1.3927  Validation loss = 3.2765  \n",
      "\n",
      "Fold: 4  Epoch: 192  Training loss = 1.3919  Validation loss = 3.2756  \n",
      "\n",
      "Fold: 4  Epoch: 193  Training loss = 1.3909  Validation loss = 3.2738  \n",
      "\n",
      "Fold: 4  Epoch: 194  Training loss = 1.3904  Validation loss = 3.2737  \n",
      "\n",
      "Fold: 4  Epoch: 195  Training loss = 1.3896  Validation loss = 3.2657  \n",
      "\n",
      "Fold: 4  Epoch: 196  Training loss = 1.3888  Validation loss = 3.2577  \n",
      "\n",
      "Fold: 4  Epoch: 197  Training loss = 1.3883  Validation loss = 3.2502  \n",
      "\n",
      "Fold: 4  Epoch: 198  Training loss = 1.3876  Validation loss = 3.2410  \n",
      "\n",
      "Fold: 4  Epoch: 199  Training loss = 1.3872  Validation loss = 3.2396  \n",
      "\n",
      "Fold: 4  Epoch: 200  Training loss = 1.3866  Validation loss = 3.2365  \n",
      "\n",
      "Fold: 4  Epoch: 201  Training loss = 1.3859  Validation loss = 3.2328  \n",
      "\n",
      "Fold: 4  Epoch: 202  Training loss = 1.3850  Validation loss = 3.2300  \n",
      "\n",
      "Fold: 4  Epoch: 203  Training loss = 1.3844  Validation loss = 3.2256  \n",
      "\n",
      "Fold: 4  Epoch: 204  Training loss = 1.3836  Validation loss = 3.2208  \n",
      "\n",
      "Fold: 4  Epoch: 205  Training loss = 1.3830  Validation loss = 3.2177  \n",
      "\n",
      "Fold: 4  Epoch: 206  Training loss = 1.3826  Validation loss = 3.2110  \n",
      "\n",
      "Fold: 4  Epoch: 207  Training loss = 1.3820  Validation loss = 3.2061  \n",
      "\n",
      "Fold: 4  Epoch: 208  Training loss = 1.3812  Validation loss = 3.2119  \n",
      "\n",
      "Fold: 4  Epoch: 209  Training loss = 1.3807  Validation loss = 3.2072  \n",
      "\n",
      "Fold: 4  Epoch: 210  Training loss = 1.3798  Validation loss = 3.2062  \n",
      "\n",
      "Fold: 4  Epoch: 211  Training loss = 1.3791  Validation loss = 3.1993  \n",
      "\n",
      "Fold: 4  Epoch: 212  Training loss = 1.3787  Validation loss = 3.1889  \n",
      "\n",
      "Fold: 4  Epoch: 213  Training loss = 1.3777  Validation loss = 3.1916  \n",
      "\n",
      "Fold: 4  Epoch: 214  Training loss = 1.3773  Validation loss = 3.1912  \n",
      "\n",
      "Fold: 4  Epoch: 215  Training loss = 1.3768  Validation loss = 3.1888  \n",
      "\n",
      "Fold: 4  Epoch: 216  Training loss = 1.3761  Validation loss = 3.1927  \n",
      "\n",
      "Fold: 4  Epoch: 217  Training loss = 1.3757  Validation loss = 3.1940  \n",
      "\n",
      "Fold: 4  Epoch: 218  Training loss = 1.3753  Validation loss = 3.1898  \n",
      "\n",
      "Fold: 4  Epoch: 219  Training loss = 1.3747  Validation loss = 3.1899  \n",
      "\n",
      "Fold: 4  Epoch: 220  Training loss = 1.3739  Validation loss = 3.1758  \n",
      "\n",
      "Fold: 4  Epoch: 221  Training loss = 1.3730  Validation loss = 3.1685  \n",
      "\n",
      "Fold: 4  Epoch: 222  Training loss = 1.3725  Validation loss = 3.1606  \n",
      "\n",
      "Fold: 4  Epoch: 223  Training loss = 1.3718  Validation loss = 3.1655  \n",
      "\n",
      "Fold: 4  Epoch: 224  Training loss = 1.3713  Validation loss = 3.1712  \n",
      "\n",
      "Fold: 4  Epoch: 225  Training loss = 1.3705  Validation loss = 3.1663  \n",
      "\n",
      "Fold: 4  Epoch: 226  Training loss = 1.3700  Validation loss = 3.1568  \n",
      "\n",
      "Fold: 4  Epoch: 227  Training loss = 1.3695  Validation loss = 3.1497  \n",
      "\n",
      "Fold: 4  Epoch: 228  Training loss = 1.3692  Validation loss = 3.1459  \n",
      "\n",
      "Fold: 4  Epoch: 229  Training loss = 1.3685  Validation loss = 3.1468  \n",
      "\n",
      "Fold: 4  Epoch: 230  Training loss = 1.3683  Validation loss = 3.1456  \n",
      "\n",
      "Fold: 4  Epoch: 231  Training loss = 1.3683  Validation loss = 3.1407  \n",
      "\n",
      "Fold: 4  Epoch: 232  Training loss = 1.3676  Validation loss = 3.1468  \n",
      "\n",
      "Fold: 4  Epoch: 233  Training loss = 1.3672  Validation loss = 3.1396  \n",
      "\n",
      "Fold: 4  Epoch: 234  Training loss = 1.3669  Validation loss = 3.1358  \n",
      "\n",
      "Fold: 4  Epoch: 235  Training loss = 1.3666  Validation loss = 3.1293  \n",
      "\n",
      "Fold: 4  Epoch: 236  Training loss = 1.3664  Validation loss = 3.1276  \n",
      "\n",
      "Fold: 4  Epoch: 237  Training loss = 1.3658  Validation loss = 3.1210  \n",
      "\n",
      "Fold: 4  Epoch: 238  Training loss = 1.3653  Validation loss = 3.1186  \n",
      "\n",
      "Fold: 4  Epoch: 239  Training loss = 1.3644  Validation loss = 3.1180  \n",
      "\n",
      "Fold: 4  Epoch: 240  Training loss = 1.3635  Validation loss = 3.1273  \n",
      "\n",
      "Fold: 4  Epoch: 241  Training loss = 1.3632  Validation loss = 3.1217  \n",
      "\n",
      "Fold: 4  Epoch: 242  Training loss = 1.3631  Validation loss = 3.1100  \n",
      "\n",
      "Fold: 4  Epoch: 243  Training loss = 1.3621  Validation loss = 3.1104  \n",
      "\n",
      "Fold: 4  Epoch: 244  Training loss = 1.3622  Validation loss = 3.1039  \n",
      "\n",
      "Fold: 4  Epoch: 245  Training loss = 1.3615  Validation loss = 3.1086  \n",
      "\n",
      "Fold: 4  Epoch: 246  Training loss = 1.3609  Validation loss = 3.1079  \n",
      "\n",
      "Fold: 4  Epoch: 247  Training loss = 1.3605  Validation loss = 3.1106  \n",
      "\n",
      "Fold: 4  Epoch: 248  Training loss = 1.3596  Validation loss = 3.1104  \n",
      "\n",
      "Fold: 4  Epoch: 249  Training loss = 1.3589  Validation loss = 3.1117  \n",
      "\n",
      "Fold: 4  Epoch: 250  Training loss = 1.3584  Validation loss = 3.1121  \n",
      "\n",
      "Fold: 4  Epoch: 251  Training loss = 1.3578  Validation loss = 3.1079  \n",
      "\n",
      "Fold: 4  Epoch: 252  Training loss = 1.3573  Validation loss = 3.0977  \n",
      "\n",
      "Fold: 4  Epoch: 253  Training loss = 1.3564  Validation loss = 3.0918  \n",
      "\n",
      "Fold: 4  Epoch: 254  Training loss = 1.3557  Validation loss = 3.0932  \n",
      "\n",
      "Fold: 4  Epoch: 255  Training loss = 1.3553  Validation loss = 3.0917  \n",
      "\n",
      "Fold: 4  Epoch: 256  Training loss = 1.3545  Validation loss = 3.0827  \n",
      "\n",
      "Fold: 4  Epoch: 257  Training loss = 1.3538  Validation loss = 3.0828  \n",
      "\n",
      "Fold: 4  Epoch: 258  Training loss = 1.3533  Validation loss = 3.0739  \n",
      "\n",
      "Fold: 4  Epoch: 259  Training loss = 1.3530  Validation loss = 3.0671  \n",
      "\n",
      "Fold: 4  Epoch: 260  Training loss = 1.3524  Validation loss = 3.0638  \n",
      "\n",
      "Fold: 4  Epoch: 261  Training loss = 1.3517  Validation loss = 3.0616  \n",
      "\n",
      "Fold: 4  Epoch: 262  Training loss = 1.3510  Validation loss = 3.0634  \n",
      "\n",
      "Fold: 4  Epoch: 263  Training loss = 1.3507  Validation loss = 3.0673  \n",
      "\n",
      "Fold: 4  Epoch: 264  Training loss = 1.3503  Validation loss = 3.0622  \n",
      "\n",
      "Fold: 4  Epoch: 265  Training loss = 1.3496  Validation loss = 3.0586  \n",
      "\n",
      "Fold: 4  Epoch: 266  Training loss = 1.3494  Validation loss = 3.0636  \n",
      "\n",
      "Fold: 4  Epoch: 267  Training loss = 1.3490  Validation loss = 3.0640  \n",
      "\n",
      "Fold: 4  Epoch: 268  Training loss = 1.3482  Validation loss = 3.0493  \n",
      "\n",
      "Fold: 4  Epoch: 269  Training loss = 1.3478  Validation loss = 3.0482  \n",
      "\n",
      "Fold: 4  Epoch: 270  Training loss = 1.3473  Validation loss = 3.0426  \n",
      "\n",
      "Fold: 4  Epoch: 271  Training loss = 1.3470  Validation loss = 3.0429  \n",
      "\n",
      "Fold: 4  Epoch: 272  Training loss = 1.3464  Validation loss = 3.0415  \n",
      "\n",
      "Fold: 4  Epoch: 273  Training loss = 1.3459  Validation loss = 3.0409  \n",
      "\n",
      "Fold: 4  Epoch: 274  Training loss = 1.3453  Validation loss = 3.0277  \n",
      "\n",
      "Fold: 4  Epoch: 275  Training loss = 1.3449  Validation loss = 3.0305  \n",
      "\n",
      "Fold: 4  Epoch: 276  Training loss = 1.3440  Validation loss = 3.0205  \n",
      "\n",
      "Fold: 4  Epoch: 277  Training loss = 1.3432  Validation loss = 3.0176  \n",
      "\n",
      "Fold: 4  Epoch: 278  Training loss = 1.3428  Validation loss = 3.0184  \n",
      "\n",
      "Fold: 4  Epoch: 279  Training loss = 1.3425  Validation loss = 3.0199  \n",
      "\n",
      "Fold: 4  Epoch: 280  Training loss = 1.3422  Validation loss = 3.0131  \n",
      "\n",
      "Fold: 4  Epoch: 281  Training loss = 1.3417  Validation loss = 3.0121  \n",
      "\n",
      "Fold: 4  Epoch: 282  Training loss = 1.3411  Validation loss = 3.0048  \n",
      "\n",
      "Fold: 4  Epoch: 283  Training loss = 1.3408  Validation loss = 3.0041  \n",
      "\n",
      "Fold: 4  Epoch: 284  Training loss = 1.3403  Validation loss = 3.0000  \n",
      "\n",
      "Fold: 4  Epoch: 285  Training loss = 1.3398  Validation loss = 3.0043  \n",
      "\n",
      "Fold: 4  Epoch: 286  Training loss = 1.3395  Validation loss = 3.0022  \n",
      "\n",
      "Fold: 4  Epoch: 287  Training loss = 1.3391  Validation loss = 2.9958  \n",
      "\n",
      "Fold: 4  Epoch: 288  Training loss = 1.3388  Validation loss = 2.9919  \n",
      "\n",
      "Fold: 4  Epoch: 289  Training loss = 1.3383  Validation loss = 2.9948  \n",
      "\n",
      "Fold: 4  Epoch: 290  Training loss = 1.3380  Validation loss = 2.9907  \n",
      "\n",
      "Fold: 4  Epoch: 291  Training loss = 1.3379  Validation loss = 2.9873  \n",
      "\n",
      "Fold: 4  Epoch: 292  Training loss = 1.3374  Validation loss = 2.9866  \n",
      "\n",
      "Fold: 4  Epoch: 293  Training loss = 1.3371  Validation loss = 2.9841  \n",
      "\n",
      "Fold: 4  Epoch: 294  Training loss = 1.3366  Validation loss = 2.9821  \n",
      "\n",
      "Fold: 4  Epoch: 295  Training loss = 1.3363  Validation loss = 2.9722  \n",
      "\n",
      "Fold: 4  Epoch: 296  Training loss = 1.3363  Validation loss = 2.9580  \n",
      "\n",
      "Fold: 4  Epoch: 297  Training loss = 1.3360  Validation loss = 2.9527  \n",
      "\n",
      "Fold: 4  Epoch: 298  Training loss = 1.3354  Validation loss = 2.9548  \n",
      "\n",
      "Fold: 4  Epoch: 299  Training loss = 1.3352  Validation loss = 2.9545  \n",
      "\n",
      "Fold: 4  Epoch: 300  Training loss = 1.3346  Validation loss = 2.9547  \n",
      "\n",
      "Fold: 4  Epoch: 301  Training loss = 1.3342  Validation loss = 2.9501  \n",
      "\n",
      "Fold: 4  Epoch: 302  Training loss = 1.3337  Validation loss = 2.9482  \n",
      "\n",
      "Fold: 4  Epoch: 303  Training loss = 1.3336  Validation loss = 2.9423  \n",
      "\n",
      "Fold: 4  Epoch: 304  Training loss = 1.3339  Validation loss = 2.9347  \n",
      "\n",
      "Fold: 4  Epoch: 305  Training loss = 1.3344  Validation loss = 2.9267  \n",
      "\n",
      "Fold: 4  Epoch: 306  Training loss = 1.3337  Validation loss = 2.9261  \n",
      "\n",
      "Fold: 4  Epoch: 307  Training loss = 1.3323  Validation loss = 2.9358  \n",
      "\n",
      "Fold: 4  Epoch: 308  Training loss = 1.3325  Validation loss = 2.9256  \n",
      "\n",
      "Fold: 4  Epoch: 309  Training loss = 1.3311  Validation loss = 2.9333  \n",
      "\n",
      "Fold: 4  Epoch: 310  Training loss = 1.3311  Validation loss = 2.9321  \n",
      "\n",
      "Fold: 4  Epoch: 311  Training loss = 1.3307  Validation loss = 2.9279  \n",
      "\n",
      "Fold: 4  Epoch: 312  Training loss = 1.3302  Validation loss = 2.9288  \n",
      "\n",
      "Fold: 4  Epoch: 313  Training loss = 1.3294  Validation loss = 2.9290  \n",
      "\n",
      "Fold: 4  Epoch: 314  Training loss = 1.3291  Validation loss = 2.9268  \n",
      "\n",
      "Fold: 4  Epoch: 315  Training loss = 1.3285  Validation loss = 2.9293  \n",
      "\n",
      "Fold: 4  Epoch: 316  Training loss = 1.3283  Validation loss = 2.9308  \n",
      "\n",
      "Fold: 4  Epoch: 317  Training loss = 1.3281  Validation loss = 2.9368  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 308  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.4408  Validation loss = 2.5924  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.4370  Validation loss = 2.5682  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.4361  Validation loss = 2.5644  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.4349  Validation loss = 2.5633  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.4327  Validation loss = 2.5473  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 1.4297  Validation loss = 2.5328  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 1.4274  Validation loss = 2.5220  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.4255  Validation loss = 2.5135  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 1.4232  Validation loss = 2.5059  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.4216  Validation loss = 2.4995  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 1.4205  Validation loss = 2.5010  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 1.4187  Validation loss = 2.4877  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 1.4171  Validation loss = 2.4819  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.4166  Validation loss = 2.4841  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 1.4152  Validation loss = 2.4790  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 1.4149  Validation loss = 2.4826  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 1.4118  Validation loss = 2.4573  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 1.4098  Validation loss = 2.4465  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 1.4079  Validation loss = 2.4386  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 1.4058  Validation loss = 2.4305  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 1.4049  Validation loss = 2.4294  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 1.4045  Validation loss = 2.4333  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 1.4036  Validation loss = 2.4339  \n",
      "\n",
      "Fold: 5  Epoch: 24  Training loss = 1.4020  Validation loss = 2.4234  \n",
      "\n",
      "Fold: 5  Epoch: 25  Training loss = 1.3993  Validation loss = 2.4055  \n",
      "\n",
      "Fold: 5  Epoch: 26  Training loss = 1.3979  Validation loss = 2.4032  \n",
      "\n",
      "Fold: 5  Epoch: 27  Training loss = 1.3963  Validation loss = 2.4041  \n",
      "\n",
      "Fold: 5  Epoch: 28  Training loss = 1.3949  Validation loss = 2.3962  \n",
      "\n",
      "Fold: 5  Epoch: 29  Training loss = 1.3938  Validation loss = 2.3928  \n",
      "\n",
      "Fold: 5  Epoch: 30  Training loss = 1.3938  Validation loss = 2.3983  \n",
      "\n",
      "Fold: 5  Epoch: 31  Training loss = 1.3931  Validation loss = 2.3986  \n",
      "\n",
      "Fold: 5  Epoch: 32  Training loss = 1.3926  Validation loss = 2.4012  \n",
      "\n",
      "Fold: 5  Epoch: 33  Training loss = 1.3922  Validation loss = 2.4009  \n",
      "\n",
      "Fold: 5  Epoch: 34  Training loss = 1.3901  Validation loss = 2.3879  \n",
      "\n",
      "Fold: 5  Epoch: 35  Training loss = 1.3881  Validation loss = 2.3757  \n",
      "\n",
      "Fold: 5  Epoch: 36  Training loss = 1.3872  Validation loss = 2.3726  \n",
      "\n",
      "Fold: 5  Epoch: 37  Training loss = 1.3849  Validation loss = 2.3613  \n",
      "\n",
      "Fold: 5  Epoch: 38  Training loss = 1.3827  Validation loss = 2.3416  \n",
      "\n",
      "Fold: 5  Epoch: 39  Training loss = 1.3813  Validation loss = 2.3277  \n",
      "\n",
      "Fold: 5  Epoch: 40  Training loss = 1.3799  Validation loss = 2.3141  \n",
      "\n",
      "Fold: 5  Epoch: 41  Training loss = 1.3787  Validation loss = 2.3086  \n",
      "\n",
      "Fold: 5  Epoch: 42  Training loss = 1.3771  Validation loss = 2.2976  \n",
      "\n",
      "Fold: 5  Epoch: 43  Training loss = 1.3760  Validation loss = 2.2838  \n",
      "\n",
      "Fold: 5  Epoch: 44  Training loss = 1.3744  Validation loss = 2.2868  \n",
      "\n",
      "Fold: 5  Epoch: 45  Training loss = 1.3730  Validation loss = 2.2715  \n",
      "\n",
      "Fold: 5  Epoch: 46  Training loss = 1.3723  Validation loss = 2.2692  \n",
      "\n",
      "Fold: 5  Epoch: 47  Training loss = 1.3709  Validation loss = 2.2618  \n",
      "\n",
      "Fold: 5  Epoch: 48  Training loss = 1.3698  Validation loss = 2.2670  \n",
      "\n",
      "Fold: 5  Epoch: 49  Training loss = 1.3689  Validation loss = 2.2659  \n",
      "\n",
      "Fold: 5  Epoch: 50  Training loss = 1.3685  Validation loss = 2.2693  \n",
      "\n",
      "Fold: 5  Epoch: 51  Training loss = 1.3679  Validation loss = 2.2731  \n",
      "\n",
      "Fold: 5  Epoch: 52  Training loss = 1.3660  Validation loss = 2.2614  \n",
      "\n",
      "Fold: 5  Epoch: 53  Training loss = 1.3648  Validation loss = 2.2585  \n",
      "\n",
      "Fold: 5  Epoch: 54  Training loss = 1.3626  Validation loss = 2.2493  \n",
      "\n",
      "Fold: 5  Epoch: 55  Training loss = 1.3614  Validation loss = 2.2473  \n",
      "\n",
      "Fold: 5  Epoch: 56  Training loss = 1.3608  Validation loss = 2.2462  \n",
      "\n",
      "Fold: 5  Epoch: 57  Training loss = 1.3592  Validation loss = 2.2293  \n",
      "\n",
      "Fold: 5  Epoch: 58  Training loss = 1.3580  Validation loss = 2.2171  \n",
      "\n",
      "Fold: 5  Epoch: 59  Training loss = 1.3562  Validation loss = 2.1981  \n",
      "\n",
      "Fold: 5  Epoch: 60  Training loss = 1.3550  Validation loss = 2.2028  \n",
      "\n",
      "Fold: 5  Epoch: 61  Training loss = 1.3536  Validation loss = 2.2066  \n",
      "\n",
      "Fold: 5  Epoch: 62  Training loss = 1.3523  Validation loss = 2.1920  \n",
      "\n",
      "Fold: 5  Epoch: 63  Training loss = 1.3520  Validation loss = 2.1882  \n",
      "\n",
      "Fold: 5  Epoch: 64  Training loss = 1.3511  Validation loss = 2.1852  \n",
      "\n",
      "Fold: 5  Epoch: 65  Training loss = 1.3504  Validation loss = 2.1664  \n",
      "\n",
      "Fold: 5  Epoch: 66  Training loss = 1.3490  Validation loss = 2.1602  \n",
      "\n",
      "Fold: 5  Epoch: 67  Training loss = 1.3481  Validation loss = 2.1574  \n",
      "\n",
      "Fold: 5  Epoch: 68  Training loss = 1.3465  Validation loss = 2.1496  \n",
      "\n",
      "Fold: 5  Epoch: 69  Training loss = 1.3450  Validation loss = 2.1592  \n",
      "\n",
      "Fold: 5  Epoch: 70  Training loss = 1.3436  Validation loss = 2.1561  \n",
      "\n",
      "Fold: 5  Epoch: 71  Training loss = 1.3423  Validation loss = 2.1511  \n",
      "\n",
      "Fold: 5  Epoch: 72  Training loss = 1.3415  Validation loss = 2.1585  \n",
      "\n",
      "Fold: 5  Epoch: 73  Training loss = 1.3404  Validation loss = 2.1519  \n",
      "\n",
      "Fold: 5  Epoch: 74  Training loss = 1.3395  Validation loss = 2.1451  \n",
      "\n",
      "Fold: 5  Epoch: 75  Training loss = 1.3383  Validation loss = 2.1338  \n",
      "\n",
      "Fold: 5  Epoch: 76  Training loss = 1.3371  Validation loss = 2.1345  \n",
      "\n",
      "Fold: 5  Epoch: 77  Training loss = 1.3360  Validation loss = 2.1311  \n",
      "\n",
      "Fold: 5  Epoch: 78  Training loss = 1.3350  Validation loss = 2.1316  \n",
      "\n",
      "Fold: 5  Epoch: 79  Training loss = 1.3343  Validation loss = 2.1297  \n",
      "\n",
      "Fold: 5  Epoch: 80  Training loss = 1.3331  Validation loss = 2.1192  \n",
      "\n",
      "Fold: 5  Epoch: 81  Training loss = 1.3325  Validation loss = 2.1209  \n",
      "\n",
      "Fold: 5  Epoch: 82  Training loss = 1.3326  Validation loss = 2.1259  \n",
      "\n",
      "Fold: 5  Epoch: 83  Training loss = 1.3330  Validation loss = 2.1345  \n",
      "\n",
      "Fold: 5  Epoch: 84  Training loss = 1.3322  Validation loss = 2.1299  \n",
      "\n",
      "Fold: 5  Epoch: 85  Training loss = 1.3298  Validation loss = 2.1123  \n",
      "\n",
      "Fold: 5  Epoch: 86  Training loss = 1.3286  Validation loss = 2.1046  \n",
      "\n",
      "Fold: 5  Epoch: 87  Training loss = 1.3278  Validation loss = 2.1028  \n",
      "\n",
      "Fold: 5  Epoch: 88  Training loss = 1.3262  Validation loss = 2.0917  \n",
      "\n",
      "Fold: 5  Epoch: 89  Training loss = 1.3261  Validation loss = 2.0941  \n",
      "\n",
      "Fold: 5  Epoch: 90  Training loss = 1.3265  Validation loss = 2.1022  \n",
      "\n",
      "Fold: 5  Epoch: 91  Training loss = 1.3242  Validation loss = 2.0851  \n",
      "\n",
      "Fold: 5  Epoch: 92  Training loss = 1.3239  Validation loss = 2.0890  \n",
      "\n",
      "Fold: 5  Epoch: 93  Training loss = 1.3228  Validation loss = 2.0816  \n",
      "\n",
      "Fold: 5  Epoch: 94  Training loss = 1.3221  Validation loss = 2.0750  \n",
      "\n",
      "Fold: 5  Epoch: 95  Training loss = 1.3206  Validation loss = 2.0655  \n",
      "\n",
      "Fold: 5  Epoch: 96  Training loss = 1.3199  Validation loss = 2.0623  \n",
      "\n",
      "Fold: 5  Epoch: 97  Training loss = 1.3195  Validation loss = 2.0628  \n",
      "\n",
      "Fold: 5  Epoch: 98  Training loss = 1.3178  Validation loss = 2.0511  \n",
      "\n",
      "Fold: 5  Epoch: 99  Training loss = 1.3178  Validation loss = 2.0553  \n",
      "\n",
      "Fold: 5  Epoch: 100  Training loss = 1.3162  Validation loss = 2.0426  \n",
      "\n",
      "Fold: 5  Epoch: 101  Training loss = 1.3146  Validation loss = 2.0189  \n",
      "\n",
      "Fold: 5  Epoch: 102  Training loss = 1.3134  Validation loss = 2.0170  \n",
      "\n",
      "Fold: 5  Epoch: 103  Training loss = 1.3119  Validation loss = 2.0130  \n",
      "\n",
      "Fold: 5  Epoch: 104  Training loss = 1.3110  Validation loss = 2.0102  \n",
      "\n",
      "Fold: 5  Epoch: 105  Training loss = 1.3109  Validation loss = 2.0141  \n",
      "\n",
      "Fold: 5  Epoch: 106  Training loss = 1.3097  Validation loss = 2.0051  \n",
      "\n",
      "Fold: 5  Epoch: 107  Training loss = 1.3090  Validation loss = 2.0016  \n",
      "\n",
      "Fold: 5  Epoch: 108  Training loss = 1.3085  Validation loss = 1.9980  \n",
      "\n",
      "Fold: 5  Epoch: 109  Training loss = 1.3077  Validation loss = 1.9932  \n",
      "\n",
      "Fold: 5  Epoch: 110  Training loss = 1.3066  Validation loss = 1.9879  \n",
      "\n",
      "Fold: 5  Epoch: 111  Training loss = 1.3056  Validation loss = 1.9850  \n",
      "\n",
      "Fold: 5  Epoch: 112  Training loss = 1.3044  Validation loss = 1.9730  \n",
      "\n",
      "Fold: 5  Epoch: 113  Training loss = 1.3044  Validation loss = 1.9861  \n",
      "\n",
      "Fold: 5  Epoch: 114  Training loss = 1.3035  Validation loss = 1.9801  \n",
      "\n",
      "Fold: 5  Epoch: 115  Training loss = 1.3031  Validation loss = 1.9777  \n",
      "\n",
      "Fold: 5  Epoch: 116  Training loss = 1.3022  Validation loss = 1.9679  \n",
      "\n",
      "Fold: 5  Epoch: 117  Training loss = 1.3012  Validation loss = 1.9580  \n",
      "\n",
      "Fold: 5  Epoch: 118  Training loss = 1.3006  Validation loss = 1.9650  \n",
      "\n",
      "Fold: 5  Epoch: 119  Training loss = 1.2997  Validation loss = 1.9603  \n",
      "\n",
      "Fold: 5  Epoch: 120  Training loss = 1.2987  Validation loss = 1.9542  \n",
      "\n",
      "Fold: 5  Epoch: 121  Training loss = 1.2975  Validation loss = 1.9432  \n",
      "\n",
      "Fold: 5  Epoch: 122  Training loss = 1.2968  Validation loss = 1.9467  \n",
      "\n",
      "Fold: 5  Epoch: 123  Training loss = 1.2960  Validation loss = 1.9434  \n",
      "\n",
      "Fold: 5  Epoch: 124  Training loss = 1.2963  Validation loss = 1.9490  \n",
      "\n",
      "Fold: 5  Epoch: 125  Training loss = 1.2955  Validation loss = 1.9474  \n",
      "\n",
      "Fold: 5  Epoch: 126  Training loss = 1.2951  Validation loss = 1.9470  \n",
      "\n",
      "Fold: 5  Epoch: 127  Training loss = 1.2941  Validation loss = 1.9418  \n",
      "\n",
      "Fold: 5  Epoch: 128  Training loss = 1.2931  Validation loss = 1.9320  \n",
      "\n",
      "Fold: 5  Epoch: 129  Training loss = 1.2934  Validation loss = 1.9358  \n",
      "\n",
      "Fold: 5  Epoch: 130  Training loss = 1.2917  Validation loss = 1.9201  \n",
      "\n",
      "Fold: 5  Epoch: 131  Training loss = 1.2910  Validation loss = 1.9178  \n",
      "\n",
      "Fold: 5  Epoch: 132  Training loss = 1.2902  Validation loss = 1.9121  \n",
      "\n",
      "Fold: 5  Epoch: 133  Training loss = 1.2897  Validation loss = 1.9113  \n",
      "\n",
      "Fold: 5  Epoch: 134  Training loss = 1.2885  Validation loss = 1.8935  \n",
      "\n",
      "Fold: 5  Epoch: 135  Training loss = 1.2885  Validation loss = 1.8892  \n",
      "\n",
      "Fold: 5  Epoch: 136  Training loss = 1.2874  Validation loss = 1.8791  \n",
      "\n",
      "Fold: 5  Epoch: 137  Training loss = 1.2869  Validation loss = 1.8668  \n",
      "\n",
      "Fold: 5  Epoch: 138  Training loss = 1.2869  Validation loss = 1.8571  \n",
      "\n",
      "Fold: 5  Epoch: 139  Training loss = 1.2862  Validation loss = 1.8533  \n",
      "\n",
      "Fold: 5  Epoch: 140  Training loss = 1.2856  Validation loss = 1.8476  \n",
      "\n",
      "Fold: 5  Epoch: 141  Training loss = 1.2850  Validation loss = 1.8528  \n",
      "\n",
      "Fold: 5  Epoch: 142  Training loss = 1.2842  Validation loss = 1.8465  \n",
      "\n",
      "Fold: 5  Epoch: 143  Training loss = 1.2839  Validation loss = 1.8447  \n",
      "\n",
      "Fold: 5  Epoch: 144  Training loss = 1.2827  Validation loss = 1.8440  \n",
      "\n",
      "Fold: 5  Epoch: 145  Training loss = 1.2818  Validation loss = 1.8313  \n",
      "\n",
      "Fold: 5  Epoch: 146  Training loss = 1.2821  Validation loss = 1.8145  \n",
      "\n",
      "Fold: 5  Epoch: 147  Training loss = 1.2800  Validation loss = 1.8169  \n",
      "\n",
      "Fold: 5  Epoch: 148  Training loss = 1.2794  Validation loss = 1.8206  \n",
      "\n",
      "Fold: 5  Epoch: 149  Training loss = 1.2788  Validation loss = 1.8205  \n",
      "\n",
      "Fold: 5  Epoch: 150  Training loss = 1.2776  Validation loss = 1.8205  \n",
      "\n",
      "Fold: 5  Epoch: 151  Training loss = 1.2771  Validation loss = 1.8199  \n",
      "\n",
      "Fold: 5  Epoch: 152  Training loss = 1.2761  Validation loss = 1.8169  \n",
      "\n",
      "Fold: 5  Epoch: 153  Training loss = 1.2751  Validation loss = 1.8158  \n",
      "\n",
      "Fold: 5  Epoch: 154  Training loss = 1.2742  Validation loss = 1.8136  \n",
      "\n",
      "Fold: 5  Epoch: 155  Training loss = 1.2740  Validation loss = 1.8212  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 154  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 1.3150  Validation loss = 0.6934  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 1.3135  Validation loss = 0.6920  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 1.3109  Validation loss = 0.6977  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 1.3084  Validation loss = 0.7050  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 1.3045  Validation loss = 0.7163  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 1.3023  Validation loss = 0.7257  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 1.2989  Validation loss = 0.7375  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 1.2973  Validation loss = 0.7404  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 1.2947  Validation loss = 0.7469  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 1.2915  Validation loss = 0.7636  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 1.2890  Validation loss = 0.7743  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 2  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 1.2579  Validation loss = 0.7712  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 1.2577  Validation loss = 0.7698  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 1.2564  Validation loss = 0.7705  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.2546  Validation loss = 0.7698  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 1.2535  Validation loss = 0.7684  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 1.2519  Validation loss = 0.7674  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 1.2504  Validation loss = 0.7672  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 1.2490  Validation loss = 0.7675  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 1.2481  Validation loss = 0.7720  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 1.2474  Validation loss = 0.7805  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 1.2463  Validation loss = 0.7742  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 1.2456  Validation loss = 0.7759  \n",
      "\n",
      "Fold: 7  Epoch: 13  Training loss = 1.2442  Validation loss = 0.7757  \n",
      "\n",
      "Fold: 7  Epoch: 14  Training loss = 1.2435  Validation loss = 0.7686  \n",
      "\n",
      "Fold: 7  Epoch: 15  Training loss = 1.2421  Validation loss = 0.7705  \n",
      "\n",
      "Fold: 7  Epoch: 16  Training loss = 1.2410  Validation loss = 0.7772  \n",
      "\n",
      "Fold: 7  Epoch: 17  Training loss = 1.2403  Validation loss = 0.7785  \n",
      "\n",
      "Fold: 7  Epoch: 18  Training loss = 1.2392  Validation loss = 0.7783  \n",
      "\n",
      "Fold: 7  Epoch: 19  Training loss = 1.2381  Validation loss = 0.7810  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 7  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 1.1708  Validation loss = 4.8261  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 1.1698  Validation loss = 4.8373  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 1.1691  Validation loss = 4.8286  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 1.1682  Validation loss = 4.8254  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 1.1673  Validation loss = 4.8225  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 1.1667  Validation loss = 4.8073  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 1.1664  Validation loss = 4.8043  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 1.1656  Validation loss = 4.8069  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 1.1644  Validation loss = 4.7970  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 1.1636  Validation loss = 4.7950  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 1.1630  Validation loss = 4.8036  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 1.1623  Validation loss = 4.7851  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 1.1626  Validation loss = 4.7778  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 1.1620  Validation loss = 4.7767  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 1.1609  Validation loss = 4.7859  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 1.1605  Validation loss = 4.7830  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 1.1593  Validation loss = 4.7781  \n",
      "\n",
      "Fold: 8  Epoch: 18  Training loss = 1.1584  Validation loss = 4.7977  \n",
      "\n",
      "Fold: 8  Epoch: 19  Training loss = 1.1571  Validation loss = 4.7834  \n",
      "\n",
      "Fold: 8  Epoch: 20  Training loss = 1.1565  Validation loss = 4.7957  \n",
      "\n",
      "Fold: 8  Epoch: 21  Training loss = 1.1559  Validation loss = 4.7702  \n",
      "\n",
      "Fold: 8  Epoch: 22  Training loss = 1.1547  Validation loss = 4.7730  \n",
      "\n",
      "Fold: 8  Epoch: 23  Training loss = 1.1542  Validation loss = 4.7808  \n",
      "\n",
      "Fold: 8  Epoch: 24  Training loss = 1.1541  Validation loss = 4.8062  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 21  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 1.6419  Validation loss = 8.0160  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 1.6426  Validation loss = 8.0119  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 1.6380  Validation loss = 7.9883  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 1.6280  Validation loss = 7.9563  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 1.6237  Validation loss = 7.9468  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 1.6202  Validation loss = 7.9218  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 1.6156  Validation loss = 7.8990  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 1.6120  Validation loss = 7.8741  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 1.6088  Validation loss = 7.8577  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 1.6087  Validation loss = 7.8602  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 1.6053  Validation loss = 7.8375  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 1.6019  Validation loss = 7.8061  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 1.5979  Validation loss = 7.7794  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 1.5967  Validation loss = 7.7760  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 1.5947  Validation loss = 7.7729  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 1.5926  Validation loss = 7.7456  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 1.5907  Validation loss = 7.7346  \n",
      "\n",
      "Fold: 9  Epoch: 18  Training loss = 1.5894  Validation loss = 7.7278  \n",
      "\n",
      "Fold: 9  Epoch: 19  Training loss = 1.5875  Validation loss = 7.7160  \n",
      "\n",
      "Fold: 9  Epoch: 20  Training loss = 1.5853  Validation loss = 7.6883  \n",
      "\n",
      "Fold: 9  Epoch: 21  Training loss = 1.5837  Validation loss = 7.6858  \n",
      "\n",
      "Fold: 9  Epoch: 22  Training loss = 1.5806  Validation loss = 7.6724  \n",
      "\n",
      "Fold: 9  Epoch: 23  Training loss = 1.5788  Validation loss = 7.6679  \n",
      "\n",
      "Fold: 9  Epoch: 24  Training loss = 1.5767  Validation loss = 7.6611  \n",
      "\n",
      "Fold: 9  Epoch: 25  Training loss = 1.5753  Validation loss = 7.6599  \n",
      "\n",
      "Fold: 9  Epoch: 26  Training loss = 1.5742  Validation loss = 7.6531  \n",
      "\n",
      "Fold: 9  Epoch: 27  Training loss = 1.5716  Validation loss = 7.6347  \n",
      "\n",
      "Fold: 9  Epoch: 28  Training loss = 1.5712  Validation loss = 7.6524  \n",
      "\n",
      "Fold: 9  Epoch: 29  Training loss = 1.5706  Validation loss = 7.6576  \n",
      "\n",
      "Fold: 9  Epoch: 30  Training loss = 1.5705  Validation loss = 7.6831  \n",
      "\n",
      "Fold: 9  Epoch: 31  Training loss = 1.5672  Validation loss = 7.6610  \n",
      "\n",
      "Fold: 9  Epoch: 32  Training loss = 1.5664  Validation loss = 7.6864  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 27  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 2.3949  Validation loss = 3.3207  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 2.3739  Validation loss = 3.3089  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 2.3534  Validation loss = 3.2224  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 2.3369  Validation loss = 3.1598  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 2.3241  Validation loss = 3.1290  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 2.3121  Validation loss = 3.0960  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 2.3029  Validation loss = 3.0724  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 2.2943  Validation loss = 3.0349  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 2.2843  Validation loss = 3.0150  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 2.2749  Validation loss = 2.9592  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 2.2474  Validation loss = 2.7208  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 2.2389  Validation loss = 2.6991  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 2.2294  Validation loss = 2.6584  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 2.2217  Validation loss = 2.6310  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 2.2156  Validation loss = 2.6190  \n",
      "\n",
      "Fold: 10  Epoch: 16  Training loss = 2.2083  Validation loss = 2.5968  \n",
      "\n",
      "Fold: 10  Epoch: 17  Training loss = 2.1983  Validation loss = 2.5688  \n",
      "\n",
      "Fold: 10  Epoch: 18  Training loss = 2.1930  Validation loss = 2.5520  \n",
      "\n",
      "Fold: 10  Epoch: 19  Training loss = 2.1886  Validation loss = 2.5455  \n",
      "\n",
      "Fold: 10  Epoch: 20  Training loss = 2.1818  Validation loss = 2.5302  \n",
      "\n",
      "Fold: 10  Epoch: 21  Training loss = 2.1725  Validation loss = 2.4953  \n",
      "\n",
      "Fold: 10  Epoch: 22  Training loss = 2.1650  Validation loss = 2.4487  \n",
      "\n",
      "Fold: 10  Epoch: 23  Training loss = 2.1622  Validation loss = 2.4490  \n",
      "\n",
      "Fold: 10  Epoch: 24  Training loss = 2.1570  Validation loss = 2.4298  \n",
      "\n",
      "Fold: 10  Epoch: 25  Training loss = 2.1536  Validation loss = 2.4306  \n",
      "\n",
      "Fold: 10  Epoch: 26  Training loss = 2.1453  Validation loss = 2.4043  \n",
      "\n",
      "Fold: 10  Epoch: 27  Training loss = 2.1403  Validation loss = 2.3883  \n",
      "\n",
      "Fold: 10  Epoch: 28  Training loss = 2.1381  Validation loss = 2.3847  \n",
      "\n",
      "Fold: 10  Epoch: 29  Training loss = 2.1313  Validation loss = 2.3614  \n",
      "\n",
      "Fold: 10  Epoch: 30  Training loss = 2.1246  Validation loss = 2.3520  \n",
      "\n",
      "Fold: 10  Epoch: 31  Training loss = 2.1215  Validation loss = 2.3469  \n",
      "\n",
      "Fold: 10  Epoch: 32  Training loss = 2.1159  Validation loss = 2.3299  \n",
      "\n",
      "Fold: 10  Epoch: 33  Training loss = 2.1061  Validation loss = 2.2949  \n",
      "\n",
      "Fold: 10  Epoch: 34  Training loss = 2.1008  Validation loss = 2.2813  \n",
      "\n",
      "Fold: 10  Epoch: 35  Training loss = 2.0974  Validation loss = 2.2794  \n",
      "\n",
      "Fold: 10  Epoch: 36  Training loss = 2.0950  Validation loss = 2.2755  \n",
      "\n",
      "Fold: 10  Epoch: 37  Training loss = 2.0932  Validation loss = 2.2743  \n",
      "\n",
      "Fold: 10  Epoch: 38  Training loss = 2.0888  Validation loss = 2.2487  \n",
      "\n",
      "Fold: 10  Epoch: 39  Training loss = 2.0819  Validation loss = 2.2361  \n",
      "\n",
      "Fold: 10  Epoch: 40  Training loss = 2.0827  Validation loss = 2.2657  \n",
      "\n",
      "Fold: 10  Epoch: 41  Training loss = 2.0817  Validation loss = 2.2412  \n",
      "\n",
      "Fold: 10  Epoch: 42  Training loss = 2.0734  Validation loss = 2.2123  \n",
      "\n",
      "Fold: 10  Epoch: 43  Training loss = 2.0683  Validation loss = 2.1985  \n",
      "\n",
      "Fold: 10  Epoch: 44  Training loss = 2.0699  Validation loss = 2.2095  \n",
      "\n",
      "Fold: 10  Epoch: 45  Training loss = 2.0647  Validation loss = 2.1878  \n",
      "\n",
      "Fold: 10  Epoch: 46  Training loss = 2.0656  Validation loss = 2.1973  \n",
      "\n",
      "Fold: 10  Epoch: 47  Training loss = 2.0625  Validation loss = 2.1986  \n",
      "\n",
      "Fold: 10  Epoch: 48  Training loss = 2.0615  Validation loss = 2.2092  \n",
      "\n",
      "Fold: 10  Epoch: 49  Training loss = 2.0568  Validation loss = 2.2015  \n",
      "\n",
      "Fold: 10  Epoch: 50  Training loss = 2.0497  Validation loss = 2.1767  \n",
      "\n",
      "Fold: 10  Epoch: 51  Training loss = 2.0408  Validation loss = 2.1465  \n",
      "\n",
      "Fold: 10  Epoch: 52  Training loss = 2.0348  Validation loss = 2.1260  \n",
      "\n",
      "Fold: 10  Epoch: 53  Training loss = 2.0326  Validation loss = 2.1217  \n",
      "\n",
      "Fold: 10  Epoch: 54  Training loss = 2.0284  Validation loss = 2.0953  \n",
      "\n",
      "Fold: 10  Epoch: 55  Training loss = 2.0181  Validation loss = 2.0885  \n",
      "\n",
      "Fold: 10  Epoch: 56  Training loss = 2.0154  Validation loss = 2.0977  \n",
      "\n",
      "Fold: 10  Epoch: 57  Training loss = 2.0090  Validation loss = 2.0323  \n",
      "\n",
      "Fold: 10  Epoch: 58  Training loss = 2.0025  Validation loss = 2.0500  \n",
      "\n",
      "Fold: 10  Epoch: 59  Training loss = 1.9987  Validation loss = 2.0429  \n",
      "\n",
      "Fold: 10  Epoch: 60  Training loss = 1.9942  Validation loss = 2.0259  \n",
      "\n",
      "Fold: 10  Epoch: 61  Training loss = 1.9916  Validation loss = 2.0278  \n",
      "\n",
      "Fold: 10  Epoch: 62  Training loss = 1.9894  Validation loss = 2.0273  \n",
      "\n",
      "Fold: 10  Epoch: 63  Training loss = 1.9872  Validation loss = 2.0243  \n",
      "\n",
      "Fold: 10  Epoch: 64  Training loss = 1.9804  Validation loss = 2.0049  \n",
      "\n",
      "Fold: 10  Epoch: 65  Training loss = 1.9750  Validation loss = 1.9955  \n",
      "\n",
      "Fold: 10  Epoch: 66  Training loss = 1.9721  Validation loss = 1.9831  \n",
      "\n",
      "Fold: 10  Epoch: 67  Training loss = 1.9678  Validation loss = 1.9798  \n",
      "\n",
      "Fold: 10  Epoch: 68  Training loss = 1.9652  Validation loss = 1.9785  \n",
      "\n",
      "Fold: 10  Epoch: 69  Training loss = 1.9624  Validation loss = 1.9593  \n",
      "\n",
      "Fold: 10  Epoch: 70  Training loss = 1.9593  Validation loss = 1.9476  \n",
      "\n",
      "Fold: 10  Epoch: 71  Training loss = 1.9522  Validation loss = 1.8803  \n",
      "\n",
      "Fold: 10  Epoch: 72  Training loss = 1.9456  Validation loss = 1.8468  \n",
      "\n",
      "Fold: 10  Epoch: 73  Training loss = 1.9408  Validation loss = 1.8188  \n",
      "\n",
      "Fold: 10  Epoch: 74  Training loss = 1.9362  Validation loss = 1.8135  \n",
      "\n",
      "Fold: 10  Epoch: 75  Training loss = 1.9328  Validation loss = 1.8073  \n",
      "\n",
      "Fold: 10  Epoch: 76  Training loss = 1.9309  Validation loss = 1.8031  \n",
      "\n",
      "Fold: 10  Epoch: 77  Training loss = 1.9482  Validation loss = 1.8124  \n",
      "\n",
      "Fold: 10  Epoch: 78  Training loss = 1.9459  Validation loss = 1.8187  \n",
      "\n",
      "Fold: 10  Epoch: 79  Training loss = 1.9434  Validation loss = 1.8208  \n",
      "\n",
      "Fold: 10  Epoch: 80  Training loss = 1.9411  Validation loss = 1.8170  \n",
      "\n",
      "Fold: 10  Epoch: 81  Training loss = 1.9374  Validation loss = 1.7932  \n",
      "\n",
      "Fold: 10  Epoch: 82  Training loss = 1.9358  Validation loss = 1.7938  \n",
      "\n",
      "Fold: 10  Epoch: 83  Training loss = 1.9325  Validation loss = 1.7986  \n",
      "\n",
      "Fold: 10  Epoch: 84  Training loss = 1.9288  Validation loss = 1.7987  \n",
      "\n",
      "Fold: 10  Epoch: 85  Training loss = 1.9257  Validation loss = 1.8110  \n",
      "\n",
      "Fold: 10  Epoch: 86  Training loss = 1.9197  Validation loss = 1.7821  \n",
      "\n",
      "Fold: 10  Epoch: 87  Training loss = 1.9158  Validation loss = 1.7790  \n",
      "\n",
      "Fold: 10  Epoch: 88  Training loss = 1.9113  Validation loss = 1.7810  \n",
      "\n",
      "Fold: 10  Epoch: 89  Training loss = 1.9065  Validation loss = 1.7730  \n",
      "\n",
      "Fold: 10  Epoch: 90  Training loss = 1.9023  Validation loss = 1.7814  \n",
      "\n",
      "Fold: 10  Epoch: 91  Training loss = 1.9004  Validation loss = 1.7796  \n",
      "\n",
      "Fold: 10  Epoch: 92  Training loss = 1.8952  Validation loss = 1.7587  \n",
      "\n",
      "Fold: 10  Epoch: 93  Training loss = 1.8888  Validation loss = 1.7663  \n",
      "\n",
      "Fold: 10  Epoch: 94  Training loss = 1.8837  Validation loss = 1.6926  \n",
      "\n",
      "Fold: 10  Epoch: 95  Training loss = 1.8837  Validation loss = 1.7002  \n",
      "\n",
      "Fold: 10  Epoch: 96  Training loss = 1.8805  Validation loss = 1.6967  \n",
      "\n",
      "Fold: 10  Epoch: 97  Training loss = 1.8750  Validation loss = 1.6972  \n",
      "\n",
      "Fold: 10  Epoch: 98  Training loss = 1.8739  Validation loss = 1.6942  \n",
      "\n",
      "Fold: 10  Epoch: 99  Training loss = 1.8706  Validation loss = 1.6870  \n",
      "\n",
      "Fold: 10  Epoch: 100  Training loss = 1.8665  Validation loss = 1.6902  \n",
      "\n",
      "Fold: 10  Epoch: 101  Training loss = 1.8660  Validation loss = 1.6994  \n",
      "\n",
      "Fold: 10  Epoch: 102  Training loss = 1.8605  Validation loss = 1.6784  \n",
      "\n",
      "Fold: 10  Epoch: 103  Training loss = 1.8601  Validation loss = 1.6874  \n",
      "\n",
      "Fold: 10  Epoch: 104  Training loss = 1.8554  Validation loss = 1.6695  \n",
      "\n",
      "Fold: 10  Epoch: 105  Training loss = 1.8551  Validation loss = 1.6742  \n",
      "\n",
      "Fold: 10  Epoch: 106  Training loss = 1.8502  Validation loss = 1.6584  \n",
      "\n",
      "Fold: 10  Epoch: 107  Training loss = 1.8480  Validation loss = 1.6617  \n",
      "\n",
      "Fold: 10  Epoch: 108  Training loss = 1.8475  Validation loss = 1.6555  \n",
      "\n",
      "Fold: 10  Epoch: 109  Training loss = 1.8456  Validation loss = 1.6333  \n",
      "\n",
      "Fold: 10  Epoch: 110  Training loss = 1.8423  Validation loss = 1.6296  \n",
      "\n",
      "Fold: 10  Epoch: 111  Training loss = 1.8343  Validation loss = 1.6220  \n",
      "\n",
      "Fold: 10  Epoch: 112  Training loss = 1.8307  Validation loss = 1.5695  \n",
      "\n",
      "Fold: 10  Epoch: 113  Training loss = 1.8291  Validation loss = 1.5893  \n",
      "\n",
      "Fold: 10  Epoch: 114  Training loss = 1.8247  Validation loss = 1.5928  \n",
      "\n",
      "Fold: 10  Epoch: 115  Training loss = 1.8241  Validation loss = 1.6011  \n",
      "\n",
      "Fold: 10  Epoch: 116  Training loss = 1.8228  Validation loss = 1.6056  \n",
      "\n",
      "Fold: 10  Epoch: 117  Training loss = 1.8204  Validation loss = 1.6102  \n",
      "\n",
      "Fold: 10  Epoch: 118  Training loss = 1.8180  Validation loss = 1.6015  \n",
      "\n",
      "Fold: 10  Epoch: 119  Training loss = 1.8164  Validation loss = 1.6064  \n",
      "\n",
      "Fold: 10  Epoch: 120  Training loss = 1.8161  Validation loss = 1.6097  \n",
      "\n",
      "Fold: 10  Epoch: 121  Training loss = 1.8147  Validation loss = 1.5980  \n",
      "\n",
      "Fold: 10  Epoch: 122  Training loss = 1.8114  Validation loss = 1.5958  \n",
      "\n",
      "Fold: 10  Epoch: 123  Training loss = 1.8099  Validation loss = 1.5374  \n",
      "\n",
      "Fold: 10  Epoch: 124  Training loss = 1.8080  Validation loss = 1.5453  \n",
      "\n",
      "Fold: 10  Epoch: 125  Training loss = 1.8069  Validation loss = 1.5264  \n",
      "\n",
      "Fold: 10  Epoch: 126  Training loss = 1.8044  Validation loss = 1.5319  \n",
      "\n",
      "Fold: 10  Epoch: 127  Training loss = 1.8083  Validation loss = 1.5291  \n",
      "\n",
      "Fold: 10  Epoch: 128  Training loss = 1.8094  Validation loss = 1.5304  \n",
      "\n",
      "Fold: 10  Epoch: 129  Training loss = 1.8042  Validation loss = 1.5385  \n",
      "\n",
      "Fold: 10  Epoch: 130  Training loss = 1.8030  Validation loss = 1.5369  \n",
      "\n",
      "Fold: 10  Epoch: 131  Training loss = 1.8021  Validation loss = 1.5417  \n",
      "\n",
      "Fold: 10  Epoch: 132  Training loss = 1.7979  Validation loss = 1.5459  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 125  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 1.8003  Validation loss = 1.3219  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 1.7955  Validation loss = 1.2645  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 1.7924  Validation loss = 1.3074  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 1.7917  Validation loss = 1.3102  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 1.7923  Validation loss = 1.3377  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 1.7879  Validation loss = 1.3211  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 1.7856  Validation loss = 1.3444  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 1.7842  Validation loss = 1.3366  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 1.7812  Validation loss = 1.3135  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 1.7797  Validation loss = 1.2979  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 1.7762  Validation loss = 1.3267  \n",
      "\n",
      "Fold: 11  Epoch: 12  Training loss = 1.7744  Validation loss = 1.3038  \n",
      "\n",
      "Fold: 11  Epoch: 13  Training loss = 1.7718  Validation loss = 1.3023  \n",
      "\n",
      "Fold: 11  Epoch: 14  Training loss = 1.7675  Validation loss = 1.3435  \n",
      "\n",
      "Fold: 11  Epoch: 15  Training loss = 1.7677  Validation loss = 1.3809  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 2  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 1.7660  Validation loss = 1.5566  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 1.7628  Validation loss = 1.5445  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 1.7618  Validation loss = 1.5092  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 1.7594  Validation loss = 1.4250  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 1.7614  Validation loss = 1.3374  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 1.7597  Validation loss = 1.3638  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 1.7552  Validation loss = 1.3784  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 1.7553  Validation loss = 1.2971  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 1.7532  Validation loss = 1.3515  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 1.7537  Validation loss = 1.3518  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 1.7533  Validation loss = 1.2993  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 1.7509  Validation loss = 1.3298  \n",
      "\n",
      "Fold: 12  Epoch: 13  Training loss = 1.7494  Validation loss = 1.3451  \n",
      "\n",
      "Fold: 12  Epoch: 14  Training loss = 1.7476  Validation loss = 1.3303  \n",
      "\n",
      "Fold: 12  Epoch: 15  Training loss = 1.7472  Validation loss = 1.4109  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 8  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 1.7480  Validation loss = 2.3636  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 1.7412  Validation loss = 2.2677  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 1.7379  Validation loss = 2.3318  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 1.7364  Validation loss = 2.3981  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 1.7354  Validation loss = 2.3893  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 1.7344  Validation loss = 2.4041  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 1.7424  Validation loss = 2.4009  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 1.7398  Validation loss = 2.4653  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 1.7351  Validation loss = 2.3774  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 1.7328  Validation loss = 2.4463  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 1.7303  Validation loss = 2.3199  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 1.7283  Validation loss = 2.3356  \n",
      "\n",
      "Fold: 13  Epoch: 13  Training loss = 1.7261  Validation loss = 2.3846  \n",
      "\n",
      "Fold: 13  Epoch: 14  Training loss = 1.7250  Validation loss = 2.3940  \n",
      "\n",
      "Fold: 13  Epoch: 15  Training loss = 1.7225  Validation loss = 2.3945  \n",
      "\n",
      "Fold: 13  Epoch: 16  Training loss = 1.7220  Validation loss = 2.4030  \n",
      "\n",
      "Fold: 13  Epoch: 17  Training loss = 1.7196  Validation loss = 2.3666  \n",
      "\n",
      "Fold: 13  Epoch: 18  Training loss = 1.7178  Validation loss = 2.3800  \n",
      "\n",
      "Fold: 13  Epoch: 19  Training loss = 1.7172  Validation loss = 2.2805  \n",
      "\n",
      "Fold: 13  Epoch: 20  Training loss = 1.7156  Validation loss = 2.2759  \n",
      "\n",
      "Fold: 13  Epoch: 21  Training loss = 1.7143  Validation loss = 2.2897  \n",
      "\n",
      "Fold: 13  Epoch: 22  Training loss = 1.7161  Validation loss = 2.3702  \n",
      "\n",
      "Fold: 13  Epoch: 23  Training loss = 1.7144  Validation loss = 2.3562  \n",
      "\n",
      "Fold: 13  Epoch: 24  Training loss = 1.7101  Validation loss = 2.3663  \n",
      "\n",
      "Fold: 13  Epoch: 25  Training loss = 1.7081  Validation loss = 2.3709  \n",
      "\n",
      "Fold: 13  Epoch: 26  Training loss = 1.7124  Validation loss = 2.4481  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 2  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 1.8039  Validation loss = 5.0010  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 1.7962  Validation loss = 4.9694  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 1.7933  Validation loss = 4.9265  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 1.7899  Validation loss = 4.8926  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 1.7858  Validation loss = 4.8777  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 1.7831  Validation loss = 4.8602  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 1.7829  Validation loss = 4.8547  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 1.7817  Validation loss = 4.8543  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 1.7809  Validation loss = 4.8142  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 1.7825  Validation loss = 4.8749  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 1.7791  Validation loss = 4.8488  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 1.7771  Validation loss = 4.8405  \n",
      "\n",
      "Fold: 14  Epoch: 13  Training loss = 1.7748  Validation loss = 4.8024  \n",
      "\n",
      "Fold: 14  Epoch: 14  Training loss = 1.7747  Validation loss = 4.8055  \n",
      "\n",
      "Fold: 14  Epoch: 15  Training loss = 1.7734  Validation loss = 4.8052  \n",
      "\n",
      "Fold: 14  Epoch: 16  Training loss = 1.7726  Validation loss = 4.8061  \n",
      "\n",
      "Fold: 14  Epoch: 17  Training loss = 1.7731  Validation loss = 4.7883  \n",
      "\n",
      "Fold: 14  Epoch: 18  Training loss = 1.7748  Validation loss = 4.7714  \n",
      "\n",
      "Fold: 14  Epoch: 19  Training loss = 1.7772  Validation loss = 4.8434  \n",
      "\n",
      "Fold: 14  Epoch: 20  Training loss = 1.7726  Validation loss = 4.8213  \n",
      "\n",
      "Fold: 14  Epoch: 21  Training loss = 1.7714  Validation loss = 4.7929  \n",
      "\n",
      "Fold: 14  Epoch: 22  Training loss = 1.7692  Validation loss = 4.8022  \n",
      "\n",
      "Fold: 14  Epoch: 23  Training loss = 1.7687  Validation loss = 4.8027  \n",
      "\n",
      "Fold: 14  Epoch: 24  Training loss = 1.7712  Validation loss = 4.8175  \n",
      "\n",
      "Fold: 14  Epoch: 25  Training loss = 1.7693  Validation loss = 4.7123  \n",
      "\n",
      "Fold: 14  Epoch: 26  Training loss = 1.7661  Validation loss = 4.7465  \n",
      "\n",
      "Fold: 14  Epoch: 27  Training loss = 1.7631  Validation loss = 4.7234  \n",
      "\n",
      "Fold: 14  Epoch: 28  Training loss = 1.7648  Validation loss = 4.7547  \n",
      "\n",
      "Fold: 14  Epoch: 29  Training loss = 1.7597  Validation loss = 4.7147  \n",
      "\n",
      "Fold: 14  Epoch: 30  Training loss = 1.7588  Validation loss = 4.7019  \n",
      "\n",
      "Fold: 14  Epoch: 31  Training loss = 1.7571  Validation loss = 4.6697  \n",
      "\n",
      "Fold: 14  Epoch: 32  Training loss = 1.7641  Validation loss = 4.7164  \n",
      "\n",
      "Fold: 14  Epoch: 33  Training loss = 1.7678  Validation loss = 4.7504  \n",
      "\n",
      "Fold: 14  Epoch: 34  Training loss = 1.7612  Validation loss = 4.7466  \n",
      "\n",
      "Fold: 14  Epoch: 35  Training loss = 1.7558  Validation loss = 4.7209  \n",
      "\n",
      "Fold: 14  Epoch: 36  Training loss = 1.7505  Validation loss = 4.6702  \n",
      "\n",
      "Fold: 14  Epoch: 37  Training loss = 1.7513  Validation loss = 4.6306  \n",
      "\n",
      "Fold: 14  Epoch: 38  Training loss = 1.7499  Validation loss = 4.6169  \n",
      "\n",
      "Fold: 14  Epoch: 39  Training loss = 1.7474  Validation loss = 4.5798  \n",
      "\n",
      "Fold: 14  Epoch: 40  Training loss = 1.7464  Validation loss = 4.6036  \n",
      "\n",
      "Fold: 14  Epoch: 41  Training loss = 1.7496  Validation loss = 4.5981  \n",
      "\n",
      "Fold: 14  Epoch: 42  Training loss = 1.7442  Validation loss = 4.5947  \n",
      "\n",
      "Fold: 14  Epoch: 43  Training loss = 1.7400  Validation loss = 4.5767  \n",
      "\n",
      "Fold: 14  Epoch: 44  Training loss = 1.7397  Validation loss = 4.6062  \n",
      "\n",
      "Fold: 14  Epoch: 45  Training loss = 1.7389  Validation loss = 4.5845  \n",
      "\n",
      "Fold: 14  Epoch: 46  Training loss = 1.7494  Validation loss = 4.5288  \n",
      "\n",
      "Fold: 14  Epoch: 47  Training loss = 1.7350  Validation loss = 4.5695  \n",
      "\n",
      "Fold: 14  Epoch: 48  Training loss = 1.7415  Validation loss = 4.5672  \n",
      "\n",
      "Fold: 14  Epoch: 49  Training loss = 1.7337  Validation loss = 4.5480  \n",
      "\n",
      "Fold: 14  Epoch: 50  Training loss = 1.7330  Validation loss = 4.5102  \n",
      "\n",
      "Fold: 14  Epoch: 51  Training loss = 1.7421  Validation loss = 4.4377  \n",
      "\n",
      "Fold: 14  Epoch: 52  Training loss = 1.7307  Validation loss = 4.4711  \n",
      "\n",
      "Fold: 14  Epoch: 53  Training loss = 1.7318  Validation loss = 4.4276  \n",
      "\n",
      "Fold: 14  Epoch: 54  Training loss = 1.7285  Validation loss = 4.4230  \n",
      "\n",
      "Fold: 14  Epoch: 55  Training loss = 1.7270  Validation loss = 4.4388  \n",
      "\n",
      "Fold: 14  Epoch: 56  Training loss = 1.7279  Validation loss = 4.4516  \n",
      "\n",
      "Fold: 14  Epoch: 57  Training loss = 1.7219  Validation loss = 4.4295  \n",
      "\n",
      "Fold: 14  Epoch: 58  Training loss = 1.7212  Validation loss = 4.4102  \n",
      "\n",
      "Fold: 14  Epoch: 59  Training loss = 1.7224  Validation loss = 4.4161  \n",
      "\n",
      "Fold: 14  Epoch: 60  Training loss = 1.7199  Validation loss = 4.4197  \n",
      "\n",
      "Fold: 14  Epoch: 61  Training loss = 1.7192  Validation loss = 4.4088  \n",
      "\n",
      "Fold: 14  Epoch: 62  Training loss = 1.7211  Validation loss = 4.4463  \n",
      "\n",
      "Fold: 14  Epoch: 63  Training loss = 1.7203  Validation loss = 4.4184  \n",
      "\n",
      "Fold: 14  Epoch: 64  Training loss = 1.7165  Validation loss = 4.4161  \n",
      "\n",
      "Fold: 14  Epoch: 65  Training loss = 1.7161  Validation loss = 4.4351  \n",
      "\n",
      "Fold: 14  Epoch: 66  Training loss = 1.7157  Validation loss = 4.4107  \n",
      "\n",
      "Fold: 14  Epoch: 67  Training loss = 1.7137  Validation loss = 4.3948  \n",
      "\n",
      "Fold: 14  Epoch: 68  Training loss = 1.7124  Validation loss = 4.3722  \n",
      "\n",
      "Fold: 14  Epoch: 69  Training loss = 1.7158  Validation loss = 4.3867  \n",
      "\n",
      "Fold: 14  Epoch: 70  Training loss = 1.7110  Validation loss = 4.3894  \n",
      "\n",
      "Fold: 14  Epoch: 71  Training loss = 1.7127  Validation loss = 4.4273  \n",
      "\n",
      "Fold: 14  Epoch: 72  Training loss = 1.7100  Validation loss = 4.4207  \n",
      "\n",
      "Fold: 14  Epoch: 73  Training loss = 1.7111  Validation loss = 4.4204  \n",
      "\n",
      "Fold: 14  Epoch: 74  Training loss = 1.7096  Validation loss = 4.3644  \n",
      "\n",
      "Fold: 14  Epoch: 75  Training loss = 1.7071  Validation loss = 4.3627  \n",
      "\n",
      "Fold: 14  Epoch: 76  Training loss = 1.7070  Validation loss = 4.3861  \n",
      "\n",
      "Fold: 14  Epoch: 77  Training loss = 1.7074  Validation loss = 4.4003  \n",
      "\n",
      "Fold: 14  Epoch: 78  Training loss = 1.7044  Validation loss = 4.3781  \n",
      "\n",
      "Fold: 14  Epoch: 79  Training loss = 1.7150  Validation loss = 4.3277  \n",
      "\n",
      "Fold: 14  Epoch: 80  Training loss = 1.7063  Validation loss = 4.3586  \n",
      "\n",
      "Fold: 14  Epoch: 81  Training loss = 1.7047  Validation loss = 4.3930  \n",
      "\n",
      "Fold: 14  Epoch: 82  Training loss = 1.7029  Validation loss = 4.3373  \n",
      "\n",
      "Fold: 14  Epoch: 83  Training loss = 1.7032  Validation loss = 4.3100  \n",
      "\n",
      "Fold: 14  Epoch: 84  Training loss = 1.7033  Validation loss = 4.3331  \n",
      "\n",
      "Fold: 14  Epoch: 85  Training loss = 1.7021  Validation loss = 4.3026  \n",
      "\n",
      "Fold: 14  Epoch: 86  Training loss = 1.7038  Validation loss = 4.2857  \n",
      "\n",
      "Fold: 14  Epoch: 87  Training loss = 1.7046  Validation loss = 4.3018  \n",
      "\n",
      "Fold: 14  Epoch: 88  Training loss = 1.7016  Validation loss = 4.3028  \n",
      "\n",
      "Fold: 14  Epoch: 89  Training loss = 1.7084  Validation loss = 4.2774  \n",
      "\n",
      "Fold: 14  Epoch: 90  Training loss = 1.7006  Validation loss = 4.3141  \n",
      "\n",
      "Fold: 14  Epoch: 91  Training loss = 1.7001  Validation loss = 4.3535  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 89  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 1.9956  Validation loss = 4.5104  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 1.9961  Validation loss = 4.5002  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 1.9924  Validation loss = 4.4896  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 1.9839  Validation loss = 4.4792  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 1.9810  Validation loss = 4.4739  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 1.9770  Validation loss = 4.4626  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 1.9711  Validation loss = 4.4503  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 1.9701  Validation loss = 4.4533  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 1.9599  Validation loss = 4.4242  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 1.9574  Validation loss = 4.4169  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 1.9486  Validation loss = 4.3779  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 1.9492  Validation loss = 4.3765  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 1.9478  Validation loss = 4.3645  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 1.9390  Validation loss = 4.3535  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 1.9361  Validation loss = 4.3500  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 1.9336  Validation loss = 4.3432  \n",
      "\n",
      "Fold: 15  Epoch: 17  Training loss = 1.9323  Validation loss = 4.3163  \n",
      "\n",
      "Fold: 15  Epoch: 18  Training loss = 1.9283  Validation loss = 4.3154  \n",
      "\n",
      "Fold: 15  Epoch: 19  Training loss = 1.9287  Validation loss = 4.3077  \n",
      "\n",
      "Fold: 15  Epoch: 20  Training loss = 1.9212  Validation loss = 4.2950  \n",
      "\n",
      "Fold: 15  Epoch: 21  Training loss = 1.9183  Validation loss = 4.2869  \n",
      "\n",
      "Fold: 15  Epoch: 22  Training loss = 1.9153  Validation loss = 4.2800  \n",
      "\n",
      "Fold: 15  Epoch: 23  Training loss = 1.9139  Validation loss = 4.2656  \n",
      "\n",
      "Fold: 15  Epoch: 24  Training loss = 1.9145  Validation loss = 4.2780  \n",
      "\n",
      "Fold: 15  Epoch: 25  Training loss = 1.9228  Validation loss = 4.2739  \n",
      "\n",
      "Fold: 15  Epoch: 26  Training loss = 1.9212  Validation loss = 4.2994  \n",
      "\n",
      "Fold: 15  Epoch: 27  Training loss = 1.9096  Validation loss = 4.2474  \n",
      "\n",
      "Fold: 15  Epoch: 28  Training loss = 1.9138  Validation loss = 4.3439  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 27  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 2.1608  Validation loss = 3.7517  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 2.1583  Validation loss = 3.7669  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 2.1442  Validation loss = 3.6479  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 2.1406  Validation loss = 3.5699  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 2.1320  Validation loss = 3.6212  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 2.1270  Validation loss = 3.6009  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 2.1228  Validation loss = 3.5772  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 2.1179  Validation loss = 3.5554  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 2.1123  Validation loss = 3.5823  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 2.1088  Validation loss = 3.5823  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 2.1106  Validation loss = 3.5916  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 2.1035  Validation loss = 3.5892  \n",
      "\n",
      "Fold: 16  Epoch: 13  Training loss = 2.1031  Validation loss = 3.6759  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 8  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 2.2721  Validation loss = 3.1998  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 2.2644  Validation loss = 3.2717  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 2.2573  Validation loss = 3.2844  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 2.2539  Validation loss = 3.2511  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 2.2516  Validation loss = 3.2549  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 2.2446  Validation loss = 3.2255  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 2.2361  Validation loss = 3.2029  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 2.2301  Validation loss = 3.2694  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 2.2238  Validation loss = 3.2568  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 2.2189  Validation loss = 3.2148  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 2.2246  Validation loss = 3.2231  \n",
      "\n",
      "Fold: 17  Epoch: 12  Training loss = 2.2146  Validation loss = 3.2587  \n",
      "\n",
      "Fold: 17  Epoch: 13  Training loss = 2.2157  Validation loss = 3.2609  \n",
      "\n",
      "Fold: 17  Epoch: 14  Training loss = 2.3094  Validation loss = 3.1712  \n",
      "\n",
      "Fold: 17  Epoch: 15  Training loss = 2.2091  Validation loss = 3.2490  \n",
      "\n",
      "Fold: 17  Epoch: 16  Training loss = 2.1949  Validation loss = 3.1312  \n",
      "\n",
      "Fold: 17  Epoch: 17  Training loss = 2.1872  Validation loss = 3.1502  \n",
      "\n",
      "Fold: 17  Epoch: 18  Training loss = 2.1794  Validation loss = 3.2188  \n",
      "\n",
      "Fold: 17  Epoch: 19  Training loss = 2.1759  Validation loss = 3.1348  \n",
      "\n",
      "Fold: 17  Epoch: 20  Training loss = 2.1720  Validation loss = 3.1632  \n",
      "\n",
      "Fold: 17  Epoch: 21  Training loss = 2.1663  Validation loss = 3.1332  \n",
      "\n",
      "Fold: 17  Epoch: 22  Training loss = 2.1806  Validation loss = 3.2787  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 16  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 2.3174  Validation loss = 1.6069  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 2.2877  Validation loss = 1.6139  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 2.2842  Validation loss = 1.6013  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 2.2737  Validation loss = 1.5863  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 2.3052  Validation loss = 1.5780  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 2.2639  Validation loss = 1.5973  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 2.2591  Validation loss = 1.5978  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 2.2518  Validation loss = 1.5952  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 2.2457  Validation loss = 1.5945  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 2.2408  Validation loss = 1.5834  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 2.2695  Validation loss = 1.5641  \n",
      "\n",
      "Fold: 18  Epoch: 12  Training loss = 2.2360  Validation loss = 1.5971  \n",
      "\n",
      "Fold: 18  Epoch: 13  Training loss = 2.2453  Validation loss = 1.6135  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 11  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 2.2237  Validation loss = 2.2351  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 2.2044  Validation loss = 2.2577  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 2.1956  Validation loss = 2.2448  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 2.2186  Validation loss = 2.2390  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 2.1876  Validation loss = 2.2268  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 2.1835  Validation loss = 2.2236  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 2.1680  Validation loss = 2.2187  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 2.1776  Validation loss = 2.2171  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 2.1653  Validation loss = 2.2210  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 2.1563  Validation loss = 2.2134  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 2.1549  Validation loss = 2.2136  \n",
      "\n",
      "Fold: 19  Epoch: 12  Training loss = 2.1561  Validation loss = 2.1934  \n",
      "\n",
      "Fold: 19  Epoch: 13  Training loss = 2.1498  Validation loss = 2.1788  \n",
      "\n",
      "Fold: 19  Epoch: 14  Training loss = 2.1459  Validation loss = 2.1603  \n",
      "\n",
      "Fold: 19  Epoch: 15  Training loss = 2.1460  Validation loss = 2.1757  \n",
      "\n",
      "Fold: 19  Epoch: 16  Training loss = 2.1465  Validation loss = 2.1674  \n",
      "\n",
      "Fold: 19  Epoch: 17  Training loss = 2.1342  Validation loss = 2.1772  \n",
      "\n",
      "Fold: 19  Epoch: 18  Training loss = 2.1371  Validation loss = 2.1885  \n",
      "\n",
      "Fold: 19  Epoch: 19  Training loss = 2.1341  Validation loss = 2.1755  \n",
      "\n",
      "Fold: 19  Epoch: 20  Training loss = 2.1807  Validation loss = 2.1397  \n",
      "\n",
      "Fold: 19  Epoch: 21  Training loss = 2.1255  Validation loss = 2.1755  \n",
      "\n",
      "Fold: 19  Epoch: 22  Training loss = 2.1609  Validation loss = 2.1871  \n",
      "\n",
      "Fold: 19  Epoch: 23  Training loss = 2.1363  Validation loss = 2.1721  \n",
      "\n",
      "Fold: 19  Epoch: 24  Training loss = 2.1153  Validation loss = 2.1725  \n",
      "\n",
      "Fold: 19  Epoch: 25  Training loss = 2.1152  Validation loss = 2.1625  \n",
      "\n",
      "Fold: 19  Epoch: 26  Training loss = 2.1101  Validation loss = 2.1460  \n",
      "\n",
      "Fold: 19  Epoch: 27  Training loss = 2.1121  Validation loss = 2.1565  \n",
      "\n",
      "Fold: 19  Epoch: 28  Training loss = 2.0971  Validation loss = 2.1472  \n",
      "\n",
      "Fold: 19  Epoch: 29  Training loss = 2.1018  Validation loss = 2.1463  \n",
      "\n",
      "Fold: 19  Epoch: 30  Training loss = 2.0865  Validation loss = 2.1484  \n",
      "\n",
      "Fold: 19  Epoch: 31  Training loss = 2.0837  Validation loss = 2.1533  \n",
      "\n",
      "Fold: 19  Epoch: 32  Training loss = 2.0780  Validation loss = 2.1445  \n",
      "\n",
      "Fold: 19  Epoch: 33  Training loss = 2.0757  Validation loss = 2.1327  \n",
      "\n",
      "Fold: 19  Epoch: 34  Training loss = 2.0845  Validation loss = 2.1376  \n",
      "\n",
      "Fold: 19  Epoch: 35  Training loss = 2.0872  Validation loss = 2.1233  \n",
      "\n",
      "Fold: 19  Epoch: 36  Training loss = 2.0762  Validation loss = 2.1203  \n",
      "\n",
      "Fold: 19  Epoch: 37  Training loss = 2.0679  Validation loss = 2.1169  \n",
      "\n",
      "Fold: 19  Epoch: 38  Training loss = 2.0602  Validation loss = 2.1397  \n",
      "\n",
      "Fold: 19  Epoch: 39  Training loss = 2.0718  Validation loss = 2.1477  \n",
      "\n",
      "Fold: 19  Epoch: 40  Training loss = 2.0556  Validation loss = 2.1273  \n",
      "\n",
      "Fold: 19  Epoch: 41  Training loss = 2.0713  Validation loss = 2.1327  \n",
      "\n",
      "Fold: 19  Epoch: 42  Training loss = 2.0738  Validation loss = 2.1254  \n",
      "\n",
      "Fold: 19  Epoch: 43  Training loss = 2.0596  Validation loss = 2.1274  \n",
      "\n",
      "Fold: 19  Epoch: 44  Training loss = 2.0557  Validation loss = 2.1361  \n",
      "\n",
      "Fold: 19  Epoch: 45  Training loss = 2.0586  Validation loss = 2.1528  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 37  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 2.1201  Validation loss = 0.8863  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 2.2253  Validation loss = 0.8735  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 2.0996  Validation loss = 0.8721  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 2.1001  Validation loss = 0.8491  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 2.1002  Validation loss = 0.8574  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 2.0975  Validation loss = 0.8229  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 2.0928  Validation loss = 0.8291  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 2.0989  Validation loss = 0.7977  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 2.0725  Validation loss = 0.7754  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 2.0736  Validation loss = 0.7727  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 2.0771  Validation loss = 0.7735  \n",
      "\n",
      "Fold: 20  Epoch: 12  Training loss = 2.0711  Validation loss = 0.7905  \n",
      "\n",
      "Fold: 20  Epoch: 13  Training loss = 2.0657  Validation loss = 0.7931  \n",
      "\n",
      "Fold: 20  Epoch: 14  Training loss = 2.0673  Validation loss = 0.7986  \n",
      "\n",
      "Fold: 20  Epoch: 15  Training loss = 2.0668  Validation loss = 0.8028  \n",
      "\n",
      "Fold: 20  Epoch: 16  Training loss = 2.0898  Validation loss = 0.8018  \n",
      "\n",
      "Fold: 20  Epoch: 17  Training loss = 2.0686  Validation loss = 0.7664  \n",
      "\n",
      "Fold: 20  Epoch: 18  Training loss = 2.0567  Validation loss = 0.7515  \n",
      "\n",
      "Fold: 20  Epoch: 19  Training loss = 2.0535  Validation loss = 0.7629  \n",
      "\n",
      "Fold: 20  Epoch: 20  Training loss = 2.0465  Validation loss = 0.7804  \n",
      "\n",
      "Fold: 20  Epoch: 21  Training loss = 2.0422  Validation loss = 0.7385  \n",
      "\n",
      "Fold: 20  Epoch: 22  Training loss = 2.0438  Validation loss = 0.7402  \n",
      "\n",
      "Fold: 20  Epoch: 23  Training loss = 2.0454  Validation loss = 0.7296  \n",
      "\n",
      "Fold: 20  Epoch: 24  Training loss = 2.0264  Validation loss = 0.7184  \n",
      "\n",
      "Fold: 20  Epoch: 25  Training loss = 2.0235  Validation loss = 0.7278  \n",
      "\n",
      "Fold: 20  Epoch: 26  Training loss = 2.0439  Validation loss = 0.6791  \n",
      "\n",
      "Fold: 20  Epoch: 27  Training loss = 2.0193  Validation loss = 0.6786  \n",
      "\n",
      "Fold: 20  Epoch: 28  Training loss = 2.0365  Validation loss = 0.6521  \n",
      "\n",
      "Fold: 20  Epoch: 29  Training loss = 2.0148  Validation loss = 0.6510  \n",
      "\n",
      "Fold: 20  Epoch: 30  Training loss = 2.0051  Validation loss = 0.6397  \n",
      "\n",
      "Fold: 20  Epoch: 31  Training loss = 2.0032  Validation loss = 0.6552  \n",
      "\n",
      "Fold: 20  Epoch: 32  Training loss = 2.0041  Validation loss = 0.6537  \n",
      "\n",
      "Fold: 20  Epoch: 33  Training loss = 2.0154  Validation loss = 0.6407  \n",
      "\n",
      "Fold: 20  Epoch: 34  Training loss = 1.9995  Validation loss = 0.6338  \n",
      "\n",
      "Fold: 20  Epoch: 35  Training loss = 2.0138  Validation loss = 0.6102  \n",
      "\n",
      "Fold: 20  Epoch: 36  Training loss = 2.0024  Validation loss = 0.6271  \n",
      "\n",
      "Fold: 20  Epoch: 37  Training loss = 1.9991  Validation loss = 0.6137  \n",
      "\n",
      "Fold: 20  Epoch: 38  Training loss = 2.0043  Validation loss = 0.6219  \n",
      "\n",
      "Fold: 20  Epoch: 39  Training loss = 2.0094  Validation loss = 0.6328  \n",
      "\n",
      "Fold: 20  Epoch: 40  Training loss = 2.0609  Validation loss = 0.6423  \n",
      "\n",
      "Fold: 20  Epoch: 41  Training loss = 1.9966  Validation loss = 0.6179  \n",
      "\n",
      "Fold: 20  Epoch: 42  Training loss = 1.9929  Validation loss = 0.6025  \n",
      "\n",
      "Fold: 20  Epoch: 43  Training loss = 1.9963  Validation loss = 0.5920  \n",
      "\n",
      "Fold: 20  Epoch: 44  Training loss = 1.9818  Validation loss = 0.5847  \n",
      "\n",
      "Fold: 20  Epoch: 45  Training loss = 1.9799  Validation loss = 0.5909  \n",
      "\n",
      "Fold: 20  Epoch: 46  Training loss = 1.9789  Validation loss = 0.5925  \n",
      "\n",
      "Fold: 20  Epoch: 47  Training loss = 1.9779  Validation loss = 0.5915  \n",
      "\n",
      "Fold: 20  Epoch: 48  Training loss = 1.9813  Validation loss = 0.5866  \n",
      "\n",
      "Fold: 20  Epoch: 49  Training loss = 1.9766  Validation loss = 0.6062  \n",
      "\n",
      "Fold: 20  Epoch: 50  Training loss = 1.9647  Validation loss = 0.6137  \n",
      "\n",
      "Fold: 20  Epoch: 51  Training loss = 1.9647  Validation loss = 0.5933  \n",
      "\n",
      "Fold: 20  Epoch: 52  Training loss = 1.9717  Validation loss = 0.5915  \n",
      "\n",
      "Fold: 20  Epoch: 53  Training loss = 1.9683  Validation loss = 0.5938  \n",
      "\n",
      "Fold: 20  Epoch: 54  Training loss = 1.9743  Validation loss = 0.5949  \n",
      "\n",
      "Fold: 20  Epoch: 55  Training loss = 1.9702  Validation loss = 0.6157  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 44  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 1.9343  Validation loss = 3.2893  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 1.9343  Validation loss = 3.3136  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 1.9358  Validation loss = 3.3862  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 1.9194  Validation loss = 3.3130  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 1.9279  Validation loss = 3.2809  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 1.9277  Validation loss = 3.2730  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 1.9240  Validation loss = 3.2971  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 1.9262  Validation loss = 3.2280  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 1.9226  Validation loss = 3.2227  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 1.9203  Validation loss = 3.2959  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 1.9057  Validation loss = 3.3048  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 1.9482  Validation loss = 3.3630  \n",
      "\n",
      "Fold: 21  Epoch: 13  Training loss = 1.9296  Validation loss = 3.3388  \n",
      "\n",
      "Fold: 21  Epoch: 14  Training loss = 1.9041  Validation loss = 3.2413  \n",
      "\n",
      "Fold: 21  Epoch: 15  Training loss = 1.9149  Validation loss = 3.1984  \n",
      "\n",
      "Fold: 21  Epoch: 16  Training loss = 1.9007  Validation loss = 3.2914  \n",
      "\n",
      "Fold: 21  Epoch: 17  Training loss = 1.8967  Validation loss = 3.2391  \n",
      "\n",
      "Fold: 21  Epoch: 18  Training loss = 1.9115  Validation loss = 3.1743  \n",
      "\n",
      "Fold: 21  Epoch: 19  Training loss = 1.8895  Validation loss = 3.2215  \n",
      "\n",
      "Fold: 21  Epoch: 20  Training loss = 1.8882  Validation loss = 3.1952  \n",
      "\n",
      "Fold: 21  Epoch: 21  Training loss = 1.9435  Validation loss = 3.2324  \n",
      "\n",
      "Fold: 21  Epoch: 22  Training loss = 1.8728  Validation loss = 3.2377  \n",
      "\n",
      "Fold: 21  Epoch: 23  Training loss = 1.8734  Validation loss = 3.2467  \n",
      "\n",
      "Fold: 21  Epoch: 24  Training loss = 1.8774  Validation loss = 3.3155  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 18  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 1.9293  Validation loss = 3.3165  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 1.9391  Validation loss = 3.6048  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 1.9207  Validation loss = 3.5765  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 1.9050  Validation loss = 3.7391  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 1.8927  Validation loss = 3.5200  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 1.9432  Validation loss = 3.8213  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 1.8875  Validation loss = 3.4064  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 1.9392  Validation loss = 3.9068  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 1.8753  Validation loss = 3.6088  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 1.8724  Validation loss = 3.3512  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 1.8713  Validation loss = 3.5933  \n",
      "\n",
      "Fold: 22  Epoch: 12  Training loss = 1.9068  Validation loss = 3.3260  \n",
      "\n",
      "Fold: 22  Epoch: 13  Training loss = 1.8819  Validation loss = 3.3665  \n",
      "\n",
      "Fold: 22  Epoch: 14  Training loss = 1.8631  Validation loss = 3.8586  \n",
      "\n",
      "Fold: 22  Epoch: 15  Training loss = 1.8723  Validation loss = 3.4363  \n",
      "\n",
      "Fold: 22  Epoch: 16  Training loss = 1.8691  Validation loss = 3.8927  \n",
      "\n",
      "Fold: 22  Epoch: 17  Training loss = 1.8614  Validation loss = 4.1380  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 1  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 2.0221  Validation loss = 2.5547  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 2.0153  Validation loss = 2.5031  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 1.9597  Validation loss = 2.2231  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 2.0189  Validation loss = 2.5633  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 1.9613  Validation loss = 2.2699  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 1.9398  Validation loss = 1.8118  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 1.9506  Validation loss = 1.9314  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 1.9411  Validation loss = 1.7232  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 1.9209  Validation loss = 1.7097  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 1.9364  Validation loss = 2.1548  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 1.9889  Validation loss = 2.4287  \n",
      "\n",
      "Fold: 23  Epoch: 12  Training loss = 1.9166  Validation loss = 1.7862  \n",
      "\n",
      "Fold: 23  Epoch: 13  Training loss = 1.8987  Validation loss = 1.8226  \n",
      "\n",
      "Fold: 23  Epoch: 14  Training loss = 1.9056  Validation loss = 2.0707  \n",
      "\n",
      "Fold: 23  Epoch: 15  Training loss = 1.9209  Validation loss = 2.1043  \n",
      "\n",
      "Fold: 23  Epoch: 16  Training loss = 1.9093  Validation loss = 1.5858  \n",
      "\n",
      "Fold: 23  Epoch: 17  Training loss = 1.8996  Validation loss = 1.8312  \n",
      "\n",
      "Fold: 23  Epoch: 18  Training loss = 1.8736  Validation loss = 1.7535  \n",
      "\n",
      "Fold: 23  Epoch: 19  Training loss = 1.8636  Validation loss = 1.6605  \n",
      "\n",
      "Fold: 23  Epoch: 20  Training loss = 1.8881  Validation loss = 1.9535  \n",
      "\n",
      "Fold: 23  Epoch: 21  Training loss = 1.8505  Validation loss = 1.5485  \n",
      "\n",
      "Fold: 23  Epoch: 22  Training loss = 1.8981  Validation loss = 1.9824  \n",
      "\n",
      "Fold: 23  Epoch: 23  Training loss = 1.8477  Validation loss = 1.6892  \n",
      "\n",
      "Fold: 23  Epoch: 24  Training loss = 1.8670  Validation loss = 1.8514  \n",
      "\n",
      "Fold: 23  Epoch: 25  Training loss = 1.8425  Validation loss = 1.4160  \n",
      "\n",
      "Fold: 23  Epoch: 26  Training loss = 1.8437  Validation loss = 1.2789  \n",
      "\n",
      "Fold: 23  Epoch: 27  Training loss = 1.8907  Validation loss = 1.3144  \n",
      "\n",
      "Fold: 23  Epoch: 28  Training loss = 1.8225  Validation loss = 1.6176  \n",
      "\n",
      "Fold: 23  Epoch: 29  Training loss = 1.8127  Validation loss = 1.3776  \n",
      "\n",
      "Fold: 23  Epoch: 30  Training loss = 1.8127  Validation loss = 1.4399  \n",
      "\n",
      "Fold: 23  Epoch: 31  Training loss = 1.8036  Validation loss = 1.4974  \n",
      "\n",
      "Fold: 23  Epoch: 32  Training loss = 1.8180  Validation loss = 1.4419  \n",
      "\n",
      "Fold: 23  Epoch: 33  Training loss = 1.8133  Validation loss = 1.5852  \n",
      "\n",
      "Fold: 23  Epoch: 34  Training loss = 1.8009  Validation loss = 1.3906  \n",
      "\n",
      "Fold: 23  Epoch: 35  Training loss = 1.7974  Validation loss = 1.4864  \n",
      "\n",
      "Fold: 23  Epoch: 36  Training loss = 1.8226  Validation loss = 1.7610  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 26  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 1.8371  Validation loss = 1.5052  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 1.8242  Validation loss = 1.4824  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 1.8088  Validation loss = 1.4424  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 1.9555  Validation loss = 1.2633  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 1.9289  Validation loss = 1.5631  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 1.8829  Validation loss = 1.4983  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 1.8500  Validation loss = 1.2888  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 1.8331  Validation loss = 1.2440  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 1.9939  Validation loss = 1.4836  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 1.8145  Validation loss = 1.4274  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 1.8265  Validation loss = 1.4445  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 1.8436  Validation loss = 1.4903  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 1.7983  Validation loss = 1.3978  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 1.8051  Validation loss = 1.3614  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 1.7868  Validation loss = 1.3155  \n",
      "\n",
      "Fold: 24  Epoch: 16  Training loss = 1.7932  Validation loss = 1.4222  \n",
      "\n",
      "Fold: 24  Epoch: 17  Training loss = 1.7795  Validation loss = 1.4141  \n",
      "\n",
      "Fold: 24  Epoch: 18  Training loss = 1.7909  Validation loss = 1.4509  \n",
      "\n",
      "Fold: 24  Epoch: 19  Training loss = 1.8157  Validation loss = 1.2954  \n",
      "\n",
      "Fold: 24  Epoch: 20  Training loss = 1.8152  Validation loss = 1.3149  \n",
      "\n",
      "Fold: 24  Epoch: 21  Training loss = 1.8332  Validation loss = 1.2968  \n",
      "\n",
      "Fold: 24  Epoch: 22  Training loss = 1.8501  Validation loss = 1.5213  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 8  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 1.7234  Validation loss = 2.2862  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 1.7677  Validation loss = 2.4583  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 1.7291  Validation loss = 2.5068  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 1.7291  Validation loss = 2.3988  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 1.7282  Validation loss = 2.2017  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 1.7250  Validation loss = 2.1157  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 1.7328  Validation loss = 2.2377  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 1.7442  Validation loss = 2.1048  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 1.6996  Validation loss = 2.2732  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 1.7026  Validation loss = 2.1981  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 1.6939  Validation loss = 2.1742  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 1.7078  Validation loss = 2.1507  \n",
      "\n",
      "Fold: 25  Epoch: 13  Training loss = 1.6917  Validation loss = 2.2342  \n",
      "\n",
      "Fold: 25  Epoch: 14  Training loss = 1.7118  Validation loss = 2.3061  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 8  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 1.7704  Validation loss = 2.0858  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 1.8406  Validation loss = 1.9167  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 1.8071  Validation loss = 2.2103  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 1.7490  Validation loss = 2.4308  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 1.8657  Validation loss = 2.2088  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 1.7472  Validation loss = 2.5548  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 1.7427  Validation loss = 2.9373  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 1.7652  Validation loss = 3.2647  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 1.7884  Validation loss = 1.8501  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 1.8120  Validation loss = 1.9852  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 1.9653  Validation loss = 1.6209  \n",
      "\n",
      "Fold: 26  Epoch: 12  Training loss = 2.0151  Validation loss = 1.0529  \n",
      "\n",
      "Fold: 26  Epoch: 13  Training loss = 1.8473  Validation loss = 2.5279  \n",
      "\n",
      "Fold: 26  Epoch: 14  Training loss = 1.8438  Validation loss = 2.7335  \n",
      "\n",
      "Fold: 26  Epoch: 15  Training loss = 1.8106  Validation loss = 2.4053  \n",
      "\n",
      "Fold: 26  Epoch: 16  Training loss = 1.7770  Validation loss = 2.6253  \n",
      "\n",
      "Fold: 26  Epoch: 17  Training loss = 1.8528  Validation loss = 3.1951  \n",
      "\n",
      "Fold: 26  Epoch: 18  Training loss = 1.7972  Validation loss = 2.0006  \n",
      "\n",
      "Fold: 26  Epoch: 19  Training loss = 1.7969  Validation loss = 2.0643  \n",
      "\n",
      "Fold: 26  Epoch: 20  Training loss = 1.7690  Validation loss = 2.2283  \n",
      "\n",
      "Fold: 26  Epoch: 21  Training loss = 1.7593  Validation loss = 2.2454  \n",
      "\n",
      "Fold: 26  Epoch: 22  Training loss = 1.7570  Validation loss = 2.4718  \n",
      "\n",
      "Fold: 26  Epoch: 23  Training loss = 1.9059  Validation loss = 3.2233  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 12  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 1.8531  Validation loss = 0.8051  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 1.7318  Validation loss = 0.8607  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 1.8519  Validation loss = 0.9363  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 1.7409  Validation loss = 0.9317  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 1.7541  Validation loss = 0.9720  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 1.7081  Validation loss = 0.9472  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 1.7092  Validation loss = 0.8817  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 1.6785  Validation loss = 0.9680  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 1.6461  Validation loss = 0.8982  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 1.6167  Validation loss = 0.9375  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 1.6407  Validation loss = 0.8771  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 1.6452  Validation loss = 0.8496  \n",
      "\n",
      "Fold: 27  Epoch: 13  Training loss = 1.6264  Validation loss = 0.9628  \n",
      "\n",
      "Fold: 27  Epoch: 14  Training loss = 1.6388  Validation loss = 0.8446  \n",
      "\n",
      "Fold: 27  Epoch: 15  Training loss = 1.6063  Validation loss = 0.8899  \n",
      "\n",
      "Fold: 27  Epoch: 16  Training loss = 1.5907  Validation loss = 0.9843  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 1  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 1.5949  Validation loss = 0.4322  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 1.5719  Validation loss = 0.4572  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 1.5778  Validation loss = 0.4651  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 1.6071  Validation loss = 0.4499  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 1.5823  Validation loss = 0.5055  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 1.5955  Validation loss = 0.4311  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 1.6273  Validation loss = 0.5234  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 1.6310  Validation loss = 0.4153  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 1.6003  Validation loss = 0.4682  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 1.6412  Validation loss = 0.5258  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 1.6458  Validation loss = 0.6233  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 8  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 1.5713  Validation loss = 0.6833  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 1.5499  Validation loss = 0.7284  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 1.5928  Validation loss = 0.7284  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 1.5226  Validation loss = 0.7462  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 1.5790  Validation loss = 0.7837  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 1.5448  Validation loss = 0.6876  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 1.5842  Validation loss = 0.6860  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 1.5294  Validation loss = 0.7278  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 1.5181  Validation loss = 0.7104  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 1.5427  Validation loss = 0.7108  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 1.5147  Validation loss = 0.6964  \n",
      "\n",
      "Fold: 29  Epoch: 12  Training loss = 1.5235  Validation loss = 0.6996  \n",
      "\n",
      "Fold: 29  Epoch: 13  Training loss = 1.5620  Validation loss = 0.6565  \n",
      "\n",
      "Fold: 29  Epoch: 14  Training loss = 1.5759  Validation loss = 0.6800  \n",
      "\n",
      "Fold: 29  Epoch: 15  Training loss = 1.5684  Validation loss = 0.6572  \n",
      "\n",
      "Fold: 29  Epoch: 16  Training loss = 1.5517  Validation loss = 0.6718  \n",
      "\n",
      "Fold: 29  Epoch: 17  Training loss = 1.4936  Validation loss = 0.6746  \n",
      "\n",
      "Fold: 29  Epoch: 18  Training loss = 1.5359  Validation loss = 0.6843  \n",
      "\n",
      "Fold: 29  Epoch: 19  Training loss = 1.5422  Validation loss = 0.7233  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 13  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 1.5112  Validation loss = 1.5073  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 1.5039  Validation loss = 1.4140  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 1.5086  Validation loss = 1.5188  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 1.5252  Validation loss = 1.3638  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 1.4705  Validation loss = 1.4067  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 1.5105  Validation loss = 1.4292  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 1.4920  Validation loss = 1.1590  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 1.4887  Validation loss = 1.1667  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 1.4629  Validation loss = 1.0989  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 1.4508  Validation loss = 1.2710  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 1.4448  Validation loss = 1.2275  \n",
      "\n",
      "Fold: 30  Epoch: 12  Training loss = 1.4507  Validation loss = 1.2200  \n",
      "\n",
      "Fold: 30  Epoch: 13  Training loss = 1.5300  Validation loss = 1.3624  \n",
      "\n",
      "Fold: 30  Epoch: 14  Training loss = 1.4723  Validation loss = 1.1238  \n",
      "\n",
      "Fold: 30  Epoch: 15  Training loss = 1.4604  Validation loss = 1.1176  \n",
      "\n",
      "Fold: 30  Epoch: 16  Training loss = 1.4611  Validation loss = 1.1441  \n",
      "\n",
      "Fold: 30  Epoch: 17  Training loss = 1.4615  Validation loss = 1.1676  \n",
      "\n",
      "Fold: 30  Epoch: 18  Training loss = 1.5014  Validation loss = 1.4746  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 9  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 1.4877  Validation loss = 1.3780  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 1.4961  Validation loss = 1.3999  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 1.5370  Validation loss = 1.4222  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 1.5001  Validation loss = 1.2872  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 1.4513  Validation loss = 1.2732  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 1.4940  Validation loss = 1.2065  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 1.5064  Validation loss = 1.1482  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 1.5861  Validation loss = 1.2273  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 1.4583  Validation loss = 1.2129  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 1.5081  Validation loss = 1.3490  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 1.4602  Validation loss = 1.2486  \n",
      "\n",
      "Fold: 31  Epoch: 12  Training loss = 1.4447  Validation loss = 1.2559  \n",
      "\n",
      "Fold: 31  Epoch: 13  Training loss = 1.5180  Validation loss = 1.2408  \n",
      "\n",
      "Fold: 31  Epoch: 14  Training loss = 1.4756  Validation loss = 1.2853  \n",
      "\n",
      "Fold: 31  Epoch: 15  Training loss = 1.4314  Validation loss = 1.2183  \n",
      "\n",
      "Fold: 31  Epoch: 16  Training loss = 1.4135  Validation loss = 1.2107  \n",
      "\n",
      "Fold: 31  Epoch: 17  Training loss = 1.4992  Validation loss = 1.1826  \n",
      "\n",
      "Fold: 31  Epoch: 18  Training loss = 1.4270  Validation loss = 1.1774  \n",
      "\n",
      "Fold: 31  Epoch: 19  Training loss = 1.4306  Validation loss = 1.1731  \n",
      "\n",
      "Fold: 31  Epoch: 20  Training loss = 1.4221  Validation loss = 1.1822  \n",
      "\n",
      "Fold: 31  Epoch: 21  Training loss = 1.4281  Validation loss = 1.2000  \n",
      "\n",
      "Fold: 31  Epoch: 22  Training loss = 1.4581  Validation loss = 1.2845  \n",
      "\n",
      "Fold: 31  Epoch: 23  Training loss = 1.4531  Validation loss = 1.2987  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 7  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 1.3053  Validation loss = 2.2224  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 1.2694  Validation loss = 1.8367  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 1.2735  Validation loss = 1.7763  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 1.2642  Validation loss = 1.8459  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 1.2705  Validation loss = 1.7194  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 1.2695  Validation loss = 2.1681  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 1.2751  Validation loss = 2.3100  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 1.2614  Validation loss = 1.6540  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 1.2715  Validation loss = 2.2763  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 1.2427  Validation loss = 1.8614  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 1.2495  Validation loss = 1.6027  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 1.2527  Validation loss = 2.1721  \n",
      "\n",
      "Fold: 32  Epoch: 13  Training loss = 1.2417  Validation loss = 1.9467  \n",
      "\n",
      "Fold: 32  Epoch: 14  Training loss = 1.2436  Validation loss = 1.5783  \n",
      "\n",
      "Fold: 32  Epoch: 15  Training loss = 1.2460  Validation loss = 2.0131  \n",
      "\n",
      "Fold: 32  Epoch: 16  Training loss = 1.2424  Validation loss = 1.7112  \n",
      "\n",
      "Fold: 32  Epoch: 17  Training loss = 1.2580  Validation loss = 1.8495  \n",
      "\n",
      "Fold: 32  Epoch: 18  Training loss = 1.2332  Validation loss = 1.9367  \n",
      "\n",
      "Fold: 32  Epoch: 19  Training loss = 1.2393  Validation loss = 1.7012  \n",
      "\n",
      "Fold: 32  Epoch: 20  Training loss = 1.2333  Validation loss = 1.6743  \n",
      "\n",
      "Fold: 32  Epoch: 21  Training loss = 1.2242  Validation loss = 1.7355  \n",
      "\n",
      "Fold: 32  Epoch: 22  Training loss = 1.2319  Validation loss = 1.6876  \n",
      "\n",
      "Fold: 32  Epoch: 23  Training loss = 1.2237  Validation loss = 1.9656  \n",
      "\n",
      "Fold: 32  Epoch: 24  Training loss = 1.2226  Validation loss = 1.7762  \n",
      "\n",
      "Fold: 32  Epoch: 25  Training loss = 1.2277  Validation loss = 2.0811  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 14  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    # Load model with lowest validation error for each fold\n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 36\n",
      "Average validation error: 2.34777\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 1.2468  Test loss = 2.6733  \n",
      "\n",
      "Epoch: 2  Training loss = 1.2315  Test loss = 2.6467  \n",
      "\n",
      "Epoch: 3  Training loss = 1.2218  Test loss = 2.6254  \n",
      "\n",
      "Epoch: 4  Training loss = 1.2158  Test loss = 2.6090  \n",
      "\n",
      "Epoch: 5  Training loss = 1.2118  Test loss = 2.5966  \n",
      "\n",
      "Epoch: 6  Training loss = 1.2091  Test loss = 2.5873  \n",
      "\n",
      "Epoch: 7  Training loss = 1.2070  Test loss = 2.5805  \n",
      "\n",
      "Epoch: 8  Training loss = 1.2053  Test loss = 2.5755  \n",
      "\n",
      "Epoch: 9  Training loss = 1.2039  Test loss = 2.5719  \n",
      "\n",
      "Epoch: 10  Training loss = 1.2027  Test loss = 2.5694  \n",
      "\n",
      "Epoch: 11  Training loss = 1.2015  Test loss = 2.5675  \n",
      "\n",
      "Epoch: 12  Training loss = 1.2005  Test loss = 2.5662  \n",
      "\n",
      "Epoch: 13  Training loss = 1.1995  Test loss = 2.5654  \n",
      "\n",
      "Epoch: 14  Training loss = 1.1986  Test loss = 2.5648  \n",
      "\n",
      "Epoch: 15  Training loss = 1.1978  Test loss = 2.5645  \n",
      "\n",
      "Epoch: 16  Training loss = 1.1969  Test loss = 2.5643  \n",
      "\n",
      "Epoch: 17  Training loss = 1.1962  Test loss = 2.5642  \n",
      "\n",
      "Epoch: 18  Training loss = 1.1954  Test loss = 2.5643  \n",
      "\n",
      "Epoch: 19  Training loss = 1.1947  Test loss = 2.5644  \n",
      "\n",
      "Epoch: 20  Training loss = 1.1940  Test loss = 2.5645  \n",
      "\n",
      "Epoch: 21  Training loss = 1.1934  Test loss = 2.5647  \n",
      "\n",
      "Epoch: 22  Training loss = 1.1927  Test loss = 2.5649  \n",
      "\n",
      "Epoch: 23  Training loss = 1.1921  Test loss = 2.5651  \n",
      "\n",
      "Epoch: 24  Training loss = 1.1915  Test loss = 2.5654  \n",
      "\n",
      "Epoch: 25  Training loss = 1.1909  Test loss = 2.5656  \n",
      "\n",
      "Epoch: 26  Training loss = 1.1904  Test loss = 2.5659  \n",
      "\n",
      "Epoch: 27  Training loss = 1.1898  Test loss = 2.5662  \n",
      "\n",
      "Epoch: 28  Training loss = 1.1893  Test loss = 2.5664  \n",
      "\n",
      "Epoch: 29  Training loss = 1.1888  Test loss = 2.5667  \n",
      "\n",
      "Epoch: 30  Training loss = 1.1883  Test loss = 2.5670  \n",
      "\n",
      "Epoch: 31  Training loss = 1.1878  Test loss = 2.5673  \n",
      "\n",
      "Epoch: 32  Training loss = 1.1873  Test loss = 2.5675  \n",
      "\n",
      "Epoch: 33  Training loss = 1.1869  Test loss = 2.5678  \n",
      "\n",
      "Epoch: 34  Training loss = 1.1864  Test loss = 2.5681  \n",
      "\n",
      "Epoch: 35  Training loss = 1.1860  Test loss = 2.5684  \n",
      "\n",
      "Epoch: 36  Training loss = 1.1855  Test loss = 2.5687  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcc1d160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXlcVPX6xz/fgWERZRPcFxBQE0JMFFPL3UzTSq1b1yX3\npTKzxW6W3Vv3dn9tXm/duqaVqaXlci2XUlNLzV3EDRQVlUVJwQUQFVnm+f3xnTPMzsDMMDA879fL\nF3LmzDnfGWY+5zmf5/k+X0FEYBiGYdwHlasHwDAMwzgWFnaGYRg3g4WdYRjGzWBhZxiGcTNY2BmG\nYdwMFnaGYRg3g4WdYRjGzWBhZxiGcTNY2BmGYdwMT1ecNCQkhMLCwlxxaoZhmFrL4cOHrxJRaEX7\nuUTYw8LCkJiY6IpTMwzD1FqEEBm27MdWDMMwjJvBws4wDONmsLAzDMO4GSzsDMMwbgYLO8MwjJvB\nws4wDONmsLAzDMO4GSzsDOMg8vPzsXTpUlcPg2FY2BnGUXz88ccYN24cLl686OqhMHUchwi7ECJQ\nCLFGCJEqhDglhLjfEcdl6hbHjx/HtGnTUFZW5uqhVImNGzcCAO7cuePikTB1HUdF7B8D2ExE7QF0\nBHDKQcdl6hDr1q3DwoULkZFh06zpGsUff/yBQ4cOAQBKSkpcPBqmrmO3sAshAgA8COArACCiYiLK\ns/e4TN3jypUrAID09HTXDqQK/PTTT7r/s7AzrsYREXs4gFwAXwshjgghvhRC+DnguEwdIycnBwBw\n4cIFF4+k8ig2DMDCzrgeRwi7J4D7ACwgok4AbgH4i/FOQogpQohEIURibm6uA07LuBu1NWIvKirC\n1q1bERERAYCFnXE9jhD2iwAuEtEB7e9rIIXeACJaRETxRBQfGlphO2GmDqIIe22L2H/77Tfcvn0b\nw4cPB8DCzrgeu4WdiC4DyBJCtNNu6gfgpL3HZeoetTVi37BhA/z8/DBw4EAAQHFxsYtHxNR1HLXQ\nxgwAy4UQXgDOAxjvoOMydYTi4mLk5cmce22K2IkIGzduxIABA1C/fn0ARhF7WRng4eGi0TF1FYeU\nOxLRUa3NEktEjxHRDUccl6k7KInTFi1aIDs7G0VFRS4ekW0cP34cWVlZGDp0KNRqNQA9YV+1CggM\nBGph+SZTu+GZp0yNQLFhEhISAACZmZmuHI7NKNUwgwcPNhR2IuCdd4DCQmDRIlcOkamDsLAzNQIl\nYleEvbbYMRs2bEDXrl3RpEkTeHl5AdAK+6ZNQEoKEBICfPklwL47U42wsDM1AuOIvTYkUK9cuYKD\nBw/ikUceAQBdxF5cXAx8+CHQogXw1VdATg7www+uHCpTx2BhZ2oEirDHxcVBrVbXioh9w4YNICIT\nYQ84exbYsQN48UXgkUeA8HBgwQIXjpSpa7CwMzWCnJwc1KtXD/7+/mjdunWtiNhXrFiByMhIxMXF\nASgX9uhNmwB/f2DyZEClAqZOBXbuBE5yFTBTPbCwMzWCK1euoHHjxgCAsLCwGh+xX7x4ETt27MDo\n0aMhhAAghT0MQJukJCnm/v5y5wkTAC8v4PPPXTZepm7Bws7UCPSFPTw8vMYL+3fffQciwqhRo3Tb\nvLy8MAsAqVTAzJnlO4eGAiNHAkuXArduVf9gmToHC7sjOXgQ+Pe/AY3G1SOpdeTk5KBRo0YApLDn\n5ubilotF8MKFC3j11Vdx9+5dk8eWL1+OhIQEREZG6rapc3IwEcDJjh2B5s0NnzB9OlBQAHz3nZNH\nzTAs7I5j3z6gXz9g1izglVdkHTNjM8ZWDOD6ypg1a9bgo48+wr/+9S+D7cnJyTh27JiM1vPzgWXL\ngMGD4XPPPfACsOd+M+vM9OgBREfLJCp/Nhgnw8LuCA4dAgYNApo0kX7q/PnA+++7elS1hrKyMuTm\n5hpYMYDrhV2xg/7xj38YLHe3fPlyeHh4YFSbNvJv/swzMjH60kvoBCA7ONj0YEIA06YBSUmyvp1h\nnAgLu70cOQIMHAg0bAj8+ivwxRfA008Dr78uJ6YwFXLt2jVoNBqdFaNE7K722dPT09GiRQtoNBq8\n8sorAACNRoMVK1Zg4MCBCN6yRVa97N0LXLgA8f77OKNWW+7u+Pjj8qfeohwM4wxY2O3h5ElgwACg\nQQMp6i1byi/6kiXAQw/Jyoi1a109yhqPMutUidgbN24MHx8fpwv75cuX8dlnn4EsWCMXLlxA165d\n8Ze//AUrV67Ejh07sHv3bmRmZkob5vffgW7dgPvvlxE5ZGWMRWFv3hzo2BH4+WdnvaRKcePGDXz1\n1VfQcE7I7WBhryoFBcBjjwFqtRR1bZQJQJa2/e9/QNeuwIgRwFNPAWfOuGyoNR1lcpIi7EIIhIWF\nOd2KefPNN/H8888jLS3N5DEiQnp6OsLCwjB79myEhYVhxowZWLp0Kfz8/PBYnz7A8ePAAw8YPM/L\ny8t6P/YhQ4A9e4Abru2Td/PmTTz00EOYNGmSbq1Wxn1gYa8KRHLyyfnzsoOfXmWEDj8/4JdfgDff\nBDZuBDp0ACZNAmpJc6vqRBF2xYoBnF/ymJubi2+//RYAcPbsWbNjKioqQnh4OHx9ffGvf/0LycnJ\nWLx4MR577DH4HT8uq5969jR4nlqttt6PffBg2cp361aHvp7KcOfOHQwdOlQn6KmpqS4bC+McWNir\nwoIFUtDffdckYjOgQQPg738Hzp0Dnn8e+OYbIDYWMCMkdRljKwaA0yP2hQsX6soYz5i5m1LOrfj9\njz32GAYMGAAAGD16NLB7t+yz3q2bwfOsWjGA3D842GU+e3FxMZ544gns2rULS5cuhaenJ06fPu2S\nsTDOg4W9shw+LEsaBw8GXn3Vtuc0bizr25OTpRiMHAncvu3ccdYAdu3ahWHDhqG0tNTqfleuXIGn\npyeCgoJ028LDw3Hjxg3k5+c7fFzFxcX47LPPMHDgQAQEBJiN2JW7BaVCRwiBL7/8Eu+8844U+N27\ngU6dAO3iGgoVCruHh6yg2rSp2uc7lJWVYezYsfjpp5+wYMECjB07FhERESzsbggLe2XIywOeeEIK\n9bJlMlFaGaKigOXLgRMngGefdft65jVr1mDDhg0V9la/cuUKGjVqpJuaD1Sxlv377+VcgsJCq7ut\nWrUKly9fxqxZs9C2bVuzwq6ct3Xr1rptrVq1wty5c+FRWgocOGBiwwA2CDsgg4LcXCAxseLX5EBW\nrlyJlStX4r333sPUqVMBAO3atWMrxg1xmLALITyEEEeEEBsddcwaxwcfSI981SpZ3lgVBg0C5s6V\n08uruRzy+vXr+O2336rtfMnJyQCA8+fPW90vJyfHwIYByiNlm332HTuAsWNlInv1aou7ERHmz5+P\n9u3bY+DAgYiKirJoxYSEhOiWuzMgKQkoKjJrw1WYPAXkZ0CIaq+OWbduHZo0aYJX9e4027Vrh7S0\nNJSVlVXrWBjn4siIfSaAUw48Xs1j/37gvvtMfNVK89ZbsvZ9xgxp7VQT8+bNw8CBA8unyK9YAdx7\nL1CREFURRdgrEmf9WacK4eHh6ACg9bx5FVeQpKbKGvGICJnI/vpri7vu2bMHSUlJmDlzJlQqFdq2\nbYvMzEyTpfguXLigu7iY8Pvv8mePHiYPVZg8BWRQ0K1btQp7SUkJNm/ejCFDhkCld6fZvn17FBcX\nu3wyGONYHCLsQogWAIYAcN8ZOUTQJCXhun5ZY1Xx8JCWTKNG8rb8738HsrPtP24FHDt2DKWlpbpF\no7F8ufT9k5Icfq6cnBzk5uYCqDhiNyfswcHB+NTDA5127wZ69wYuXzb/5NxcWUKoVkuhnDhRCq+Z\nEkYA+PjjjxEUFIQxY8YAAKKiokBEJmNUSh3Nsnu3tNWMxgzYaMUAcsyHDgHaiiBns3v3bhQUFOh6\nxyu0a9cOANhndzMcFbH/G8BsAO470yE9Har8fLz1ww+44Yga5JAQWQYZGysj+FatZNS5Z4/9x7aA\nEkHn5eXJKH3XLvmA8tMJ5wKsCzsRGTQAUxAHD6JPWRl2NWsmq4p69gSMI39lLkF2NrB+vVzQYswY\nmftYutTkXBkZGVi7di2mTJkCPz8/AFLYAcPKGI1Gg4yMDPMRu0Yj/0YWqqFsFvbBg+XPzZsr3tcB\nbNy4EV5eXujfv7/BdhZ298RuYRdCPAIgh4isegpCiClCiEQhRKISydUmSGuZHCgtxWorHq41Tp8+\njSVLlpRviI2V9cxnzwIvvywjwb59gatXHTBiQ27evImMjAwAWmE/dKg8yehEYe/YsaNVYS8oKMDd\nu3dNInb885+4qVbj1cBAYPt2acf06CG99EWLpDCGhsrp/N98U26PNW8uba6lS2W9uB4//fQTNBoN\nJk2apNumCLt+AvWPP/5AcXGx+Yj91Cng+nWziVOgEsIeFwc0bVptZY8bN25Enz59THIGISEhCA4O\n5gSqm+GIiL0HgGFCiHQA3wPoK4T41ngnIlpERPFEFB8aGuqA01YvlzdvRimAFCHwzTffVOkYb731\nFsaPH49Lly4ZPhAZKZuG/fKLXPT4f/+zf8BGnNRbvScvL0+KpRDyLmH3boeX3iUnJyMkJATdunUr\n99jv3pW9dfQwnnUKQFYNrV+P3ffdh5OZmaCuXeXFRwigTx/ZquHMGTk34MABYORI5OXlYfbs2Rg4\ncCBKx4wBsrJkIlUPxYJq2bKlbltgYCBCQ0MNhN241NGA3bvlTwsRu03JU0C+lsGDgS1bnL7Q9Zkz\nZ3DmzBkTG0ahXbt2HLG7GXYLOxG9TkQtiCgMwFMAfiWi0XaPzBZKSqqtHrxw1y6cBDDtxRexe/du\nswnB27dvm52eDgB3797Fpk2bAAA/W0qaxcUB7do5pWe3vjVy48YNKexxcVLY8/Kk1+7g80VHRyMi\nIgLXrl2T9egvvCCTz3qVOeZmneL//g+oXx/pQ4eisLBQTmCKjpbJ688/l8J/9iwwbx5KOnXCJ598\ngoiICHz44YfYunUrsuPjgaAgkyRqQUEBvLy84O3tbbDduDLGeHKSAb//Lr31iAizr9um5KnCU09J\nO+kvf7Ft/yryk/auYMiQIWYfb9++PQu7m1E769ivXAHeeUf60tHR1TLRIyg9HVmhoXjxxRcBQDcd\nXZ9x48ahY8eO5clJPXbu3ImbN29CpVLpvmgmCCG/7Lt2AcZRvZ3oC3thTk55/3gl8nSgHUNESE5O\nRkxMDNq0aQMAuLRzJ/DVV3KHadNk9A4zs07T0oCVK4Fnn0Vbrb1y4sQJ+VjLljJaj4kBhEBaWhqi\no6Mxc+ZMdOrUCXPnzgUAFBQXyw6bP/wgL1paCgoK4K8sV6eHcS27ctHWr2HXsXu3fM/0au71sdmK\nAYD+/WVl1Pz5soTWSWzcuBHR0dEWq3zatWuHy5cvO2UyGOMaHCrsRLSDiMzf7zmCI0eAceOkoP/1\nr0BgIJCeDhw96rRTAkDOsWMIKSmBV9euaNWqFXr37o1ly5YZdAXcuXMnVq9ejdu3b2OVmS/punXr\nUK9ePYwdOxbbtm0zuyoPAClIRA7/oisRNADUP3pU3v736we0bi0FUynhcwBZWVm4efMmYmJidGJS\nf9482Rxt6VJpo7z3HgAzVsz778sKl1mzEBsbC0BW85hj8eLFuHDhAjZu3IitW7eip9b3LigoAMaP\nl7XmK1fq9s/Pzzcr7FFRUcjOzkZhYSGg0SA9PR1NmjSBr6+v8QsDMjIs+utAJYUdAD76SHaHnDBB\n+vcOJj8/H7t27bJowwCcQHVHalfE/tVXwJo1sgFXamq5h+rkhkpHFy8GAISPHAkAGDNmDNLS0nDg\nwAEAcqr2zJkz0apVK7Rr1w5LjSoyiAjr16/HQw89hJEjR+LWrVvYuXOn+ZO1ayenqjvYjklOTkaX\nLl3g7e2NJikpUjyVyPOBB2TE7qCZsMrdgRKxdwDQ8vffpSc+dqy8eP3zn8Dp07hy5QqEEAgJCZGT\nv5Yulc3SmjRBaGgomjZtiuPHj5s9T1JSEqKjozFkyBAIIXSinZ+fD3TuLCN7PTumoKAAAQEBJsdR\nEqg3Z88GWrbEjdOnzUe3yl2NI4Xdy0tOqPLzAz3+OA5u326xjXBV2LJlC0pLS1nY6xi1S9jfegu4\neBH49FMpgE2bygk2v/zi1NNe1V44IrQLJYwcORI+Pj66JOrixYtx7NgxfPDBB5g4cSL27t1r4Nke\nOXIEFy9exLBhw9CnTx/4+PhYtmMAKXyHDskyP2Oq8KW/du0aLl++jJiYGAQGBiL8/HlZRaIt+cOD\nD8o6cXPn0yMtLQ1z586VEbEVUrQrBEVHRyMwMBDvqdUo8vQEXntN7vCvfwH16gHTpyPnyhWEBAfD\n85tvgPh4WeOvNzOyY8eOZiN2IsLhw4dx33336bYpwl5QUCAvWJMmyeTqnDkAkVUrZiaApgsWANnZ\niE5NNe+v//CDXDEpLs7ia7c5eapP8+bA99+DzpxBev/+mPfRR5V7vhU2btyI4OBgdLMyqS4iIgIe\nHh4s7O4EEVX7v86dO5PDePllIi8volu3HHdMPe7cuUPrPDzockCAwfY//elPFBwcTDk5ORQaGko9\ne/YkjUZD2dnZpFKp6I033tDt+9Zbb5FKpaLc3FwiIho8eDBFRESQRqMxf9KMDCKA6B//KN+m0cjX\n2r49UX5+pV7Dzp07CQBt3ryZukZGUhlA9Le/le+QkiLP99VXVo/zyiuvEAC655576MyZM6Y7aDRE\nt27R2LFjqXnz5nJbYiIRQN9ERBjuu3AhEUAr27WjJF9fef7u3YmOHDHY7bXXXiO1Wk1379412J6Z\nmUkA6NNPP9Vtu3jxIgGghQsXyg2lpURTp8pjP/MMxXfsSEOHDjUZdtGCBUQAnYqOJk27drRdCJoz\nZ47hTjdvEvn6Ej33nNX3aOLEieWvvZKsiosjAug+gL777ruKn6DREF27ZvHh0tJSatiwIY0aNarC\nQ0VFRdHIkSMrM1zGBQBIJBs0tnZF7OYYMED6xbYm/zQaufLR4sXAlCnAk08C//mPrAoxEw3v3LkT\n95aVoaxjR4PtY8eOxfXr1zFo0CBcvXoVH3/8MYQQaNq0KR566CEsW7ZMtzLNunXr0KNHD2k3QFYn\nnDt3zmyPEgAyh9Cjh6EdM3cuMG+etKDmz7fttWpRrJHo6Gj0UankH71fv/Id7rlHTpiq4D08efIk\nmjRpgpycHHTt2hVbtmwx3OGtt4AGDTD5xx/xXMOGwJ07wNy5uOnlBZMRT5oEdO+OJ0+fRlhpqVx1\n6vffTaLh2NhYlJSUmESTh7XzCixG7ICM/hcskIn2pUvx0ZkzCDX2zdeuhfdzz2GHtzc+6tQJ+QMH\nohcR2huvW/rTT/L1PPGE1feoUlUxRnx4/ToAYEpYGJ555hns2LHD+hN++kneQVj4HB08eBDXrl2z\nasMotGvXDlkpKbK5XXV2Hq2g86e7UW13Rbaov6P/OTRiv3WLyNubaNasivddtYooIEBGcABRYCBR\nq1blv4eGEr30koyEtMyeNIkIoOK//93gUCUlJdSoUSMCQBMnTjR47PvvvycAtG3bNkpPTycA9OGH\nH+oeV7bNmzfP8lg//VSO6cQJog8+kP+fNIno8ceJGjQgunqVLl++TFu2bKnwZU+fPp0CAgJIo9HQ\n+lat6JZKRWQUAdPjjxO1aWP1OOHh4fTUU0/R+fPnKTY2llQqVflr2LmTSAjS3H8/ZSrvZ/36RAD9\n1KsXeXl5UVlZmeEBL1yg9xs2pAnDh1s8Z3JyMgGgb775xmD73LlzSaVS0S29OzWNRkNCCJo7d67p\ngb74gkoBuhIQQNS3L1GvXkQ9esi7vfvvp0EPPEDdu3enQ19+SQRQygsvGD5/xAiiJk3kXYAVZsyY\nQYGBgVb3MUd2djYBoJymTan4wQepQ4cOFBAQQCdOnLD8pFdeke/ze++Zffg///kPAaBLly5VeP7X\nX3iB9gghj9emDdGvv1b6NVSaAwfkdzc11fnncjG3bt2iWbNmkRCC1q1bV+XjwMaIvfYLOxFR//5E\nMTEV7xcTQ9SuHdGSJfLDpAjN+fPShhg6VL4lP/5IRFIo/ty4sdz2yy8mh5s9ezYFBgbS5cuXDbbf\nuXOHAgICaPTo0fTJJ58QABPrIjo6mvr27Wt5rFeuEHl4EHXuLM//pz9JUUlJIRKC6NVXafbs2TZ9\ncR944AHq0aMHERFd9PenHfXqme70r3/J82RlmT3GrVu3SAhBb7/9NhERFRYW0vDhwwkAHdq6VV4g\nIyPp7JEjJADaNHs20bhxRP360Zcff0wAKMvMsRs0aEAzZ860OPaSkhLy8vKiV155xWD7kCFDKDo6\n2mT/gIAAesFYlLU85ulJ51u0kILeq5cU+DFjiK5fp8mTJ1NoaCh9vXgxnQboVvfu5U+00YYhInrp\npZfIz8+vwv2MWb16NQGg7KefJvLyoszUVGrWrBm1aNGCCgsLzT+pb1/5N9P+bY154YUXqH79+pYt\nP4WiIsq65x4qBej6rFlEERHyuFOnEuXlEZ08SfTJJ/L70aED0fLlBsFPlfnoI3me//7X/mPVFE6e\nNHlvduzYQREREQSApk+fTgUFBVU+fN0S9vffly/FmsAdPy730fNkMzIy6LPPPqPly5fT5s2b6dC+\nfXQ3IoJKIyNJc/cuJScn08tK9Kn1x/UpLi6maxY8zqlTp5Kvry8lJCTQPffcY/L47NmzydPTk/Kt\n+eUDB8pzDxlCVFxcvn3sWCIfHxrdt6+hp2wGjUZDQUFBNHXqVKKLF4kA+qs54dF64RkWor+kpCQC\nQKtXr9ZtKygooJCQENretKm8CO3fT2vXrpVif+iQbr9ffvmFANDOnTsNjnn79m0CQO+++67l94CI\nOnXqRAMHDjTY1qRJExozZozJvi1btqRx48aZbC8qKrJ6rg8++IAA0MyZM+ldgDQeHuV/8++/l3+H\nHTusjpNI5gS8vLwq3M+YF198kXx8fKh4/Xp5rs2bac2aNQSA9u3bZ/oEjUbecarV8kKfk2Oyy8MP\nP0xxcXHWT1xSQvTYY0QAPQPQpk2b5F3wyy8TqVTy+Mp3oE0bothY+f8RI8yes1KMH09K/sMtWLJE\nvp5Vq4hI5jief/55AkBt2rShXx1wF1S3hP3IEflSliyxvM9f/iLF58oV3aaJEycSAIN/Q7Qf4hlC\nUP369Wk5QCXNmlV6SPv27dMd87XXXjN5XElorlmzxvJB9u8neuEFotu3DbefP0+kVtOyBg3kmIcM\nsXiIS5cuEQD6z3/+Q7RsGRFA8R4eplFcSQnd9famBSoV3bhxw+Q4y5cvp1CALnzxhcFFbv3TT8sL\ngtaOeuedd0gIYRBlnj17lgDQ119/bXBMxZL68ssvLb8HRDRu3Dhq3Lix7nfFtvj3v/9tsm9MTAw9\n/vjjJttzcnLK3wcz/PjjjwSAOnToQAMbNZKfp0WL5IM22jBE0iISQlQcJRsRHx9PvXr1kqLq5UX0\n8ss6G2rFihWmTzh/Xo5xyhSLn/3IyEh68sknLZ+0rIxo9GgigArefdf0Pd2/n2jaNKLPPyc6d05u\nKy2VgZSXl7QuV62SF4eq0KWLHHv79lV7fk0iJ4coOFi+noQEIiLauHEjAaBp06ZZvuuqJHVL2MvK\niBo1IrKU/S8rI2rdmujhhw02x8XFUZ8+fSg1NZX27NlD69ato8VffUXpkZFU6OtLM8eNo6uhoUSP\nPlrpIWk0Gmrbti0BoL1795o8XlJSQoGBgTR+/PhKH5uIqHjKFCoGqL2XF/n4+Fj84GzZsoUA0G+/\n/UbUrx/lBwSQAMzuf7JVK0oFaPeyZYa3k4mJdDg2loqUyA0giooiGjuWNAEBdNDTk/r37k1ERE8+\n+SRFGFXA3L17l1QqlYn3feDAAQJAGzZssPpa58+fTwB0lteGDRsIAO3atctk3+7du1O/fv1Mtqel\npREAWrp0qdlzpKSk6C7EPXv0kHbEwIGVsmGI5IUNAJVUQuwKCwvJw8OjvBKnTx+i2FgqLCy0fJex\nZo38Oxw8SNSsGZFRnqK4uJg8PDwMqrNM+PFHeYy//500Gg0FBgbStGnTbBv0iRNEnTrJ5zdoQDR4\nMNGHHxIdPWrb88vKiPz8yu8IzAQTdnHnjqwqi4khSk937LHNMWaMfC3PPitfz7599Morr5C3tzfd\nuXPHYaexVdhrf1UMINu09u8vJyqZay+wb5+cMfjnP+s2FRUVITk5Gd26dUO7du3QvXt3DBs2DOMn\nTEDrVavgV1SEf/v5oeHVq3LCUCURQuDVV19F9+7d0bVrV5PHPT09MWjQIPz888/yCltJUkeMQAmA\nb1q2hKaoCNu2bTO7n1JTHuvhAWzfjtQ+fUCA2bYHiUFBaAegx9ixQECAnIhz//1AfDzuOXkSawID\nZc/z996TlTSbNkEIgROzZ2Pbjh3YsWOHrpWAPl5eXmjZsqVJfx2zfWLMYDwDNSkpCUIIxJmpJw8I\nCDBbZ69MlzdXxw4Abdq00S3NF96mjax+2b5ddo60oRpGQa1WA0ClatkPHjyIsrIy9FAW7hgwADh+\nHH6FhWjUqJH5hUqSkgBPTzmPY9gw2UxMb7GQCxcuoKysTDf5yizHjsl6/5dfhhCics3AYmLkHIGV\nK4FRo+QciFdflVVNSgsIa2RkALduybbLgJy34QiI5HyDDh2AN9+U1W7aHk1OY+tW+Tl57TU5czog\nAJg/Hzt27EBCQgJ8fHyce34zuIewA7JVa04OYG6W4ooVgK8v8Oijuk3JyckoLS01KJfT0amTnCH5\n2Wfyg2JuHxuYNGkS9uzZAw8PD7OPP/DAA7hy5Yppt0cbOJ6bi3kA4s+dwx9CwG/OHJPOiYB8nY0a\nNULwihWAtzeytY2gzAn70uBgdAXw+X33yb7mgCx9mz8fvdq0wZrevYGHH5Yf4HXrZM+eK1fw9Jtv\nomnTppgzZw7OnDljIuyAFE7j9r0mfWIs0FFbaqrMQD18+DDatm2LBg0amOzr7+9vtueJIvaWhN3H\nx0fXGyYsLEwKeVmZfK1NmlidbapPVYR9j7YH//333y83DBggf27fjvDwcPOrGyUlyT5JPj7A0KFS\nJPXKI5X3l0SCAAAgAElEQVTeN1aFPTUVCAuT3w1UocujWi3LhRcskMdSVgPTzsi2ijbgwLhxtj+n\nIgoKpA4MHy4n323fLhez2bev8sdSLhCPPipnRFvi9m3Z+ygqCnjjDbm4+eTJoP/9D7mHD6N3795V\nfjn24D7CrnwZjNsLlJTIvivDhgF6QqDUQXfu3Nn88f7xD/mlAaos7BVRUS8Ua5w6dQrvqFQo2bAB\nqS1boufJk3KcPXsaLOacnJyMbu3ayfrkUaPgpxUvc4uFXMnNxSEA/3f1qryo7d4NHDuGkueew5Hz\n53HPPfcYPkEIwMsLvr6+mDNnDvbt24fS0lKzwh4eHm4i7Nu2bYO/vz+aNm1q9bU2bNgQzZs3N4jY\nLf3d/P39zUbsyjZzLQUUFBEMDw+XF/fwcODmTWDECFkTbwNVFfbo6GgEBQXJDZ06ye6UW7ciPDzc\nNGInkiKqfC779pUzedev1+2izJFo27at5ROnpgLt2+t+bd++PbKzs3Hz5k2bx25AXJz8jmkDDI1G\ng32WRFVpSte9uxyDDcJeUlKClStXWl6f9fvvgW3b5DyPI0fk+9Ktm+wKWhlOnSq/QKxfD/ztb5b3\n/fvfgfPngYULy/VixgwQEZ4lYmG3m2bNZASzaZPhRKNt2+TCFXo2DCDFISgoyPLyZy1ayIktCQny\n2E7g3nvvBQCLvVCscfLkSURERUH9yCPIeO89NAWQ/sILcuGJF14AIL9YKSkpGE8kI4sZMxAYGAjA\nfMSem5sLDw8PZGZm6qJpQLYSKC0tRYcOHSyOZ9KkSWjRogUAWIzYL1++jNvayS+ZmZlYvXo1Jk+e\nDC8vrwpfr9JaICcnBxcvXjR/pwUp3FWJ2IFyYQ8LC5MXLcV+sdGGAaB7LbYKe1lZGfbu3atrYAZA\nXkT69QO2bkVY69bIzMw0FLPsbLkkoPIe+PgADz0EbNig++yfPXsWgYGBaGhp0XWNBjh92kDYlc/j\n3r17bXy1RqhU8qKkXWpx2bJl6N69u/kF1FNS5HcsMFB+xw4eNDtBUJ+VK1fiqaeeMttkD4D87rdu\nDcycKW0qQAr7mTPAtWsVj7+sTNpJsbFAYiLwySeyv9GyZeaXWjx2TDZxGz9erhOg0KoVTkRFYTKA\nbma+C9WB+wg7ICOr336TNooSdaxYIaOfQYMMdlX6jAgL7VcByD/y/v0WW7TaS0BAAMLCwqok7KdO\nndJF0IMGDcJNDw8s8vOTfVG+/hpYuRIZGRm4c+sW+qamykZfcXEWhV2j0eDq1avo3r07ACAxMdHg\nXABMI3Y9fHx88MEHHyAuLs5slKi071VshU8//RREhBkzZtj0emNjY3Hq1Cns10ZfloTd398fd+7c\nMRFWW4RdeX0RSq/1V16RfYksLKphDiVit3X2aUpKCgoKCsr9dYUBA4BLl9DJ1xclJSWGdp2yRq3+\nezB0qOyjpO10evbsWURFRVn+fF+8KC/2esI+YMAABAcH42sri4FXSKdO0g4tK8NibfM8syuOJSfL\nQAwAunaVNqp2hS9LbNYuI2jcZA+AnH2+fbv8nuu/ZsXessXq+flnKdSjRsmLwYwZ0l7x8pJ38PqU\nlEhBDw4GPvzQ5FCfqFQIAuDrxHbM1nAvYX/rLeDtt6WYd+4s16b84Qdg5Ej5x9FSXFyMEydOWBSH\n6iQ2NrbSVkxxcTHS0tJ0EXRQUBAefPBBrF+/XrYz7tYNmDIF57ZvxxAA/lev6qJ45Xbf2Iq5ceMG\nysrK8NBDD0EIgUN6ySxl9aX2eiJgjqeffhpHjhwxG4Erwn7+/HkUFhZi0aJFGDFihPme52bo2LEj\nSktLsXz5cgBAJwsJbcVqMbZjbBH2CRMmYPPmzWjVqpXcEBoKPPecjERtpLJWjOKvmxV2ALHaBLOB\nHZOUJMek3+ZiyBApaBs2ACgXdosoS+Hp/U29vb0xatQo/PDDD7hmS4Rrjk6dgNu3kbF1K37//Xd4\neXnhhx9+0LXXACAj49TUcmFPSJA/rYivRqPBli1boFarsXXrVtO81L59Mph7+GHD7fHx8r2yxY7Z\nt09G+gsWyL89IPMr06fL5Kh+64YPPpB2z4IFgNFdUUFBAZakpuJis2bAxx9Xy3oRxriXsHt4SHH/\n9VeZTOrZU/40smFSUlJQXFxs2V+vRmJjY3H69GkU6VU0VIRijehH0MOGDUNKSgrOZWYCK1ZAQ4Sm\nL7+MFwFomjfXVR8owmccsSvr0LZp0wbt27c3EPZTp06hdevWugWgq4K+sC9ZsgT5+fmYNWuWzc9X\nEqg//vgjIiIidHcexpj0i9FSUFAAT09PqxUK9erVw0MPPWTzmMxRFWFv0qSJaZvg8HAgIgIttclM\nE2Fv3768Oycgk4TdugHr16OoqAiZmZkV++uAgbADwMSJE1FcXIwVK1bYNH4TtBfcgwsXQqVS4R//\n+AcuX76su9MCID3poiJZWQNI68PHx7ywa+2ZpKQkXL16FW+++SY0Go3pQjebNklR7tvXcHv9+vL4\nZrz+0tJSfP311+V3V/v3y4ulcT+h2bMBb2/ppwPSRnrnHeDJJ3HdjIe+Z88eaIiQN368XOmrimsk\n24N7CbtCr17S/3r0URm5G91KJ2lvZWtKxK7RaAzWJK0Ic9bI0KFDAQAbNmzAj8eOYSqA6IIC9AWg\neu45neeoVqvh5+dnIuyKpx4aGoouXbogMTFRV4apb/tUlZCQEPj5+SEtLQ0ff/wxEhISyqtAbCAq\nKgre3t4VXpCtCbu/v791680BVFbYd+/ejR49epgf14ABqHfwINQwI+zmPrvDhgGHDyNj3z4QUcUR\ne1BQeWSqpWPHjujcuTO++uqrKpXh4p57QN7euL59OwYOHIipU6fCy8sL/9Nfx1dJnCoRu1otX4+x\nsO/YIatNkpOxefNmCCEwffp09OzZE0uWLDEc3+bNMpAzUymFbt3ksY2Srrt27cKECROwbNky+dih\nQ+V3D/o0bizv3FaskKI+fjzg74+U6dMREhKCNWvWGA17B9RqNdq89ppMKL/yigwwqxG7hV0I0VII\n8ZsQ4qQQIkUIMdMRA7ObkBDgxx9lEsSoouHw4cPw9/cv91JdiHEpny0owq5vjURERCA6Ohp//etf\n8fjjj+NwZCRujBghk1OTJxs8PzAw0GLE3qhRI3Tp0gVXrlzBxYsXodFokJqaarewCyHQpk0bfPvt\nt0hLS8NLL71Uqed7enrqkrLWLsjKHYlxAtVSL3ZHU5nk6aVLl5CRkWGYONVn2DCIwkK8EhRULuw5\nOdIfN/ceDBwIALiptWMqFPb27c3mjyZMmIBjx47pAiCF69ev47PPPrNclQIAajVutm6NyJs3MW7c\nOPj7+6N///5Yu3ZtuRArpY76yfiEBHnBUt63u3flMojnzgH/93/YvHkz4uPjERoaimeeeQapqanl\nd5XZ2TKQM7ZhFLp1kzaNcpei5erVqwAgcwonT8pqMkt96199Vd5V9O8vLwCffYbEjAwQEV566SXc\n0hNupX69XoMGMkdz8SLw7ruW3zMn4IiIvRTAy0TUAUA3AM8JISyXT9QAkpKS0KlTJ6gq4Z06i4iI\nCPj6+lbKZz958qRZa+TJJ59EYWEh5syZg/379yNo9Wq5dKC2XbBCUFCQiceuCHtoaCji4+MBAIcO\nHZIJ2Dt3rFbE2EqbNm1w48YNtGrVCsOHD6/085WLoD0Ru7OpTMSurMClJKxNGDQI6N0br928ievK\nmqzKXAVzwt6xIxAQALXWt7dJ2M3w5z//GT4+PvhKWaMWwJ07dzB06FA8//zz+FVZucwChzUa3CcE\nHh02DAAwYsQIpKen46iyhGVysqyfr1+//EkJCUBREcZ07Ihz587JJOaZM0DPnqCVK5G9d6/OJnvi\niSfg6+uLJUuWyOcq7aONCiR0KHeGRnaMEtzs3bsXl5VSUUvC3qiRrJC5fFmWQT7xhG7h+qysLLyn\nXerx5s2bOKxfv96jhyzmUF5PNWG3shHRH0SUpP3/TQCnADS397hV4YsvvkDPnj0tzsIEpK927Nix\nGuGvA4CHhwdiYmIqHbGbE9o5c+YgKysL7777rowchZCz4IwwF7ErVkxISAji4uLg6emJQ4cO2VQR\nYyuKzz5jxgx4KuVoleCBBx6An5+f1b+dqyP2ylTFXL58GQAsl9wKAXzyCeqXleEJZTanEkWbW8XJ\nwwN44AE0Pn0aoaGhFvMQyM8H/vjDorAHBgZixIgRWLFiBe7cuYOysjKMHj0a+/btgxACu3fvtvia\nCgoK8GNGBoKI4KP9TA0bNgweHh7ldkxKSrm/rqC1QBqcOoVPZs2SVSgjRwLffQcCMIsIg7TCHRAQ\ngOHDh+O7776TuanNm2VJsrZc04SoKFm9YpRAVb4DQghkrV4t94mMtPja8Prrcjbr558DQuDcuXMI\nCwvDqFGj8OGHH+L8+fPYs2cPysrKDOvX339f+vYzZ1ZY0ukoHBqyCiHCAHQC4IBpZJWjpKQEf/vb\n37Bnzx4MGDAAjz76qO6Kqs+pU6dQVFRUI/x1BaUyxhZPs6yszKI14unpiWY21NxbsmKCgoKgVqvh\n4+ODe++9F4mJiTrv3xHC/uCDDyIqKgqTJk2q0vPHjh2LrKys8ok8ZrAUsVtayNrRVCZiV/4G1iZN\n4d57kdilC0bfuoXiQ4eksEdGmr1gAwB690aT/Hx0s1ZtpMwutVLlNGHCBOTn52Pt2rV4+eWXsXbt\nWsybNw9xcXFWhX3VqlU4oLx27d1FSEgIevXqhbVr10qr5fTpcn9doXVrFPr5IQFAvw0bUCaEnGjU\nogX2tWmDiQAStIEBADzzzDPIy8vDxh9/lJMSjcsc9RHC7ESlvLw8eHp6YvDgwWiQkgLq2tV6aXNg\noEygavMSaWlpiIyMxPvvvw9PT0+89NJLOn/dIH/UpIms1tu82WASmTNxmLALIeoD+B+AF4nIZOqf\nEGKKECJRCJGo3PY7kg0bNiA7OxsrV67EP//5T/z666/o0KED/va3vxkIZoUzTl1Ax44ddeuSVkRG\nRgaKiorsEtrAwEATKyYnJweheok0JYF68uRJNG7cGMHGKwpVgcceewxnzpyxHElWgEqlsirqgHUr\nxqqAOojKCruvry+8vb2t7nf+mWdwHUDZs88azjg1R69eAICH69WzvI+Fihh9evfujfDwcMycORMf\nf/wxZs6ciVmzZqFnz544cOCAxde3ZMkSFLdrB1KpDFpcDB8+HKdOncK5zZuluBtH7EIgrWFDjAQw\nDMCyNm2AFi1ARHgjPx/1AHh+/rlu9759+6JFixbY/8knwI0blm0YhW7dpI+udyeXl5eHwMBATP7T\nn9C2tBTnjBLJFaEIe/PmzTF37lysW7cOX375Jbp27Yp6xu//c8/Ji9mLL8reQ07GIcIuhFBDivpy\nIlprbh8iWkRE8UQUH1rJN9AW/vvf/6JVq1YYMWIEXn/9dZw5cwYjRozA22+/LbPeWpKSkuDn52fd\nf6xmKtNaQLFG7PG8g4KCzEbs+s244uPjkZeXh02bNjkkWq8ufH194enp6XIrxlZht+Ui1zwmBnMA\n+CYmAhcuWBX2wshIFABIsFY+m5oqK1GMSyz1UKlUGD9+PK5du4bhw4dj3rx5AICePXvi1q1bZj+r\nZ8+exZ49e/DUhAkQ7duX20YAHtcuBH9UKaM0jtgBHFWr4Qcgp1EjTElJwcGDB5GSkoKdOTnIiIuT\niUhtktLDwwNjxoxB4P79IA+P8pYilujWTdogBw/qNt24cQNBQUF4OCQEKgCrKpggpc/169dx48YN\nRGqtmxdffBFRUVG4du2a+TYCarUcf3q685uSwTFVMQLAVwBOEdG/7B9S5Tl9+jS2b9+OqVOn6hpu\nNW3aFN9++y169+6N5557Ttc74/Dhw+jUqZPFxlyuoDKtBRxhjQQGBiI/P99g0oi5iB2QPnBtEnYh\nhNl+MTWxKsZWYQ8PD8diALnKxCkrwp6Wno7fAURaayyXmirtHO1FyBKzZs3C559/jm+//Vb3fVEm\nUpmzY1auXAkAGDVqlKxn14vYmzVrhvvvvx/XduyQE4bM3C1sLClBkYcH6n/zDQJDQvDGG2/oZpv6\n/vWvwPXrgF5Cd/z48XiICBebN5c2iTUUm0Uvgaq8/17acX6yf7/ZHkrmOHfuHIDyWcre3t745JNP\noFKp8LCl6pzevWUfmioUDlQWR0TsPQCMAdBXCHFU+2+wA45rM59//jnUajUmTpxosN3DwwPffvst\nfHx88NRTT+H27ds4evRojfLXASA4OBgtWrSwSdhPnTqFxo0bV2hJWCMwMBBEZNDsKTc310DYo6Oj\ndZN5HFERU50Y94spLi5GUVFRjUue2irszZo1g4daje/69JEzTK3U/589exY7AfhfuiS7b5rDSkWM\nPvXr18fUqVPhqzdhp3nz5ggPDzcr7GvWrEH37t3RvHlzKeyXLsmeNlqGDx+O4MuXUdK6tckkICLC\nz7m5eHPGDNQbOBCvv/46tm3bhvnz5yMmJgaNHntMVph89BHwxRfAf/+LqA0b0BnA93l51kswAZmT\n6NDBwGfXvf/796MoLAxXiovx/fffV/i+ANDl7yL1kq2DBg3C9evXTWcR62PD++4IHFEVs5uIBBHF\nElGc9t/PjhicLdy6dQtff/01RowYYbb9a/PmzfH111/jyJEjeOKJJ3D79u0a5a8rdOzY0eaI3V6h\nNW4roPSJ0bdi1Gq1rt95bYrYAdMOj8oFrLZaMR4eHmjVqhX2FhUBGzcalgkacfbsWexQftm1y3SH\nkhLZ0MoOgenZsyd2795tkLtKS0vDsWPHMHLkSLlBafmgF7U/8MADiAFw1cz39Nq1a7hz5w5aae2h\n6dOno1mzZsjOztZVw+DNN4GsLGDKFOlZv/wy4OmJJQUF+OmnnyoeuJJA1Y47Ly8PgQEBwIED8O7V\nC7GxsTb3yVEi9jZ6CV2ggkR4NeL6Qm47+f7775Gfn49nn33W4j5Dhw7FCy+8gJ9/ltebmhaxA+VN\nru7evWtxHyJyyCxQ40Zg169fh0ajgXHuQ7Fjaruw29InxlE4Q9gBmG/fa4YzZ87gctOmUvz1+rPr\nuHBBirudwn7lyhWduAHQlTKOGDFCblDKMfWEPaplS0QCSDdzYcrU9jxX+vT4+vrirbfeAgAM0a4h\ngEGDZJlmVpasJ796FZrr15HXrBkWLFhQ8cDvv18mWrX5gby8PER4eAA5ORDdumH8+PE4dOiQTbPA\n09LS0KxZM9MkaQ2hVgs7EeG///0vYmJiLM/e06J0Hqxfv36FzaxcQWxsLEpLS5FqNDtOnz/++AMF\nBQV2R+zGwq4/61Sf559/Hh988EGF/dJrGsZWTF0S9rNnz6JN27bStti503QHGypiKkL5runbMWvW\nrEGXLl3KG6gFB8tJSHoJ1OALF+AJINlMWW+GNnGpez6AKVOmIDEx0TAZ2aSJbPfbuDHQsCE8GzTA\n5MmTsWXLFpN+/yYMHizbKPz5z0B+Pm7cuIF7lRmj3brpLkpblAlPVlAqYmoqtVrYDx06hKSkJEyf\nPr3CHiDe3t7YsmULfv311ypNjnE2SmWMNTvGUTXlipgoVox+nxh92rZti1dffdXp/VUcjSsjdluT\np0RUaWHPzc1Fod4iKubQdXXs3VtOBDIuLVaEvV07m85rjvbt2yM4OFgn7BkZGUhMTCy3YRSUBOrV\nq8CsWUDfvrilUmG7mYod44gdkIlwW2zTyZMnQ6VSYeHChdZ3bNoUWLsWOHsWZSNHovTuXURdvy4X\nKYmJQcuWLREVFVXhzFpAWjEs7E7i3XffRf369TF69Gib9lf6oNRE2rZtC29vb6vC7qhZoIrHXlHE\nXlupCRF7RclTpWe8rZ6s0v3R7DJ5WvLy8pCbmyuFXVvPbuKzp6ZKgbPDC1apVOjRo4dO2E1sGIVO\nnWR3wzZt5KIVY8fiL48+in1ZWSbHzMzMhK+vr+WFQazQvHlzDBs2DIsXL664S2rv3sCCBfDYtg3/\nBtAqO1u29tUGe3379sXOnTtRWlpq8RCFhYW4fPlyjeg1ZYlaK+zr1q3D+vXrMXfu3Gr5wjobT09P\nREdHW61lP378OIKDg9GkSRO7zmXJinHG/AJXUBs8duW9r0zEDsCqHaOsc9q2bVspVvXqmdoxNlbE\nVETPnj1x+vRp5ObmYs2aNYiLizMVOsVC6d9f9of54guEdOyIrKwsEwHOzMxE69atq3x3OH36dFy9\netWk06JZJk3CtQkT8DyAxunpBh0d+/btq+v3Ygklt8ARu4MpLCzEjBkzEBMTU6me3jWdihbdSEpK\nqnjVJxtQ2tcq4qJYMVWJlmoi/v7+KC4u1iWilei9Js08raywK/1krAm7ckfXtm1bWaPeo4dhApXI\nYcKulPStXr0a+/btM7VhANkuu6BA2h/au8yoqCgQkYkfnpGRYWDDVJZ+/fohMjLStiQqgLMTJmCd\n8ote4y/Fz7dmx7CwO4m3334bWVlZWLhwoe6L5A506tRJt6anMcXFxUhOTnZIRY9KpYK/v7/OY8/N\nzUVwcLDbvJfGjcDcIWJv1KgR6tWrZ1XY9+/fjwYNGqCd4p/36gWcOCEXY05MlHXtN244RNjj4+Ph\n7e2Nv2kXejYr7IBJf3Rlxrdyd6GQmZlpl7CrVCpMmzYNe/fu1V3grHGjoACjAFx45RXgkUd02xs1\naoR7773XqrArNexsxTiQY8eOYf78+Zg8ebLldqe1lATtLeEBMyvJnDx5EsXFxQ4r1dRvK2A867S2\nY9wvpqCgACqVqlpK04QQ8PT0dLiwCyEQFhZm1WPfu3cvunXrVj6revx4GTW/8w7QpUt5wtQBwu7t\n7Y0uXbogNzcX0dHR5ReTClCiXH1hLyoqwpUrV+wSdgAYPFjOi9Rfr9cSeXl5uAXg7sSJBstmAtKO\n2b17t8XS47S0NISEhNSYmnVz1Cph12g0mDZtGoKDg3X9j92JuLg4eHl54aBePwsFZdEDS2t9Vhb9\nDo/GfWJqO+Yi9upYPUlBrVZXmDytrLAD1kseCwoKcOLECcNgp1kzmTy9cgVYtkzWgXfuLEXeAShl\njxajdTMEBQWhYcOGBsKu3KHauv6tJSIjI+Hl5YVkZYUmK1h7//v27YuioiLD5fz0qOkVMUAtE/Yv\nvvgC+/fvx7x58xzSbbCm4e3tjbi4OLMR+5EjR1C/fn2HfaD0OzzWhYi9OhPsarXa4RE7UC7s5to7\nHzx4EBqNxvxdbGgoMGYMsHKltGTsaEehzyOPPAJfX188/fTTlXpeVFSUQUttczXsVUGtVqN9+/Z2\nC/uDDz4IlUpl0Y6p6TXsQC0T9rt372LIkCE2lzfWRhISEpCYmGjS+8LRqz7pWzHGfWJqO4qIG0fs\n1UVlhL0yt/Ph4eEoKCgw26hq7969EELo7LzqoEePHrh586bNNoxCVFSUQcRuroa9qkRHRyNFWXrP\nCjdu3ICPj4/Zxc0DAwPRuXNns8J+9+5dZGVl1Wh/Hahlwv7CCy9gw4YNtW7CTGXo2rUrbt26ZfDh\nLCsrw9GjRx1mwwDlVkxZWRmuXbvmllZMTY/YLQmLJZQuoOYacO3duxcxMTHV7vtWpUtqZGQksrKy\ncEfblzwzMxNCCNk8zE5iYmKQkZFh0ODOHBVNDuvbty/2799vsJYpAN0dE0fsDsadRR0wn0A9c+YM\nbt++7dAeN4qwW+oTU5txtRXj5eVlk7BXdsGR3r17IygoCKtXrzbYrtFosG/fvlpTTKBUxiglj5mZ\nmWjSpEmFC47YgrLgeUX9XmwR9tLSUpOLqLmujjWRWifs7k5kZCSCg4MNEqhHtI2UHC3shYWFyM7O\nBuA+s06B2mPFVFbY1Wo1Hn/8caxfv96gYuPkyZMoKCiodcKu2DHK5CRHEK1dwKMin72i979Hjx5Q\nq9UmdgwLO1MlhBDo2rWrQcSelJQEb29vhzYvU9oKKF8ud4rYvb294e3t7VIrxpaqmKosEfjEE0+g\noKAAv/zyi27b3r17AaDWCLtxyaO9k5P0CQ8Ph6+vr93C7ufnh27dupkI+7lz5+Dv71/jJ/OxsNdA\nEhISkJKSomv4lJSUhNjYWIdOIFI+1O4o7ICM2pWIPT8/v1q9Z2dF7ICcYWlsx+zduxehoaE1PqGn\nEBgYiJCQEJw9exZEZPfkJH1UKpVNCVRlWTxr9O3bF0lJSQbJaqUipqZbwizsNZCEhARoNBokJiaC\niHDkyBGH95BXREVZMtCdrBhAJlALCgpQWlqK27dvu4UVoxz7sccew7p163R2zN69e9GjR48aLzb6\nKCWPubm5uHv3rsOEHZB2jL0ROyAnPGk0GgwaNEiXD6gNpY6A4xazHiSEOC2ESBNC/MURx6zLKB0o\nDxw4gPT0dOTl5Tm0IgYwFfaafmtZWZRGYNW5epKCM4UdMLRjcnNzcfbs2VpjwygoJY9KqaOjPHZA\nJlD/+OMPXL9+3ezjtrZM7tq1K9asWYMzZ86gU6dOWL58OdLT02vFnZEjFrP2APAZgIcBdADwtBCi\ndi2SWcMICQlBREQEDh48qJtx6uiIXd9jb9iwYY3sUW8PSuve6uwTo1BRVUxle7Ebo2/H7NMuzlzb\nhD0yMhIXL17ULSzj6IgdgEU75vbt2ygtLbXp/R8xYgSOHj2K6OhojB49GqWlpXUmYu8KII2IzhNR\nMYDvATzqgOPWaRISEnDgwAEcOXIEHh4euhpmR6F8qN1tcpKCErG7QtgrSp4qvdirKuxeXl46O+a3\n336DWq2ukev4WkOpjPntt98AOFbYlZJHS3ZMZWf9tm7dGjt37sScOXNQr169ap0EVlUcIezNAeh3\nzr+o3cbYQUJCAi5duoQNGzYgOjq6UhNZbEH/Q+2uwu6qiL0iK6Yq7QSMUeyYRYsWoXPnzg7/fDgb\nRdi3b98OPz+/ChOZlaFFixbw9/e3GLErydDKnFOtVuPdd99FYWGh7o6gJlNtyVMhxBQhRKIQIjHX\neAeH0akAABAJSURBVLkuxgQlKjh+/LjD/XVAlnMp9ou7JU6B8uSpuwp7v379EBgYiNu3b9c6GwYo\nL3nMyMiwa4ENcwghrCZQ7Xn/a0uC2hHCfglAS73fW2i3GUBEi4gonoji3TFCdDRKp0fA8f46ID+g\nygfbHf8eihWjlDzWJGFXxmSPsCt2DFD7/HVAXniVz50jbRiFmJgYJCcnm22Y5ogLa03HEcJ+CECU\nECJcCOEF4CkA6x1w3DqN0ukRcI6wA+UfbHeN2MvKynD58mUANSt56ihhmTZtGu69917dqj+1DcWO\ncZawX7t2Tbc6mD4s7DZARKUAngewBcApAKuIqOL2akyFdOvWDSqVCh07dnTK8d09YgfKe33XpORp\nVTo7miMhIQHHjx+vtaWqzhR2a60FquKx1zYc4rET0c9E1JaIIojoXUcckwHmzJmDzZs3o4HR8mKO\nQvlgu7OwZ2VlQQiB+vXrV9u5q8NjdwecHbED5kseHXVhrcnwzNMaTOPGjTFgwACnHd/drRhACnuD\nBg0c1sfeFljYbUMRdmWhbkfSqFEjhISEmI3Y8/Ly4Ofn5zZr/JqDhb0OU1esmOq0YQDbhN3b27vW\nlSg6mkcffRQLFy50SvLXWmWMPZPDagss7HUYxYpx54g9Ozu72oXdluSpuwuLLXh7e2PKlClVWqzD\nFmJiYpCSkmJSGVMX3n8W9jpMx44dERkZWWuTb9ZQxLysrKxGRuzuLiw1gejoaBQUFOgS6Aq2dHas\n7bCw12H+/Oc/4+zZs06LmFyJvpi7QtgrqophYXc+lloL1IX3n4WdcUtcLewajQYajcbs43VBWGoC\nLOwM42Z4enqiXr16AFwj7AAs2jF1QVhqAkFBQWjevDkLO8O4E0oC1RXJU4CFvSagtBZQ0Gg0yM/P\nd/v3n4WdcVsUQa9JEbu9vdiZyhETE4OTJ0+irKwMAHDz5k1oNBpOnjJMbcXVwm4ugVpUVITi4mIW\n9moiJiYGRUVFOHfuHIC6MzmMhZ1xWxQrprqnjluL2OuKsNQUjBOodeX9Z2Fn3BZXR+ws7K6nQ4cO\nEEKwsDOMu+Cq5CkLe82hXr16iIiIYGFnGHfBVRG7taqYuiIsNQn9ypi60LIXYGFn3BhXWzHmkqcs\n7NVPTEwMzpw5g7t379aZ95+FnXFb2IphACnsZWVlSE1N1b3/1f2ZqG5Y2Bm3ZeDAgRg1ahSaNWtW\nredlYa9Z6FfG5OXlwd/f3y37I+ljl7ALIT4UQqQKIY4LIX4QQvCnlakx3Hvvvfj222/h6elZreet\nSNi5F3v10rZtW6jVaiQnJ9eJzo6A/RH7VgAxRBQL4AyA1+0fEsPUbipKnnK0Xr2o1Wq0b99eF7HX\nhfffLmEnol+0i1kDwH4ALewfEsPUbiqK2OuCsNQ0YmJicOLEiTrz/jvSY58AYJMDj8cwtZKKqmLc\neRHlmkpMTAwyMjKQmZnJwg4AQohtQohkM/8e1dvnDQClAJZbOc4UIUSiECIxNzfXMaNnmBoIR+w1\nDyWBmp6eXife/wqzSkTU39rjQohxAB4B0I+MFxc0PM4iAIsAID4+3uJ+DFPbqUjYw8LCqnlEjCLs\ngPtPTgLsr4oZBGA2gGFEdNsxQ2KY2g0nT2seYWFh8PPzA1A3Sk3t9dg/BdAAwFYhxFEhxOcOGBPD\n1GosRezci911qFQqREdHA6gbwm5XgS8RRTpqIAzjLlhKnnIvdtcSExODgwcP1on3n2eeMoyDsRSx\n86xT16L47HXh/WdhZxgHw8JeM0lISAAAtG7d2sUjcT7VO9eaYeoAlpKn+fn5AFjYXUX37t1x/vx5\nhIeHu3ooTocjdoZxMByx11zqgqgDLOwM43BUKhVUKpVJ8pSFnakuWNgZxgmo1WqLVoy79wJnXA8L\nO8M4AXPCfvPmTQBAgwYNXDEkpg7Bws4wTsCasNevX98VQ2LqECzsDOMEvLy8zAq7n58fVCr+2jHO\nhT9hDOME1Gq1SfL05s2bbMMw1QILO8M4AUtWDAs7Ux2wsDOME2BhZ1wJCzvDOAEWdsaVsLAzjBOw\nlDxlYWeqAxZ2hnECHLEzroSFnWGcAFfFMK6EhZ1hnABH7IwrcYiwCyFeFkKQECLEEcdjmNqOsbCX\nlpbizp07LOxMtWC3sAshWgIYCCDT/uEwjHtgnDwtLCwEwH1imOrBERH7fACzAZADjsUwboFxxM4N\nwJjqxC5hF0I8CuASER1z0HgYxi0wTp6ysDPVSYVL4wkhtgFoYuahNwDMgbRhKkQIMQXAFABo1apV\nJYbIMLUPjtgZV1KhsBNRf3PbhRD3AggHcEwIAQAtACQJIboS0WUzx1kEYBEAxMfHs23DuDUs7Iwr\nqfJi1kR0AkAj5XchRDqAeCK66oBxMUythoWdcSVcx84wTsC4KoaFnalOqhyxG0NEYY46FsPUdjh5\nyrgSjtgZxgmwFcO4EhZ2hnEC5oRdpVLB19fXhaNi6gos7AzjBNRqNUpLS0EkC8CUPjHaCjKGcSos\n7AzjBLy8vADIHjEANwBjqhcWdoZxAmq1GgB0dgwLO1OdsLAzjBNQhF2pjGFhZ6oTFnaGcQIcsTOu\nhIWdYZwACzvjSljYGcYJKMlTFnbGFbCwM4wT4IidcSUs7AzjBDh5yrgSFnaGcQL6Efvdu3dRUlLC\nws5UGyzsDOME9IWd+8Qw1Q0LO8M4Af3kKQs7U92wsDOME+CInXElLOwM4wT0k6cs7Ex147CFNhiG\nKUc/YlcagbGwM9WF3RG7EGKGECJVCJEihPjAEYNimNoOWzGMK7ErYhdC9AHwKICORHRXCNGooucw\nTF2AhZ1xJfZG7NMBvEdEdwGAiHLsHxLD1H64KoZxJfYKe1sADwghDgghdgohujhiUAxT2+GInXEl\nFVoxQohtAJqYeegN7fODAXQD0AXAKiFEG1LWAzM8zhQAUwCgVatW9oyZYWo8xlUxXl5euiieYZxN\nhcJORP0tPSaEmA5grVbIDwohNABCAOSaOc4iAIsAID4+3kT4GcadMI7YOVpnqhN7rZgfAfQBACFE\nWwBeAK7aOyiGqe2wsDOuxN469sUAFgshkgEUA3jGnA3DMHUN4+QpCztTndgl7ERUDGC0g8bCMG4D\nR+yMK+GWAgzjBIyTpyzsTHXCws4wTsDDwwMAR+yMa2BhZxgnIISAWq1mYWdcAgs7wzgJLy8vFnbG\nJbCwM4yTUKvVKC4uRmFhIQs7U62wsDOMk1Cr1cjPz4dGo2FhZ6oVFnaGcRJqtRrXr18HwH1imOqF\nhZ1hnIRarcaNGzcAsLAz1QsLO8M4CS8vL47YGZfAws4wToKtGMZVsLAzjJNQq9W4du0aABZ2pnph\nYWcYJ6FWq3kha8YlsLAzjJNQ+sUALOxM9cLCzjBOgoWdcRUs7AzjJPSXwqtfv74LR8LUNVjYGcZJ\nKBF7vXr1dN0eGaY6YGFnGCehCDvbMEx1Y5ewCyHihBD7hRBHhRCJQoiujhoYw9R2WNgZV2FvxP4B\ngLeJKA7AW9rfGYYBCzvjOuwVdgLgr/1/AIBsO4/HMG6DkjxlYWeqG7sWswbwIoAtQoiPIC8S3e0f\nEsO4BxyxM66iQmEXQmwD0MTMQ28A6AdgFhH9TwjxJICvAPS3cJwpAKYAQKtWrao8YIapLbCwM66i\nQmEnIrNCDQBCiGUAZmp/XQ3gSyvHWQRgEQDEx8dT5YbJMLUPFnbGVdjrsWcD6KX9f18AZ+08HsO4\nDSzsjKuw12OfDOBjIYQngCJorRaGYTh5yrgOu4SdiHYD6OygsTCMW8ERO+MqeOYpwzgJFnbGVbCw\nM4yTYGFnXAULO8M4CRZ2xlWwsDOMk2BhZ1wFCzvDOAmuimFcBQs7wzgJFnbGVbCwM4yTePjhh/HG\nG28gIiLC1UNh6hiCqPpn98fHx1NiYmK1n5dhGKY2I4Q4TETxFe3HETvDMIybwcLOMAzjZrCwMwzD\nuBks7AzDMG4GCzvDMIybwcLOMAzjZrCwMwzDuBks7AzDMG6GSyYoCSFyAWRU8ekhAK46cDiOhsdn\nHzw+++Dx2U9NHmNrIgqtaCeXCLs9CCESbZl55Sp4fPbB47MPHp/91IYxVgRbMQzDMG4GCzvDMIyb\nURuFfZGrB1ABPD774PHZB4/PfmrDGK1S6zx2hmEYxjq1MWJnGIZhrFCrhF0IMUgIcVoIkSaE+EsN\nGM9iIUSOEP/fvtmEWFmFcfz3x8k+pnC0QoZGGCNRZqGjgSlJlFGohKsWSQsXQhsXCoE0BEHLNpWL\naFPUJiyyL5lFZZOrFmN+jDU6TRYNOKJORCIURNa/xTmXXi4SjS7OuZfnB4f3nOfcxY/3ufe5933e\n92qyEVsi6bCks/m4uKDfMklHJJ2RdFrSnpocJd0i6aikU9nvxRxfLmk85/k9SQtL+DU8F0g6KWm0\nNj9JM5K+lTQh6ViOVZHf7NIn6aCk7yRNSdpYi5+klfm8tcYVSXtr8bsROqawS1oAvAZsBYaAHZKG\nylrxNrClLfYcMGZ7BTCW16W4CjxrewjYAOzO56wWxz+AzbbXAMPAFkkbgJeAV2zfB/wK7Crk12IP\nMNVY1+b3iO3hxiN6teQXYD/wqe1VwBrSeazCz/Z0Pm/DwP3A78BHtfjdELY7YgAbgc8a6xFgpAKv\nQWCysZ4G+vO8H5gu7dhw+wR4rEZH4DbgBPAA6c8hPdfKewGvAdKHezMwCqgyvxngrrZYFfkFFgE/\nke/l1ebX5vQ48FWtfvMdHfOLHbgHONdYz+ZYbSy1fSHPLwJLS8q0kDQIrAXGqcgxtzkmgDngMPAj\ncNn21fyS0nl+FdgH/J3Xd1KXn4HPJR2X9EyO1ZLf5cDPwFu5lfWGpN6K/Jo8BRzI8xr95kUnFfaO\nw+krv/hjR5JuBz4A9tq+0twr7Wj7L6dL4QFgPbCqlEs7kp4A5mwfL+3yH2yyvY7Uotwt6aHmZuH8\n9gDrgNdtrwV+o62tUfr9B5DvkWwH3m/fq8Hveuikwn4eWNZYD+RYbVyS1A+Qj3MlZSTdRCrq79j+\nMIercgSwfRk4Qmpt9EnqyVsl8/wgsF3SDPAuqR2zn3r8sH0+H+dI/eH11JPfWWDW9nheHyQV+lr8\nWmwFTti+lNe1+c2bTirsXwMr8hMJC0mXTocKO12LQ8DOPN9J6msXQZKAN4Ep2y83tqpwlHS3pL48\nv5XU/58iFfgnS/vZHrE9YHuQ9H770vbTtfhJ6pV0R2tO6hNPUkl+bV8EzklamUOPAmeoxK/BDv5t\nw0B9fvOndJN/njc4tgHfk/qwz1fgcwC4APxJ+nWyi9SDHQPOAl8ASwr6bSJdRn4DTOSxrRZHYDVw\nMvtNAi/k+L3AUeAH0uXxzRXk+mFgtCa/7HEqj9Otz0Qt+c0uw8CxnOOPgcWV+fUCvwCLGrFq/K53\nxD9PgyAIuoxOasUEQRAE/4Mo7EEQBF1GFPYgCIIuIwp7EARBlxGFPQiCoMuIwh4EQdBlRGEPgiDo\nMqKwB0EQdBn/AIOvYm1i+VCcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcaedb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 2.56865040263 \n",
      "Fixed scheme MAE:  1.6413753593\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 1.1855  Test loss = 2.7357  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 1.2191  Test loss = 2.3722  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 1.2381  Test loss = 0.5005  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 1.2395  Test loss = 0.4501  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 1.1501  Test loss = 1.0663  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 1.1197  Test loss = 0.2494  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 1.0956  Test loss = 0.2488  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 1.0527  Test loss = 0.1906  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 0.9854  Test loss = 0.1459  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 0.9784  Test loss = 1.8772  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 0.9735  Test loss = 1.6978  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 0.9898  Test loss = 2.0306  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 0.9252  Test loss = 0.4891  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 0.9266  Test loss = 1.8053  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 0.9532  Test loss = 2.8394  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 1.0137  Test loss = 3.2973  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 1.0222  Test loss = 0.7120  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 1.0167  Test loss = 0.4019  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 1.0168  Test loss = 0.7228  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 0.9465  Test loss = 0.3729  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 0.8779  Test loss = 1.4880  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 0.8957  Test loss = 4.3306  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 1.0373  Test loss = 0.9138  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 1.0362  Test loss = 1.3379  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 0.9651  Test loss = 0.9041  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 0.9647  Test loss = 0.0277  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 0.9645  Test loss = 0.4465  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 0.9635  Test loss = 1.8041  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 0.9521  Test loss = 0.1485  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 0.9512  Test loss = 0.5474  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 0.9487  Test loss = 4.0457  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 1.0657  Test loss = 0.5549  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 0.9959  Test loss = 1.4746  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 1.0125  Test loss = 0.0095  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 0.9727  Test loss = 0.0115  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 0.9626  Test loss = 5.0905  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 1.1028  Test loss = 1.3637  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 1.1039  Test loss = 2.4661  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 1.1351  Test loss = 0.8586  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 1.1400  Test loss = 2.7063  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 1.1097  Test loss = 1.1888  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 1.1191  Test loss = 1.7844  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 1.1407  Test loss = 2.1203  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 1.1689  Test loss = 13.1049  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 1.9910  Test loss = 7.1939  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.1811  Test loss = 1.5598  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.1895  Test loss = 0.6328  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.1903  Test loss = 0.3230  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 1.7751  Test loss = 2.0090  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 1.7870  Test loss = 1.6863  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 1.7991  Test loss = 2.2851  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 1.8203  Test loss = 1.4352  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 1.7855  Test loss = 3.4425  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 1.8344  Test loss = 2.6601  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 1.8619  Test loss = 0.4329  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 1.8550  Test loss = 1.3998  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 1.7657  Test loss = 1.0732  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 1.7707  Test loss = 2.4905  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 1.7964  Test loss = 0.6116  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 1.7978  Test loss = 0.3537  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.7300  Test loss = 0.2089  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.7302  Test loss = 2.7861  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 1.7561  Test loss = 0.6385  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 1.7503  Test loss = 0.0078  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 1.6828  Test loss = 0.3003  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 1.6828  Test loss = 0.2933  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.6792  Test loss = 1.4177  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 1.6878  Test loss = 2.6966  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 1.6625  Test loss = 5.3283  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 1.7886  Test loss = 0.0082  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 1.7886  Test loss = 1.2180  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 1.7919  Test loss = 2.6126  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 1.7335  Test loss = 2.5944  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 1.7627  Test loss = 0.3686  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 1.7608  Test loss = 0.9051  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 1.7642  Test loss = 0.7110  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 1.6844  Test loss = 2.0555  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd427a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.50590951494 \n",
      "Updating scheme MAE:  1.64512468614\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_test_fixed.csv\", yhat_test_fixed)\n",
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_update.csv\", yhat_update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
