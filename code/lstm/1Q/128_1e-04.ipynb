{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"1Q/128_units/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 500\n",
    "learning_rate = 1e-4\n",
    "batch_size = 5\n",
    "early_stop_iters = 10\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 128 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag1',\n",
    "                                       'inflation.lag2',\n",
    "                                       'inflation.lag3',\n",
    "                                       'inflation.lag4']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag1',\n",
    "                                   'unemp.lag2',\n",
    "                                   'unemp.lag3',\n",
    "                                   'unemp.lag4']])\n",
    "train_4lag_oil = np.array(train[['oil.lag1',\n",
    "                                 'oil.lag2',\n",
    "                                 'oil.lag3',\n",
    "                                 'oil.lag4']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag1',\n",
    "                                     'inflation.lag2',\n",
    "                                     'inflation.lag3',\n",
    "                                     'inflation.lag4']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag1',\n",
    "                                 'unemp.lag2',\n",
    "                                 'unemp.lag3',\n",
    "                                 'unemp.lag4']])\n",
    "test_4lag_oil = np.array(test[['oil.lag1',\n",
    "                               'oil.lag2',\n",
    "                               'oil.lag3',\n",
    "                               'oil.lag4']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 128 \n",
      "Learning rate = 0.0001 \n",
      "Epochs = 500 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 10 \n",
      "Learning rate = 0.0001\n",
      "Fold: 1  Epoch: 1  Training loss = 3.2847  Validation loss = 3.4321  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 3.2542  Validation loss = 3.3812  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 3.2325  Validation loss = 3.3449  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 3.2028  Validation loss = 3.2952  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 3.1640  Validation loss = 3.2257  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 3.1376  Validation loss = 3.1791  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 3.1085  Validation loss = 3.1267  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 3.0718  Validation loss = 3.0569  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 3.0497  Validation loss = 3.0138  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 3.0269  Validation loss = 2.9698  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 3.0101  Validation loss = 2.9366  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 2.9860  Validation loss = 2.8873  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 2.9658  Validation loss = 2.8461  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 2.9487  Validation loss = 2.8098  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 2.9354  Validation loss = 2.7807  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 2.9121  Validation loss = 2.7281  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 2.8992  Validation loss = 2.6980  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 2.8834  Validation loss = 2.6622  \n",
      "\n",
      "Fold: 1  Epoch: 19  Training loss = 2.8658  Validation loss = 2.6206  \n",
      "\n",
      "Fold: 1  Epoch: 20  Training loss = 2.8454  Validation loss = 2.5711  \n",
      "\n",
      "Fold: 1  Epoch: 21  Training loss = 2.8324  Validation loss = 2.5376  \n",
      "\n",
      "Fold: 1  Epoch: 22  Training loss = 2.8225  Validation loss = 2.5119  \n",
      "\n",
      "Fold: 1  Epoch: 23  Training loss = 2.8121  Validation loss = 2.4853  \n",
      "\n",
      "Fold: 1  Epoch: 24  Training loss = 2.7961  Validation loss = 2.4407  \n",
      "\n",
      "Fold: 1  Epoch: 25  Training loss = 2.7898  Validation loss = 2.4228  \n",
      "\n",
      "Fold: 1  Epoch: 26  Training loss = 2.7795  Validation loss = 2.3943  \n",
      "\n",
      "Fold: 1  Epoch: 27  Training loss = 2.7738  Validation loss = 2.3777  \n",
      "\n",
      "Fold: 1  Epoch: 28  Training loss = 2.7668  Validation loss = 2.3569  \n",
      "\n",
      "Fold: 1  Epoch: 29  Training loss = 2.7622  Validation loss = 2.3434  \n",
      "\n",
      "Fold: 1  Epoch: 30  Training loss = 2.7536  Validation loss = 2.3174  \n",
      "\n",
      "Fold: 1  Epoch: 31  Training loss = 2.7449  Validation loss = 2.2895  \n",
      "\n",
      "Fold: 1  Epoch: 32  Training loss = 2.7406  Validation loss = 2.2764  \n",
      "\n",
      "Fold: 1  Epoch: 33  Training loss = 2.7361  Validation loss = 2.2616  \n",
      "\n",
      "Fold: 1  Epoch: 34  Training loss = 2.7292  Validation loss = 2.2402  \n",
      "\n",
      "Fold: 1  Epoch: 35  Training loss = 2.7260  Validation loss = 2.2306  \n",
      "\n",
      "Fold: 1  Epoch: 36  Training loss = 2.7114  Validation loss = 2.1771  \n",
      "\n",
      "Fold: 1  Epoch: 37  Training loss = 2.7084  Validation loss = 2.1671  \n",
      "\n",
      "Fold: 1  Epoch: 38  Training loss = 2.7042  Validation loss = 2.1505  \n",
      "\n",
      "Fold: 1  Epoch: 39  Training loss = 2.7008  Validation loss = 2.1380  \n",
      "\n",
      "Fold: 1  Epoch: 40  Training loss = 2.6959  Validation loss = 2.1193  \n",
      "\n",
      "Fold: 1  Epoch: 41  Training loss = 2.6927  Validation loss = 2.1062  \n",
      "\n",
      "Fold: 1  Epoch: 42  Training loss = 2.6872  Validation loss = 2.0844  \n",
      "\n",
      "Fold: 1  Epoch: 43  Training loss = 2.6846  Validation loss = 2.0736  \n",
      "\n",
      "Fold: 1  Epoch: 44  Training loss = 2.6815  Validation loss = 2.0604  \n",
      "\n",
      "Fold: 1  Epoch: 45  Training loss = 2.6765  Validation loss = 2.0385  \n",
      "\n",
      "Fold: 1  Epoch: 46  Training loss = 2.6733  Validation loss = 2.0238  \n",
      "\n",
      "Fold: 1  Epoch: 47  Training loss = 2.6680  Validation loss = 1.9999  \n",
      "\n",
      "Fold: 1  Epoch: 48  Training loss = 2.6620  Validation loss = 1.9700  \n",
      "\n",
      "Fold: 1  Epoch: 49  Training loss = 2.6565  Validation loss = 1.9420  \n",
      "\n",
      "Fold: 1  Epoch: 50  Training loss = 2.6529  Validation loss = 1.9209  \n",
      "\n",
      "Fold: 1  Epoch: 51  Training loss = 2.6492  Validation loss = 1.9001  \n",
      "\n",
      "Fold: 1  Epoch: 52  Training loss = 2.6446  Validation loss = 1.8708  \n",
      "\n",
      "Fold: 1  Epoch: 53  Training loss = 2.6401  Validation loss = 1.8430  \n",
      "\n",
      "Fold: 1  Epoch: 54  Training loss = 2.6423  Validation loss = 1.8600  \n",
      "\n",
      "Fold: 1  Epoch: 55  Training loss = 2.6430  Validation loss = 1.8658  \n",
      "\n",
      "Fold: 1  Epoch: 56  Training loss = 2.6405  Validation loss = 1.8502  \n",
      "\n",
      "Fold: 1  Epoch: 57  Training loss = 2.6373  Validation loss = 1.8294  \n",
      "\n",
      "Fold: 1  Epoch: 58  Training loss = 2.6323  Validation loss = 1.7943  \n",
      "\n",
      "Fold: 1  Epoch: 59  Training loss = 2.6330  Validation loss = 1.8003  \n",
      "\n",
      "Fold: 1  Epoch: 60  Training loss = 2.6315  Validation loss = 1.7903  \n",
      "\n",
      "Fold: 1  Epoch: 61  Training loss = 2.6310  Validation loss = 1.7877  \n",
      "\n",
      "Fold: 1  Epoch: 62  Training loss = 2.6294  Validation loss = 1.7777  \n",
      "\n",
      "Fold: 1  Epoch: 63  Training loss = 2.6249  Validation loss = 1.7411  \n",
      "\n",
      "Fold: 1  Epoch: 64  Training loss = 2.6248  Validation loss = 1.7412  \n",
      "\n",
      "Fold: 1  Epoch: 65  Training loss = 2.6237  Validation loss = 1.7332  \n",
      "\n",
      "Fold: 1  Epoch: 66  Training loss = 2.6252  Validation loss = 1.7510  \n",
      "\n",
      "Fold: 1  Epoch: 67  Training loss = 2.6238  Validation loss = 1.7416  \n",
      "\n",
      "Fold: 1  Epoch: 68  Training loss = 2.6204  Validation loss = 1.7125  \n",
      "\n",
      "Fold: 1  Epoch: 69  Training loss = 2.6212  Validation loss = 1.7228  \n",
      "\n",
      "Fold: 1  Epoch: 70  Training loss = 2.6208  Validation loss = 1.7234  \n",
      "\n",
      "Fold: 1  Epoch: 71  Training loss = 2.6201  Validation loss = 1.7160  \n",
      "\n",
      "Fold: 1  Epoch: 72  Training loss = 2.6183  Validation loss = 1.7014  \n",
      "\n",
      "Fold: 1  Epoch: 73  Training loss = 2.6169  Validation loss = 1.6905  \n",
      "\n",
      "Fold: 1  Epoch: 74  Training loss = 2.6152  Validation loss = 1.6768  \n",
      "\n",
      "Fold: 1  Epoch: 75  Training loss = 2.6149  Validation loss = 1.6743  \n",
      "\n",
      "Fold: 1  Epoch: 76  Training loss = 2.6143  Validation loss = 1.6700  \n",
      "\n",
      "Fold: 1  Epoch: 77  Training loss = 2.6135  Validation loss = 1.6650  \n",
      "\n",
      "Fold: 1  Epoch: 78  Training loss = 2.6132  Validation loss = 1.6649  \n",
      "\n",
      "Fold: 1  Epoch: 79  Training loss = 2.6128  Validation loss = 1.6619  \n",
      "\n",
      "Fold: 1  Epoch: 80  Training loss = 2.6117  Validation loss = 1.6539  \n",
      "\n",
      "Fold: 1  Epoch: 81  Training loss = 2.6110  Validation loss = 1.6495  \n",
      "\n",
      "Fold: 1  Epoch: 82  Training loss = 2.6118  Validation loss = 1.6633  \n",
      "\n",
      "Fold: 1  Epoch: 83  Training loss = 2.6112  Validation loss = 1.6590  \n",
      "\n",
      "Fold: 1  Epoch: 84  Training loss = 2.6110  Validation loss = 1.6585  \n",
      "\n",
      "Fold: 1  Epoch: 85  Training loss = 2.6105  Validation loss = 1.6546  \n",
      "\n",
      "Fold: 1  Epoch: 86  Training loss = 2.6096  Validation loss = 1.6476  \n",
      "\n",
      "Fold: 1  Epoch: 87  Training loss = 2.6070  Validation loss = 1.6197  \n",
      "\n",
      "Fold: 1  Epoch: 88  Training loss = 2.6075  Validation loss = 1.6281  \n",
      "\n",
      "Fold: 1  Epoch: 89  Training loss = 2.6061  Validation loss = 1.6146  \n",
      "\n",
      "Fold: 1  Epoch: 90  Training loss = 2.6061  Validation loss = 1.6166  \n",
      "\n",
      "Fold: 1  Epoch: 91  Training loss = 2.6057  Validation loss = 1.6112  \n",
      "\n",
      "Fold: 1  Epoch: 92  Training loss = 2.6050  Validation loss = 1.6066  \n",
      "\n",
      "Fold: 1  Epoch: 93  Training loss = 2.6055  Validation loss = 1.6159  \n",
      "\n",
      "Fold: 1  Epoch: 94  Training loss = 2.6045  Validation loss = 1.6044  \n",
      "\n",
      "Fold: 1  Epoch: 95  Training loss = 2.6044  Validation loss = 1.6044  \n",
      "\n",
      "Fold: 1  Epoch: 96  Training loss = 2.6038  Validation loss = 1.6006  \n",
      "\n",
      "Fold: 1  Epoch: 97  Training loss = 2.6033  Validation loss = 1.6001  \n",
      "\n",
      "Fold: 1  Epoch: 98  Training loss = 2.6035  Validation loss = 1.6071  \n",
      "\n",
      "Fold: 1  Epoch: 99  Training loss = 2.6029  Validation loss = 1.6019  \n",
      "\n",
      "Fold: 1  Epoch: 100  Training loss = 2.6042  Validation loss = 1.6233  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 97  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.4940  Validation loss = 1.8696  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.4939  Validation loss = 1.8709  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.4933  Validation loss = 1.8682  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.4900  Validation loss = 1.8463  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.4897  Validation loss = 1.8463  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.4895  Validation loss = 1.8451  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.4887  Validation loss = 1.8424  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.4875  Validation loss = 1.8354  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.4868  Validation loss = 1.8290  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.4867  Validation loss = 1.8344  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.4855  Validation loss = 1.8278  \n",
      "\n",
      "Fold: 2  Epoch: 12  Training loss = 2.4851  Validation loss = 1.8257  \n",
      "\n",
      "Fold: 2  Epoch: 13  Training loss = 2.4847  Validation loss = 1.8242  \n",
      "\n",
      "Fold: 2  Epoch: 14  Training loss = 2.4837  Validation loss = 1.8218  \n",
      "\n",
      "Fold: 2  Epoch: 15  Training loss = 2.4831  Validation loss = 1.8204  \n",
      "\n",
      "Fold: 2  Epoch: 16  Training loss = 2.4828  Validation loss = 1.8203  \n",
      "\n",
      "Fold: 2  Epoch: 17  Training loss = 2.4835  Validation loss = 1.8290  \n",
      "\n",
      "Fold: 2  Epoch: 18  Training loss = 2.4821  Validation loss = 1.8209  \n",
      "\n",
      "Fold: 2  Epoch: 19  Training loss = 2.4818  Validation loss = 1.8194  \n",
      "\n",
      "Fold: 2  Epoch: 20  Training loss = 2.4809  Validation loss = 1.8158  \n",
      "\n",
      "Fold: 2  Epoch: 21  Training loss = 2.4803  Validation loss = 1.8139  \n",
      "\n",
      "Fold: 2  Epoch: 22  Training loss = 2.4805  Validation loss = 1.8186  \n",
      "\n",
      "Fold: 2  Epoch: 23  Training loss = 2.4795  Validation loss = 1.8139  \n",
      "\n",
      "Fold: 2  Epoch: 24  Training loss = 2.4795  Validation loss = 1.8165  \n",
      "\n",
      "Fold: 2  Epoch: 25  Training loss = 2.4793  Validation loss = 1.8163  \n",
      "\n",
      "Fold: 2  Epoch: 26  Training loss = 2.4785  Validation loss = 1.8154  \n",
      "\n",
      "Fold: 2  Epoch: 27  Training loss = 2.4769  Validation loss = 1.8023  \n",
      "\n",
      "Fold: 2  Epoch: 28  Training loss = 2.4760  Validation loss = 1.7992  \n",
      "\n",
      "Fold: 2  Epoch: 29  Training loss = 2.4753  Validation loss = 1.7955  \n",
      "\n",
      "Fold: 2  Epoch: 30  Training loss = 2.4745  Validation loss = 1.7906  \n",
      "\n",
      "Fold: 2  Epoch: 31  Training loss = 2.4735  Validation loss = 1.7825  \n",
      "\n",
      "Fold: 2  Epoch: 32  Training loss = 2.4733  Validation loss = 1.7878  \n",
      "\n",
      "Fold: 2  Epoch: 33  Training loss = 2.4725  Validation loss = 1.7797  \n",
      "\n",
      "Fold: 2  Epoch: 34  Training loss = 2.4719  Validation loss = 1.7763  \n",
      "\n",
      "Fold: 2  Epoch: 35  Training loss = 2.4717  Validation loss = 1.7804  \n",
      "\n",
      "Fold: 2  Epoch: 36  Training loss = 2.4713  Validation loss = 1.7728  \n",
      "\n",
      "Fold: 2  Epoch: 37  Training loss = 2.4707  Validation loss = 1.7736  \n",
      "\n",
      "Fold: 2  Epoch: 38  Training loss = 2.4700  Validation loss = 1.7648  \n",
      "\n",
      "Fold: 2  Epoch: 39  Training loss = 2.4699  Validation loss = 1.7680  \n",
      "\n",
      "Fold: 2  Epoch: 40  Training loss = 2.4690  Validation loss = 1.7676  \n",
      "\n",
      "Fold: 2  Epoch: 41  Training loss = 2.4688  Validation loss = 1.7709  \n",
      "\n",
      "Fold: 2  Epoch: 42  Training loss = 2.4688  Validation loss = 1.7799  \n",
      "\n",
      "Fold: 2  Epoch: 43  Training loss = 2.4682  Validation loss = 1.7772  \n",
      "\n",
      "Fold: 2  Epoch: 44  Training loss = 2.4677  Validation loss = 1.7793  \n",
      "\n",
      "Fold: 2  Epoch: 45  Training loss = 2.4673  Validation loss = 1.7800  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 38  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.4782  Validation loss = 2.4544  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.4755  Validation loss = 2.4547  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.4730  Validation loss = 2.4646  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.4711  Validation loss = 2.4714  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.4685  Validation loss = 2.4800  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.4669  Validation loss = 2.4891  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.4658  Validation loss = 2.4891  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.4642  Validation loss = 2.4935  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.4628  Validation loss = 2.4940  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.4610  Validation loss = 2.5044  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.4607  Validation loss = 2.4998  \n",
      "\n",
      "Fold: 3  Epoch: 12  Training loss = 1.4601  Validation loss = 2.4909  \n",
      "\n",
      "Fold: 3  Epoch: 13  Training loss = 1.4580  Validation loss = 2.4967  \n",
      "\n",
      "Fold: 3  Epoch: 14  Training loss = 1.4563  Validation loss = 2.5013  \n",
      "\n",
      "Fold: 3  Epoch: 15  Training loss = 1.4556  Validation loss = 2.4934  \n",
      "\n",
      "Fold: 3  Epoch: 16  Training loss = 1.4542  Validation loss = 2.4988  \n",
      "\n",
      "Fold: 3  Epoch: 17  Training loss = 1.4533  Validation loss = 2.5002  \n",
      "\n",
      "Fold: 3  Epoch: 18  Training loss = 1.4522  Validation loss = 2.5024  \n",
      "\n",
      "Fold: 3  Epoch: 19  Training loss = 1.4514  Validation loss = 2.5022  \n",
      "\n",
      "Fold: 3  Epoch: 20  Training loss = 1.4504  Validation loss = 2.4989  \n",
      "\n",
      "Fold: 3  Epoch: 21  Training loss = 1.4483  Validation loss = 2.5110  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 1  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.4930  Validation loss = 3.6135  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.4918  Validation loss = 3.6165  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.4902  Validation loss = 3.6091  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.4898  Validation loss = 3.6087  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.4879  Validation loss = 3.5988  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.4870  Validation loss = 3.5840  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.4859  Validation loss = 3.5747  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.4843  Validation loss = 3.5649  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.4832  Validation loss = 3.5661  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 1.4821  Validation loss = 3.5714  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.4818  Validation loss = 3.5792  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 1.4807  Validation loss = 3.5680  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.4795  Validation loss = 3.5591  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 1.4786  Validation loss = 3.5470  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.4776  Validation loss = 3.5444  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 1.4757  Validation loss = 3.5221  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 1.4749  Validation loss = 3.5151  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 1.4742  Validation loss = 3.5170  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 1.4728  Validation loss = 3.5034  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 1.4715  Validation loss = 3.5060  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 1.4702  Validation loss = 3.5017  \n",
      "\n",
      "Fold: 4  Epoch: 22  Training loss = 1.4696  Validation loss = 3.4998  \n",
      "\n",
      "Fold: 4  Epoch: 23  Training loss = 1.4685  Validation loss = 3.4958  \n",
      "\n",
      "Fold: 4  Epoch: 24  Training loss = 1.4675  Validation loss = 3.4964  \n",
      "\n",
      "Fold: 4  Epoch: 25  Training loss = 1.4666  Validation loss = 3.4806  \n",
      "\n",
      "Fold: 4  Epoch: 26  Training loss = 1.4659  Validation loss = 3.4853  \n",
      "\n",
      "Fold: 4  Epoch: 27  Training loss = 1.4649  Validation loss = 3.4815  \n",
      "\n",
      "Fold: 4  Epoch: 28  Training loss = 1.4645  Validation loss = 3.4787  \n",
      "\n",
      "Fold: 4  Epoch: 29  Training loss = 1.4631  Validation loss = 3.4729  \n",
      "\n",
      "Fold: 4  Epoch: 30  Training loss = 1.4619  Validation loss = 3.4733  \n",
      "\n",
      "Fold: 4  Epoch: 31  Training loss = 1.4612  Validation loss = 3.4801  \n",
      "\n",
      "Fold: 4  Epoch: 32  Training loss = 1.4604  Validation loss = 3.4769  \n",
      "\n",
      "Fold: 4  Epoch: 33  Training loss = 1.4595  Validation loss = 3.4735  \n",
      "\n",
      "Fold: 4  Epoch: 34  Training loss = 1.4586  Validation loss = 3.4686  \n",
      "\n",
      "Fold: 4  Epoch: 35  Training loss = 1.4578  Validation loss = 3.4699  \n",
      "\n",
      "Fold: 4  Epoch: 36  Training loss = 1.4571  Validation loss = 3.4689  \n",
      "\n",
      "Fold: 4  Epoch: 37  Training loss = 1.4565  Validation loss = 3.4705  \n",
      "\n",
      "Fold: 4  Epoch: 38  Training loss = 1.4553  Validation loss = 3.4622  \n",
      "\n",
      "Fold: 4  Epoch: 39  Training loss = 1.4542  Validation loss = 3.4521  \n",
      "\n",
      "Fold: 4  Epoch: 40  Training loss = 1.4534  Validation loss = 3.4470  \n",
      "\n",
      "Fold: 4  Epoch: 41  Training loss = 1.4523  Validation loss = 3.4434  \n",
      "\n",
      "Fold: 4  Epoch: 42  Training loss = 1.4509  Validation loss = 3.4247  \n",
      "\n",
      "Fold: 4  Epoch: 43  Training loss = 1.4503  Validation loss = 3.4073  \n",
      "\n",
      "Fold: 4  Epoch: 44  Training loss = 1.4493  Validation loss = 3.4039  \n",
      "\n",
      "Fold: 4  Epoch: 45  Training loss = 1.4481  Validation loss = 3.4030  \n",
      "\n",
      "Fold: 4  Epoch: 46  Training loss = 1.4475  Validation loss = 3.4118  \n",
      "\n",
      "Fold: 4  Epoch: 47  Training loss = 1.4467  Validation loss = 3.4067  \n",
      "\n",
      "Fold: 4  Epoch: 48  Training loss = 1.4458  Validation loss = 3.4090  \n",
      "\n",
      "Fold: 4  Epoch: 49  Training loss = 1.4445  Validation loss = 3.3960  \n",
      "\n",
      "Fold: 4  Epoch: 50  Training loss = 1.4436  Validation loss = 3.3933  \n",
      "\n",
      "Fold: 4  Epoch: 51  Training loss = 1.4426  Validation loss = 3.3860  \n",
      "\n",
      "Fold: 4  Epoch: 52  Training loss = 1.4421  Validation loss = 3.3699  \n",
      "\n",
      "Fold: 4  Epoch: 53  Training loss = 1.4410  Validation loss = 3.3580  \n",
      "\n",
      "Fold: 4  Epoch: 54  Training loss = 1.4401  Validation loss = 3.3579  \n",
      "\n",
      "Fold: 4  Epoch: 55  Training loss = 1.4393  Validation loss = 3.3480  \n",
      "\n",
      "Fold: 4  Epoch: 56  Training loss = 1.4386  Validation loss = 3.3498  \n",
      "\n",
      "Fold: 4  Epoch: 57  Training loss = 1.4377  Validation loss = 3.3458  \n",
      "\n",
      "Fold: 4  Epoch: 58  Training loss = 1.4370  Validation loss = 3.3366  \n",
      "\n",
      "Fold: 4  Epoch: 59  Training loss = 1.4366  Validation loss = 3.3205  \n",
      "\n",
      "Fold: 4  Epoch: 60  Training loss = 1.4357  Validation loss = 3.3199  \n",
      "\n",
      "Fold: 4  Epoch: 61  Training loss = 1.4346  Validation loss = 3.3259  \n",
      "\n",
      "Fold: 4  Epoch: 62  Training loss = 1.4341  Validation loss = 3.3262  \n",
      "\n",
      "Fold: 4  Epoch: 63  Training loss = 1.4332  Validation loss = 3.3179  \n",
      "\n",
      "Fold: 4  Epoch: 64  Training loss = 1.4326  Validation loss = 3.3166  \n",
      "\n",
      "Fold: 4  Epoch: 65  Training loss = 1.4319  Validation loss = 3.3177  \n",
      "\n",
      "Fold: 4  Epoch: 66  Training loss = 1.4313  Validation loss = 3.3170  \n",
      "\n",
      "Fold: 4  Epoch: 67  Training loss = 1.4301  Validation loss = 3.3194  \n",
      "\n",
      "Fold: 4  Epoch: 68  Training loss = 1.4288  Validation loss = 3.3119  \n",
      "\n",
      "Fold: 4  Epoch: 69  Training loss = 1.4280  Validation loss = 3.3076  \n",
      "\n",
      "Fold: 4  Epoch: 70  Training loss = 1.4269  Validation loss = 3.3064  \n",
      "\n",
      "Fold: 4  Epoch: 71  Training loss = 1.4264  Validation loss = 3.3116  \n",
      "\n",
      "Fold: 4  Epoch: 72  Training loss = 1.4254  Validation loss = 3.3040  \n",
      "\n",
      "Fold: 4  Epoch: 73  Training loss = 1.4243  Validation loss = 3.2938  \n",
      "\n",
      "Fold: 4  Epoch: 74  Training loss = 1.4232  Validation loss = 3.2793  \n",
      "\n",
      "Fold: 4  Epoch: 75  Training loss = 1.4227  Validation loss = 3.2841  \n",
      "\n",
      "Fold: 4  Epoch: 76  Training loss = 1.4221  Validation loss = 3.2777  \n",
      "\n",
      "Fold: 4  Epoch: 77  Training loss = 1.4209  Validation loss = 3.2696  \n",
      "\n",
      "Fold: 4  Epoch: 78  Training loss = 1.4203  Validation loss = 3.2791  \n",
      "\n",
      "Fold: 4  Epoch: 79  Training loss = 1.4196  Validation loss = 3.2693  \n",
      "\n",
      "Fold: 4  Epoch: 80  Training loss = 1.4189  Validation loss = 3.2737  \n",
      "\n",
      "Fold: 4  Epoch: 81  Training loss = 1.4182  Validation loss = 3.2581  \n",
      "\n",
      "Fold: 4  Epoch: 82  Training loss = 1.4176  Validation loss = 3.2465  \n",
      "\n",
      "Fold: 4  Epoch: 83  Training loss = 1.4168  Validation loss = 3.2390  \n",
      "\n",
      "Fold: 4  Epoch: 84  Training loss = 1.4161  Validation loss = 3.2396  \n",
      "\n",
      "Fold: 4  Epoch: 85  Training loss = 1.4155  Validation loss = 3.2467  \n",
      "\n",
      "Fold: 4  Epoch: 86  Training loss = 1.4150  Validation loss = 3.2480  \n",
      "\n",
      "Fold: 4  Epoch: 87  Training loss = 1.4143  Validation loss = 3.2488  \n",
      "\n",
      "Fold: 4  Epoch: 88  Training loss = 1.4138  Validation loss = 3.2462  \n",
      "\n",
      "Fold: 4  Epoch: 89  Training loss = 1.4136  Validation loss = 3.2479  \n",
      "\n",
      "Fold: 4  Epoch: 90  Training loss = 1.4127  Validation loss = 3.2337  \n",
      "\n",
      "Fold: 4  Epoch: 91  Training loss = 1.4115  Validation loss = 3.2448  \n",
      "\n",
      "Fold: 4  Epoch: 92  Training loss = 1.4111  Validation loss = 3.2484  \n",
      "\n",
      "Fold: 4  Epoch: 93  Training loss = 1.4106  Validation loss = 3.2563  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 90  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.5623  Validation loss = 2.9607  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.5588  Validation loss = 2.9424  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.5555  Validation loss = 2.9292  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.5526  Validation loss = 2.9190  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.5484  Validation loss = 2.8963  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 1.5452  Validation loss = 2.8773  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 1.5437  Validation loss = 2.8777  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.5402  Validation loss = 2.8572  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 1.5362  Validation loss = 2.8345  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.5329  Validation loss = 2.8154  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 1.5313  Validation loss = 2.8179  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 1.5276  Validation loss = 2.7966  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 1.5222  Validation loss = 2.7603  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.5190  Validation loss = 2.7484  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 1.5171  Validation loss = 2.7370  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 1.5148  Validation loss = 2.7155  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 1.5125  Validation loss = 2.7143  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 1.5107  Validation loss = 2.7129  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 1.5091  Validation loss = 2.7078  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 1.5076  Validation loss = 2.7024  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 1.5049  Validation loss = 2.6897  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 1.5028  Validation loss = 2.6861  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 1.5002  Validation loss = 2.6705  \n",
      "\n",
      "Fold: 5  Epoch: 24  Training loss = 1.4974  Validation loss = 2.6614  \n",
      "\n",
      "Fold: 5  Epoch: 25  Training loss = 1.4951  Validation loss = 2.6440  \n",
      "\n",
      "Fold: 5  Epoch: 26  Training loss = 1.4939  Validation loss = 2.6352  \n",
      "\n",
      "Fold: 5  Epoch: 27  Training loss = 1.4924  Validation loss = 2.6333  \n",
      "\n",
      "Fold: 5  Epoch: 28  Training loss = 1.4891  Validation loss = 2.6176  \n",
      "\n",
      "Fold: 5  Epoch: 29  Training loss = 1.4881  Validation loss = 2.6230  \n",
      "\n",
      "Fold: 5  Epoch: 30  Training loss = 1.4873  Validation loss = 2.6278  \n",
      "\n",
      "Fold: 5  Epoch: 31  Training loss = 1.4868  Validation loss = 2.6305  \n",
      "\n",
      "Fold: 5  Epoch: 32  Training loss = 1.4855  Validation loss = 2.6287  \n",
      "\n",
      "Fold: 5  Epoch: 33  Training loss = 1.4831  Validation loss = 2.6092  \n",
      "\n",
      "Fold: 5  Epoch: 34  Training loss = 1.4814  Validation loss = 2.5998  \n",
      "\n",
      "Fold: 5  Epoch: 35  Training loss = 1.4805  Validation loss = 2.6036  \n",
      "\n",
      "Fold: 5  Epoch: 36  Training loss = 1.4777  Validation loss = 2.5886  \n",
      "\n",
      "Fold: 5  Epoch: 37  Training loss = 1.4763  Validation loss = 2.5785  \n",
      "\n",
      "Fold: 5  Epoch: 38  Training loss = 1.4745  Validation loss = 2.5769  \n",
      "\n",
      "Fold: 5  Epoch: 39  Training loss = 1.4731  Validation loss = 2.5735  \n",
      "\n",
      "Fold: 5  Epoch: 40  Training loss = 1.4705  Validation loss = 2.5423  \n",
      "\n",
      "Fold: 5  Epoch: 41  Training loss = 1.4689  Validation loss = 2.5339  \n",
      "\n",
      "Fold: 5  Epoch: 42  Training loss = 1.4666  Validation loss = 2.5283  \n",
      "\n",
      "Fold: 5  Epoch: 43  Training loss = 1.4640  Validation loss = 2.5121  \n",
      "\n",
      "Fold: 5  Epoch: 44  Training loss = 1.4623  Validation loss = 2.5023  \n",
      "\n",
      "Fold: 5  Epoch: 45  Training loss = 1.4610  Validation loss = 2.5010  \n",
      "\n",
      "Fold: 5  Epoch: 46  Training loss = 1.4586  Validation loss = 2.4879  \n",
      "\n",
      "Fold: 5  Epoch: 47  Training loss = 1.4569  Validation loss = 2.4790  \n",
      "\n",
      "Fold: 5  Epoch: 48  Training loss = 1.4555  Validation loss = 2.4756  \n",
      "\n",
      "Fold: 5  Epoch: 49  Training loss = 1.4545  Validation loss = 2.4715  \n",
      "\n",
      "Fold: 5  Epoch: 50  Training loss = 1.4538  Validation loss = 2.4802  \n",
      "\n",
      "Fold: 5  Epoch: 51  Training loss = 1.4518  Validation loss = 2.4786  \n",
      "\n",
      "Fold: 5  Epoch: 52  Training loss = 1.4503  Validation loss = 2.4724  \n",
      "\n",
      "Fold: 5  Epoch: 53  Training loss = 1.4490  Validation loss = 2.4771  \n",
      "\n",
      "Fold: 5  Epoch: 54  Training loss = 1.4467  Validation loss = 2.4667  \n",
      "\n",
      "Fold: 5  Epoch: 55  Training loss = 1.4439  Validation loss = 2.4415  \n",
      "\n",
      "Fold: 5  Epoch: 56  Training loss = 1.4417  Validation loss = 2.4265  \n",
      "\n",
      "Fold: 5  Epoch: 57  Training loss = 1.4409  Validation loss = 2.4321  \n",
      "\n",
      "Fold: 5  Epoch: 58  Training loss = 1.4401  Validation loss = 2.4357  \n",
      "\n",
      "Fold: 5  Epoch: 59  Training loss = 1.4391  Validation loss = 2.4383  \n",
      "\n",
      "Fold: 5  Epoch: 60  Training loss = 1.4359  Validation loss = 2.4252  \n",
      "\n",
      "Fold: 5  Epoch: 61  Training loss = 1.4336  Validation loss = 2.4115  \n",
      "\n",
      "Fold: 5  Epoch: 62  Training loss = 1.4319  Validation loss = 2.4108  \n",
      "\n",
      "Fold: 5  Epoch: 63  Training loss = 1.4310  Validation loss = 2.4126  \n",
      "\n",
      "Fold: 5  Epoch: 64  Training loss = 1.4299  Validation loss = 2.4139  \n",
      "\n",
      "Fold: 5  Epoch: 65  Training loss = 1.4275  Validation loss = 2.3977  \n",
      "\n",
      "Fold: 5  Epoch: 66  Training loss = 1.4250  Validation loss = 2.3752  \n",
      "\n",
      "Fold: 5  Epoch: 67  Training loss = 1.4232  Validation loss = 2.3571  \n",
      "\n",
      "Fold: 5  Epoch: 68  Training loss = 1.4227  Validation loss = 2.3704  \n",
      "\n",
      "Fold: 5  Epoch: 69  Training loss = 1.4220  Validation loss = 2.3708  \n",
      "\n",
      "Fold: 5  Epoch: 70  Training loss = 1.4197  Validation loss = 2.3525  \n",
      "\n",
      "Fold: 5  Epoch: 71  Training loss = 1.4180  Validation loss = 2.3439  \n",
      "\n",
      "Fold: 5  Epoch: 72  Training loss = 1.4151  Validation loss = 2.3131  \n",
      "\n",
      "Fold: 5  Epoch: 73  Training loss = 1.4143  Validation loss = 2.3170  \n",
      "\n",
      "Fold: 5  Epoch: 74  Training loss = 1.4129  Validation loss = 2.3076  \n",
      "\n",
      "Fold: 5  Epoch: 75  Training loss = 1.4106  Validation loss = 2.2988  \n",
      "\n",
      "Fold: 5  Epoch: 76  Training loss = 1.4094  Validation loss = 2.2953  \n",
      "\n",
      "Fold: 5  Epoch: 77  Training loss = 1.4065  Validation loss = 2.2588  \n",
      "\n",
      "Fold: 5  Epoch: 78  Training loss = 1.4053  Validation loss = 2.2620  \n",
      "\n",
      "Fold: 5  Epoch: 79  Training loss = 1.4037  Validation loss = 2.2384  \n",
      "\n",
      "Fold: 5  Epoch: 80  Training loss = 1.4017  Validation loss = 2.2362  \n",
      "\n",
      "Fold: 5  Epoch: 81  Training loss = 1.4000  Validation loss = 2.2407  \n",
      "\n",
      "Fold: 5  Epoch: 82  Training loss = 1.3990  Validation loss = 2.2391  \n",
      "\n",
      "Fold: 5  Epoch: 83  Training loss = 1.3973  Validation loss = 2.2217  \n",
      "\n",
      "Fold: 5  Epoch: 84  Training loss = 1.3958  Validation loss = 2.2179  \n",
      "\n",
      "Fold: 5  Epoch: 85  Training loss = 1.3938  Validation loss = 2.2126  \n",
      "\n",
      "Fold: 5  Epoch: 86  Training loss = 1.3914  Validation loss = 2.1890  \n",
      "\n",
      "Fold: 5  Epoch: 87  Training loss = 1.3904  Validation loss = 2.2008  \n",
      "\n",
      "Fold: 5  Epoch: 88  Training loss = 1.3886  Validation loss = 2.1901  \n",
      "\n",
      "Fold: 5  Epoch: 89  Training loss = 1.3870  Validation loss = 2.2017  \n",
      "\n",
      "Fold: 5  Epoch: 90  Training loss = 1.3856  Validation loss = 2.1912  \n",
      "\n",
      "Fold: 5  Epoch: 91  Training loss = 1.3847  Validation loss = 2.1883  \n",
      "\n",
      "Fold: 5  Epoch: 92  Training loss = 1.3830  Validation loss = 2.1824  \n",
      "\n",
      "Fold: 5  Epoch: 93  Training loss = 1.3816  Validation loss = 2.1779  \n",
      "\n",
      "Fold: 5  Epoch: 94  Training loss = 1.3798  Validation loss = 2.1658  \n",
      "\n",
      "Fold: 5  Epoch: 95  Training loss = 1.3784  Validation loss = 2.1706  \n",
      "\n",
      "Fold: 5  Epoch: 96  Training loss = 1.3761  Validation loss = 2.1569  \n",
      "\n",
      "Fold: 5  Epoch: 97  Training loss = 1.3753  Validation loss = 2.1549  \n",
      "\n",
      "Fold: 5  Epoch: 98  Training loss = 1.3733  Validation loss = 2.1298  \n",
      "\n",
      "Fold: 5  Epoch: 99  Training loss = 1.3707  Validation loss = 2.1133  \n",
      "\n",
      "Fold: 5  Epoch: 100  Training loss = 1.3691  Validation loss = 2.0938  \n",
      "\n",
      "Fold: 5  Epoch: 101  Training loss = 1.3677  Validation loss = 2.0730  \n",
      "\n",
      "Fold: 5  Epoch: 102  Training loss = 1.3671  Validation loss = 2.0546  \n",
      "\n",
      "Fold: 5  Epoch: 103  Training loss = 1.3652  Validation loss = 2.0627  \n",
      "\n",
      "Fold: 5  Epoch: 104  Training loss = 1.3640  Validation loss = 2.0524  \n",
      "\n",
      "Fold: 5  Epoch: 105  Training loss = 1.3635  Validation loss = 2.0737  \n",
      "\n",
      "Fold: 5  Epoch: 106  Training loss = 1.3627  Validation loss = 2.0780  \n",
      "\n",
      "Fold: 5  Epoch: 107  Training loss = 1.3606  Validation loss = 2.0544  \n",
      "\n",
      "Fold: 5  Epoch: 108  Training loss = 1.3589  Validation loss = 2.0537  \n",
      "\n",
      "Fold: 5  Epoch: 109  Training loss = 1.3572  Validation loss = 2.0336  \n",
      "\n",
      "Fold: 5  Epoch: 110  Training loss = 1.3555  Validation loss = 2.0178  \n",
      "\n",
      "Fold: 5  Epoch: 111  Training loss = 1.3538  Validation loss = 1.9997  \n",
      "\n",
      "Fold: 5  Epoch: 112  Training loss = 1.3528  Validation loss = 1.9987  \n",
      "\n",
      "Fold: 5  Epoch: 113  Training loss = 1.3515  Validation loss = 2.0053  \n",
      "\n",
      "Fold: 5  Epoch: 114  Training loss = 1.3503  Validation loss = 1.9923  \n",
      "\n",
      "Fold: 5  Epoch: 115  Training loss = 1.3490  Validation loss = 1.9922  \n",
      "\n",
      "Fold: 5  Epoch: 116  Training loss = 1.3474  Validation loss = 1.9738  \n",
      "\n",
      "Fold: 5  Epoch: 117  Training loss = 1.3458  Validation loss = 1.9463  \n",
      "\n",
      "Fold: 5  Epoch: 118  Training loss = 1.3444  Validation loss = 1.9399  \n",
      "\n",
      "Fold: 5  Epoch: 119  Training loss = 1.3424  Validation loss = 1.9442  \n",
      "\n",
      "Fold: 5  Epoch: 120  Training loss = 1.3415  Validation loss = 1.9491  \n",
      "\n",
      "Fold: 5  Epoch: 121  Training loss = 1.3401  Validation loss = 1.9463  \n",
      "\n",
      "Fold: 5  Epoch: 122  Training loss = 1.3387  Validation loss = 1.9350  \n",
      "\n",
      "Fold: 5  Epoch: 123  Training loss = 1.3371  Validation loss = 1.9155  \n",
      "\n",
      "Fold: 5  Epoch: 124  Training loss = 1.3356  Validation loss = 1.9168  \n",
      "\n",
      "Fold: 5  Epoch: 125  Training loss = 1.3340  Validation loss = 1.9118  \n",
      "\n",
      "Fold: 5  Epoch: 126  Training loss = 1.3328  Validation loss = 1.8908  \n",
      "\n",
      "Fold: 5  Epoch: 127  Training loss = 1.3313  Validation loss = 1.8940  \n",
      "\n",
      "Fold: 5  Epoch: 128  Training loss = 1.3293  Validation loss = 1.8837  \n",
      "\n",
      "Fold: 5  Epoch: 129  Training loss = 1.3277  Validation loss = 1.8808  \n",
      "\n",
      "Fold: 5  Epoch: 130  Training loss = 1.3259  Validation loss = 1.8798  \n",
      "\n",
      "Fold: 5  Epoch: 131  Training loss = 1.3250  Validation loss = 1.8794  \n",
      "\n",
      "Fold: 5  Epoch: 132  Training loss = 1.3245  Validation loss = 1.8866  \n",
      "\n",
      "Fold: 5  Epoch: 133  Training loss = 1.3233  Validation loss = 1.8654  \n",
      "\n",
      "Fold: 5  Epoch: 134  Training loss = 1.3214  Validation loss = 1.8589  \n",
      "\n",
      "Fold: 5  Epoch: 135  Training loss = 1.3199  Validation loss = 1.8701  \n",
      "\n",
      "Fold: 5  Epoch: 136  Training loss = 1.3186  Validation loss = 1.8717  \n",
      "\n",
      "Fold: 5  Epoch: 137  Training loss = 1.3175  Validation loss = 1.8603  \n",
      "\n",
      "Fold: 5  Epoch: 138  Training loss = 1.3160  Validation loss = 1.8315  \n",
      "\n",
      "Fold: 5  Epoch: 139  Training loss = 1.3144  Validation loss = 1.8104  \n",
      "\n",
      "Fold: 5  Epoch: 140  Training loss = 1.3132  Validation loss = 1.8125  \n",
      "\n",
      "Fold: 5  Epoch: 141  Training loss = 1.3121  Validation loss = 1.8087  \n",
      "\n",
      "Fold: 5  Epoch: 142  Training loss = 1.3113  Validation loss = 1.7939  \n",
      "\n",
      "Fold: 5  Epoch: 143  Training loss = 1.3102  Validation loss = 1.7924  \n",
      "\n",
      "Fold: 5  Epoch: 144  Training loss = 1.3089  Validation loss = 1.8040  \n",
      "\n",
      "Fold: 5  Epoch: 145  Training loss = 1.3082  Validation loss = 1.7900  \n",
      "\n",
      "Fold: 5  Epoch: 146  Training loss = 1.3075  Validation loss = 1.7616  \n",
      "\n",
      "Fold: 5  Epoch: 147  Training loss = 1.3066  Validation loss = 1.7457  \n",
      "\n",
      "Fold: 5  Epoch: 148  Training loss = 1.3058  Validation loss = 1.7400  \n",
      "\n",
      "Fold: 5  Epoch: 149  Training loss = 1.3044  Validation loss = 1.7456  \n",
      "\n",
      "Fold: 5  Epoch: 150  Training loss = 1.3032  Validation loss = 1.7506  \n",
      "\n",
      "Fold: 5  Epoch: 151  Training loss = 1.3018  Validation loss = 1.7412  \n",
      "\n",
      "Fold: 5  Epoch: 152  Training loss = 1.3000  Validation loss = 1.7387  \n",
      "\n",
      "Fold: 5  Epoch: 153  Training loss = 1.2996  Validation loss = 1.7275  \n",
      "\n",
      "Fold: 5  Epoch: 154  Training loss = 1.2985  Validation loss = 1.7340  \n",
      "\n",
      "Fold: 5  Epoch: 155  Training loss = 1.2975  Validation loss = 1.7377  \n",
      "\n",
      "Fold: 5  Epoch: 156  Training loss = 1.2969  Validation loss = 1.7204  \n",
      "\n",
      "Fold: 5  Epoch: 157  Training loss = 1.2966  Validation loss = 1.7026  \n",
      "\n",
      "Fold: 5  Epoch: 158  Training loss = 1.2944  Validation loss = 1.7118  \n",
      "\n",
      "Fold: 5  Epoch: 159  Training loss = 1.2936  Validation loss = 1.7111  \n",
      "\n",
      "Fold: 5  Epoch: 160  Training loss = 1.2927  Validation loss = 1.7202  \n",
      "\n",
      "Fold: 5  Epoch: 161  Training loss = 1.2917  Validation loss = 1.6948  \n",
      "\n",
      "Fold: 5  Epoch: 162  Training loss = 1.2911  Validation loss = 1.6812  \n",
      "\n",
      "Fold: 5  Epoch: 163  Training loss = 1.2898  Validation loss = 1.6829  \n",
      "\n",
      "Fold: 5  Epoch: 164  Training loss = 1.2885  Validation loss = 1.6802  \n",
      "\n",
      "Fold: 5  Epoch: 165  Training loss = 1.2878  Validation loss = 1.6688  \n",
      "\n",
      "Fold: 5  Epoch: 166  Training loss = 1.2868  Validation loss = 1.6577  \n",
      "\n",
      "Fold: 5  Epoch: 167  Training loss = 1.2861  Validation loss = 1.6728  \n",
      "\n",
      "Fold: 5  Epoch: 168  Training loss = 1.2849  Validation loss = 1.6759  \n",
      "\n",
      "Fold: 5  Epoch: 169  Training loss = 1.2840  Validation loss = 1.6756  \n",
      "\n",
      "Fold: 5  Epoch: 170  Training loss = 1.2829  Validation loss = 1.6700  \n",
      "\n",
      "Fold: 5  Epoch: 171  Training loss = 1.2820  Validation loss = 1.6539  \n",
      "\n",
      "Fold: 5  Epoch: 172  Training loss = 1.2812  Validation loss = 1.6399  \n",
      "\n",
      "Fold: 5  Epoch: 173  Training loss = 1.2803  Validation loss = 1.6315  \n",
      "\n",
      "Fold: 5  Epoch: 174  Training loss = 1.2789  Validation loss = 1.6152  \n",
      "\n",
      "Fold: 5  Epoch: 175  Training loss = 1.2781  Validation loss = 1.5943  \n",
      "\n",
      "Fold: 5  Epoch: 176  Training loss = 1.2776  Validation loss = 1.5762  \n",
      "\n",
      "Fold: 5  Epoch: 177  Training loss = 1.2768  Validation loss = 1.5710  \n",
      "\n",
      "Fold: 5  Epoch: 178  Training loss = 1.2760  Validation loss = 1.5686  \n",
      "\n",
      "Fold: 5  Epoch: 179  Training loss = 1.2749  Validation loss = 1.5629  \n",
      "\n",
      "Fold: 5  Epoch: 180  Training loss = 1.2741  Validation loss = 1.5657  \n",
      "\n",
      "Fold: 5  Epoch: 181  Training loss = 1.2730  Validation loss = 1.5676  \n",
      "\n",
      "Fold: 5  Epoch: 182  Training loss = 1.2724  Validation loss = 1.5529  \n",
      "\n",
      "Fold: 5  Epoch: 183  Training loss = 1.2710  Validation loss = 1.5577  \n",
      "\n",
      "Fold: 5  Epoch: 184  Training loss = 1.2714  Validation loss = 1.5411  \n",
      "\n",
      "Fold: 5  Epoch: 185  Training loss = 1.2700  Validation loss = 1.5435  \n",
      "\n",
      "Fold: 5  Epoch: 186  Training loss = 1.2691  Validation loss = 1.5459  \n",
      "\n",
      "Fold: 5  Epoch: 187  Training loss = 1.2688  Validation loss = 1.5356  \n",
      "\n",
      "Fold: 5  Epoch: 188  Training loss = 1.2678  Validation loss = 1.5323  \n",
      "\n",
      "Fold: 5  Epoch: 189  Training loss = 1.2678  Validation loss = 1.5192  \n",
      "\n",
      "Fold: 5  Epoch: 190  Training loss = 1.2662  Validation loss = 1.5320  \n",
      "\n",
      "Fold: 5  Epoch: 191  Training loss = 1.2654  Validation loss = 1.5380  \n",
      "\n",
      "Fold: 5  Epoch: 192  Training loss = 1.2646  Validation loss = 1.5339  \n",
      "\n",
      "Fold: 5  Epoch: 193  Training loss = 1.2645  Validation loss = 1.5254  \n",
      "\n",
      "Fold: 5  Epoch: 194  Training loss = 1.2633  Validation loss = 1.5229  \n",
      "\n",
      "Fold: 5  Epoch: 195  Training loss = 1.2622  Validation loss = 1.5316  \n",
      "\n",
      "Fold: 5  Epoch: 196  Training loss = 1.2611  Validation loss = 1.5357  \n",
      "\n",
      "Fold: 5  Epoch: 197  Training loss = 1.2607  Validation loss = 1.5689  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 189  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 1.2877  Validation loss = 0.7083  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 1.2856  Validation loss = 0.7138  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 1.2815  Validation loss = 0.7251  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 1.2786  Validation loss = 0.7353  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 1.2729  Validation loss = 0.7638  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 1.2723  Validation loss = 0.7615  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 1.2691  Validation loss = 0.7803  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 1.2664  Validation loss = 0.7914  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 1.2640  Validation loss = 0.8167  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 1.2621  Validation loss = 0.8227  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 1.2600  Validation loss = 0.8349  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 1  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 1.2425  Validation loss = 0.7398  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 1.2413  Validation loss = 0.7430  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 1.2402  Validation loss = 0.7445  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.2385  Validation loss = 0.7462  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 1.2374  Validation loss = 0.7472  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 1.2366  Validation loss = 0.7519  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 1.2355  Validation loss = 0.7547  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 1.2350  Validation loss = 0.7576  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 1.2332  Validation loss = 0.7576  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 1.2324  Validation loss = 0.7593  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 1.2317  Validation loss = 0.7604  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 1  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 1.1556  Validation loss = 4.7051  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 1.1561  Validation loss = 4.7371  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 1.1555  Validation loss = 4.7342  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 1.1547  Validation loss = 4.7360  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 1.1539  Validation loss = 4.7291  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 1.1526  Validation loss = 4.7253  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 1.1520  Validation loss = 4.7344  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 1.1505  Validation loss = 4.7165  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 1.1489  Validation loss = 4.6951  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 1.1490  Validation loss = 4.7101  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 1.1476  Validation loss = 4.6987  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 1.1463  Validation loss = 4.6811  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 1.1461  Validation loss = 4.6895  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 1.1452  Validation loss = 4.6601  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 1.1449  Validation loss = 4.6671  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 1.1444  Validation loss = 4.6618  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 1.1442  Validation loss = 4.6567  \n",
      "\n",
      "Fold: 8  Epoch: 18  Training loss = 1.1434  Validation loss = 4.6639  \n",
      "\n",
      "Fold: 8  Epoch: 19  Training loss = 1.1429  Validation loss = 4.6572  \n",
      "\n",
      "Fold: 8  Epoch: 20  Training loss = 1.1428  Validation loss = 4.6415  \n",
      "\n",
      "Fold: 8  Epoch: 21  Training loss = 1.1425  Validation loss = 4.6160  \n",
      "\n",
      "Fold: 8  Epoch: 22  Training loss = 1.1416  Validation loss = 4.6174  \n",
      "\n",
      "Fold: 8  Epoch: 23  Training loss = 1.1421  Validation loss = 4.5946  \n",
      "\n",
      "Fold: 8  Epoch: 24  Training loss = 1.1405  Validation loss = 4.6126  \n",
      "\n",
      "Fold: 8  Epoch: 25  Training loss = 1.1397  Validation loss = 4.6180  \n",
      "\n",
      "Fold: 8  Epoch: 26  Training loss = 1.1389  Validation loss = 4.6313  \n",
      "\n",
      "Fold: 8  Epoch: 27  Training loss = 1.1384  Validation loss = 4.6321  \n",
      "\n",
      "Fold: 8  Epoch: 28  Training loss = 1.1376  Validation loss = 4.6310  \n",
      "\n",
      "Fold: 8  Epoch: 29  Training loss = 1.1372  Validation loss = 4.6354  \n",
      "\n",
      "Fold: 8  Epoch: 30  Training loss = 1.1366  Validation loss = 4.6546  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 23  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 1.5936  Validation loss = 8.0954  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 1.5913  Validation loss = 8.0986  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 1.5896  Validation loss = 8.0827  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 1.5859  Validation loss = 8.0158  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 1.5866  Validation loss = 8.1171  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 1.5808  Validation loss = 7.9730  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 1.5769  Validation loss = 7.9206  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 1.5744  Validation loss = 7.8837  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 1.5759  Validation loss = 7.9482  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 1.5737  Validation loss = 7.8976  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 1.5718  Validation loss = 7.8496  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 1.5677  Validation loss = 7.7972  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 1.5643  Validation loss = 7.7361  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 1.5614  Validation loss = 7.7135  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 1.5611  Validation loss = 7.7396  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 1.5574  Validation loss = 7.6976  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 1.5558  Validation loss = 7.6900  \n",
      "\n",
      "Fold: 9  Epoch: 18  Training loss = 1.5543  Validation loss = 7.6717  \n",
      "\n",
      "Fold: 9  Epoch: 19  Training loss = 1.5526  Validation loss = 7.6535  \n",
      "\n",
      "Fold: 9  Epoch: 20  Training loss = 1.5505  Validation loss = 7.5983  \n",
      "\n",
      "Fold: 9  Epoch: 21  Training loss = 1.5492  Validation loss = 7.6009  \n",
      "\n",
      "Fold: 9  Epoch: 22  Training loss = 1.5479  Validation loss = 7.5861  \n",
      "\n",
      "Fold: 9  Epoch: 23  Training loss = 1.5465  Validation loss = 7.5666  \n",
      "\n",
      "Fold: 9  Epoch: 24  Training loss = 1.5447  Validation loss = 7.5206  \n",
      "\n",
      "Fold: 9  Epoch: 25  Training loss = 1.5435  Validation loss = 7.5166  \n",
      "\n",
      "Fold: 9  Epoch: 26  Training loss = 1.5428  Validation loss = 7.4940  \n",
      "\n",
      "Fold: 9  Epoch: 27  Training loss = 1.5415  Validation loss = 7.4785  \n",
      "\n",
      "Fold: 9  Epoch: 28  Training loss = 1.5411  Validation loss = 7.5104  \n",
      "\n",
      "Fold: 9  Epoch: 29  Training loss = 1.5396  Validation loss = 7.4841  \n",
      "\n",
      "Fold: 9  Epoch: 30  Training loss = 1.5378  Validation loss = 7.4697  \n",
      "\n",
      "Fold: 9  Epoch: 31  Training loss = 1.5376  Validation loss = 7.4900  \n",
      "\n",
      "Fold: 9  Epoch: 32  Training loss = 1.5363  Validation loss = 7.4948  \n",
      "\n",
      "Fold: 9  Epoch: 33  Training loss = 1.5353  Validation loss = 7.5088  \n",
      "\n",
      "Fold: 9  Epoch: 34  Training loss = 1.5346  Validation loss = 7.5681  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 30  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 2.3111  Validation loss = 3.0236  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 2.2892  Validation loss = 2.9483  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 2.2356  Validation loss = 2.6643  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 2.1982  Validation loss = 2.5323  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 2.1882  Validation loss = 2.5086  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 2.1816  Validation loss = 2.4982  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 2.1550  Validation loss = 2.4435  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 2.1228  Validation loss = 2.3362  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 2.1180  Validation loss = 2.3251  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 2.1126  Validation loss = 2.3034  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 2.1022  Validation loss = 2.2420  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 2.0932  Validation loss = 2.1984  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 2.0787  Validation loss = 2.1503  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 2.0741  Validation loss = 2.1382  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 2.0504  Validation loss = 2.0463  \n",
      "\n",
      "Fold: 10  Epoch: 16  Training loss = 2.0233  Validation loss = 1.8740  \n",
      "\n",
      "Fold: 10  Epoch: 17  Training loss = 2.0143  Validation loss = 1.8442  \n",
      "\n",
      "Fold: 10  Epoch: 18  Training loss = 2.0088  Validation loss = 1.8313  \n",
      "\n",
      "Fold: 10  Epoch: 19  Training loss = 1.9984  Validation loss = 1.8097  \n",
      "\n",
      "Fold: 10  Epoch: 20  Training loss = 1.9947  Validation loss = 1.8068  \n",
      "\n",
      "Fold: 10  Epoch: 21  Training loss = 1.9903  Validation loss = 1.8029  \n",
      "\n",
      "Fold: 10  Epoch: 22  Training loss = 1.9912  Validation loss = 1.7973  \n",
      "\n",
      "Fold: 10  Epoch: 23  Training loss = 1.9886  Validation loss = 1.7918  \n",
      "\n",
      "Fold: 10  Epoch: 24  Training loss = 1.9865  Validation loss = 1.7869  \n",
      "\n",
      "Fold: 10  Epoch: 25  Training loss = 1.9792  Validation loss = 1.7280  \n",
      "\n",
      "Fold: 10  Epoch: 26  Training loss = 1.9767  Validation loss = 1.7080  \n",
      "\n",
      "Fold: 10  Epoch: 27  Training loss = 1.9681  Validation loss = 1.7008  \n",
      "\n",
      "Fold: 10  Epoch: 28  Training loss = 1.9653  Validation loss = 1.7084  \n",
      "\n",
      "Fold: 10  Epoch: 29  Training loss = 1.9558  Validation loss = 1.6874  \n",
      "\n",
      "Fold: 10  Epoch: 30  Training loss = 1.9520  Validation loss = 1.6830  \n",
      "\n",
      "Fold: 10  Epoch: 31  Training loss = 1.9508  Validation loss = 1.6807  \n",
      "\n",
      "Fold: 10  Epoch: 32  Training loss = 1.9401  Validation loss = 1.6461  \n",
      "\n",
      "Fold: 10  Epoch: 33  Training loss = 1.9337  Validation loss = 1.6460  \n",
      "\n",
      "Fold: 10  Epoch: 34  Training loss = 1.9243  Validation loss = 1.6043  \n",
      "\n",
      "Fold: 10  Epoch: 35  Training loss = 1.9146  Validation loss = 1.5849  \n",
      "\n",
      "Fold: 10  Epoch: 36  Training loss = 1.9091  Validation loss = 1.5324  \n",
      "\n",
      "Fold: 10  Epoch: 37  Training loss = 1.9103  Validation loss = 1.5331  \n",
      "\n",
      "Fold: 10  Epoch: 38  Training loss = 1.9042  Validation loss = 1.5315  \n",
      "\n",
      "Fold: 10  Epoch: 39  Training loss = 1.8967  Validation loss = 1.5141  \n",
      "\n",
      "Fold: 10  Epoch: 40  Training loss = 1.9115  Validation loss = 1.5656  \n",
      "\n",
      "Fold: 10  Epoch: 41  Training loss = 1.8853  Validation loss = 1.4986  \n",
      "\n",
      "Fold: 10  Epoch: 42  Training loss = 1.8835  Validation loss = 1.4981  \n",
      "\n",
      "Fold: 10  Epoch: 43  Training loss = 1.8780  Validation loss = 1.4972  \n",
      "\n",
      "Fold: 10  Epoch: 44  Training loss = 1.8705  Validation loss = 1.4977  \n",
      "\n",
      "Fold: 10  Epoch: 45  Training loss = 1.8683  Validation loss = 1.5000  \n",
      "\n",
      "Fold: 10  Epoch: 46  Training loss = 1.8666  Validation loss = 1.4995  \n",
      "\n",
      "Fold: 10  Epoch: 47  Training loss = 1.8610  Validation loss = 1.4981  \n",
      "\n",
      "Fold: 10  Epoch: 48  Training loss = 1.8608  Validation loss = 1.4888  \n",
      "\n",
      "Fold: 10  Epoch: 49  Training loss = 1.8557  Validation loss = 1.4786  \n",
      "\n",
      "Fold: 10  Epoch: 50  Training loss = 1.8498  Validation loss = 1.4774  \n",
      "\n",
      "Fold: 10  Epoch: 51  Training loss = 1.8589  Validation loss = 1.5047  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 50  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 1.8457  Validation loss = 1.3112  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 1.8397  Validation loss = 1.3035  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 1.8347  Validation loss = 1.3143  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 1.8290  Validation loss = 1.2924  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 1.8216  Validation loss = 1.2820  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 1.8191  Validation loss = 1.2622  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 1.8347  Validation loss = 1.2364  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 1.8156  Validation loss = 1.2522  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 1.8106  Validation loss = 1.2184  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 1.8028  Validation loss = 1.2433  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 1.7980  Validation loss = 1.2316  \n",
      "\n",
      "Fold: 11  Epoch: 12  Training loss = 1.7952  Validation loss = 1.2256  \n",
      "\n",
      "Fold: 11  Epoch: 13  Training loss = 1.7888  Validation loss = 1.2320  \n",
      "\n",
      "Fold: 11  Epoch: 14  Training loss = 1.7864  Validation loss = 1.2648  \n",
      "\n",
      "Fold: 11  Epoch: 15  Training loss = 1.7841  Validation loss = 1.2873  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 9  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 1.7780  Validation loss = 1.3366  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 1.7815  Validation loss = 1.3080  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 1.7735  Validation loss = 1.2757  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 1.7717  Validation loss = 1.3051  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 1.7910  Validation loss = 1.2536  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 1.7642  Validation loss = 1.2607  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 1.7616  Validation loss = 1.3161  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 1.7600  Validation loss = 1.3149  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 1.7554  Validation loss = 1.2938  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 1.7483  Validation loss = 1.3060  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 1.7408  Validation loss = 1.3001  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 1.7368  Validation loss = 1.3067  \n",
      "\n",
      "Fold: 12  Epoch: 13  Training loss = 1.7353  Validation loss = 1.3087  \n",
      "\n",
      "Fold: 12  Epoch: 14  Training loss = 1.7340  Validation loss = 1.3036  \n",
      "\n",
      "Fold: 12  Epoch: 15  Training loss = 1.7299  Validation loss = 1.3059  \n",
      "\n",
      "Fold: 12  Epoch: 16  Training loss = 1.7253  Validation loss = 1.3076  \n",
      "\n",
      "Fold: 12  Epoch: 17  Training loss = 1.7234  Validation loss = 1.3062  \n",
      "\n",
      "Fold: 12  Epoch: 18  Training loss = 1.7201  Validation loss = 1.3055  \n",
      "\n",
      "Fold: 12  Epoch: 19  Training loss = 1.7196  Validation loss = 1.3067  \n",
      "\n",
      "Fold: 12  Epoch: 20  Training loss = 1.7163  Validation loss = 1.3110  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 5  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 1.7126  Validation loss = 2.2104  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 1.7081  Validation loss = 2.2451  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 1.7071  Validation loss = 2.2326  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 1.7048  Validation loss = 2.3960  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 1.6982  Validation loss = 2.3938  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 1.6951  Validation loss = 2.4161  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 1.6924  Validation loss = 2.3307  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 1.6906  Validation loss = 2.4065  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 1.6883  Validation loss = 2.3351  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 1.6872  Validation loss = 2.3898  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 1.6857  Validation loss = 2.3389  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 1.6866  Validation loss = 2.5074  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 1  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 1.7817  Validation loss = 4.9758  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 1.7857  Validation loss = 4.9981  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 1.7797  Validation loss = 4.9645  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 1.7742  Validation loss = 4.9450  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 1.7697  Validation loss = 4.9180  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 1.7650  Validation loss = 4.8664  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 1.7631  Validation loss = 4.8489  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 1.7631  Validation loss = 4.8191  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 1.7632  Validation loss = 4.7989  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 1.7628  Validation loss = 4.7731  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 1.7612  Validation loss = 4.7646  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 1.7598  Validation loss = 4.7637  \n",
      "\n",
      "Fold: 14  Epoch: 13  Training loss = 1.7563  Validation loss = 4.7978  \n",
      "\n",
      "Fold: 14  Epoch: 14  Training loss = 1.7588  Validation loss = 4.8242  \n",
      "\n",
      "Fold: 14  Epoch: 15  Training loss = 1.7532  Validation loss = 4.7780  \n",
      "\n",
      "Fold: 14  Epoch: 16  Training loss = 1.7518  Validation loss = 4.7693  \n",
      "\n",
      "Fold: 14  Epoch: 17  Training loss = 1.7478  Validation loss = 4.7109  \n",
      "\n",
      "Fold: 14  Epoch: 18  Training loss = 1.7462  Validation loss = 4.7051  \n",
      "\n",
      "Fold: 14  Epoch: 19  Training loss = 1.7466  Validation loss = 4.7143  \n",
      "\n",
      "Fold: 14  Epoch: 20  Training loss = 1.7478  Validation loss = 4.6861  \n",
      "\n",
      "Fold: 14  Epoch: 21  Training loss = 1.7433  Validation loss = 4.6777  \n",
      "\n",
      "Fold: 14  Epoch: 22  Training loss = 1.7406  Validation loss = 4.6478  \n",
      "\n",
      "Fold: 14  Epoch: 23  Training loss = 1.7428  Validation loss = 4.6429  \n",
      "\n",
      "Fold: 14  Epoch: 24  Training loss = 1.7446  Validation loss = 4.6311  \n",
      "\n",
      "Fold: 14  Epoch: 25  Training loss = 1.7468  Validation loss = 4.5874  \n",
      "\n",
      "Fold: 14  Epoch: 26  Training loss = 1.7373  Validation loss = 4.6076  \n",
      "\n",
      "Fold: 14  Epoch: 27  Training loss = 1.7361  Validation loss = 4.5776  \n",
      "\n",
      "Fold: 14  Epoch: 28  Training loss = 1.7412  Validation loss = 4.5746  \n",
      "\n",
      "Fold: 14  Epoch: 29  Training loss = 1.7413  Validation loss = 4.5857  \n",
      "\n",
      "Fold: 14  Epoch: 30  Training loss = 1.7409  Validation loss = 4.6001  \n",
      "\n",
      "Fold: 14  Epoch: 31  Training loss = 1.7383  Validation loss = 4.5586  \n",
      "\n",
      "Fold: 14  Epoch: 32  Training loss = 1.7365  Validation loss = 4.5528  \n",
      "\n",
      "Fold: 14  Epoch: 33  Training loss = 1.7354  Validation loss = 4.4945  \n",
      "\n",
      "Fold: 14  Epoch: 34  Training loss = 1.7336  Validation loss = 4.5004  \n",
      "\n",
      "Fold: 14  Epoch: 35  Training loss = 1.7344  Validation loss = 4.4735  \n",
      "\n",
      "Fold: 14  Epoch: 36  Training loss = 1.7303  Validation loss = 4.4649  \n",
      "\n",
      "Fold: 14  Epoch: 37  Training loss = 1.7302  Validation loss = 4.4907  \n",
      "\n",
      "Fold: 14  Epoch: 38  Training loss = 1.7309  Validation loss = 4.5196  \n",
      "\n",
      "Fold: 14  Epoch: 39  Training loss = 1.7300  Validation loss = 4.5100  \n",
      "\n",
      "Fold: 14  Epoch: 40  Training loss = 1.7337  Validation loss = 4.4828  \n",
      "\n",
      "Fold: 14  Epoch: 41  Training loss = 1.7329  Validation loss = 4.4955  \n",
      "\n",
      "Fold: 14  Epoch: 42  Training loss = 1.7467  Validation loss = 4.5814  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 36  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 2.0519  Validation loss = 4.5297  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 2.0244  Validation loss = 4.4625  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 2.0263  Validation loss = 4.4141  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 2.0103  Validation loss = 4.3849  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 2.0066  Validation loss = 4.3354  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 1.9957  Validation loss = 4.3517  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 1.9905  Validation loss = 4.3220  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 1.9870  Validation loss = 4.3030  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 1.9807  Validation loss = 4.2796  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 1.9759  Validation loss = 4.2620  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 1.9772  Validation loss = 4.2320  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 1.9729  Validation loss = 4.2181  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 1.9628  Validation loss = 4.2155  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 1.9678  Validation loss = 4.1839  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 1.9591  Validation loss = 4.1690  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 1.9538  Validation loss = 4.1664  \n",
      "\n",
      "Fold: 15  Epoch: 17  Training loss = 1.9539  Validation loss = 4.1342  \n",
      "\n",
      "Fold: 15  Epoch: 18  Training loss = 1.9523  Validation loss = 4.1119  \n",
      "\n",
      "Fold: 15  Epoch: 19  Training loss = 1.9544  Validation loss = 4.1166  \n",
      "\n",
      "Fold: 15  Epoch: 20  Training loss = 1.9495  Validation loss = 4.1281  \n",
      "\n",
      "Fold: 15  Epoch: 21  Training loss = 1.9412  Validation loss = 4.1269  \n",
      "\n",
      "Fold: 15  Epoch: 22  Training loss = 1.9367  Validation loss = 4.1346  \n",
      "\n",
      "Fold: 15  Epoch: 23  Training loss = 1.9284  Validation loss = 4.0846  \n",
      "\n",
      "Fold: 15  Epoch: 24  Training loss = 1.9257  Validation loss = 4.0826  \n",
      "\n",
      "Fold: 15  Epoch: 25  Training loss = 1.9291  Validation loss = 4.1477  \n",
      "\n",
      "Fold: 15  Epoch: 26  Training loss = 1.9306  Validation loss = 4.0781  \n",
      "\n",
      "Fold: 15  Epoch: 27  Training loss = 1.9338  Validation loss = 4.0141  \n",
      "\n",
      "Fold: 15  Epoch: 28  Training loss = 1.9147  Validation loss = 4.0177  \n",
      "\n",
      "Fold: 15  Epoch: 29  Training loss = 1.9206  Validation loss = 3.9709  \n",
      "\n",
      "Fold: 15  Epoch: 30  Training loss = 1.9139  Validation loss = 3.9610  \n",
      "\n",
      "Fold: 15  Epoch: 31  Training loss = 1.9083  Validation loss = 3.9454  \n",
      "\n",
      "Fold: 15  Epoch: 32  Training loss = 1.9044  Validation loss = 3.9568  \n",
      "\n",
      "Fold: 15  Epoch: 33  Training loss = 1.9326  Validation loss = 4.2001  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 31  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 2.1267  Validation loss = 3.2891  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 2.1233  Validation loss = 3.3146  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 2.1189  Validation loss = 3.3118  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 2.1085  Validation loss = 3.2773  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 2.1038  Validation loss = 3.2116  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 2.1063  Validation loss = 3.1737  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 2.0967  Validation loss = 3.1793  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 2.0884  Validation loss = 3.2362  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 2.0915  Validation loss = 3.2605  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 2.0703  Validation loss = 3.2598  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 2.0673  Validation loss = 3.3308  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 6  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 2.2131  Validation loss = 3.1735  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 2.2040  Validation loss = 3.1945  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 2.2093  Validation loss = 3.1489  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 2.1932  Validation loss = 3.2626  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 2.1859  Validation loss = 3.2395  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 2.2064  Validation loss = 3.2537  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 2.2524  Validation loss = 3.2052  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 2.1958  Validation loss = 3.1686  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 2.1946  Validation loss = 3.0873  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 2.1870  Validation loss = 3.0816  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 2.1777  Validation loss = 3.0799  \n",
      "\n",
      "Fold: 17  Epoch: 12  Training loss = 2.1721  Validation loss = 3.0745  \n",
      "\n",
      "Fold: 17  Epoch: 13  Training loss = 2.1574  Validation loss = 3.1346  \n",
      "\n",
      "Fold: 17  Epoch: 14  Training loss = 2.2334  Validation loss = 3.0733  \n",
      "\n",
      "Fold: 17  Epoch: 15  Training loss = 2.1582  Validation loss = 3.0800  \n",
      "\n",
      "Fold: 17  Epoch: 16  Training loss = 2.1516  Validation loss = 3.1248  \n",
      "\n",
      "Fold: 17  Epoch: 17  Training loss = 2.1441  Validation loss = 3.1587  \n",
      "\n",
      "Fold: 17  Epoch: 18  Training loss = 2.1562  Validation loss = 3.2458  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 14  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 2.2575  Validation loss = 1.8272  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 2.2555  Validation loss = 1.8252  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 2.3508  Validation loss = 1.8038  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 2.3296  Validation loss = 1.7850  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 2.2617  Validation loss = 1.8116  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 2.2776  Validation loss = 1.8502  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 2.2678  Validation loss = 1.8656  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 2.2554  Validation loss = 1.8393  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 2.2618  Validation loss = 1.8310  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 2.2484  Validation loss = 1.8299  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 2.2570  Validation loss = 1.8173  \n",
      "\n",
      "Fold: 18  Epoch: 12  Training loss = 2.2409  Validation loss = 1.8284  \n",
      "\n",
      "Fold: 18  Epoch: 13  Training loss = 2.2377  Validation loss = 1.8168  \n",
      "\n",
      "Fold: 18  Epoch: 14  Training loss = 2.2187  Validation loss = 1.8526  \n",
      "\n",
      "Fold: 18  Epoch: 15  Training loss = 2.2133  Validation loss = 1.8440  \n",
      "\n",
      "Fold: 18  Epoch: 16  Training loss = 2.2273  Validation loss = 1.7997  \n",
      "\n",
      "Fold: 18  Epoch: 17  Training loss = 2.2073  Validation loss = 1.8118  \n",
      "\n",
      "Fold: 18  Epoch: 18  Training loss = 2.2053  Validation loss = 1.8335  \n",
      "\n",
      "Fold: 18  Epoch: 19  Training loss = 2.2065  Validation loss = 1.8458  \n",
      "\n",
      "Fold: 18  Epoch: 20  Training loss = 2.2103  Validation loss = 1.8550  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 4  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 2.1900  Validation loss = 1.9104  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 2.1860  Validation loss = 1.9185  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 2.1751  Validation loss = 1.9149  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 2.1749  Validation loss = 1.8915  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 2.1589  Validation loss = 1.8530  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 2.1496  Validation loss = 1.8343  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 2.1422  Validation loss = 1.8211  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 2.1424  Validation loss = 1.8249  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 2.1399  Validation loss = 1.7925  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 2.1368  Validation loss = 1.7905  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 2.1263  Validation loss = 1.8058  \n",
      "\n",
      "Fold: 19  Epoch: 12  Training loss = 2.1154  Validation loss = 1.7907  \n",
      "\n",
      "Fold: 19  Epoch: 13  Training loss = 2.1228  Validation loss = 1.7567  \n",
      "\n",
      "Fold: 19  Epoch: 14  Training loss = 2.1166  Validation loss = 1.7448  \n",
      "\n",
      "Fold: 19  Epoch: 15  Training loss = 2.1069  Validation loss = 1.7547  \n",
      "\n",
      "Fold: 19  Epoch: 16  Training loss = 2.1028  Validation loss = 1.7481  \n",
      "\n",
      "Fold: 19  Epoch: 17  Training loss = 2.1008  Validation loss = 1.7508  \n",
      "\n",
      "Fold: 19  Epoch: 18  Training loss = 2.0984  Validation loss = 1.7319  \n",
      "\n",
      "Fold: 19  Epoch: 19  Training loss = 2.1377  Validation loss = 1.7399  \n",
      "\n",
      "Fold: 19  Epoch: 20  Training loss = 2.1143  Validation loss = 1.7229  \n",
      "\n",
      "Fold: 19  Epoch: 21  Training loss = 2.1237  Validation loss = 1.7463  \n",
      "\n",
      "Fold: 19  Epoch: 22  Training loss = 2.1164  Validation loss = 1.7535  \n",
      "\n",
      "Fold: 19  Epoch: 23  Training loss = 2.0804  Validation loss = 1.7519  \n",
      "\n",
      "Fold: 19  Epoch: 24  Training loss = 2.0790  Validation loss = 1.7472  \n",
      "\n",
      "Fold: 19  Epoch: 25  Training loss = 2.0755  Validation loss = 1.7068  \n",
      "\n",
      "Fold: 19  Epoch: 26  Training loss = 2.0693  Validation loss = 1.7039  \n",
      "\n",
      "Fold: 19  Epoch: 27  Training loss = 2.0632  Validation loss = 1.6959  \n",
      "\n",
      "Fold: 19  Epoch: 28  Training loss = 2.0664  Validation loss = 1.6866  \n",
      "\n",
      "Fold: 19  Epoch: 29  Training loss = 2.0656  Validation loss = 1.6651  \n",
      "\n",
      "Fold: 19  Epoch: 30  Training loss = 2.0903  Validation loss = 1.6886  \n",
      "\n",
      "Fold: 19  Epoch: 31  Training loss = 2.0973  Validation loss = 1.6855  \n",
      "\n",
      "Fold: 19  Epoch: 32  Training loss = 2.0621  Validation loss = 1.6886  \n",
      "\n",
      "Fold: 19  Epoch: 33  Training loss = 2.0613  Validation loss = 1.6751  \n",
      "\n",
      "Fold: 19  Epoch: 34  Training loss = 2.0529  Validation loss = 1.6662  \n",
      "\n",
      "Fold: 19  Epoch: 35  Training loss = 2.0925  Validation loss = 1.6620  \n",
      "\n",
      "Fold: 19  Epoch: 36  Training loss = 2.0540  Validation loss = 1.6849  \n",
      "\n",
      "Fold: 19  Epoch: 37  Training loss = 2.0526  Validation loss = 1.6813  \n",
      "\n",
      "Fold: 19  Epoch: 38  Training loss = 2.0592  Validation loss = 1.6439  \n",
      "\n",
      "Fold: 19  Epoch: 39  Training loss = 2.0453  Validation loss = 1.6523  \n",
      "\n",
      "Fold: 19  Epoch: 40  Training loss = 2.0888  Validation loss = 1.6689  \n",
      "\n",
      "Fold: 19  Epoch: 41  Training loss = 2.1483  Validation loss = 1.6736  \n",
      "\n",
      "Fold: 19  Epoch: 42  Training loss = 2.0772  Validation loss = 1.6883  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 38  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 2.1074  Validation loss = 0.7442  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 2.1001  Validation loss = 0.7344  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 2.1019  Validation loss = 0.7310  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 2.1042  Validation loss = 0.7732  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 2.1118  Validation loss = 0.8208  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 2.0938  Validation loss = 0.7721  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 2.0914  Validation loss = 0.7892  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 2.0914  Validation loss = 0.7126  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 2.0922  Validation loss = 0.6762  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 2.0777  Validation loss = 0.6736  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 2.0805  Validation loss = 0.6734  \n",
      "\n",
      "Fold: 20  Epoch: 12  Training loss = 2.0773  Validation loss = 0.6724  \n",
      "\n",
      "Fold: 20  Epoch: 13  Training loss = 2.0933  Validation loss = 0.6605  \n",
      "\n",
      "Fold: 20  Epoch: 14  Training loss = 2.0645  Validation loss = 0.6647  \n",
      "\n",
      "Fold: 20  Epoch: 15  Training loss = 2.0594  Validation loss = 0.6539  \n",
      "\n",
      "Fold: 20  Epoch: 16  Training loss = 2.0925  Validation loss = 0.6318  \n",
      "\n",
      "Fold: 20  Epoch: 17  Training loss = 2.0658  Validation loss = 0.6399  \n",
      "\n",
      "Fold: 20  Epoch: 18  Training loss = 2.0501  Validation loss = 0.6614  \n",
      "\n",
      "Fold: 20  Epoch: 19  Training loss = 2.0463  Validation loss = 0.6528  \n",
      "\n",
      "Fold: 20  Epoch: 20  Training loss = 2.0410  Validation loss = 0.6585  \n",
      "\n",
      "Fold: 20  Epoch: 21  Training loss = 2.0593  Validation loss = 0.6400  \n",
      "\n",
      "Fold: 20  Epoch: 22  Training loss = 2.0701  Validation loss = 0.6421  \n",
      "\n",
      "Fold: 20  Epoch: 23  Training loss = 2.0655  Validation loss = 0.6415  \n",
      "\n",
      "Fold: 20  Epoch: 24  Training loss = 2.0437  Validation loss = 0.6527  \n",
      "\n",
      "Fold: 20  Epoch: 25  Training loss = 2.0398  Validation loss = 0.6505  \n",
      "\n",
      "Fold: 20  Epoch: 26  Training loss = 2.0299  Validation loss = 0.6626  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 16  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 2.0011  Validation loss = 3.3048  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 1.9998  Validation loss = 3.3319  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 1.9987  Validation loss = 3.3098  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 2.0037  Validation loss = 3.2155  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 1.9978  Validation loss = 3.3650  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 1.9933  Validation loss = 3.3513  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 1.9937  Validation loss = 3.2813  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 2.0021  Validation loss = 3.4248  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 1.9964  Validation loss = 3.3959  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 2.0082  Validation loss = 3.4620  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 1.9894  Validation loss = 3.3315  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 1.9833  Validation loss = 3.3094  \n",
      "\n",
      "Fold: 21  Epoch: 13  Training loss = 2.0570  Validation loss = 3.8054  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 4  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 2.1103  Validation loss = 2.6895  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 2.0878  Validation loss = 2.5016  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 2.0760  Validation loss = 3.0762  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 2.0520  Validation loss = 3.2252  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 2.0587  Validation loss = 2.5476  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 2.0241  Validation loss = 2.9211  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 2.0175  Validation loss = 2.7024  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 2.0180  Validation loss = 2.7007  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 2.0055  Validation loss = 2.7458  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 2.0057  Validation loss = 2.5918  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 2.0103  Validation loss = 2.7975  \n",
      "\n",
      "Fold: 22  Epoch: 12  Training loss = 2.0086  Validation loss = 2.7351  \n",
      "\n",
      "Fold: 22  Epoch: 13  Training loss = 1.9896  Validation loss = 2.7571  \n",
      "\n",
      "Fold: 22  Epoch: 14  Training loss = 1.9957  Validation loss = 2.6487  \n",
      "\n",
      "Fold: 22  Epoch: 15  Training loss = 1.9926  Validation loss = 2.6224  \n",
      "\n",
      "Fold: 22  Epoch: 16  Training loss = 1.9851  Validation loss = 2.7996  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 2  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 2.1213  Validation loss = 2.5304  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 2.0775  Validation loss = 2.2930  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 2.1347  Validation loss = 2.4786  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 2.0983  Validation loss = 2.1530  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 2.0287  Validation loss = 2.0249  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 2.1285  Validation loss = 1.6470  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 2.0623  Validation loss = 2.2680  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 2.0610  Validation loss = 2.2318  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 2.1163  Validation loss = 1.6282  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 2.0397  Validation loss = 2.1704  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 2.0079  Validation loss = 1.8419  \n",
      "\n",
      "Fold: 23  Epoch: 12  Training loss = 1.9940  Validation loss = 1.9246  \n",
      "\n",
      "Fold: 23  Epoch: 13  Training loss = 1.9887  Validation loss = 1.8958  \n",
      "\n",
      "Fold: 23  Epoch: 14  Training loss = 2.0042  Validation loss = 1.6215  \n",
      "\n",
      "Fold: 23  Epoch: 15  Training loss = 1.9744  Validation loss = 1.8862  \n",
      "\n",
      "Fold: 23  Epoch: 16  Training loss = 1.9596  Validation loss = 1.8103  \n",
      "\n",
      "Fold: 23  Epoch: 17  Training loss = 1.9797  Validation loss = 1.9560  \n",
      "\n",
      "Fold: 23  Epoch: 18  Training loss = 1.9965  Validation loss = 1.8883  \n",
      "\n",
      "Fold: 23  Epoch: 19  Training loss = 1.9765  Validation loss = 1.7348  \n",
      "\n",
      "Fold: 23  Epoch: 20  Training loss = 1.9470  Validation loss = 1.5733  \n",
      "\n",
      "Fold: 23  Epoch: 21  Training loss = 1.9435  Validation loss = 1.5966  \n",
      "\n",
      "Fold: 23  Epoch: 22  Training loss = 1.9691  Validation loss = 1.8384  \n",
      "\n",
      "Fold: 23  Epoch: 23  Training loss = 1.9271  Validation loss = 1.5267  \n",
      "\n",
      "Fold: 23  Epoch: 24  Training loss = 1.9323  Validation loss = 1.6759  \n",
      "\n",
      "Fold: 23  Epoch: 25  Training loss = 1.9328  Validation loss = 1.7009  \n",
      "\n",
      "Fold: 23  Epoch: 26  Training loss = 1.9504  Validation loss = 1.5097  \n",
      "\n",
      "Fold: 23  Epoch: 27  Training loss = 1.9113  Validation loss = 1.5821  \n",
      "\n",
      "Fold: 23  Epoch: 28  Training loss = 1.9160  Validation loss = 1.6511  \n",
      "\n",
      "Fold: 23  Epoch: 29  Training loss = 1.9337  Validation loss = 1.9200  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 26  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 1.9443  Validation loss = 1.4697  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 1.9131  Validation loss = 1.4425  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 1.9137  Validation loss = 1.4475  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 1.9196  Validation loss = 1.4232  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 1.9449  Validation loss = 1.3492  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 1.9101  Validation loss = 1.4025  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 1.8914  Validation loss = 1.4123  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 1.9276  Validation loss = 1.5648  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 1.8949  Validation loss = 1.4337  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 1.9039  Validation loss = 1.4155  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 1.8999  Validation loss = 1.4381  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 1.9569  Validation loss = 1.4028  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 1.9174  Validation loss = 1.4362  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 1.9578  Validation loss = 1.4778  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 2.0397  Validation loss = 1.5151  \n",
      "\n",
      "Fold: 24  Epoch: 16  Training loss = 1.9659  Validation loss = 1.4784  \n",
      "\n",
      "Fold: 24  Epoch: 17  Training loss = 1.9526  Validation loss = 1.4523  \n",
      "\n",
      "Fold: 24  Epoch: 18  Training loss = 1.9288  Validation loss = 1.4429  \n",
      "\n",
      "Fold: 24  Epoch: 19  Training loss = 1.9560  Validation loss = 1.4124  \n",
      "\n",
      "Fold: 24  Epoch: 20  Training loss = 1.9712  Validation loss = 1.4812  \n",
      "\n",
      "Fold: 24  Epoch: 21  Training loss = 1.9247  Validation loss = 1.4254  \n",
      "\n",
      "Fold: 24  Epoch: 22  Training loss = 1.9602  Validation loss = 1.4507  \n",
      "\n",
      "Fold: 24  Epoch: 23  Training loss = 1.9032  Validation loss = 1.4039  \n",
      "\n",
      "Fold: 24  Epoch: 24  Training loss = 1.8938  Validation loss = 1.4486  \n",
      "\n",
      "Fold: 24  Epoch: 25  Training loss = 1.8897  Validation loss = 1.4821  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 5  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 1.8081  Validation loss = 2.3539  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 1.8209  Validation loss = 2.3620  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 1.8357  Validation loss = 2.4425  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 1.8074  Validation loss = 2.3863  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 1.8308  Validation loss = 2.4148  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 1.8136  Validation loss = 2.4568  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 1.7821  Validation loss = 2.4328  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 1.7919  Validation loss = 2.4224  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 1.7954  Validation loss = 2.3877  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 1.7929  Validation loss = 2.4786  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 1.7808  Validation loss = 2.4640  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 1.9036  Validation loss = 2.3545  \n",
      "\n",
      "Fold: 25  Epoch: 13  Training loss = 1.9064  Validation loss = 2.6508  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 1  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 1.9678  Validation loss = 2.1717  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 1.8902  Validation loss = 2.2990  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 1.8425  Validation loss = 1.9296  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 2.0109  Validation loss = 1.5387  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 1.8748  Validation loss = 1.8984  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 1.9005  Validation loss = 2.5306  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 1.8887  Validation loss = 2.3709  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 1.8076  Validation loss = 2.0584  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 1.7963  Validation loss = 2.3104  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 1.8546  Validation loss = 2.5946  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 1.8142  Validation loss = 2.6254  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 4  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 1.7431  Validation loss = 0.8360  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 1.6936  Validation loss = 0.8759  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 1.7944  Validation loss = 0.9265  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 1.6826  Validation loss = 0.8436  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 1.6665  Validation loss = 0.8579  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 1.6670  Validation loss = 0.9001  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 1.6588  Validation loss = 0.8675  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 1.6970  Validation loss = 0.8252  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 1.6751  Validation loss = 0.8724  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 1.6426  Validation loss = 0.8895  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 1.6381  Validation loss = 0.9080  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 1.7234  Validation loss = 0.9432  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 8  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 1.6452  Validation loss = 0.5060  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 1.6559  Validation loss = 0.3621  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 1.6260  Validation loss = 0.4714  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 1.6018  Validation loss = 0.4451  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 1.6060  Validation loss = 0.4131  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 1.6015  Validation loss = 0.5194  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 1.6183  Validation loss = 0.4210  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 1.5926  Validation loss = 0.5675  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 1.5904  Validation loss = 0.4875  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 1.5784  Validation loss = 0.4977  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 1.5986  Validation loss = 0.5029  \n",
      "\n",
      "Fold: 28  Epoch: 12  Training loss = 1.5834  Validation loss = 0.4829  \n",
      "\n",
      "Fold: 28  Epoch: 13  Training loss = 1.6288  Validation loss = 0.6171  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 2  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 1.5980  Validation loss = 0.6812  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 1.5733  Validation loss = 0.6768  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 1.5667  Validation loss = 0.6890  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 1.5669  Validation loss = 0.6828  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 1.6293  Validation loss = 0.6881  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 1.5607  Validation loss = 0.6781  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 1.5638  Validation loss = 0.7004  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 1.5583  Validation loss = 0.6866  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 1.5776  Validation loss = 0.6800  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 1.5437  Validation loss = 0.6808  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 1.5493  Validation loss = 0.6811  \n",
      "\n",
      "Fold: 29  Epoch: 12  Training loss = 1.5568  Validation loss = 0.6980  \n",
      "\n",
      "Fold: 29  Epoch: 13  Training loss = 1.5454  Validation loss = 0.6972  \n",
      "\n",
      "Fold: 29  Epoch: 14  Training loss = 1.5558  Validation loss = 0.6988  \n",
      "\n",
      "Fold: 29  Epoch: 15  Training loss = 1.5461  Validation loss = 0.6805  \n",
      "\n",
      "Fold: 29  Epoch: 16  Training loss = 1.5336  Validation loss = 0.7342  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 2  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 1.5235  Validation loss = 1.2951  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 1.5302  Validation loss = 1.2348  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 1.4981  Validation loss = 1.1969  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 1.4844  Validation loss = 1.2007  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 1.5164  Validation loss = 1.2805  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 1.4807  Validation loss = 1.2188  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 1.5281  Validation loss = 1.2981  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 1.5430  Validation loss = 1.2557  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 1.4950  Validation loss = 1.3417  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 1.4880  Validation loss = 1.2996  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 1.5579  Validation loss = 1.3029  \n",
      "\n",
      "Fold: 30  Epoch: 12  Training loss = 1.5229  Validation loss = 1.2166  \n",
      "\n",
      "Fold: 30  Epoch: 13  Training loss = 1.4777  Validation loss = 1.2077  \n",
      "\n",
      "Fold: 30  Epoch: 14  Training loss = 1.4940  Validation loss = 1.2498  \n",
      "\n",
      "Fold: 30  Epoch: 15  Training loss = 1.4822  Validation loss = 1.1822  \n",
      "\n",
      "Fold: 30  Epoch: 16  Training loss = 1.4734  Validation loss = 1.1651  \n",
      "\n",
      "Fold: 30  Epoch: 17  Training loss = 1.4607  Validation loss = 1.1852  \n",
      "\n",
      "Fold: 30  Epoch: 18  Training loss = 1.4626  Validation loss = 1.1126  \n",
      "\n",
      "Fold: 30  Epoch: 19  Training loss = 1.4854  Validation loss = 1.0844  \n",
      "\n",
      "Fold: 30  Epoch: 20  Training loss = 1.4554  Validation loss = 1.0480  \n",
      "\n",
      "Fold: 30  Epoch: 21  Training loss = 1.4558  Validation loss = 1.0967  \n",
      "\n",
      "Fold: 30  Epoch: 22  Training loss = 1.4483  Validation loss = 1.1047  \n",
      "\n",
      "Fold: 30  Epoch: 23  Training loss = 1.4897  Validation loss = 1.1309  \n",
      "\n",
      "Fold: 30  Epoch: 24  Training loss = 1.4400  Validation loss = 1.0527  \n",
      "\n",
      "Fold: 30  Epoch: 25  Training loss = 1.4430  Validation loss = 1.0733  \n",
      "\n",
      "Fold: 30  Epoch: 26  Training loss = 1.4683  Validation loss = 1.0434  \n",
      "\n",
      "Fold: 30  Epoch: 27  Training loss = 1.4727  Validation loss = 1.1134  \n",
      "\n",
      "Fold: 30  Epoch: 28  Training loss = 1.4335  Validation loss = 1.1740  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 26  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 1.4333  Validation loss = 1.0345  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 1.4503  Validation loss = 0.9961  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 1.4381  Validation loss = 1.0933  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 1.4338  Validation loss = 1.1079  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 1.4437  Validation loss = 1.1183  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 1.4227  Validation loss = 1.0384  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 1.4285  Validation loss = 1.0847  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 1.4173  Validation loss = 1.0787  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 1.4269  Validation loss = 1.1107  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 1.4203  Validation loss = 1.1060  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 1.4124  Validation loss = 1.0696  \n",
      "\n",
      "Fold: 31  Epoch: 12  Training loss = 1.4145  Validation loss = 1.0972  \n",
      "\n",
      "Fold: 31  Epoch: 13  Training loss = 1.4068  Validation loss = 1.0525  \n",
      "\n",
      "Fold: 31  Epoch: 14  Training loss = 1.4380  Validation loss = 1.0338  \n",
      "\n",
      "Fold: 31  Epoch: 15  Training loss = 1.4049  Validation loss = 1.0501  \n",
      "\n",
      "Fold: 31  Epoch: 16  Training loss = 1.4239  Validation loss = 1.0196  \n",
      "\n",
      "Fold: 31  Epoch: 17  Training loss = 1.4018  Validation loss = 1.0487  \n",
      "\n",
      "Fold: 31  Epoch: 18  Training loss = 1.4074  Validation loss = 1.0658  \n",
      "\n",
      "Fold: 31  Epoch: 19  Training loss = 1.4198  Validation loss = 1.1016  \n",
      "\n",
      "Fold: 31  Epoch: 20  Training loss = 1.4110  Validation loss = 1.0248  \n",
      "\n",
      "Fold: 31  Epoch: 21  Training loss = 1.4238  Validation loss = 0.9943  \n",
      "\n",
      "Fold: 31  Epoch: 22  Training loss = 1.3905  Validation loss = 1.0147  \n",
      "\n",
      "Fold: 31  Epoch: 23  Training loss = 1.4120  Validation loss = 1.0258  \n",
      "\n",
      "Fold: 31  Epoch: 24  Training loss = 1.4426  Validation loss = 1.1690  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 21  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 1.3475  Validation loss = 1.7012  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 1.3487  Validation loss = 1.6309  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 1.3528  Validation loss = 1.6186  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 1.3472  Validation loss = 1.8952  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 1.3362  Validation loss = 1.6266  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 1.3284  Validation loss = 1.7672  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 1.3305  Validation loss = 1.6593  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 1.3503  Validation loss = 1.7632  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 1.3334  Validation loss = 1.6367  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 1.3318  Validation loss = 1.6706  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 1.3192  Validation loss = 1.7882  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 1.3220  Validation loss = 1.8588  \n",
      "\n",
      "Fold: 32  Epoch: 13  Training loss = 1.3364  Validation loss = 1.5417  \n",
      "\n",
      "Fold: 32  Epoch: 14  Training loss = 1.3204  Validation loss = 1.9722  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 13  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    # Load model with lowest validation error for each fold\n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 24\n",
      "Average validation error: 2.26952\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 1.3037  Test loss = 2.7559  \n",
      "\n",
      "Epoch: 2  Training loss = 1.2953  Test loss = 2.7259  \n",
      "\n",
      "Epoch: 3  Training loss = 1.2901  Test loss = 2.7028  \n",
      "\n",
      "Epoch: 4  Training loss = 1.2866  Test loss = 2.6848  \n",
      "\n",
      "Epoch: 5  Training loss = 1.2842  Test loss = 2.6706  \n",
      "\n",
      "Epoch: 6  Training loss = 1.2824  Test loss = 2.6593  \n",
      "\n",
      "Epoch: 7  Training loss = 1.2810  Test loss = 2.6503  \n",
      "\n",
      "Epoch: 8  Training loss = 1.2799  Test loss = 2.6430  \n",
      "\n",
      "Epoch: 9  Training loss = 1.2790  Test loss = 2.6372  \n",
      "\n",
      "Epoch: 10  Training loss = 1.2782  Test loss = 2.6324  \n",
      "\n",
      "Epoch: 11  Training loss = 1.2774  Test loss = 2.6285  \n",
      "\n",
      "Epoch: 12  Training loss = 1.2767  Test loss = 2.6253  \n",
      "\n",
      "Epoch: 13  Training loss = 1.2761  Test loss = 2.6227  \n",
      "\n",
      "Epoch: 14  Training loss = 1.2754  Test loss = 2.6206  \n",
      "\n",
      "Epoch: 15  Training loss = 1.2748  Test loss = 2.6188  \n",
      "\n",
      "Epoch: 16  Training loss = 1.2743  Test loss = 2.6174  \n",
      "\n",
      "Epoch: 17  Training loss = 1.2737  Test loss = 2.6161  \n",
      "\n",
      "Epoch: 18  Training loss = 1.2731  Test loss = 2.6151  \n",
      "\n",
      "Epoch: 19  Training loss = 1.2726  Test loss = 2.6142  \n",
      "\n",
      "Epoch: 20  Training loss = 1.2721  Test loss = 2.6135  \n",
      "\n",
      "Epoch: 21  Training loss = 1.2715  Test loss = 2.6129  \n",
      "\n",
      "Epoch: 22  Training loss = 1.2710  Test loss = 2.6123  \n",
      "\n",
      "Epoch: 23  Training loss = 1.2705  Test loss = 2.6119  \n",
      "\n",
      "Epoch: 24  Training loss = 1.2700  Test loss = 2.6114  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nx/gbiOpRkhS9UJQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc5ace80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc6bee80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 2.61143783573 \n",
      "Fixed scheme MAE:  1.6484348456\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 1.2700  Test loss = 2.5016  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 1.2993  Test loss = 1.8991  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 1.2759  Test loss = 0.0276  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 1.2757  Test loss = 0.0675  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 1.2065  Test loss = 0.8610  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 1.1614  Test loss = 0.3066  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 1.1186  Test loss = 0.1362  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 1.0943  Test loss = 0.1312  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 1.0447  Test loss = 0.1027  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 1.0361  Test loss = 0.9651  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 1.0129  Test loss = 1.5480  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 1.0249  Test loss = 2.3877  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 1.0022  Test loss = 0.4273  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 1.0027  Test loss = 1.0102  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 1.0098  Test loss = 2.5761  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 1.0551  Test loss = 2.9925  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 1.0696  Test loss = 0.7241  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 1.0662  Test loss = 0.4643  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 1.0660  Test loss = 1.0386  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 0.9761  Test loss = 0.8647  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 0.9407  Test loss = 1.5897  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 0.9608  Test loss = 4.0432  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 1.0779  Test loss = 0.9237  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 1.0763  Test loss = 1.1256  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 1.0254  Test loss = 1.1786  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 1.0321  Test loss = 0.1934  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 1.0309  Test loss = 0.5470  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 1.0319  Test loss = 1.4754  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 1.0276  Test loss = 0.3622  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 1.0263  Test loss = 0.1785  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 1.0230  Test loss = 3.6675  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 1.1080  Test loss = 0.2970  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 1.0643  Test loss = 1.6260  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 1.0824  Test loss = 0.1816  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 1.0350  Test loss = 0.0470  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 1.0317  Test loss = 5.2081  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 1.1651  Test loss = 1.4386  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 1.1675  Test loss = 2.3829  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 1.1951  Test loss = 0.9739  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 1.2011  Test loss = 2.0015  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 1.1602  Test loss = 1.4615  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 1.1743  Test loss = 2.1406  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 1.2039  Test loss = 2.9563  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 1.2559  Test loss = 12.5197  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 2.0026  Test loss = 7.1494  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.1876  Test loss = 1.1721  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.1923  Test loss = 0.3554  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.1916  Test loss = 0.0533  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 1.9150  Test loss = 2.3102  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 1.9320  Test loss = 1.8951  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 1.9462  Test loss = 1.4243  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 1.9528  Test loss = 2.0896  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 1.9004  Test loss = 3.8814  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 1.9588  Test loss = 3.2882  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 1.9996  Test loss = 0.2754  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 1.9951  Test loss = 0.2970  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 1.8950  Test loss = 0.6396  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 1.8965  Test loss = 1.9067  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 1.9095  Test loss = 0.5915  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 1.9092  Test loss = 0.5997  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.8399  Test loss = 0.2923  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.8402  Test loss = 2.5371  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 1.8618  Test loss = 0.5301  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 1.8566  Test loss = 0.3015  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 1.7976  Test loss = 0.8134  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 1.8001  Test loss = 0.2350  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.7975  Test loss = 1.0985  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 1.8013  Test loss = 2.3775  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 1.7785  Test loss = 4.7599  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 1.8733  Test loss = 0.5207  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 1.8744  Test loss = 1.2237  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 1.8723  Test loss = 2.5221  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 1.8382  Test loss = 2.3292  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 1.8607  Test loss = 0.6233  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 1.8605  Test loss = 0.9540  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 1.8635  Test loss = 0.7159  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 1.8033  Test loss = 1.5997  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfwPZkXpUdxb2/wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd3d4ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.40733540917 \n",
      "Updating scheme MAE:  1.57031788531\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_test_fixed.csv\", yhat_test_fixed)\n",
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_update.csv\", yhat_update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
