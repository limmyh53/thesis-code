{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"1Q/128_units/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 500\n",
    "learning_rate = 1e-3\n",
    "batch_size = 5\n",
    "early_stop_iters = 10\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 128 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag1',\n",
    "                                       'inflation.lag2',\n",
    "                                       'inflation.lag3',\n",
    "                                       'inflation.lag4']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag1',\n",
    "                                   'unemp.lag2',\n",
    "                                   'unemp.lag3',\n",
    "                                   'unemp.lag4']])\n",
    "train_4lag_oil = np.array(train[['oil.lag1',\n",
    "                                 'oil.lag2',\n",
    "                                 'oil.lag3',\n",
    "                                 'oil.lag4']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag1',\n",
    "                                     'inflation.lag2',\n",
    "                                     'inflation.lag3',\n",
    "                                     'inflation.lag4']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag1',\n",
    "                                 'unemp.lag2',\n",
    "                                 'unemp.lag3',\n",
    "                                 'unemp.lag4']])\n",
    "test_4lag_oil = np.array(test[['oil.lag1',\n",
    "                               'oil.lag2',\n",
    "                               'oil.lag3',\n",
    "                               'oil.lag4']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 128 \n",
      "Learning rate = 0.001 \n",
      "Epochs = 500 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 10 \n",
      "Learning rate = 0.001\n",
      "Fold: 1  Epoch: 1  Training loss = 2.9992  Validation loss = 2.9153  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 2.9200  Validation loss = 2.7374  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 2.7867  Validation loss = 2.4111  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 2.7300  Validation loss = 2.2373  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 2.6507  Validation loss = 1.9006  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 2.6337  Validation loss = 1.8159  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 2.6323  Validation loss = 1.8072  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 2.6275  Validation loss = 1.7838  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 2.6274  Validation loss = 1.7975  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 2.6126  Validation loss = 1.6944  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 2.6074  Validation loss = 1.6502  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 2.6076  Validation loss = 1.6586  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 2.6010  Validation loss = 1.5822  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 2.5973  Validation loss = 1.4426  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 2.5942  Validation loss = 1.4564  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 2.5930  Validation loss = 1.4687  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 2.5939  Validation loss = 1.5469  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 2.5921  Validation loss = 1.5572  \n",
      "\n",
      "Fold: 1  Epoch: 19  Training loss = 2.5884  Validation loss = 1.5482  \n",
      "\n",
      "Fold: 1  Epoch: 20  Training loss = 2.5848  Validation loss = 1.5037  \n",
      "\n",
      "Fold: 1  Epoch: 21  Training loss = 2.5793  Validation loss = 1.4529  \n",
      "\n",
      "Fold: 1  Epoch: 22  Training loss = 2.5761  Validation loss = 1.4571  \n",
      "\n",
      "Fold: 1  Epoch: 23  Training loss = 2.5783  Validation loss = 1.2404  \n",
      "\n",
      "Fold: 1  Epoch: 24  Training loss = 2.5675  Validation loss = 1.2902  \n",
      "\n",
      "Fold: 1  Epoch: 25  Training loss = 2.5623  Validation loss = 1.3369  \n",
      "\n",
      "Fold: 1  Epoch: 26  Training loss = 2.5602  Validation loss = 1.2827  \n",
      "\n",
      "Fold: 1  Epoch: 27  Training loss = 2.5553  Validation loss = 1.4005  \n",
      "\n",
      "Fold: 1  Epoch: 28  Training loss = 2.5510  Validation loss = 1.4251  \n",
      "\n",
      "Fold: 1  Epoch: 29  Training loss = 2.5465  Validation loss = 1.1926  \n",
      "\n",
      "Fold: 1  Epoch: 30  Training loss = 2.5402  Validation loss = 1.3301  \n",
      "\n",
      "Fold: 1  Epoch: 31  Training loss = 2.5438  Validation loss = 1.0630  \n",
      "\n",
      "Fold: 1  Epoch: 32  Training loss = 2.5268  Validation loss = 1.2618  \n",
      "\n",
      "Fold: 1  Epoch: 33  Training loss = 2.5286  Validation loss = 1.3878  \n",
      "\n",
      "Fold: 1  Epoch: 34  Training loss = 2.5292  Validation loss = 1.4572  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 31  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.3986  Validation loss = 1.7807  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.3939  Validation loss = 1.7517  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.3884  Validation loss = 1.7297  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.3788  Validation loss = 1.6700  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.3676  Validation loss = 1.6640  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.3673  Validation loss = 1.6117  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.3520  Validation loss = 1.6184  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.3297  Validation loss = 1.6820  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.3186  Validation loss = 1.6792  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.3206  Validation loss = 1.6991  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.3165  Validation loss = 1.6883  \n",
      "\n",
      "Fold: 2  Epoch: 12  Training loss = 2.2983  Validation loss = 1.6659  \n",
      "\n",
      "Fold: 2  Epoch: 13  Training loss = 2.3031  Validation loss = 1.6681  \n",
      "\n",
      "Fold: 2  Epoch: 14  Training loss = 2.2837  Validation loss = 1.6935  \n",
      "\n",
      "Fold: 2  Epoch: 15  Training loss = 2.2811  Validation loss = 1.6890  \n",
      "\n",
      "Fold: 2  Epoch: 16  Training loss = 2.2746  Validation loss = 1.6864  \n",
      "\n",
      "Fold: 2  Epoch: 17  Training loss = 2.2690  Validation loss = 1.6688  \n",
      "\n",
      "Fold: 2  Epoch: 18  Training loss = 2.2597  Validation loss = 1.6747  \n",
      "\n",
      "Fold: 2  Epoch: 19  Training loss = 2.2560  Validation loss = 1.6938  \n",
      "\n",
      "Fold: 2  Epoch: 20  Training loss = 2.2517  Validation loss = 1.7069  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 6  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.2966  Validation loss = 1.4067  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.2947  Validation loss = 1.3514  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.2839  Validation loss = 1.4264  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.2856  Validation loss = 1.4835  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.2811  Validation loss = 1.3816  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.2864  Validation loss = 1.3266  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.2794  Validation loss = 1.3737  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.2963  Validation loss = 1.5599  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.2894  Validation loss = 1.5689  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.2944  Validation loss = 1.2741  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.2790  Validation loss = 1.3006  \n",
      "\n",
      "Fold: 3  Epoch: 12  Training loss = 1.2726  Validation loss = 1.3674  \n",
      "\n",
      "Fold: 3  Epoch: 13  Training loss = 1.2819  Validation loss = 1.4616  \n",
      "\n",
      "Fold: 3  Epoch: 14  Training loss = 1.2779  Validation loss = 1.4782  \n",
      "\n",
      "Fold: 3  Epoch: 15  Training loss = 1.2717  Validation loss = 1.3471  \n",
      "\n",
      "Fold: 3  Epoch: 16  Training loss = 1.2663  Validation loss = 1.3749  \n",
      "\n",
      "Fold: 3  Epoch: 17  Training loss = 1.2784  Validation loss = 1.4915  \n",
      "\n",
      "Fold: 3  Epoch: 18  Training loss = 1.2655  Validation loss = 1.4343  \n",
      "\n",
      "Fold: 3  Epoch: 19  Training loss = 1.2654  Validation loss = 1.4089  \n",
      "\n",
      "Fold: 3  Epoch: 20  Training loss = 1.2714  Validation loss = 1.4276  \n",
      "\n",
      "Fold: 3  Epoch: 21  Training loss = 1.2776  Validation loss = 1.3636  \n",
      "\n",
      "Fold: 3  Epoch: 22  Training loss = 1.2716  Validation loss = 1.2175  \n",
      "\n",
      "Fold: 3  Epoch: 23  Training loss = 1.2652  Validation loss = 1.3785  \n",
      "\n",
      "Fold: 3  Epoch: 24  Training loss = 1.2590  Validation loss = 1.3381  \n",
      "\n",
      "Fold: 3  Epoch: 25  Training loss = 1.2578  Validation loss = 1.3441  \n",
      "\n",
      "Fold: 3  Epoch: 26  Training loss = 1.2578  Validation loss = 1.3874  \n",
      "\n",
      "Fold: 3  Epoch: 27  Training loss = 1.2548  Validation loss = 1.3084  \n",
      "\n",
      "Fold: 3  Epoch: 28  Training loss = 1.2542  Validation loss = 1.3017  \n",
      "\n",
      "Fold: 3  Epoch: 29  Training loss = 1.2541  Validation loss = 1.3668  \n",
      "\n",
      "Fold: 3  Epoch: 30  Training loss = 1.2524  Validation loss = 1.2648  \n",
      "\n",
      "Fold: 3  Epoch: 31  Training loss = 1.2499  Validation loss = 1.2482  \n",
      "\n",
      "Fold: 3  Epoch: 32  Training loss = 1.2489  Validation loss = 1.2736  \n",
      "\n",
      "Fold: 3  Epoch: 33  Training loss = 1.2548  Validation loss = 1.3366  \n",
      "\n",
      "Fold: 3  Epoch: 34  Training loss = 1.2484  Validation loss = 1.3111  \n",
      "\n",
      "Fold: 3  Epoch: 35  Training loss = 1.2517  Validation loss = 1.3259  \n",
      "\n",
      "Fold: 3  Epoch: 36  Training loss = 1.2469  Validation loss = 1.2741  \n",
      "\n",
      "Fold: 3  Epoch: 37  Training loss = 1.2472  Validation loss = 1.2560  \n",
      "\n",
      "Fold: 3  Epoch: 38  Training loss = 1.2480  Validation loss = 1.2249  \n",
      "\n",
      "Fold: 3  Epoch: 39  Training loss = 1.2432  Validation loss = 1.2630  \n",
      "\n",
      "Fold: 3  Epoch: 40  Training loss = 1.2420  Validation loss = 1.2162  \n",
      "\n",
      "Fold: 3  Epoch: 41  Training loss = 1.2415  Validation loss = 1.2048  \n",
      "\n",
      "Fold: 3  Epoch: 42  Training loss = 1.2412  Validation loss = 1.2731  \n",
      "\n",
      "Fold: 3  Epoch: 43  Training loss = 1.2383  Validation loss = 1.2406  \n",
      "\n",
      "Fold: 3  Epoch: 44  Training loss = 1.2452  Validation loss = 1.2898  \n",
      "\n",
      "Fold: 3  Epoch: 45  Training loss = 1.2815  Validation loss = 1.4765  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 41  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.2032  Validation loss = 2.3544  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.2223  Validation loss = 2.4492  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.2030  Validation loss = 2.2384  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.2001  Validation loss = 2.2293  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.2003  Validation loss = 2.2116  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.1992  Validation loss = 2.2712  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.2005  Validation loss = 2.3478  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.1939  Validation loss = 2.2638  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.1981  Validation loss = 2.1952  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 1.1884  Validation loss = 2.2830  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.1907  Validation loss = 2.2929  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 1.1907  Validation loss = 2.2987  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.1980  Validation loss = 2.3161  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 1.1872  Validation loss = 2.3190  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.1951  Validation loss = 2.3445  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 1.1881  Validation loss = 2.3373  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 1.1855  Validation loss = 2.3081  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 1.1784  Validation loss = 2.2379  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 1.1925  Validation loss = 2.3649  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 9  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.1977  Validation loss = 1.6141  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.1788  Validation loss = 1.5355  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.1673  Validation loss = 1.4675  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.1699  Validation loss = 1.4890  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.1598  Validation loss = 1.4249  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 1.1580  Validation loss = 1.4147  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 1.1482  Validation loss = 1.3222  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.1583  Validation loss = 1.2112  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 1.1378  Validation loss = 1.2107  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.1347  Validation loss = 1.2232  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 1.1316  Validation loss = 1.2372  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 1.1285  Validation loss = 1.1921  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 1.1568  Validation loss = 1.3128  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.1630  Validation loss = 1.3341  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 1.1231  Validation loss = 1.1380  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 1.1186  Validation loss = 1.1476  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 1.1173  Validation loss = 1.1170  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 1.1103  Validation loss = 1.1540  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 1.1063  Validation loss = 1.1090  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 1.1116  Validation loss = 1.1493  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 1.1092  Validation loss = 1.0756  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 1.1162  Validation loss = 1.0448  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 1.1053  Validation loss = 1.1120  \n",
      "\n",
      "Fold: 5  Epoch: 24  Training loss = 1.0992  Validation loss = 1.0680  \n",
      "\n",
      "Fold: 5  Epoch: 25  Training loss = 1.0979  Validation loss = 1.0936  \n",
      "\n",
      "Fold: 5  Epoch: 26  Training loss = 1.1200  Validation loss = 1.0358  \n",
      "\n",
      "Fold: 5  Epoch: 27  Training loss = 1.0960  Validation loss = 1.0466  \n",
      "\n",
      "Fold: 5  Epoch: 28  Training loss = 1.0996  Validation loss = 1.0350  \n",
      "\n",
      "Fold: 5  Epoch: 29  Training loss = 1.1077  Validation loss = 1.0310  \n",
      "\n",
      "Fold: 5  Epoch: 30  Training loss = 1.1162  Validation loss = 1.0247  \n",
      "\n",
      "Fold: 5  Epoch: 31  Training loss = 1.0890  Validation loss = 1.0668  \n",
      "\n",
      "Fold: 5  Epoch: 32  Training loss = 1.0876  Validation loss = 1.0621  \n",
      "\n",
      "Fold: 5  Epoch: 33  Training loss = 1.0912  Validation loss = 1.0474  \n",
      "\n",
      "Fold: 5  Epoch: 34  Training loss = 1.0840  Validation loss = 1.0501  \n",
      "\n",
      "Fold: 5  Epoch: 35  Training loss = 1.1028  Validation loss = 1.0391  \n",
      "\n",
      "Fold: 5  Epoch: 36  Training loss = 1.0915  Validation loss = 1.0617  \n",
      "\n",
      "Fold: 5  Epoch: 37  Training loss = 1.0911  Validation loss = 1.0358  \n",
      "\n",
      "Fold: 5  Epoch: 38  Training loss = 1.0828  Validation loss = 1.0455  \n",
      "\n",
      "Fold: 5  Epoch: 39  Training loss = 1.0829  Validation loss = 1.0578  \n",
      "\n",
      "Fold: 5  Epoch: 40  Training loss = 1.0769  Validation loss = 1.0701  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 30  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 1.1000  Validation loss = 0.9179  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 1.1091  Validation loss = 1.0419  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 1.0961  Validation loss = 0.8939  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 1.0938  Validation loss = 0.8759  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 1.0952  Validation loss = 0.9297  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 1.0928  Validation loss = 0.8576  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 1.0987  Validation loss = 0.9994  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 1.1001  Validation loss = 1.0240  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 1.0926  Validation loss = 0.9148  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 1.0884  Validation loss = 0.9274  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 1.0913  Validation loss = 0.7830  \n",
      "\n",
      "Fold: 6  Epoch: 12  Training loss = 1.0874  Validation loss = 0.9263  \n",
      "\n",
      "Fold: 6  Epoch: 13  Training loss = 1.1351  Validation loss = 1.1878  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 11  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 1.0687  Validation loss = 1.0062  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 1.0589  Validation loss = 1.0641  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 1.0792  Validation loss = 1.2531  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.0569  Validation loss = 1.0211  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 1.0538  Validation loss = 1.0890  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 1.1012  Validation loss = 1.3457  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 1.0588  Validation loss = 1.0529  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 1.0488  Validation loss = 1.0954  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 1.0650  Validation loss = 1.0616  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 1.0859  Validation loss = 0.9405  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 1.0465  Validation loss = 1.1588  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 1.0662  Validation loss = 1.3552  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 10  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 0.9989  Validation loss = 4.4091  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 0.9933  Validation loss = 4.3388  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 0.9943  Validation loss = 4.3298  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 0.9958  Validation loss = 4.4408  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 0.9989  Validation loss = 4.4639  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 1.0156  Validation loss = 4.6477  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 0.9941  Validation loss = 4.2118  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 0.9874  Validation loss = 4.2685  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 0.9869  Validation loss = 4.3093  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 0.9963  Validation loss = 4.1452  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 0.9886  Validation loss = 4.2485  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 0.9901  Validation loss = 4.4727  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 0.9924  Validation loss = 4.1670  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 0.9868  Validation loss = 4.2217  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 0.9860  Validation loss = 4.2241  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 0.9845  Validation loss = 4.3434  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 0.9928  Validation loss = 4.4624  \n",
      "\n",
      "Fold: 8  Epoch: 18  Training loss = 0.9953  Validation loss = 4.0581  \n",
      "\n",
      "Fold: 8  Epoch: 19  Training loss = 0.9917  Validation loss = 4.0699  \n",
      "\n",
      "Fold: 8  Epoch: 20  Training loss = 0.9840  Validation loss = 4.1707  \n",
      "\n",
      "Fold: 8  Epoch: 21  Training loss = 0.9820  Validation loss = 4.1848  \n",
      "\n",
      "Fold: 8  Epoch: 22  Training loss = 0.9831  Validation loss = 4.3376  \n",
      "\n",
      "Fold: 8  Epoch: 23  Training loss = 0.9840  Validation loss = 4.3718  \n",
      "\n",
      "Fold: 8  Epoch: 24  Training loss = 0.9829  Validation loss = 4.1627  \n",
      "\n",
      "Fold: 8  Epoch: 25  Training loss = 0.9805  Validation loss = 4.1796  \n",
      "\n",
      "Fold: 8  Epoch: 26  Training loss = 1.0053  Validation loss = 3.9387  \n",
      "\n",
      "Fold: 8  Epoch: 27  Training loss = 0.9789  Validation loss = 4.3357  \n",
      "\n",
      "Fold: 8  Epoch: 28  Training loss = 0.9894  Validation loss = 4.4296  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 26  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 1.4311  Validation loss = 6.7096  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 1.3621  Validation loss = 6.9728  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 1.3556  Validation loss = 7.2957  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 1.3433  Validation loss = 6.7500  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 1.3505  Validation loss = 6.7817  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 1.3541  Validation loss = 6.6037  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 1.3400  Validation loss = 6.5994  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 1.3295  Validation loss = 6.6622  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 1.3618  Validation loss = 7.8386  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 1.3557  Validation loss = 8.1530  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 1.3924  Validation loss = 8.6596  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 7  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 1.9941  Validation loss = 2.1894  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 1.8443  Validation loss = 1.6240  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 1.7942  Validation loss = 1.6692  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 1.6918  Validation loss = 1.5859  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 1.6546  Validation loss = 1.5997  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 1.6258  Validation loss = 1.6992  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 1.5879  Validation loss = 1.7526  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 1.5959  Validation loss = 1.9041  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 1.5603  Validation loss = 1.7288  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 1.5340  Validation loss = 1.8082  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 1.5471  Validation loss = 1.9353  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 4  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 1.6143  Validation loss = 1.3341  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 1.5879  Validation loss = 1.3801  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 1.5575  Validation loss = 1.3425  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 1.5425  Validation loss = 1.3357  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 1.5338  Validation loss = 1.3116  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 1.5153  Validation loss = 1.3289  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 1.5255  Validation loss = 1.2515  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 1.4847  Validation loss = 1.6340  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 1.4990  Validation loss = 1.4465  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 1.5225  Validation loss = 1.5602  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 1.4423  Validation loss = 1.9271  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 7  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 1.5006  Validation loss = 1.2776  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 1.4742  Validation loss = 1.0957  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 1.4804  Validation loss = 1.1153  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 1.4948  Validation loss = 1.6906  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 1.4753  Validation loss = 1.6766  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 1.4269  Validation loss = 0.9422  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 1.4416  Validation loss = 1.2824  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 1.4156  Validation loss = 0.8972  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 1.3923  Validation loss = 1.0680  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 1.4286  Validation loss = 0.9558  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 1.3922  Validation loss = 1.1281  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 1.3935  Validation loss = 1.0785  \n",
      "\n",
      "Fold: 12  Epoch: 13  Training loss = 1.3661  Validation loss = 1.1715  \n",
      "\n",
      "Fold: 12  Epoch: 14  Training loss = 1.3656  Validation loss = 1.0592  \n",
      "\n",
      "Fold: 12  Epoch: 15  Training loss = 1.3507  Validation loss = 1.3215  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 8  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 1.3741  Validation loss = 3.1552  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 1.3887  Validation loss = 2.7071  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 1.4028  Validation loss = 2.7441  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 1.4120  Validation loss = 3.8556  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 1.3881  Validation loss = 3.7222  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 1.3513  Validation loss = 3.0401  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 1.3364  Validation loss = 3.4428  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 1.4041  Validation loss = 2.7362  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 1.3216  Validation loss = 2.9541  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 1.3250  Validation loss = 2.8381  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 1.3115  Validation loss = 2.9588  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 1.3229  Validation loss = 3.0332  \n",
      "\n",
      "Fold: 13  Epoch: 13  Training loss = 1.3563  Validation loss = 3.1821  \n",
      "\n",
      "Fold: 13  Epoch: 14  Training loss = 1.3155  Validation loss = 3.1953  \n",
      "\n",
      "Fold: 13  Epoch: 15  Training loss = 1.3269  Validation loss = 2.9890  \n",
      "\n",
      "Fold: 13  Epoch: 16  Training loss = 1.3171  Validation loss = 3.2374  \n",
      "\n",
      "Fold: 13  Epoch: 17  Training loss = 1.3139  Validation loss = 3.1729  \n",
      "\n",
      "Fold: 13  Epoch: 18  Training loss = 1.3363  Validation loss = 3.5084  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 2  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 1.4739  Validation loss = 4.5562  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 1.4759  Validation loss = 4.4626  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 1.4873  Validation loss = 4.2729  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 1.5135  Validation loss = 4.2422  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 1.5263  Validation loss = 3.7867  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 1.4765  Validation loss = 4.0809  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 1.4947  Validation loss = 3.7093  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 1.4609  Validation loss = 4.2150  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 1.4550  Validation loss = 3.9065  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 1.4820  Validation loss = 3.5371  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 1.4081  Validation loss = 3.8148  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 1.4749  Validation loss = 4.3524  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 10  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 1.7091  Validation loss = 3.3726  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 1.7080  Validation loss = 3.3420  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 1.7940  Validation loss = 3.4266  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 1.8347  Validation loss = 3.6275  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 1.7156  Validation loss = 3.2594  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 1.6632  Validation loss = 3.3027  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 1.6542  Validation loss = 3.4590  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 1.6617  Validation loss = 3.2418  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 1.6111  Validation loss = 3.1429  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 1.6311  Validation loss = 3.1690  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 1.6062  Validation loss = 3.1158  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 1.7076  Validation loss = 3.0857  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 1.5382  Validation loss = 3.1707  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 1.5564  Validation loss = 3.3555  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 1.5625  Validation loss = 3.2122  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 1.5049  Validation loss = 3.0994  \n",
      "\n",
      "Fold: 15  Epoch: 17  Training loss = 1.5126  Validation loss = 3.1961  \n",
      "\n",
      "Fold: 15  Epoch: 18  Training loss = 1.5442  Validation loss = 3.2302  \n",
      "\n",
      "Fold: 15  Epoch: 19  Training loss = 1.5095  Validation loss = 3.4506  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 12  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 1.6667  Validation loss = 4.3527  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 1.7012  Validation loss = 4.4192  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 1.7415  Validation loss = 4.8418  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 1.6764  Validation loss = 4.4182  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 1.7057  Validation loss = 4.2925  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 1.7622  Validation loss = 4.7833  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 1.6364  Validation loss = 4.4231  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 1.6132  Validation loss = 4.4955  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 1.7104  Validation loss = 4.9368  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 1.5963  Validation loss = 4.4540  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 1.5755  Validation loss = 4.2295  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 1.6225  Validation loss = 4.2118  \n",
      "\n",
      "Fold: 16  Epoch: 13  Training loss = 1.8289  Validation loss = 4.8524  \n",
      "\n",
      "Fold: 16  Epoch: 14  Training loss = 1.6108  Validation loss = 4.5909  \n",
      "\n",
      "Fold: 16  Epoch: 15  Training loss = 1.6237  Validation loss = 4.4651  \n",
      "\n",
      "Fold: 16  Epoch: 16  Training loss = 1.6615  Validation loss = 4.2589  \n",
      "\n",
      "Fold: 16  Epoch: 17  Training loss = 1.6052  Validation loss = 4.1360  \n",
      "\n",
      "Fold: 16  Epoch: 18  Training loss = 1.5869  Validation loss = 4.3504  \n",
      "\n",
      "Fold: 16  Epoch: 19  Training loss = 1.6035  Validation loss = 4.2982  \n",
      "\n",
      "Fold: 16  Epoch: 20  Training loss = 1.5572  Validation loss = 4.1538  \n",
      "\n",
      "Fold: 16  Epoch: 21  Training loss = 1.5631  Validation loss = 4.0364  \n",
      "\n",
      "Fold: 16  Epoch: 22  Training loss = 1.5665  Validation loss = 4.0672  \n",
      "\n",
      "Fold: 16  Epoch: 23  Training loss = 1.6361  Validation loss = 4.8053  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 21  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 1.6809  Validation loss = 2.8924  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 1.7609  Validation loss = 3.2009  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 1.7230  Validation loss = 3.1402  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 1.6379  Validation loss = 3.0126  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 1.6261  Validation loss = 2.9066  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 1.7004  Validation loss = 3.0462  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 1.6471  Validation loss = 3.0218  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 1.6340  Validation loss = 3.4061  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 1.5766  Validation loss = 3.3061  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 1.5613  Validation loss = 3.0453  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 1.5462  Validation loss = 2.9557  \n",
      "\n",
      "Fold: 17  Epoch: 12  Training loss = 1.5558  Validation loss = 2.8311  \n",
      "\n",
      "Fold: 17  Epoch: 13  Training loss = 1.5812  Validation loss = 2.8386  \n",
      "\n",
      "Fold: 17  Epoch: 14  Training loss = 1.5777  Validation loss = 2.9744  \n",
      "\n",
      "Fold: 17  Epoch: 15  Training loss = 1.5476  Validation loss = 3.0093  \n",
      "\n",
      "Fold: 17  Epoch: 16  Training loss = 1.6644  Validation loss = 2.7545  \n",
      "\n",
      "Fold: 17  Epoch: 17  Training loss = 1.6444  Validation loss = 2.7797  \n",
      "\n",
      "Fold: 17  Epoch: 18  Training loss = 1.5632  Validation loss = 2.7539  \n",
      "\n",
      "Fold: 17  Epoch: 19  Training loss = 1.5589  Validation loss = 2.8771  \n",
      "\n",
      "Fold: 17  Epoch: 20  Training loss = 1.6349  Validation loss = 2.6394  \n",
      "\n",
      "Fold: 17  Epoch: 21  Training loss = 1.8170  Validation loss = 2.5334  \n",
      "\n",
      "Fold: 17  Epoch: 22  Training loss = 1.4967  Validation loss = 2.7372  \n",
      "\n",
      "Fold: 17  Epoch: 23  Training loss = 1.5123  Validation loss = 2.6908  \n",
      "\n",
      "Fold: 17  Epoch: 24  Training loss = 1.4627  Validation loss = 2.8237  \n",
      "\n",
      "Fold: 17  Epoch: 25  Training loss = 1.6520  Validation loss = 2.6407  \n",
      "\n",
      "Fold: 17  Epoch: 26  Training loss = 1.6172  Validation loss = 2.7417  \n",
      "\n",
      "Fold: 17  Epoch: 27  Training loss = 1.4626  Validation loss = 2.8900  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 21  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 1.6054  Validation loss = 1.5296  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 1.6427  Validation loss = 1.3320  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 1.6088  Validation loss = 1.4811  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 1.6508  Validation loss = 1.7538  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 1.5228  Validation loss = 1.5018  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 1.5280  Validation loss = 1.4341  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 1.5396  Validation loss = 1.3111  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 1.5377  Validation loss = 1.2882  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 1.7759  Validation loss = 1.1990  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 1.5926  Validation loss = 1.3200  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 1.5111  Validation loss = 1.0731  \n",
      "\n",
      "Fold: 18  Epoch: 12  Training loss = 1.5288  Validation loss = 1.0681  \n",
      "\n",
      "Fold: 18  Epoch: 13  Training loss = 1.5095  Validation loss = 1.3313  \n",
      "\n",
      "Fold: 18  Epoch: 14  Training loss = 1.5305  Validation loss = 1.4572  \n",
      "\n",
      "Fold: 18  Epoch: 15  Training loss = 1.5359  Validation loss = 1.0164  \n",
      "\n",
      "Fold: 18  Epoch: 16  Training loss = 1.6396  Validation loss = 1.4327  \n",
      "\n",
      "Fold: 18  Epoch: 17  Training loss = 1.5071  Validation loss = 1.3356  \n",
      "\n",
      "Fold: 18  Epoch: 18  Training loss = 1.4253  Validation loss = 1.2749  \n",
      "\n",
      "Fold: 18  Epoch: 19  Training loss = 1.4663  Validation loss = 1.3899  \n",
      "\n",
      "Fold: 18  Epoch: 20  Training loss = 1.4707  Validation loss = 1.3033  \n",
      "\n",
      "Fold: 18  Epoch: 21  Training loss = 1.4573  Validation loss = 1.0480  \n",
      "\n",
      "Fold: 18  Epoch: 22  Training loss = 1.4532  Validation loss = 1.1336  \n",
      "\n",
      "Fold: 18  Epoch: 23  Training loss = 1.5221  Validation loss = 0.9930  \n",
      "\n",
      "Fold: 18  Epoch: 24  Training loss = 1.4640  Validation loss = 1.2759  \n",
      "\n",
      "Fold: 18  Epoch: 25  Training loss = 1.4146  Validation loss = 1.4437  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 23  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 1.3895  Validation loss = 2.0520  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 1.3736  Validation loss = 2.0161  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 1.4794  Validation loss = 2.1369  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 1.3675  Validation loss = 2.3207  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 1.3442  Validation loss = 2.2622  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 1.3805  Validation loss = 2.1103  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 1.3407  Validation loss = 2.4012  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 1.3915  Validation loss = 2.4047  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 1.3045  Validation loss = 2.1513  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 1.3297  Validation loss = 2.3987  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 1.3167  Validation loss = 2.5497  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 2  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 1.4360  Validation loss = 1.5578  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 1.4310  Validation loss = 0.7759  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 1.4212  Validation loss = 0.9289  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 1.3799  Validation loss = 1.2719  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 1.4549  Validation loss = 0.9074  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 1.4268  Validation loss = 1.5944  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 1.3458  Validation loss = 0.9805  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 1.3644  Validation loss = 1.2311  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 1.3513  Validation loss = 1.3226  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 1.3298  Validation loss = 1.2417  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 1.3290  Validation loss = 1.3791  \n",
      "\n",
      "Fold: 20  Epoch: 12  Training loss = 1.3148  Validation loss = 1.4379  \n",
      "\n",
      "Fold: 20  Epoch: 13  Training loss = 1.4219  Validation loss = 1.4714  \n",
      "\n",
      "Fold: 20  Epoch: 14  Training loss = 1.2638  Validation loss = 0.9878  \n",
      "\n",
      "Fold: 20  Epoch: 15  Training loss = 1.4006  Validation loss = 0.8253  \n",
      "\n",
      "Fold: 20  Epoch: 16  Training loss = 1.2779  Validation loss = 0.9570  \n",
      "\n",
      "Fold: 20  Epoch: 17  Training loss = 1.3367  Validation loss = 1.2700  \n",
      "\n",
      "Fold: 20  Epoch: 18  Training loss = 1.2948  Validation loss = 1.1774  \n",
      "\n",
      "Fold: 20  Epoch: 19  Training loss = 1.2674  Validation loss = 0.8305  \n",
      "\n",
      "Fold: 20  Epoch: 20  Training loss = 1.3031  Validation loss = 1.1521  \n",
      "\n",
      "Fold: 20  Epoch: 21  Training loss = 1.3918  Validation loss = 0.9259  \n",
      "\n",
      "Fold: 20  Epoch: 22  Training loss = 1.3968  Validation loss = 1.1250  \n",
      "\n",
      "Fold: 20  Epoch: 23  Training loss = 1.3256  Validation loss = 1.0382  \n",
      "\n",
      "Fold: 20  Epoch: 24  Training loss = 1.3162  Validation loss = 1.1436  \n",
      "\n",
      "Fold: 20  Epoch: 25  Training loss = 1.2618  Validation loss = 0.8093  \n",
      "\n",
      "Fold: 20  Epoch: 26  Training loss = 1.2258  Validation loss = 0.7565  \n",
      "\n",
      "Fold: 20  Epoch: 27  Training loss = 1.2145  Validation loss = 0.8287  \n",
      "\n",
      "Fold: 20  Epoch: 28  Training loss = 1.2079  Validation loss = 0.8436  \n",
      "\n",
      "Fold: 20  Epoch: 29  Training loss = 1.4548  Validation loss = 0.8602  \n",
      "\n",
      "Fold: 20  Epoch: 30  Training loss = 1.1982  Validation loss = 1.0168  \n",
      "\n",
      "Fold: 20  Epoch: 31  Training loss = 1.1860  Validation loss = 0.8338  \n",
      "\n",
      "Fold: 20  Epoch: 32  Training loss = 1.1788  Validation loss = 0.7663  \n",
      "\n",
      "Fold: 20  Epoch: 33  Training loss = 1.1834  Validation loss = 1.0011  \n",
      "\n",
      "Fold: 20  Epoch: 34  Training loss = 1.3305  Validation loss = 0.8925  \n",
      "\n",
      "Fold: 20  Epoch: 35  Training loss = 1.1749  Validation loss = 0.9947  \n",
      "\n",
      "Fold: 20  Epoch: 36  Training loss = 1.2294  Validation loss = 1.2524  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 26  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 1.2006  Validation loss = 3.1147  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 1.1351  Validation loss = 3.2273  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 1.2751  Validation loss = 3.0130  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 1.2886  Validation loss = 3.4054  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 1.1300  Validation loss = 3.0439  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 1.1269  Validation loss = 3.1959  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 1.1284  Validation loss = 3.5114  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 1.1610  Validation loss = 3.3085  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 1.1050  Validation loss = 3.3480  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 1.1566  Validation loss = 3.4532  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 1.1476  Validation loss = 3.2655  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 1.2044  Validation loss = 3.5897  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 3  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 1.2955  Validation loss = 4.8304  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 1.2914  Validation loss = 4.4369  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 1.2577  Validation loss = 5.1503  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 1.2372  Validation loss = 4.6036  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 1.2505  Validation loss = 5.0855  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 1.2204  Validation loss = 4.9915  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 1.2404  Validation loss = 4.3140  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 1.1961  Validation loss = 4.4760  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 1.1830  Validation loss = 5.1402  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 1.1537  Validation loss = 5.0429  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 1.2002  Validation loss = 5.1004  \n",
      "\n",
      "Fold: 22  Epoch: 12  Training loss = 1.1426  Validation loss = 5.0104  \n",
      "\n",
      "Fold: 22  Epoch: 13  Training loss = 1.1727  Validation loss = 3.9418  \n",
      "\n",
      "Fold: 22  Epoch: 14  Training loss = 1.1395  Validation loss = 4.4364  \n",
      "\n",
      "Fold: 22  Epoch: 15  Training loss = 1.1484  Validation loss = 3.7420  \n",
      "\n",
      "Fold: 22  Epoch: 16  Training loss = 1.1560  Validation loss = 3.3046  \n",
      "\n",
      "Fold: 22  Epoch: 17  Training loss = 1.3326  Validation loss = 3.3139  \n",
      "\n",
      "Fold: 22  Epoch: 18  Training loss = 1.1121  Validation loss = 4.4883  \n",
      "\n",
      "Fold: 22  Epoch: 19  Training loss = 1.0728  Validation loss = 3.9033  \n",
      "\n",
      "Fold: 22  Epoch: 20  Training loss = 1.0572  Validation loss = 3.9267  \n",
      "\n",
      "Fold: 22  Epoch: 21  Training loss = 1.0928  Validation loss = 4.4099  \n",
      "\n",
      "Fold: 22  Epoch: 22  Training loss = 1.1341  Validation loss = 3.5523  \n",
      "\n",
      "Fold: 22  Epoch: 23  Training loss = 1.0871  Validation loss = 4.6818  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 16  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 1.5544  Validation loss = 2.5646  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 1.5082  Validation loss = 2.9388  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 1.5500  Validation loss = 2.2363  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 1.5360  Validation loss = 2.4687  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 1.7277  Validation loss = 2.1896  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 1.4604  Validation loss = 2.2077  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 1.6198  Validation loss = 2.7551  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 1.4227  Validation loss = 2.4808  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 1.7209  Validation loss = 1.5226  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 1.3916  Validation loss = 2.5822  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 1.6084  Validation loss = 3.1328  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 9  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 1.7498  Validation loss = 2.2208  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 1.5618  Validation loss = 1.7202  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 1.6996  Validation loss = 1.2350  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 1.7569  Validation loss = 1.0023  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 1.5613  Validation loss = 2.0761  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 1.7345  Validation loss = 1.9520  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 1.3198  Validation loss = 1.5964  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 1.4109  Validation loss = 1.4007  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 1.3425  Validation loss = 1.3630  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 1.2846  Validation loss = 1.4876  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 1.3352  Validation loss = 2.0325  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 1.2490  Validation loss = 1.4084  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 1.4371  Validation loss = 1.8036  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 1.2449  Validation loss = 1.8466  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 1.3078  Validation loss = 2.0434  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 4  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 1.2180  Validation loss = 2.9425  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 1.1109  Validation loss = 2.7448  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 1.1625  Validation loss = 2.8293  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 1.0705  Validation loss = 2.6757  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 1.0764  Validation loss = 3.0309  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 1.0717  Validation loss = 2.6270  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 1.2863  Validation loss = 2.8363  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 1.3044  Validation loss = 2.7935  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 1.2553  Validation loss = 2.8357  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 1.3802  Validation loss = 2.5602  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 1.2702  Validation loss = 3.0205  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 1.3167  Validation loss = 2.9199  \n",
      "\n",
      "Fold: 25  Epoch: 13  Training loss = 1.5665  Validation loss = 2.6721  \n",
      "\n",
      "Fold: 25  Epoch: 14  Training loss = 1.0664  Validation loss = 2.7073  \n",
      "\n",
      "Fold: 25  Epoch: 15  Training loss = 1.1838  Validation loss = 2.9575  \n",
      "\n",
      "Fold: 25  Epoch: 16  Training loss = 1.1780  Validation loss = 2.4352  \n",
      "\n",
      "Fold: 25  Epoch: 17  Training loss = 1.3161  Validation loss = 3.3053  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 16  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 1.2707  Validation loss = 2.7742  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 1.2465  Validation loss = 2.0065  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 1.3003  Validation loss = 2.6746  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 1.5831  Validation loss = 3.5919  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 1.3552  Validation loss = 1.6206  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 1.1537  Validation loss = 1.7868  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 1.2392  Validation loss = 2.7071  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 1.0805  Validation loss = 2.3621  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 1.3388  Validation loss = 2.5113  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 1.4185  Validation loss = 3.1657  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 1.1525  Validation loss = 2.5651  \n",
      "\n",
      "Fold: 26  Epoch: 12  Training loss = 1.0482  Validation loss = 2.5658  \n",
      "\n",
      "Fold: 26  Epoch: 13  Training loss = 1.1128  Validation loss = 1.7611  \n",
      "\n",
      "Fold: 26  Epoch: 14  Training loss = 1.0819  Validation loss = 1.8349  \n",
      "\n",
      "Fold: 26  Epoch: 15  Training loss = 1.2001  Validation loss = 1.9908  \n",
      "\n",
      "Fold: 26  Epoch: 16  Training loss = 1.0889  Validation loss = 2.0860  \n",
      "\n",
      "Fold: 26  Epoch: 17  Training loss = 1.0972  Validation loss = 1.9435  \n",
      "\n",
      "Fold: 26  Epoch: 18  Training loss = 1.0234  Validation loss = 2.0666  \n",
      "\n",
      "Fold: 26  Epoch: 19  Training loss = 1.0524  Validation loss = 2.0578  \n",
      "\n",
      "Fold: 26  Epoch: 20  Training loss = 1.0184  Validation loss = 2.4962  \n",
      "\n",
      "Fold: 26  Epoch: 21  Training loss = 1.1235  Validation loss = 2.9251  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 5  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 1.0526  Validation loss = 1.1040  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 0.9975  Validation loss = 1.1238  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 1.0645  Validation loss = 0.9696  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 0.9960  Validation loss = 1.2233  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 1.0727  Validation loss = 0.9789  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 1.1045  Validation loss = 1.5524  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 1.0056  Validation loss = 1.3552  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 0.9741  Validation loss = 1.2482  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 1.1302  Validation loss = 1.1135  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 1.1962  Validation loss = 1.0467  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 1.0460  Validation loss = 1.3666  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 1.6207  Validation loss = 0.9686  \n",
      "\n",
      "Fold: 27  Epoch: 13  Training loss = 1.1977  Validation loss = 1.5462  \n",
      "\n",
      "Fold: 27  Epoch: 14  Training loss = 0.9838  Validation loss = 1.4384  \n",
      "\n",
      "Fold: 27  Epoch: 15  Training loss = 1.1939  Validation loss = 1.0649  \n",
      "\n",
      "Fold: 27  Epoch: 16  Training loss = 1.1204  Validation loss = 1.3129  \n",
      "\n",
      "Fold: 27  Epoch: 17  Training loss = 1.0667  Validation loss = 1.1782  \n",
      "\n",
      "Fold: 27  Epoch: 18  Training loss = 1.0116  Validation loss = 1.2917  \n",
      "\n",
      "Fold: 27  Epoch: 19  Training loss = 1.0032  Validation loss = 1.3110  \n",
      "\n",
      "Fold: 27  Epoch: 20  Training loss = 0.9832  Validation loss = 1.4285  \n",
      "\n",
      "Fold: 27  Epoch: 21  Training loss = 1.0064  Validation loss = 1.3708  \n",
      "\n",
      "Fold: 27  Epoch: 22  Training loss = 0.9992  Validation loss = 1.3076  \n",
      "\n",
      "Fold: 27  Epoch: 23  Training loss = 0.9437  Validation loss = 1.2970  \n",
      "\n",
      "Fold: 27  Epoch: 24  Training loss = 1.0043  Validation loss = 1.6654  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 12  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 1.0922  Validation loss = 0.9627  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 0.9720  Validation loss = 0.6948  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 0.9143  Validation loss = 0.6315  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 0.9662  Validation loss = 0.4440  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 0.9413  Validation loss = 0.5411  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 0.8556  Validation loss = 0.6206  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 0.8957  Validation loss = 0.4237  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 0.9510  Validation loss = 0.5055  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 0.9611  Validation loss = 0.7743  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 0.8831  Validation loss = 0.7205  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 1.0288  Validation loss = 0.3378  \n",
      "\n",
      "Fold: 28  Epoch: 12  Training loss = 0.9537  Validation loss = 0.7376  \n",
      "\n",
      "Fold: 28  Epoch: 13  Training loss = 0.9164  Validation loss = 0.6094  \n",
      "\n",
      "Fold: 28  Epoch: 14  Training loss = 0.9428  Validation loss = 0.7780  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 11  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 0.8838  Validation loss = 1.1654  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 0.9027  Validation loss = 1.2052  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 0.7865  Validation loss = 1.2231  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 0.7984  Validation loss = 1.1348  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 0.7791  Validation loss = 1.3689  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 1.0451  Validation loss = 1.1982  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 0.8892  Validation loss = 1.3892  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 0.8874  Validation loss = 1.2724  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 0.8000  Validation loss = 1.4661  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 0.7806  Validation loss = 1.2452  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 0.8922  Validation loss = 1.1056  \n",
      "\n",
      "Fold: 29  Epoch: 12  Training loss = 0.8385  Validation loss = 1.1645  \n",
      "\n",
      "Fold: 29  Epoch: 13  Training loss = 0.8938  Validation loss = 1.2301  \n",
      "\n",
      "Fold: 29  Epoch: 14  Training loss = 0.8746  Validation loss = 1.2464  \n",
      "\n",
      "Fold: 29  Epoch: 15  Training loss = 0.8235  Validation loss = 1.3010  \n",
      "\n",
      "Fold: 29  Epoch: 16  Training loss = 0.9687  Validation loss = 1.0727  \n",
      "\n",
      "Fold: 29  Epoch: 17  Training loss = 0.8473  Validation loss = 1.2663  \n",
      "\n",
      "Fold: 29  Epoch: 18  Training loss = 0.9428  Validation loss = 1.1766  \n",
      "\n",
      "Fold: 29  Epoch: 19  Training loss = 0.7809  Validation loss = 1.0442  \n",
      "\n",
      "Fold: 29  Epoch: 20  Training loss = 0.8633  Validation loss = 1.2285  \n",
      "\n",
      "Fold: 29  Epoch: 21  Training loss = 0.8525  Validation loss = 1.1598  \n",
      "\n",
      "Fold: 29  Epoch: 22  Training loss = 0.7656  Validation loss = 1.2395  \n",
      "\n",
      "Fold: 29  Epoch: 23  Training loss = 0.8208  Validation loss = 1.0444  \n",
      "\n",
      "Fold: 29  Epoch: 24  Training loss = 0.7884  Validation loss = 1.4159  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 19  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 0.8832  Validation loss = 1.2014  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 0.8686  Validation loss = 1.3785  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 0.7573  Validation loss = 1.2531  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 0.8099  Validation loss = 1.2396  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 0.8960  Validation loss = 1.5245  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 0.7605  Validation loss = 1.3736  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 1.0063  Validation loss = 1.6411  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 0.7215  Validation loss = 1.5338  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 0.7326  Validation loss = 1.3778  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 0.6896  Validation loss = 1.4198  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 0.7319  Validation loss = 1.3683  \n",
      "\n",
      "Fold: 30  Epoch: 12  Training loss = 0.8772  Validation loss = 1.6468  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 1  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 0.9435  Validation loss = 2.0269  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 0.8136  Validation loss = 1.6715  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 0.7603  Validation loss = 1.6927  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 0.7884  Validation loss = 1.8347  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 0.8157  Validation loss = 1.7721  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 0.7974  Validation loss = 2.0943  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 0.7578  Validation loss = 1.8154  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 0.7218  Validation loss = 1.7887  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 0.6776  Validation loss = 1.7752  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 0.6928  Validation loss = 1.7299  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 0.6690  Validation loss = 1.7537  \n",
      "\n",
      "Fold: 31  Epoch: 12  Training loss = 0.7361  Validation loss = 1.7999  \n",
      "\n",
      "Fold: 31  Epoch: 13  Training loss = 0.7612  Validation loss = 1.7222  \n",
      "\n",
      "Fold: 31  Epoch: 14  Training loss = 0.6777  Validation loss = 1.6929  \n",
      "\n",
      "Fold: 31  Epoch: 15  Training loss = 0.6635  Validation loss = 1.6052  \n",
      "\n",
      "Fold: 31  Epoch: 16  Training loss = 0.6956  Validation loss = 1.5467  \n",
      "\n",
      "Fold: 31  Epoch: 17  Training loss = 0.6616  Validation loss = 1.5898  \n",
      "\n",
      "Fold: 31  Epoch: 18  Training loss = 0.7111  Validation loss = 1.6994  \n",
      "\n",
      "Fold: 31  Epoch: 19  Training loss = 0.6742  Validation loss = 1.5682  \n",
      "\n",
      "Fold: 31  Epoch: 20  Training loss = 0.6455  Validation loss = 1.7029  \n",
      "\n",
      "Fold: 31  Epoch: 21  Training loss = 0.7559  Validation loss = 1.7490  \n",
      "\n",
      "Fold: 31  Epoch: 22  Training loss = 0.6138  Validation loss = 1.6643  \n",
      "\n",
      "Fold: 31  Epoch: 23  Training loss = 0.6709  Validation loss = 1.6796  \n",
      "\n",
      "Fold: 31  Epoch: 24  Training loss = 0.6206  Validation loss = 1.6740  \n",
      "\n",
      "Fold: 31  Epoch: 25  Training loss = 0.6320  Validation loss = 1.5872  \n",
      "\n",
      "Fold: 31  Epoch: 26  Training loss = 0.6469  Validation loss = 1.8471  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 16  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 0.7424  Validation loss = 1.7915  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 0.6892  Validation loss = 2.4104  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 0.7307  Validation loss = 2.1237  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 0.6288  Validation loss = 2.0717  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 0.6443  Validation loss = 2.0271  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 0.6989  Validation loss = 2.5991  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 0.8551  Validation loss = 2.9917  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 0.6650  Validation loss = 1.6558  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 0.6429  Validation loss = 2.4025  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 0.6714  Validation loss = 1.9197  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 0.6096  Validation loss = 2.0690  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 0.6633  Validation loss = 1.7999  \n",
      "\n",
      "Fold: 32  Epoch: 13  Training loss = 0.6146  Validation loss = 2.3057  \n",
      "\n",
      "Fold: 32  Epoch: 14  Training loss = 0.6873  Validation loss = 1.7120  \n",
      "\n",
      "Fold: 32  Epoch: 15  Training loss = 0.6356  Validation loss = 2.2384  \n",
      "\n",
      "Fold: 32  Epoch: 16  Training loss = 0.6516  Validation loss = 1.5978  \n",
      "\n",
      "Fold: 32  Epoch: 17  Training loss = 0.6064  Validation loss = 2.3489  \n",
      "\n",
      "Fold: 32  Epoch: 18  Training loss = 0.5674  Validation loss = 2.2467  \n",
      "\n",
      "Fold: 32  Epoch: 19  Training loss = 0.5891  Validation loss = 2.1713  \n",
      "\n",
      "Fold: 32  Epoch: 20  Training loss = 0.5941  Validation loss = 1.9257  \n",
      "\n",
      "Fold: 32  Epoch: 21  Training loss = 0.5504  Validation loss = 1.9807  \n",
      "\n",
      "Fold: 32  Epoch: 22  Training loss = 0.5691  Validation loss = 1.6665  \n",
      "\n",
      "Fold: 32  Epoch: 23  Training loss = 0.6069  Validation loss = 2.4241  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 16  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    # Load model with lowest validation error for each fold\n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 13\n",
      "Average validation error: 2.58123\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 0.6846  Test loss = 3.0375  \n",
      "\n",
      "Epoch: 2  Training loss = 0.6699  Test loss = 2.9999  \n",
      "\n",
      "Epoch: 3  Training loss = 0.6643  Test loss = 2.9879  \n",
      "\n",
      "Epoch: 4  Training loss = 0.6599  Test loss = 2.9831  \n",
      "\n",
      "Epoch: 5  Training loss = 0.6560  Test loss = 2.9806  \n",
      "\n",
      "Epoch: 6  Training loss = 0.6524  Test loss = 2.9791  \n",
      "\n",
      "Epoch: 7  Training loss = 0.6490  Test loss = 2.9783  \n",
      "\n",
      "Epoch: 8  Training loss = 0.6458  Test loss = 2.9778  \n",
      "\n",
      "Epoch: 9  Training loss = 0.6428  Test loss = 2.9775  \n",
      "\n",
      "Epoch: 10  Training loss = 0.6400  Test loss = 2.9775  \n",
      "\n",
      "Epoch: 11  Training loss = 0.6374  Test loss = 2.9775  \n",
      "\n",
      "Epoch: 12  Training loss = 0.6348  Test loss = 2.9775  \n",
      "\n",
      "Epoch: 13  Training loss = 0.6324  Test loss = 2.9776  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc9a0ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xca9dc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 2.9776403855 \n",
      "Fixed scheme MAE:  1.92320047088\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 0.6324  Test loss = 2.8660  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 0.7216  Test loss = 3.2001  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 0.8235  Test loss = 1.3718  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 0.8409  Test loss = 0.5808  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 0.5736  Test loss = 0.4911  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 0.5723  Test loss = 1.1698  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 0.5875  Test loss = 0.5969  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 0.5915  Test loss = 0.0655  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 0.5091  Test loss = 2.4247  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 0.5911  Test loss = 3.7115  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 0.7479  Test loss = 4.0216  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 0.8990  Test loss = 3.7009  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 0.5498  Test loss = 0.6330  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 0.5553  Test loss = 0.7679  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 0.5634  Test loss = 2.5243  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 0.6441  Test loss = 2.9359  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 0.6395  Test loss = 0.2145  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 0.6342  Test loss = 0.3879  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 0.6357  Test loss = 0.8966  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 0.6386  Test loss = 0.9270  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 0.5413  Test loss = 1.9679  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 0.5935  Test loss = 3.5949  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 0.7418  Test loss = 2.4970  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 0.8027  Test loss = 0.7389  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 0.6611  Test loss = 1.0152  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 0.6691  Test loss = 0.4116  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 0.6710  Test loss = 0.5540  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 0.6738  Test loss = 1.2802  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 0.6075  Test loss = 0.8086  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 0.6156  Test loss = 0.2451  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 0.6161  Test loss = 4.2051  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 0.8056  Test loss = 1.0880  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 0.7252  Test loss = 0.6062  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 0.7285  Test loss = 0.5387  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 0.7115  Test loss = 1.8805  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 0.7483  Test loss = 4.9878  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 0.9223  Test loss = 0.3065  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 0.9220  Test loss = 1.8849  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 0.9466  Test loss = 1.4831  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 0.9639  Test loss = 1.6728  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 0.7973  Test loss = 3.1371  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 0.8869  Test loss = 2.9376  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 0.9587  Test loss = 3.4364  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 1.0466  Test loss = 11.9505  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 1.8173  Test loss = 7.6703  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.0509  Test loss = 0.8811  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.0538  Test loss = 0.5731  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.0549  Test loss = 0.6996  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 1.5337  Test loss = 2.7889  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 1.5693  Test loss = 2.7370  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 1.6045  Test loss = 0.8577  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 1.6080  Test loss = 2.5349  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 1.4875  Test loss = 5.3819  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 1.6292  Test loss = 3.6159  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 1.6892  Test loss = 0.3140  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 1.6894  Test loss = 0.5226  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 1.4082  Test loss = 0.2730  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 1.4077  Test loss = 1.7150  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 1.4208  Test loss = 1.3440  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 1.4304  Test loss = 0.6576  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.3156  Test loss = 1.1548  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.3233  Test loss = 2.6486  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 1.3521  Test loss = 0.1459  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 1.3498  Test loss = 1.1934  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 1.3471  Test loss = 1.1747  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 1.3491  Test loss = 0.5578  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.3509  Test loss = 1.4286  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 1.3622  Test loss = 2.2579  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 1.2848  Test loss = 5.2924  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 1.4427  Test loss = 0.5177  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 1.4429  Test loss = 2.0002  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 1.4634  Test loss = 2.9459  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 1.2098  Test loss = 2.4025  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 1.2457  Test loss = 0.5508  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 1.2475  Test loss = 0.9358  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 1.2490  Test loss = 0.7301  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 1.1810  Test loss = 1.9266  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXlclNX+xz9n2DdZxAVNREEBAUEhzUTL1DRcyyWXzGsu\nWVq/0tteVre61b11Lb3lVpY3Sy01UzP3pbQUN3AXWdzZZBcQBub7++PMM8zyzDDAMIPMeb9evBie\n5zznOfMwcz7nu5xzGBFBIBAIBPaHwtYNEAgEAoFtEAIgEAgEdooQAIFAILBThAAIBAKBnSIEQCAQ\nCOwUIQACgUBgpwgBEAgEAjtFCIBAIBDYKUIABAKBwE5xtHUDTOHv709BQUG2boZAIBDcNRw/fvwW\nEbUyp2yTFoCgoCAcO3bM1s0QCASCuwbG2BVzywoXkEAgENgpQgAEAoHAThECIBAIBHaKEACBQCCw\nU4QACAQCgZ0iBEAgEAjsFCEAAoFAYKc0WAAYY6GMsSStn2LG2At6ZR5kjBVplVnQ0Pve1WzZAly4\nYOtWCAQCO6fBE8GI6CKAGABgjDkAuAHgZ5mifxDR8Ibe764nKwt47DFg6FAuBAKBQGAjLO0CGggg\njYjMnolmd3z7LVBVBezZA5SX27o1AoHAjrG0AEwAsMbIuT6MsWTG2G+MsQgL3/fuQKUCVqwAfH15\n579/v61bJBAI7BiLCQBjzBnASAA/yZw+AaAjEUUDWAxgk4l6ZjHGjjHGjuXm5lqqeU2DvXuB9HTg\nk08Ad3fg119t3SKBQGDHWNICeATACSLK1j9BRMVEdFv9ehsAJ8aYv1wlRLSciOKIKK5VK7MWtLt7\nWLEC8PMDJk0CBg7kAkBk61YJBAI7xZICMBFG3D+MsbaMMaZ+3Ut93zwL3rvpk5MD/PwzMHUq4OoK\nDBsGXL4MnD9v65YJBAI7xSICwBjzADAYwEatY7MZY7PVf44FcIYxlgxgEYAJRHY29F21ClAqgZkz\n+d8JCfy3cAMJBAIbwZpyPxwXF0fNYj8AIqBrVyAgAPj995rj0dE8ICyCwQKBwEIwxo4TUZw5ZcVM\nYGuwfz+Qmloz+pcYNgw4eBAoLLRJswQCgX0jBMAaLF8O+PgAY8fqHh82DKiuBnbutE27BAKBXSME\noLHZswdYtw546inAzU333H338awgEQcQCAQ2QAhAY3LzJk/5DAsD3n3X8LyDA18S4rff+CQxgUAg\nsCJCABqLqipg4kTg9m1g/XrA01O+3LBhQG4ucPSoddsnEAjsHiEAjcWCBTzjZ9kyoFs34+WGDgUU\nCmDbNuu1TSAQCCAEoHHYtg348EOe9fPEE6bL+vkBwcFieWiBQGB1hAA0BnPmAN27A59/bl75du14\nvEAgEAisiBAAS6NUAleu8DX/9bN+jBEQAGRmNm67BE2GDz/8EAsXLrR1MwQCIQAWJyeHz/xt29b8\nayQLoAnPym5K5OXlobi42NbNqBdlZWX44IMPsGmT0QVxBQKrIQTA0mRl8d91FYDycqCoqHHa1MwY\nPXo0nnnmGVs3o17s2LEDpaWlqKystHVTBIKGbwkp0ENy5QQEmH9Nu3b8982bfMawwCQXL15Efn6+\nrZtRLzZs2AAAUCqVNm6JQCAsAMtTXwsAEHEAM1AqlcjNzUV6ejpUd9nkuYqKCmzevBkAhAUgaBII\nAbA0kgC0aWP+NZK1IDKBaiU7m+83dOfOHdy8y57Xrl27UFJSgpYtWwoBEDQJhABYmsxMntvv4mL+\nNUIAzCZTy0pKS0uzYUvqzoYNG+Dt7Y2BAwcKF5CgSSAEwNJkZdXN/QMAXl78RwhArWRJFhaA1NRU\nG7akbiiVSvzyyy8YNWoUPD09hQUgaBIIAbA0mZl1FwCAxwFEDKBW7lYLYN++fSgoKMCYMWPg5OQk\nBEDQJBACYGmysuqWASQRECAsADOQBKBjx453lQWwfv16eHp64uGHH4azs7NwAQmaBEIALAlR/VxA\ngFgOwkwyMzPh7++P8PDwu8YCqKqqws8//4zhw4fD1dUVzs7OwgIQNAmEAFiS4mI+oas+FoDkAhKz\ngU2SlZWFgIAAhISEIDU1FU15T2uJP/74A7du3cJY9Y5wwgUkaCoIAbAk9ZkDINGuHXDnjtgfuBYy\nMzM1AlBcXIxbt27Zukm1sn79eri5uWHo0KEAoHEB3Q3iJWjeWEwAGGOXGWOnGWNJjLFjMucZY2wR\nYyyVMXaKMdbTUvduMjREAEQqqFlkZmaibdu2CA4OBtD0A8EVFRVYt24dhg8fDg8PDwBcAADuGhII\nbImlLYABRBRDRHEy5x4B0EX9MwvAEgvf2/bUZxkICe3lIASyEJGOCwho+qmgmzdvRl5eHqZPn645\n5uTkBEDMBhbYHmu6gEYB+B9xDgPwYYzVo6dswjTUBQSIVFAT5OfnQ6lUIiAgAJ06dQJjrMlbAF99\n9RUCAwMxaNAgzTHJAhCZQAJbY0kBIAA7GWPHGWOzZM63B3BN6+/r6mPNh8xMwNkZ8PWt+7XCBVQr\nUgpoQEAAXFxc0KFDhyZhARw6dAh//vmnwfErV65g165dmDZtGhwcHDTHJQEQFoDA1lhyNdB4IrrB\nGGsNYBdj7AIR/V7XStTiMQsAAgMDLdg8KyClgDJW92s9PABvbyEAJtAWAAAIDg5uEhbA3LlzkZGR\ngXPnzqGdZMkB+OabbwAA06ZN0ylfqwvoxx95NtjjjzdOgwUCNRazAIjohvp3DoCfAfTSK3IDQAet\nv+9RH9OvZzkRxRFRXKtWrSzVPOtQ3zkAEmIymEkkAWirfsZSKqgtISJcunQJRUVFmDNnjiazp7q6\nGitXrsTDDz+Mjh076lxTqwtowQLg7bcbtd0CAWAhAWCMeTDGvKTXAB4GcEav2GYAT6qzge4DUERE\nzcvhnZlZvwCwhFgOwiT6FkBISAhyc3NtujtYTk4OSktLER4ejk2bNmnW+9+1axeuXbuGGTNmGFxj\n0gV0+zaQkgJcugSUlTVq2wUCS1kAbQAcZIwlA0gE8CsRbWeMzWaMzVaX2QYgHUAqgBUAnrXQvZsO\nDbUAxGxgk2RlZcHT0xOenp4AYLVU0MWLF+O7776TPSdZIB9//DF69uyJuXPnIj8/H1999RX8/f0x\ncuRIg2tMuoBOn+buH5UKOHfOcm+iAaxcuRIrV660dTMEjYBFBICI0okoWv0TQUQfqI8vJaKl6tdE\nRHOIKJiIoojIYK5Ao3H1auOPppRK4NYtywiAmCAkizQJTEJKBW1MAbh69SrmzZtndBN36d6hoaH4\n+uuvcevWLUyfPh2bN2/G1KlTNaN9bUy6gE6erHl96lTD30AD+ec//4np06fj3XfftXVTBI1A858J\nXFgIREYCr73WuPeRNoNviAsoIACorATu0u0OGxtpEphE586dATTuXIBPPvkEVVVVuHDhguwOZKmp\nqVAoFAgKCkJMTAxeeuklbNq0CUqlUif3XxuTLqCkJJ5F5u5uUwEgIrzzzjt444030Lp1a1y9ehUl\nJSU2a4+gcWj+ArB6NVBSAmzY0Lgj64bMAZCw07kAKSkpZu3upW8BeHl5oU2bNo0mANnZ2VixYgX8\n/PxQXl6Oq1evGpRJS0tDYGCgplNfsGABwsLC0L9/f4SHh8vWa9IFlJQE9OjBBy02EgAiwuuvv453\n330X06ZNw5IlfM7m+fPnbdIeQePRvAWACFi2DHByAm7cAI4fb7x7SZ22JQTAzuIAI0aMwPPPP19r\nOWkWsDYhISGN5gL67LPPUFFRgX/9618A5DvA1NRUjSsKANzc3HD48GFs3brVaL1GXUBVVTwG0KMH\nEB3NBcAG7sAPP/wQH330EWbPno2vvvoKUVFRAICzZ89avS2CxqV5C8CffwJnzgD/+AegUAC//NJ4\n95IsgIZmAQE2F4DKykqrbbheVFSElJSUWjuX0tJSlJSUGAhAcHBwo1gAhYWF+OKLLzBu3DiMGjUK\nAHDhwgWDcmlpaZpgtIS3tze8vLyM1m3UBXTxIl8QMCYG6N4dyMuz+mehuroan3/+ORISEvDll19C\noVCgc+fOcHV1FQLQDGneArBsGd9qce5cID7eOgJQl83g9Wkis4H79++PV155xSr3OqV2c6Snp6O6\nutpoOf05ABIhISG4fv06ysvLLdquL774AiUlJXjttdfg7+8Pf39/AwugsLAQeXl5OhaAORh1AUkB\n4B49uAAAVncDHTx4EDk5OZg6dSqYekKjg4MDwsLChAA0Q5qvAOTn8xmVU6YAnp7AqFHcvE5Pb5z7\nZWby4J2LC3Jzc3Hnzp261+HmBvj42DQGUFJSgsTERNnRbmOQlJQEgHeGcj52Cf05ABLS6DsjI8Ni\nbSotLcXChQuRkJCAmJgYAEBYWFiNAGRmAjk5GteTvgVQG0ZdQElJgIsLEBoKqN0u1haADRs2wNXV\nFQkJCTrHIyIihAA0Q5qvAKxaBVRUAE8/zf9Wm/GNZgWot4JUKpXo3r27WT5tWWw8F+D06dMgIuRb\nKRNJEgDAdDaPtBm8XAwAsGwq6IoVK5CXl4c33nhDcyw8PLxGAEaOBIYMQeqlSwDqLwAGFkBSEu/4\nHR35YKJDB6sKgEqlwsaNGzFkyBDNXAuJbt264dq1azaddCewPM1TAKTgb58+NaZ0cDDPrGhMAWjb\nFvv27UNWVhZWr16NoqKiutfTrh1yk5Nx9OhRy7fRDKQO2ZoC0K1bNwDAJXWHKkdtFoAl4wBr165F\nr169cP/992uOhYeHIy8vD3nnzwPHjgFJSVDt2KHTBnORdQERcRdQjx41x7p3t6oAJCYm4saNGxgz\nZozBuYiICAAiE6i50TwF4MABHlCTRv8So0YBf/zBg2uWRr0MxIYNG+Do6Ijy8nJ8//33da6mws8P\nZWlpePnlly3fRjOQBKCgoKDR76VUKnHmzBkkJCTA3d29VgFwdHSEn5+fznE/Pz/4+PiYZQGoVCps\n2LABcXFxWLBggdFyeXl5mjkGElJKZ+66dfyAqyu679yJtm3bajZ6MRdZF9D169xtqXY5AeACcOEC\nt2RN8cwzwKBBDf5cb9iwAU5OThgxYoTBOUkAhBuoedE8BWDZMu5LHz9e9/ioUXyK/a+/WvZ+6s3g\nVa1bY9OmTXjsscfQs2dPLF++XHbbvy1bthj1sV8oLkYAgN8PHEBubq5l22kG2hZAY29ZeOHCBVRW\nVqJHjx4ICQmpVQDatm0LhUL3I8sYQ9euXU2OTIkImzdvRs+ePTF27FgcP34cv/9ufKHawsJC+Pj4\n6BwLCwvjL3bv5qu2vvUWIm7exCP1SPuVdQFpB4AloqN5aqipeMytW8BXXwF79gB9+wJXrtS5PQB/\nRhs2bMCgQYMM3jsAdOrUSWQCNUOanwCUlHA3z9SpPKiqTWws97Fb2g2k3gw+o7wcOTk5GDNmDGbO\nnInk5GQcO6a74kViYiJGjRqF2bNny1Z1KCMDzgB81Z0WACAjA/j2W+Cll4CEBO7O+uwzy74H8C0K\nT58+rdmztrS01OL30EYSm5iYGHTp0qVWAdB3/0j06NEDJ06cMCpYM2bMwKhRo1BaWorvvvsOw4YN\nM+qeo+pqTMvPR7xeID4wMBDubm5ofeoU8NBDwLPP4jZjmFkPN5+sCygpiS8jLgV/gRr3ZXKy8crW\nreMi8d//AtnZwP3382SHOpKUlISMjAxZ9w/AM4HCw8OFADQzmp8AeHlx94+cC0Wh4AG8HTt4vrWl\nUAcoD6WnazIoJk2aBHd3dyxfvlxTTKlUYtasWSAiHDhwwGDUmpubi/3qTvDedu2wceNG4NAhHruY\nNg1YvJgHiKuqgM8/t/gkoZSUFNy5cwd9+vQB0PhxgKSkJLi6uqJr167o0qUL0tPTje6TKzcJTCI2\nNhaFhYVIl8nwqq6uxrp16zBx4kScP38eTzzxBPz8/FBYWGhY0e3bqH70UfxLpcLo3bv5shxqFAoF\nBgUFwa+kBBg8GOUuLlhGhN5XrtR51C3rAkpKArp04RlrEl268KwgU3GA1au5aMyZA0hWTb9+wMGD\ndWrT+vXr4eDgoJnzIIfIBFJz8yZgoxidpWl+AgDw7AmtjTl0GDUKKC3lJrOlUAvAr8ePY+jQofD0\n9ESLFi0wceJErFmzRrOGymeffYbk5GQsXboUTk5OWLZsmU41P//8M66rJ2A92rs3bu3cCUpIAO65\nh4/qbt/mHcXbbwOXLwMnTljuPaBmRD5gwAAA9ReA27dvm7XbVVJSEqKiouDo6IguXbqgqqoKV4x0\npqYsgNjYWADAcZmZ3mfPnkVpaSmGDRsGR0e+/5GPj4+hAFy7BvTrB4dff8X3ADxKSw1chaOkznnQ\nIKSnp+MzgI/a62iNGXUBabt/AJ4NFBFhIABLly7FO++8A9XFi8DhwzzVGeBC8OefgL8/HzCYOUCQ\n3D8PPPAA/P39jZaLiIjA9evX65fc0FwgAsaMAXr1AubNqz0+08RpngJgigEDgBYt+NpAlkLtLjiT\nl6djQs+cOROlpaVYs2YNMjIy8Pbbb2PUqFF4+umnMXbsWKxatQplWquU/vjjj3Dr1AkAMMzVFb9W\nVaHU2Zn7nSMjeYcAcBFzcADWr7fcewDvkJ2dnTXZL/UNBA8cOBBhYWEmM5mICElJSZo8eymdU84N\npFQqkZubazAJTCIyMhLOzs6yAnDkyBEAQO/evTXHfHx8UFRUVDPb+dgx/oVOS8P1JUswFUCZry+g\ntwRyn9JSXAFQGhCA1NRUXAeQ//DDwIoVQB2elbQ9pEYACgu5oGsHgCX0MoGICO+++y7effdd7Jgy\nBcQYMHFiTfmOHfnCh6mpZg8Qzp07h4sXLxp1/0hI2Vrnmsgy1TZhxw4uuvffDyxcyOMuTWBb0vpi\nfwLg4gKMHg1s3Gg59VZbALccHTF8+HDN4V69eqF79+5YtmwZnn32WTg4OGDx4sUAgNmzZ6OwsBDr\n1FklOTk52LdvH/qrtwFsv2YNqhUKvBYXxy0abVq25H7o9esb7ga6dQuYPh344QckJycjMjISbdSz\nmetjAZSXl+PYsWO4fPky+vbti4ULF8r65q9fv478/HyNAHTp0gWAvABkZ2cDMEwBlXB2dkZUVJSs\nABw+fBgtW7bUSdX08fEBEeH27dv8wFNP8fWi/voLN7p3RzWAm4MHA9u21czJqK5G8JUr2A0g5dIl\nTdaR02uvcYty6VKzng/AA9dSnAUAt+oAQwsA4AKQnc1/wN10WVlZiIqMRNejR5HaoQOovd7W2o8+\nyt/P2rVmtWfDhg1gjOHRRx81WU7KBLJbASAC3nkHCAwE9u0DNm3iE0t79AB++smitzI1K96S2J8A\nAMCECUBREbB9u0Wqo8xMVACI1cugYIxh1qxZOHHiBLZv344PPvgAHdSdeb9+/RAeHo6l6o5j48aN\nUKlUeGzSJG7Ct2yJZWPHYuXvv8svczB2LB95mJEnnp2dXdPZabN3L+9gVq4EffABTp48iZiYGE2q\nZX0E4Ny5c1CpVFi6dCmGDRuGefPmYeTIkcjTS1HUDgADfIkHT09P2Xx+Y5PAtImNjZUNBB85cgS9\nevXSLGsA8LV6AJ7tg7w87l575hkgIkLj3ih69FGeMSZtBHPiBJxLS7EbPBc+NTUVPj4+8O7Xjy8z\n8vPPdXhKXLQ0FoAkAMYsAEAT2N23bx8A4Nc33kAwgPevXsXb+ttH+vkBQ4bwALEZazrt3bsXcXFx\nJp8vwDOB3Nzc7DcOsGMHcOQI8MYbgLMzt8STk7mb7sknLZJeXlxcjBdffBEPPfSQVdbjsk8BGDSI\nj6LNHCHVRv65c8gCMGbsWINzkydPhru7O+69917MmTNHc5wxhtmzZyMxMREnTpzAjz/+iLCwMERG\nRgI//AAcPIg+M2agrKwMO3fuNLzp6NE8qG2GG6hfv3545plnag4olfxDPGgQd4fNng127hxa5OY2\nWABOqzuq/v37Y+PGjVi0aBF27tyJsWPH1nTOq1ah5ccfYzaAmJwcIC0NjMhoKqixSWDaxMbGoqCg\nQGdJiOLiYpw7dw4T/P25m0eNJNKFhYU1wdJ+/WqOAXCJjOTHVq7kI7/duwEABxQKnD9/HmlpaTVr\nAPXsyXfvqsMX1snJqUYADh/m60DJrSOltybQvn370L59e9yzfz/IzQ2eU6bgvffew+eff6573YQJ\nPK7x11+1tuXcuXPoLt3HBAqFwjKZQDNm8M/v3YQ0+u/YEfjb32qOd+jA03Dv3OGuwHpXT1izZg3C\nwsLw+eefIywsDBXWiC8QUZP9iY2NpUZj9mwid3ei27drLXrgwAFatWoVbdmyhQ4ePEjnzp2jzMxM\nunPnDhERpXTuTEcAys3Nlb3+6NGjdPPmTYPjBQUF5ObmRiNHjiSFQkELFizQOV9ZWUm+vr705JNP\nyjdswACi0FAilcpo2/Pz8wkAubu7U2lpKVFmJlHfvkQA0fTp/P2npxMB9AJAv//+O6lUKnJ2dqaX\nX35Zts4rV65QXl6e7Ll58+aRq6srKZVKzbH//ve/BIB++eUXok8/JQKoijHeBuknJoZeGDKEQkJC\nDOpctmwZAaCrV68afZ/Hjh0jAPTjjz9qju3Zs4f6AVTt4EDUo4fm+O7duwkAHThwgGj+fCIXFyL1\n/1K617Vr14i++Ya37eBB/qy7d6cuXbrQmDFjqHPnzvT4449LDeTlMjKMtk+fNm3a0LwnnyR6/HF+\n7bRpxgsHBBCNGkWq8nJq3bo1TZ0wgcjXl2jSJKqqqqL4+Hjq1q2b7jXFxUSurkRz55psx61btwgA\nffLJJ2a1e8qUKdS+fXuzysqydy9/v4wRGfm+NEm2bePtXr5c/vygQUTt2xNVVta56suXL9OAAQMI\nAMXGxtKRI0ca1FQAx8jMPtbmnbypn0YVgAMH+Nv/4QeTxSoqKsjNzY0AyP64ubnRKYAO+vvXqxlP\nPfWUpq7Tp08bnJ86dSr5+PhQpdwH64sv+Hs4c8Zo/QcOHNDUv/vjj4nuuYcLn977zm7ThvYCVFhY\nSEREbdu2pZkzZ/KTp08TXb+uKRsZGUkTJkyQvd/gwYOpZ8+eOscqKyspNDSUXmvblrd33Djq2qkT\nzRo+nP8fvviCyN+fKhwdaTZjVFlRwS+sribasYOSo6JoGkAV0nEZ7ty5Q05OTvTKK69ojv335Zcp\nGyCVQsHvm5lJRDVisXnzZqJevYj69dNc869//YsAUHFxMVFJCZGnJ9GECUTOzkTz5tHIkSOpa9eu\n5ODgQG+88Qa/6NAhXv/WrUbbp89T/v5U6OpK5ORE9P77RFqCaVj4KSKAlN7etAigxDFj+P22bSMi\novnz55OrqytVV1frXjd2LFHr1ibrPnjwIAGgrWa2/aOPPiIAVFBQYFZ5HZRKoshIohYtzPruNRlU\nKv456diRyNhncOtW/p7WrKlz9ePGjSMPDw9asmQJVVVVNaytJATAPKqruWKPGGGy2NGjRwkALVq0\niI4cOULbt2+nNWvW0Jdffknvv/8+vfnss1Tu5ESZo0fXqxmJiYkEgMLDw0klM5L/5ZdfCADt3LnT\n8OKbN/lI6p13jNYvjb7/5uJCFQ4ORIGBRCdPGpTbEB5OSoBIPbLv1q0bjRkzhqioiMjbm2jiRE1Z\nDw8Patu2rWx727ZtS1OnTjU4fuTVV6kKoKthYVSYnU0A6IMPPtB5L9cjIogAKhk0iOgf/+BfOLWF\ncFOh4P8zE/Ts2ZMGDRrE/ygtpVRvbypWKIjWruX1fPMNERFdunSJANAPK1YQOToSvf66po7XX3+d\nFApFzXubPr3GSvntN3rllVc0grpy5UpepqCAn//oI5Pt0/D3vxMBlOHrS5ScXHv5qiqi7dsppWdP\nKpfa0qaNpmNfsmRJjdWizfr1vOzu3UarXrFiBQGgtLQ0s5q+ZcsWAkCHDh0yq7wOn3/O27N+PVHL\nlkRTpsiXUyo1FpnNKSoiWrqUt3vFCuPlqquJunQh6t27TtVXVVWRr68vPfXUUw1saA1CAMxl3jw+\nAsvPN1pE6kAvX74sX2DqVCIHB6ITJ+rVBJVKRdOnT6f//e9/sufLysrIw8OD5hoz5fv146MqIzz9\n9NP0D1dXIoAOKhRUZuR9jO3QgX8cVq8mIqL4+HgaMGAA0Sef8ONqF0pJSYmmA8zQc3nk5OTIuxP2\n7CGVszOd8fKiji1bajqRX3/9VafYwd9/pxcBqnZ05PccNIho7Vpa2KMH//vAAaPvk4ho5syZ5Ovr\nS6rqalJNnEjVAH3y4IN8BBcQQDR+PBER5ebmEgDaOGeOpmOXmDNnDvn6+tZUKo3unZyIbt+mb7/9\nVvP+f//995py7dsb79D0ngUBtM7bmyaOGVN7eS3Gjh1Lke3bk2rpUiKtAcGuXbsIAO3bt0/3grIy\nIi8vohkzjNYpWQ/mjjzT0tIIAC035goxRnY2H0gMHsz/H5MmcetETtQnTybq3t20VdSYJCURzZrF\nv1eSqzI83PjoX2LxYl72r7/MvtWRI0cIAK2ph+VgDKsKAIAOAPYBOAfgLID/kynzIIAiAEnqnwXm\n1N3oAnD0KH8EX31ltMiUKVOoTZs2sqNdjV9QcgU0Eg8++CD1NjaykEZV58/Lnn6gd2+6wxjl9OlD\nTgBt2LDBoExJSQkpACrx9OQ+aSIaMWIExUZF8Y4N4B2JSkWpqamaDvD777/XqWfPnj0EgHbs2FFz\n8K+/iDw8iCIi6ITa996hQwcCQDdu3NC5PisriwDQV2+/TaQlVPdHR1O5QkH0zDPyz+DwYaL//IcO\njx5N7wJUMnIkEUCvA7R48WJe5qmniHx8iJRKqqysJAC0/8EHiRQKPspT88QTT1BQUFBN3SoVUUQE\nFyMiOnz4sOb967T/4YeJ9FxfBpSVEYWEEAUH072RkTS6DlZjdXU1+fv7y8aDMjIyCACtkBuhTpnC\n4wVGOq+EhATq3r17ndrh7u5O//d//2f2NUTELSlHx5rP6f/+xz9Xx47plsvK4uW0LDar8/DDRG5u\nREOGEL37LhdbM2KFVFLCRc6Ie1TDDz8Q/e1vRJWV9N577xFjzGj8sD5YWwACAPRUv/YCkAKgm16Z\nBwFsrWt9keanAAAgAElEQVTdjS4AKhX/Qg4caLRIly5daOTIkYYnCgt559itW6Obq3KBVQ3XrvF/\n4/vvG5xSqVSU4OZGBFDVpk3k7+8v67v/888/uZUzeDD3z1ZU0NSpU+kFPz9e95Ah/HdODh06dEjT\nAepbJZ999hkBoEy1r52Sk3mnGxzM3VXEO1gA1KpVKwNRValU5OXlRXPmzNEcO378OAGgc1FRRK1a\nGY4KS0qIpHaqf6qcnCjtoYcIACUmJvJyP/3Ez//xBxERubu7U0pgoE5wmIho+PDhFBMTo3uPrCxN\nwLKwsFAT+9Fp/7x5POhqaiT9+uskuWRiY2MpISHBeFk9Tp06pet20qKqqsog/qHh11/5Pbdska23\nU6dONcFsM4mNjaWBJr4zBiQm8pH0/Pk1x7KzNZ/bNWvW0H/+8x9+/F//4sc7dSIKCqp91C3DihUr\naN68eXW+joiIysv5/7GuAicxfz73COi74yRUKv59AIheeIH69etHlu7nbOoCAvALgMF6x5qmABAR\nvfUWHwVKnZYWeXl5hr5qiZkz+XUNjNibw/fff08A6NSpU/IFevfmQSo9MjIy6B8AVatHubNmzSIP\nDw8qKyvTKffll18SAMr5+mv+kdi1i1584QU6rVBwM3jLFn78zz9pw4YNmg5cP9g7ffp08vf35x3j\nxYvcxG/fXic75sqVK+Ti4kKDBw+WfSs9e/akIUOGaP5+7LHHyMfHh0q/+463QT8WIrmo9u+nO4WF\n5OToSK+++irNnz+fXFxcagLHhYX8i/naa0REFBgQQHccHYmef16nun79+tEDDzwg/5zVtGvXjiL1\n3W7Ss7t0Sf6i5GQ+slXHR/r06WP0GcixaNEiWbebRGhoKI/Z6FNRwQVSppMvKysjxhi9YyKGJMe8\nefPI0dFRNrPNAJWKuynbtNGxtIiIKDaWqvv2pdatW5OzszPl5+URde1KFB/P3XIA0Zdf1qltKpWK\nOnXqRADo4sWL8oUqKrg1JofaRWdMMGslI4P3C6++Kn8+MZE0LiWAJigU9KqxsvWkLgJg0XkAjLEg\nAD0AHJE53YcxlswY+40xFmHJ+zaICRN4/rbMTL7ExEQAwH333ad7YvdunvM7fz5fQqCR6dmzJwD5\ntW4AAMOG8cWp1LNFJU6fPo2HAJSGhwMtWmDcuHEoLS3Fb7/9plMuKSkJvr6+8H/8cb6C6ubNiMvP\nR6RKhaoXXgCkfPe0NM2s3JEjRyI5OVlnxdDTp08jKioK7No1PseAiD+roCBNmcDAQGzZsgX//ve/\nZd+K9qqgZ8+excaNG/Hcc8/BfcwYvtCftB4/wHOvP/0UGDgQeOABuHh7I1I9I/jw4cPo2bOnZt0d\neHvzafvq997HxQUuVVWa/H8JuaWg9Zk8eTLG6s/5UM+ShVyOfHU1MGsWX6L8008B6E0EM4N9+/Yh\nKCgIQVrPUpvg4GD5PRGcnflM73Xr+MKCWqSkpICIapa6NpPZs2ejqqpKZ6FDo+zfz/fgeOMNPudE\nm0ceAf76CxU5OaisrMShjz8GUlL4PIEhQ/j/6/33gTrs93z27FnNXBCdtbaI+DpJs2fz+Ra9e/Nj\n+uzaxZdceeABs++pQ1AQn4m9bBmfIa7PmjV8lvb+/cgPC8MKlQqju3at370sgblKUdsPAE8AxwE8\nJnOuBQBP9esEAJdM1DMLwDEAxwIDAy2qjEbp0oVo1CiDw++88w4xxqhIe+RSWspN065djY8iLEx1\ndTV5enrSc889J1/gxAmS85n+e8ECqgTozt//TkRESqXSwA1069YtCg0N5QFfIqKRI4k6dqRrXbvS\nNYCyrl7lLi7GiN5+m95++21ijGmyk6TAY3V1NXV2c6Pt997LMzy8vWWzjWrjzTffJIVCQRUVFTR5\n8mTy8PCgW7du8ZNPPKHrz16yhL/vPXs018+YMYN8fX3Jzc2NXnjhBd3KP/yQl79xgxZLGUZZWTpF\nAgMDjc+7MEVxMa9PzlqUgoPqADsR0aBBg6hPnz5mVV1dXU1+fn70t7/9zWiZ559/njw9PeVjVSUl\nPKMqLEzHXblmzRoCQMnmZCLp8djAgTTA35+UmzbxUbqxwOcDD/AAfHm54bmDB4kAerZ1awoJCaEd\nbdtyF6Tkb9+/nz83M+coEBG9//77BIAeeOAB8vPz49butm01bhd3d24xAzwGqE9cHLdAtCgrKzM6\n70UW9fui//5X93hVFVG7dvw7RkRvTptG2QBVh4byz4+FgLVdQACcAOwAMM/M8pcB+NdWziouICJu\nlrdqZTCh6pFHHqGIiAjdsq+9pnE5WJP4+Hi6//775U+qVPyDNXaszuEP+/Uj/TTAmTNnatxAq1ev\nJn9/f3J0dKwJ6K5YQZIvfT5A586d48cDA4meeIKefvppatWqla577MIFKh49mioAUjHGP+BJSfV6\nn6tWrdJkCCkUCvq7WryIqMYVtXUrjwUEBfEvs9b/TUqJhFxmRVISv/7rr+nP1q3psouLwf29vb2N\nC21tdOyoky5LRLxtHTvyjlCrnQkJCRQXF2dWtUlJSQSAVq1aZbSM5CLK0hM0DVLCwttvaw4tWLCA\nGGNULtc5m2LuXM1nRPPj7GyYCSd14J99JlvNmaQkygcoOS6O/vnyy1QK0G39TKrBg4n8/Ws6yJQU\nXt/GjbJ19urVi3r16kV79+7lz+zbb/kALziYaNUqXk9+Ps/q0o5JEBHdusUHOu++q3P4ueeeo8DA\nQPm5OHKoVPxzGRJCVFVFS5cupZEjR1LV7t38eag/l6GhofRKr17cZTRpknl1m4FVBQAAA/A/AJ+Z\nKNMWAFO/7gXgqvS3qR+rCYCU55uaqjmkUqmoZcuWuvm5587xD059RogN5P/+7//I3d3deLrejBk8\nU0craLbSz48qFQodS2Xnzp0EgLp160YAqHfv3rqxhcxMIoAq3d3JC1r53g89RHTffTR69GiN/zss\nLIyGDx9OFBpKSldX+hygpPXrG/Q+pYB0p06dyMXFpSagTMTfm48Pz2yRYgKbN+tcL82rAEDp6em6\nlUtCOWYMFTs7049eXjqnq6uriTFGb731Vv0an5DA0xe1OX5cIzrajB492uzsm4ULFxIAunLlitEy\n27ZtIwB08OBB4xVNnsw/v+qJg+PHj6fOnTub1QYN1dVErVqRql8/mtOqFT0dHc2/F+3b8062qIjS\n09Pp/PnzfPZ027ZGLeW5c+fSTwoFVbVtS5lvvkkE0OoXX9QtdOQIf34PPsg7cklw2rQxqO/GjRua\nQYlKpaLQ0FCaERnJy+sHz0eM4JMitdNQpUQBvTkOw4cPJwD0008/mf+cfvyR1/XzzzRkyBACQEm9\ne2tWH7hy5QoBoIULFxK9/DIXHr2suPpibQGIV3/hTqEmzTMBwGwAs9Vl5oKniCYDOAzgfnPqtpoA\nnDrFH4VWLr40WWjZsmX8gErFP9A+PjyDwcpII+OzZ8/KF9i0ibTdIXfu3KHjAKV17KhTTKlUUuvW\nrcnT05MWLVokLyizZ9Pl+fMJAG2RgmEzZxK1akV9+vTRTLaaNm0aBfv6EgG0e/BgYozRbXPS5Uwg\n5egD0MkG0vDUU1zowsKIoqIM8sjLy8vJ0dFRNsuIiGrSEQGa6+mpc0rK8DF3WQQDXnqJj4S1M5Xe\nfJOP8HJydIqOGzeOwsLCzKr20UcfrbWjTklJIQD07bffGi+Uk8Pdc/fdR1RVRVFRUTRs2DCz2qAh\nOZl/zr79lv7973/XuJD++IPIwYHyBw8m7xYtaHxAAC8nZffoUVJSQi1atKBl993Hy7VuTefd3ChO\n7js/fjx/rkOGEC1eTMlaWWnaLF26lICaGfULFy6kLwCqdnU1DED/8AOvQ3sux6xZ/LOll2nWt29f\nAlDjJjUHpZJbfvHxFBQURE4A3QKoWD3x9KuvviIAdObMGS6gANGiRebXbwKru4Aa66cxBKCqqooG\nDx5MEydOpJSUFOkg9z3Onq0pt3r1aq7akivj++/541qyxOJtMoczZ84QAKMTxqikhH9J1COo0/v3\nUzVAp9STn7RJTU3VHVnLIOX7a9wOH39MBFBUx440efJkIuLpdgPUI7J/xMdTcHBw/d+gGpVKRd7e\n3uTo6Cg/4t2xo2YUaGQpgfj4eKNLVWhmxwIU6uioIxKXL18mAPSViXkhJlm1itetPScjIoK7f/SY\nPHmy2c8rLi6Ohg4darJMRUUFOTg40Jtvvmm6MrXlVP355+Ti4qLrYjOHhQv5e1SvB+Xq6kqzZs0i\nIqKsF18kAuhZxmgXQEp/fx4zk2H58uUEgBKlgQtAux99VD57p6pKJ4bw9+hoIoAOf/ihTrGEhATq\n3Lmz5n+ad/Mm5QF0RGZ9KSop4bn+zz5bc6xzZ41/Xpvw8HBijBG0XaLmoH5WvQBaNHQoEUBvdO9O\nKpWKxo8fT+3atav5/EVF8TW6LIAQABP89NNPBIAcHR3JwcGBZs6cyafQDx5MFB2tKffcc8+Ru7s7\nz70vLOQm5733ms7zbkSUSqV8YFObIUN4cJqI9j/3HBFA6VqBx7og+fgXLlzID6g7zj6urpoc67Nn\nz9I89Zf3vuDgOk1sMsW0adOMp8YpldwnrPavylFcXGyQ6qqhsJDI0ZGKvbwIAF8gT01ycnLdTX1t\njh3jXynJDZaSQsZ84NOmTaMOHTqYVW1wcDBNMsNH3LlzZ+PCJ6FSEfXvT5XqyXh1FrsRI/izVzN9\n+nRyd3enI0eOUNvWrWmXiwtffA+gw+PGGWmCinr06EHd1Z0hRUcTubrSjTNnzEpLjbvnHiKAPmjT\nRuOXLykpIRcXF93vh/ozO9LVla/tpM/48TVzS9LSjI7C27RpQ6NGjSJnZ2d6Xi9t2CTFxVTl6Ulr\nAMro25fK3d3JCXzCnp+fn+6SKe+9x+9vbP5AHaiLANjVctBEhE8++QSdO3fGlStXMGfOHKxatQoh\nISE44erK11xXb994+PBhxMXF8W0E33oLyMkBlizhO3HZAEdHR8TExBhPBQV4OmhKCnDpEpz++AO3\nAdxTyyYfxvD29gZjrGZXMHUqaPs7dzQ7c4WFhaG3kxPy3N2RmJFh1pLC5rBy5Up8+OGH8icdHYFf\nfuEb+hj5X3h5ecHNzU3+em9vYPJkpKlTe7W3hpT2ApD2C6gz4eF8i0gpFVTaI0Bm6WOd5aBrIT8/\nH76+vrWWCwkJkd1PQQfGgDFj4HTtGoIAhIeHm9UGAHwv6gMH+GZEaubMmYOysjL07dsX1UQI3LsX\nLCAAtxjDcoV895KYmIiTJ0/imWee4fs0/PvfwIoVaBcRgQcffBA//PADH53KoFQqceLGDRQ6OMA/\nO1uTirpz505UVFTo7mm8ahUqW7XC1jt38P333xtWNnEikJvL98VQL/eNwYN1ihARCgoKEBYWptnF\nr1QuvVMOLy+kDhiAsQA6HD8Ol4kT0ad/f8yZMwf5+fl4+OGHa8qOH89/W3hjmVoxVyls8WNpC0Ba\n+fC/WulZly9fpkGDBtEj0oqRu3dTeXk5OTk50UsvvcT9jI6ORE8/bdG21Ic5c+aQp6en4aqPEtIo\nZuFCuuLhQX/oBTnriq+vb81sX3Wa4yt62ShXPDxoq0LRsJGzDVi7dq1BTGXz5s3cLSHNHq4PnTtr\n1hyi++4jMvIZnjt3Lvn5+dVanRSYrtW1Q0TPPvus7jpGxjhzhgig6QDlm1gHywApILt2rc7h/v37\nk4+PT4279OZNen7YMF0XhxbTpk0jT09P2VG5tDjdMf0lItRIaxHdDA2lMy1akJ+fH+Xn59OTTz5J\nvr6+NbPls7OJHB1J9dJLFBMTQ1FRUYZtKS/nrt9p03gGXfv2BpmAt2/fJgD00UcfafoP2SU3jPD5\n3/9OlZLLcs8eunDhArm4uBAAytaPJcbE8M9MA4GwAOT59NNP4evri79pbejQsWNHrF+/HjmdOkEF\noOi335CUlASlUsn3kV29mo98nn/eZu2WiI2Nxe3bt2U3TQEAdO7MR6Fff43A0lJcU2+zWF/8/Pxq\nNoXx8kKlnx9CAM2WkSgrwz1lZTim3gglKiqqQfezJtIoX3uD8wZbAADfu/nMGb6V5OHDfFKQDOZO\nBCsuLgYRaTbpMUVISAgKCgpq38inWzcUurlhmIuLWZaFhr17+e8BA3QO//LLL0hJSUF0dDQ/EBCA\niJEjcfPmTaSkpOiULSsrw/r16zF+/Hh4eXkZ3GLMmDFwcnLCDz/8INsEaZJXdXg4wqqqUJCfj3fe\neQdbt27FsGHDuMUO8AlXVVVgTz6J5557DqdPn8aePXt0K3N15f+fjRuBPXv46F9r5zigZlMkPz8/\n3H///YiKisKXX35p1ELR51h2Nn52dwc6dQIeeAChoaFYvHgxnnnmGbRu3Vq38OOP88/MlStm1W0J\n7EYALl26hE2bNuHZZ5+Fh4eHzjlvb298t2ULLigUOPvVV/j9998BAL179eI7QvXuDag3xLYl0ozg\nE6Y2+x42jHdAACr1ZrnWFV9fX53OpLhVKwQDNZuznzoFBRFOAnB1da3ZIesuQGdXMDXS69pmApsk\nIoK74aSd2owIgLkuIOn5m+sCAmCWG+iIhwceUKn42NRc9uwBoqIAvY7Lx8cHrVq10jk2QC0S0haW\nElu2bEFJSQmeeOIJ2Vv4+vri4YcfxtatW2XPSwLgeu+9cCgrw8uPP45FixYhPz9f1/3zv//xndoi\nIzF58mS0adMGn6pnYesgbQ9bUMBnr+shuUD9/PzAGMOzzz6LkydPalYJqI2LFy/i6169+Lafapfl\nzJkz8eWXXxoWtoEbyG4E4LPPPoOTkxPmzp0rez48PBzuAwcivKgIC958k2+7l53NO9Np06zcWnm6\ndesGFxcX03EA9ab0+QDaaPsY64GOBQDglo+PrgVw8iT/pW6bg43iI/VBTgAsYgFERHCL8T//Abp2\n5RaZDNKm8LWNJLU7oNqQNr6vTQCICFtKS+GnVGoGC7VSUcG3z9Ty/5siJCQE99xzD/ZKVoOa77//\nHu3bt8cDJpZa6NGjB9LS0mQFMiMjAw4ODvCJjwcAvDJsGDw9PeHs7IwhQ4bwQmfOACdOAFOnAgBc\nXFwwd+5cbN++3XA7y4ED+R7cgKwAaFsAAF8GxNPTU74D14OIcPHiRQSrl2Kplc6dgbg44Mcfay9r\nIexCAPLy8vDNN9/giSeeqBm9yhA0aRJ8AXSW3D8rV3IzccIE6zXWBE5OTujevbtpC+D++3HH3R17\nAERJJnk98fPzqwkCA7jp6ooOAFp5evIDJ04Afn6ISkhAQkJCg+5lbYwJgIuLC1xcXOpfcWQk/33l\nCh/967kUJJydnUFEqK6uNlldXSyAzp07gzFWqwDk5ORgi7S+jr5bxBiHD/O1l8wUAMYYBgwYgP37\n92s2N7916xZ+++03TJo0CQojAWKAJxdUV1fLvo+MjAwEBgbCMSYGAOB7/TpWrFiBf/7znzUupf/9\njycLaH1vZ8+eDTc3NyxcuFC3QicnYM4cPnCS2ZNZ//l7eXlh4sSJWL9+fa179ubk5KCoqAihoaEm\ny+kwfjxf10trb+vGxC4EYMmSJSgvL8e8efNMF+zTBwDwfkICXnj6ae5HHDuWZ440EWJjY3HixAnj\nI0cnJ3yckIC3fXzQrl27Bt1L3wJIV4/wHa5e5QdOngR69MDWX3/Fe++916B7WRu5GIA5C8HVSmgo\nIHVuJjKwnJycAKBWN1BdLABXV1fcc889tQrA+fPncRVAabt25gvA3r38ffXvb155AA899BByc3M1\no+6ffvoJVVVVmDx5ssnrpMykCxcuGJzLyMhAp06d+Ig6MBA4cwYTJkzA/PnzeQEinn01cKCOq8rf\n3x9Tp07Fd999p1nQUMM77wBbtsi2Rd8CAIARI0agrKwMh/QW19Pn4sWLAFB3AQCsZgU0ewHIy8vD\nokWL8MgjjyAiopZFSLt2Bfz88FjbtuiXlwcUFjYZ949Ez549UVRUhPT0dKNldty4gdbR0TzFrgFI\nFoA0gruoVPITaWmAUsnTZnv0aNA9bIWrqyucnZ0NLIAGuX94xTxltl074N57jRaTVilVSs/UCHWx\nAADzUkHPnz8PAKABA3haZ1VV7RXv3QvExvIVTc1EPw6wevVqRERE1Jou3FW9OqZJAQB4POL0ad0C\nFy8CqanAyJEG17744ouorKzEF198YfZ7kBPgAQMGwMnJCdu3bzd5bb0EoGNHHnMUAmAZ5s6di8LC\nQuN55dowxq2Av/7i7p+gIODBBxu7iXWitqWhq6qqkJycXJOR0QB8fX2hUqlQXFwMAEiW8p9TU4Fz\n54DKSh5ouwthjMHHx8cgCNxgCwAAPvgAWLy4xhKQQRIAcy0ASwrAhQsX4OHhAY+RI/m8l6NHTVd6\n+zZ3AQ0caFYbJDp27IjOnTtj7969yMjIwJ9//onJkyfXOjDx9PREhw4dDASgrKwM2dnZNQIQGQlc\nuMAHIxLSSF4dC9Oma9euGDFiBL788kuUlZWZ9R7y8/Ph6Oiokzji6emJfv36mSUALi4uCAwMNOte\nGmbM4GJby+DAEjRrAVi/fj3Wrl2LBQsWmN8h3n8/cP48N42nTTP5JbYFkZGRcHJyMioA586dQ1lZ\nGXpZYJ8CadQjjUIv5eWh1MmJWwDqAPDdagEA3A2knwbaYAsA4G7Dxx4zWaQuLiBXV1fjE9v0CAkJ\nQW5urka05Th9+jTCw8PBJH9+bW6ggwe5lWCm/1+bAQMG4MCBA/juu+8AAJMmTTLrurCwMAMBuHz5\nMgDoCoBSybOuJDZvBqKjuXtIhvnz5yMvL0/TntrIz8/XZABpM3ToUJw+fRo3btwwem1KSgq6dOlS\n9+SIGTOA5ct5fKKRaVq9mwXJycnBM888g7i4OLz66qvmX6iOAwDQZBE0JVxcXBATE4PDhw/LnpfS\n0ywpAAUFBSAiZOfkIN/Pj1sAJ08C7u5AA+ca2BJ9C8BiAmAGdXEB1SVXX0oFld0cBkBFRQUOHz6M\nvn378uyXmJjaBWDvXt4Z9e1rdjskHnroIRQWFuLTTz9Fv3790LFjR7OukwRAO9YlpYDquICAmkym\nvDy+6YuM+0eif//+iI2NxZIlS8xqR0FBgWz8ZejQoQCAHTt2GL324sWLdXP/2IBmKQBEhNmzZ6Ok\npASrVq2qmRxiDvfey/N1Bw7k/rgmSHx8PBITE2WzEBITE+Hr62uRnHxtC6CkpATl5eUobdu2xgKI\njrbZ0hiWoNFcQGZQFxeQOQFgidrmAhw9ehTl5eU1aZgDB/JO05RL5PBhnp7o7m52OySkOEBxcXGt\nwV9twsLCUFJSgszMTM0xAwEIC+OfPykOsG0b391vxAij9TLGMGzYMJw+fdosN5BkAegTGRmJdu3a\nGeyuJ6FUKpGeni4EwBb88MMP+Pnnn/Hee++hW10ncHl68jQy/XSxJkR8fDzu3Lkjmw565MgR9OrV\nq8EBYKDG75yfn6/JnFAGBgKXL3MBuEv9/xK2tADMdQHV1QKobS7AgQMHAPCRMAAuAJWVBttF6nD+\nfM2Wl3UkICAAYWFhcHJywrhx48y+TtqmUtsNlJGRATc3t5p5KC4uPHFDsgC2bAHatuX+cxNER0dD\npVLhjBlzIIw9f8YYhg4dil27dqFKJoienp6OqqoqIQDWJj8/H3PnzsX9999fe9qnMSZNqsnnboL0\nVZviBw8e1DleWlqKM2fOWMT9A+haAFlZWQAARZcu3B98+/Zd7f8HdGMAlZWVKC8vt7oFUJsLqK4W\ngIeHBwICAowKwP79+xEVFYWWLVvyA/368Zx5Y26gW7f4T10WjdPj9ddfx/vvv1+n9yEJgJSxBHAB\nCAoK0h3cREZyC6CyEti+nY/+a4nbxajnECQlJdXaDmMWAMDdQEVFRThyxHAL9HplANmAZicAfn5+\nWL58Ob755pu7amZqXWjTpg26dOliIAAnTpyASqWymABII5+CggKNBeCqLYx3uQBoWwAWmQVcB8x1\nAdXVAgC4G0hudKtUKvHnn3/iQe3MNk9Pnna4f798ZVIH3AABmDJlCl5++eU6XRMQEAAvLy8DC0Dj\n/pGIigLS07n7p6TEpPtHIigoCC1atEBycnKtZU0J8KBBg6BQKGSzgYQA2JBx48ZpcombK/Hx8Th0\n6JBOkMySAWCA58q7u7vruIBaSJ2+o2O93QJNBR8fH5SXl6OiosLqAlCXLKC6CsCQIUOQmJioyZqR\nOHbsGMrKygyXYbj3XuDUKUBuVrIFBKA+MMYMMoFkBUAakHz8MZ+DYUaqqkKhQPfu3Wu1AJRKJYqL\ni40+f19fX9x3331GBaB169ZWsyjrS7MUAHugb9++yMvL04w0AO7/DwoKMlxlsAFIC8JlZWVBoVDA\nLyICcHPjX7yGLJnQBNCeDWyRheDqgDkuIKVSidu3b9fJdQJAE2xdvXq1zvH96lF+f/3ZvNHRQHk5\nILfK7LlzPPhb11x2CxAeHq4RgIKCAhQVFclbAAAPVA8aZHagOiYmBsnJyZpJjnJInwlTz3/o0KE4\nduwYcnJydI7fDRlAgBCAu5Z49WJY2m6gxMREi43+JaTlILKzs9GqVSs4ODkBQ4cC2isv3qVInX1R\nUVGTdAHVdRKYRFBQEPr374/vvvtOx0I8cOAAIiIiDFbuhNonDjmXyPnzPNvGBvNhwsLCcP36dZSU\nlBhmAEl06sQHJIDJ9E99YmJiUFpaanJGvTnLcEjpoLt27dI5LgRA0Kh07doV/v7+GgHIzs7GlStX\nGk0AsrKyarIvNm7k66fc5WgvCCcJgLUsAHNcQHVZB0ifKVOmICUlBUfVs3yVSiUOHjwovwpneDh3\n6RkTACu7fySkQHBKSopxAXBwqHFFysz+NYY0MdSUG0huHSB9YmNj4e/vr+MGKigoQG5urhAAQePB\nGEN8fLxGAKQveu/evS16H2k9oOzsbJMrqd6NaAuAZO43pYlgdV0HSJuxY8fCxcVFM+P1xIkTKC0t\n1QRa5aEAABQfSURBVA0AS7i48P0u9DvD27eBa9dsLgAXLlwwLgAAX3Rv/HggIMDsuiMiIuDg4GCW\nAJh6/gqFAsOHD8eaNWvw1ltvobKy8q4JAAMWEgDG2FDG2EXGWCpjzGDaLWPMhTG2Tn3+CGMsyBL3\ntXfi4+ORlpaGzMxMHDlyBA4ODuhh4cwcbRdQG5nlcu9mpM5e2wJoii6g+lgAPj4+GDlyJNauXQul\nUmmY/69PdLShBSAFYG0kAMHBwXBwcNAIgI+Pj7yF9vrrwLp1darbzc0NYWFhJjOBzH3+CxcuxBNP\nPIH3338f9957L9arNwOyCwFgjDkA+ALAIwC6AZjIGNOffTUdQAERhQBYCODjht5XUBMHOHToEBIT\nExEZGWmw21lD8fX1RV5eHrKyspqtBaAdBG5hzsYdFsAcF1BDLAAAePLJJ3Hr1i1s374d+/fvR3h4\nuHERj47m21jm5tYcs1EGkISzszOCg4Nx/vx5+QygBhIdHd1gFxDAP0fffvstNm/ejJycHHz66adw\ndHS0eHsbA0tYAL0ApBJROhFVAlgLQD9COArAKvXr9QAGMktMVbVzevToATc3N/zxxx+NEgAG+Ie/\noqICFRUVzc4C0I8BeHl5WW3uiDkuoPoGgSWGDBmCVq1a4dtvvzXu/5eQCwSfP89jAzbc6lNKBW0M\nAYiJicH169eRl5cne14SAHPjQiNGjMDZs2cxbdo0TJkyRSPyTRlLCEB7ANe0/r6uPiZbhoiqABQB\naGmBe9s1zs7O6N27N9asWYPCwkKL+/8B3dFPcxMAT09PKBQKjQBYM2fbHBdQQy0AJycnTJgwARs3\nbkRJSYm8/19CWi1XXwBCQqyyKqUxwsLCcOnSJVy+fLlRBACAUTdQfn4+WrRoUae1xPz8/LBy5Uqs\nXLnSIm1sbJpcEJgxNosxdowxdixX2xwVyBIfHw/pOTWWBSDR3FxAjDF4e3trgsDW8v8D5mcBeXl5\n1W0xQz2mTJmieW3SAvD355vY6AuAjdw/EmFhYaisrMSdO3caxQUEGM8EqusyHHcjlhCAGwA6aP19\nj/qYbBnGmCMAbwCydhcRLSeiOCKKM8hXFhggrQvk4eFR94XvzEB79NncLACAm/fSPABrCoC5LqCG\ndkBxcXEICwtDaGho7QIeE1OTCVRZyZf9bgICIGFpAWjdujUCAgJMWgDNXQDqP7So4SiALoyxTuAd\n/QQA+rs+bAYwFcBfAMYC2EtGN7UV1IU+ffqAMYbY2NhG8V83ZxcQULMeUGFhYYP3UK4L5rqA6uv+\nkWCM4aeffqp10TkA3A20cydQUcGX/K6ubtYCAJgOBFvi+Td1GiwARFTFGJsLYAcABwAriegsY+wf\nAI4R0WYAXwP4jjGWCiAfXCQEFsDb2xvPP/887jWx/2xDkATAwcGhZgXJZoTkAioqKtJsRm4NJLdO\nbS4gS4xAI81d2TYmhq/0ev48FwDA5gLg6+uLNm3aIDs7G0FBQRavPyYmBrt370ZFRQVc9JY2KSgo\nwD333GPxezYlLGEBgIi2Adimd2yB1us7AMxfDFxQJz777LNGq1vqgFq1atUsV1f18fFBWlqa1YPA\njDE4OTnVOhGsMdx6RpECwUlJwPXr/LXWCNxWhIWFgTFm9raYdSEmJgZVVVU4f/68JigsIVxAArtH\nSo1sbgFgCR8fHxQUFFg9CAxwN1BtFoBVXRAhIXxdneRkICeHLwBn4Xkl9eGFF14wufduQ9AOBGsL\nABEJF5BAwBjTmOHNER8fH2RlZaG6utrqAuDk5GRUAKQOyKojUAcHoHt3LgCFhTZ3/0iMHj260eru\n0qUL3NzcDOIAt2/fRlVVVbO3AJpcGqig6dG7d+9GmWPQFPD29tZs6WfttdudnZ2NuoDKy8tRWVlp\n/RFodDR3AV282GQEoDFxcHBA9+7dDTKBGrIMx92EsAAEtbJ161ZbN6HR0O70m5ILyNxlCCxOdDSw\nfDl/bQcCAPA4wLp160BEmu0mGzoJ725BWAACu0ZbAKxtAZhyATV0GYh6ox0ItSMBKCwsxNWrVzXH\nbCbAVkYIgMCu0R7128ICMOYCspkASDtsAXYlAABw8uRJzTF7cQEJARDYNcIFpIeXFxAcDLRqxZeH\nsAOioqLAGNMJBNuLBSBiAAK7RriAZBg3DlB3gPaAh4cHQkNDZQWguccAhAAI7BpbWwDGXEA2HYF+\n+KH172ljYmJi8Ndff2n+LigogLOzM9zN3GT+bkW4gAR2jdTpOzo6Wv3LbsoFVFBQAIVCAS8vL6u2\nyV6JiYnBlStXNJaXNAejuW9bIgRAYNdIO4B5e3tb/ctuygUkzUJVKMRX1BpIgWDJDWQPs4ABIQAC\nO8fR0RFeXl5Wd/8AtWcB2UMH1FSQE4DmHgAGhAAIBMY3G29kassCEgJgPdq0aYOAgACNANjDZjCA\nEACBAN7e3jaxAGrLArKHDqgpERMTY3cWgMgCEtg98+bNs0mwtTYXUHBwsJVbZN/06NEDu3btQkVF\nhd1YYEIABHbPtGnTbHLf2lxA9jACbUpIewMkJSXh9u3bdvH8hQtIILARxlxAKpUKhYWFdjECbUpI\ngeB9+/YBaP6zgAEhAAKBzTDmAiouLoZKpbKLDqgpERwcDE9PT+zduxdA858FDAgBEAhshjEXkE2X\ngbBjFAoFoqOjcfDgQQDCAhAIBI2IMReQvaxD0xSJiYlBeXk5ACEAAoGgETHmArKXpYibItr7AtvD\n829QFhBj7N8ARgCoBJAGYBoRFcqUuwygBEA1gCoiimvIfQWC5oCzszNUKhWqq6vh4OCgOS5cQLaj\nR48emtf28PwbagHsAhBJRN0BpAB4zUTZAUQUIzp/gYDj5OQEAAZuIHtZi74pEhERoRFjW8wOtzYN\nEgAi2klEVeo/DwO4p+FNEgjsA2dnZwAwcAMJC8B2uLq6Ijw8HD4+PjpWWXPFkjGApwD8ZuQcAdjJ\nGDvOGJtlqhLG2CzG2DHG2LHc3FwLNk8gaFpIAiBnAbi4uMDNzc0WzbJ7+vfvj86dO9u6GVah1hgA\nY2w3gLYyp94gol/UZd4AUAXgeyPVxBPRDcZYawC7GGMXiOh3uYJEtBzAcgCIi4sjM96DQHBXYswF\nJK0E2tzXom+qfPrpp0ZnaDc3ahUAIhpk6jxj7G8AhgMYSESyHTYR3VD/zmGM/QygFwBZARAI7AVj\nLiB7WYemqeLq6gpXV1dbN8MqNMgFxBgbCuBlACOJqMxIGQ/GmJf0GsDDAM405L4CQXPAmAtI7AUg\nsBYNjQH8F4AXuFsniTG2FAAYY+0YY9vUZdoAOMgYSwaQCOBXItrewPsKBHc9xlxAxcXFdpGBIrA9\nDZoHQEQhRo7fBJCgfp0OILoh9xEImiPGXEBFRUXo0qWLLZoksDPETGCBwEYYcwEVFxdr9ioWCBoT\nIQACgY0w5QISAiCwBkIABAIbIecCqqysxJ07d2yyRaXA/hACIBDYCDkXUElJCQAIC0BgFYQACAQ2\nQs4FVFxcDEAIgMA6CAEQCGyEnAuoqKgIgBAAgXUQAiAQ2Ag5F5CwAATWRAiAQGAjTLmARBBYYA2E\nAAgENkLOBSQsAIE1EQIgENgI4QIS2BohAAKBjZBzAYkgsMCaCAEQCGyEMReQg4MD3N3dbdUsgR0h\nBEAgsBHGXEAtWrQQm8EIrIIQAIHARhjLAhLuH4G1EAIgENgIxhgcHR0NXEBCAATWQgiAQGBDnJ2d\nhQUgsBlCAAQCG+Lk5GSQBSQEQGAthAAIBDbE2dnZwAUkZgELrIUQAIHAhggXkMCWCAEQCGyIvgtI\nCIDAmggBEAhsiLYLSKlUory8XAiAwGo0SAAYY+8wxm4wxpLUPwlGyg1ljF1kjKUyxl5tyD0FguaE\ntgtIrAMksDaOFqhjIRF9YuwkY8wBwBcABgO4DuAoY2wzEZ2zwL0FgrsabReQWApaYG2s4QLqBSCV\niNKJqBLAWgCjrHBfgaDJo+0CEhaAwNpYQgDmMsZOMcZWMsZ8Zc63B3BN6+/r6mOyMMZmMcaOMcaO\n5ebmWqB5AkHTRbiABLakVgFgjO1m/9/e/cVYcdZhHP8+u+zxz9qU1iJQAUElNFzItt1gG9FYxKYl\npqIxCjGmJk3woiZtQmJKmph4qUnVXpgmWKs3BhurtQRJW0qbGL2AQgu6FJCqmEIpW0CouImR5efF\nvEdPTnZZYDbzDmeeT3KyM3MOZ57swD77vjOHkUYmeHweeAz4CDAEHAceKRsoIjZFxHBEDM+aNavs\n25nV2kRTQC4Aq8qU5wAiYtWlvJGkHwNbJ3jqGDC/Y31e2mbWeK1Wi3PnzgEuAKte2auA5nasfgEY\nmeBlLwOLJS2S1ALWAlvK7NesV3ROAbVvBuOTwFaVslcBfU/SEBDAEeAbAJJuBB6PiNURcV7SN4Hn\ngH7giYjYX3K/Zj3BU0CWU6kCiIivTbL9TWB1x/o2YFuZfZn1ou6rgPr6+nw3MKuMPwlsllH3VUC+\nG5hVyQVgllH3FJCnf6xKLgCzjDqngM6ePesTwFYpF4BZRhNNAZlVxQVglpGngCwnF4BZRt1XAbkA\nrEouALOMWq0W4+PjjI+PuwCsci4As4xarRZQ3AzG9wO2qrkAzDIaGBgAYGxsjLGxMY8ArFIuALOM\n2iOAU6dOAf5vIKxaLgCzjNoFcPLkScAFYNVyAZhl1J4C8gjAcnABmGXUPQLwSWCrkgvALCNPAVlO\nLgCzjNpTQC4Ay8EFYJaRRwCWkwvALCMXgOXkAjDLqPMqoL6+PgYHBzMnsiZxAZhl1DkC8N3ArGou\nALOMugvArEqlbgov6UlgSVqdCZyJiKEJXncE+CcwDpyPiOEy+zXrFe0poNOnTzNnzpzMaaxpShVA\nRHylvSzpEeDsRV5+R0ScLLM/s17THgFcuHDBIwCrXKkCaFMxcfllYOV0vJ9ZU7QLAPwpYKvedJ0D\n+CRwIiIOT/J8AM9L2iNp/TTt0+yq154CAl8CatWbcgQg6QVgosnJhyPimbS8Dth8kbdZERHHJH0A\n2C7pYET8bpL9rQfWAyxYsGCqeGZXtc4RgAvAqjZlAUTEqos9L2kG8EXg1ou8x7H0dVTS08ByYMIC\niIhNwCaA4eHhmCqf2dXMBWA5TccU0CrgYEQcnehJSYOSrmkvA3cCI9OwX7OrnqeALKfpKIC1dE3/\nSLpR0ra0Ohv4vaR9wC7gtxHx7DTs1+yq55PAllPpq4Ai4usTbHsTWJ2W/wosK7sfs17kEYDl5E8C\nm2XU19dHf38/4AKw6rkAzDJrTwO5AKxqLgCzzFwAlosLwCyz9nkAnwS2qrkAzDLzCMBycQGYZeYC\nsFxcAGaZDQwMIMl3A7PKuQDMMmu1Wr4bmGXhAjDLrNVq+QSwZeECMMtsYGDA8/+WxbTcEMbMrlyr\n1frfp4HNquQCMMtsw4YNuSNYQ7kAzDJbs2ZN7gjWUD4HYGbWUC4AM7OGcgGYmTWUC8DMrKFcAGZm\nDeUCMDNrKBeAmVlDuQDMzBpKEZE7w6QkvQ38/Qr/+A3AyWmMM92crxznK8f5yqlzvg9FxKxLeWGt\nC6AMSbsjYjh3jsk4XznOV47zlVP3fJfKU0BmZg3lAjAza6heLoBNuQNMwfnKcb5ynK+cuue7JD17\nDsDMzC6ul0cAZmZ2ET1XAJLuknRI0uuSHsqdB0DSE5JGJY10bLte0nZJh9PX6zJlmy/pJUmvSdov\n6YGa5Xu3pF2S9qV830nbF0namY7zk5JaOfJ15OyX9KqkrTXNd0TSnyTtlbQ7bavFMU5ZZkp6StJB\nSQck3V6XfJKWpO9b+/GOpAfrkq+MnioASf3Aj4C7gaXAOklL86YC4GfAXV3bHgJ2RMRiYEdaz+E8\nsCEilgK3Afen71ld8v0bWBkRy4Ah4C5JtwHfBX4QER8F/gHclylf2wPAgY71uuUDuCMihjouX6zL\nMQZ4FHg2Im4CllF8L2uRLyIOpe/bEHArMAY8XZd8pUREzzyA24HnOtY3Ahtz50pZFgIjHeuHgLlp\neS5wKHfGlOUZ4LN1zAe8F3gF+DjFh3BmTHTcM+SaR/EDYCWwFVCd8qUMR4AburbV4hgD1wJ/I52T\nrFu+rkx3An+oa77LffTUCAD4IPBGx/rRtK2OZkfE8bT8FjA7ZxgASQuBm4Gd1Chfml7ZC4wC24G/\nAGci4nx6Se7j/EPgW8CFtP5+6pUPIIDnJe2RtD5tq8sxXgS8Dfw0TaM9LmmwRvk6rQU2p+U65rss\nvVYAV6UofoXIejmWpPcBvwIejIh3Op/LnS8ixqMYfs8DlgM35crSTdLngNGI2JM7yxRWRMQtFNOj\n90v6VOeTmY/xDOAW4LGIuBn4F13TKbn/DgKk8zj3AL/sfq4O+a5ErxXAMWB+x/q8tK2OTkiaC5C+\njuYKImmA4of/zyPi13XL1xYRZ4CXKKZUZkqakZ7KeZw/Adwj6QjwC4ppoEepTz4AIuJY+jpKMX+9\nnPoc46PA0YjYmdafoiiEuuRruxt4JSJOpPW65btsvVYALwOL0xUYLYrh2pbMmSazBbg3Ld9LMfde\nOUkCfgIciIjvdzxVl3yzJM1My++hOD9xgKIIvpQ7X0RsjIh5EbGQ4u/bixHx1brkA5A0KOma9jLF\nPPYINTnGEfEW8IakJWnTZ4DXqEm+Duv4//QP1C/f5ct9EmK6H8Bq4M8U88QP586TMm0GjgP/ofht\n5z6KeeIdwGHgBeD6TNlWUAxd/wjsTY/VNcr3MeDVlG8E+Hba/mFgF/A6xZD8XTU4zp8GttYtX8qy\nLz32t/9d1OUYpyxDwO50nH8DXFezfIPAKeDajm21yXelD38S2MysoXptCsjMzC6RC8DMrKFcAGZm\nDeUCMDNrKBeAmVlDuQDMzBrKBWBm1lAuADOzhvov/O8U4V5JyzIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd3c2320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.6687783836 \n",
      "Updating scheme MAE:  1.91099057012\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_test_fixed.csv\", yhat_test_fixed)\n",
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_update.csv\", yhat_update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
