{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"1Q/32_units/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 500\n",
    "learning_rate = 1e-4\n",
    "batch_size = 5\n",
    "early_stop_iters = 10\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 32 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag1',\n",
    "                                       'inflation.lag2',\n",
    "                                       'inflation.lag3',\n",
    "                                       'inflation.lag4']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag1',\n",
    "                                   'unemp.lag2',\n",
    "                                   'unemp.lag3',\n",
    "                                   'unemp.lag4']])\n",
    "train_4lag_oil = np.array(train[['oil.lag1',\n",
    "                                 'oil.lag2',\n",
    "                                 'oil.lag3',\n",
    "                                 'oil.lag4']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag1',\n",
    "                                     'inflation.lag2',\n",
    "                                     'inflation.lag3',\n",
    "                                     'inflation.lag4']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag1',\n",
    "                                 'unemp.lag2',\n",
    "                                 'unemp.lag3',\n",
    "                                 'unemp.lag4']])\n",
    "test_4lag_oil = np.array(test[['oil.lag1',\n",
    "                               'oil.lag2',\n",
    "                               'oil.lag3',\n",
    "                               'oil.lag4']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 32 \n",
      "Learning rate = 0.0001 \n",
      "Epochs = 500 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 10 \n",
      "Learning rate = 0.0001\n",
      "Fold: 1  Epoch: 1  Training loss = 3.5276  Validation loss = 3.9513  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 3.4989  Validation loss = 3.9028  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 3.4687  Validation loss = 3.8496  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 3.4416  Validation loss = 3.8011  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 3.4182  Validation loss = 3.7584  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 3.3955  Validation loss = 3.7163  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 3.3789  Validation loss = 3.6858  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 3.3560  Validation loss = 3.6428  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 3.3317  Validation loss = 3.5977  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 3.3127  Validation loss = 3.5618  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 3.2893  Validation loss = 3.5174  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 3.2609  Validation loss = 3.4640  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 3.2360  Validation loss = 3.4174  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 3.2178  Validation loss = 3.3838  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 3.2001  Validation loss = 3.3506  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 3.1824  Validation loss = 3.3177  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 3.1657  Validation loss = 3.2867  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 3.1539  Validation loss = 3.2635  \n",
      "\n",
      "Fold: 1  Epoch: 19  Training loss = 3.1360  Validation loss = 3.2295  \n",
      "\n",
      "Fold: 1  Epoch: 20  Training loss = 3.1218  Validation loss = 3.2031  \n",
      "\n",
      "Fold: 1  Epoch: 21  Training loss = 3.1108  Validation loss = 3.1820  \n",
      "\n",
      "Fold: 1  Epoch: 22  Training loss = 3.1001  Validation loss = 3.1607  \n",
      "\n",
      "Fold: 1  Epoch: 23  Training loss = 3.0840  Validation loss = 3.1291  \n",
      "\n",
      "Fold: 1  Epoch: 24  Training loss = 3.0698  Validation loss = 3.1019  \n",
      "\n",
      "Fold: 1  Epoch: 25  Training loss = 3.0632  Validation loss = 3.0878  \n",
      "\n",
      "Fold: 1  Epoch: 26  Training loss = 3.0537  Validation loss = 3.0684  \n",
      "\n",
      "Fold: 1  Epoch: 27  Training loss = 3.0440  Validation loss = 3.0482  \n",
      "\n",
      "Fold: 1  Epoch: 28  Training loss = 3.0336  Validation loss = 3.0254  \n",
      "\n",
      "Fold: 1  Epoch: 29  Training loss = 3.0256  Validation loss = 3.0082  \n",
      "\n",
      "Fold: 1  Epoch: 30  Training loss = 3.0192  Validation loss = 2.9945  \n",
      "\n",
      "Fold: 1  Epoch: 31  Training loss = 3.0090  Validation loss = 2.9716  \n",
      "\n",
      "Fold: 1  Epoch: 32  Training loss = 2.9990  Validation loss = 2.9490  \n",
      "\n",
      "Fold: 1  Epoch: 33  Training loss = 2.9925  Validation loss = 2.9346  \n",
      "\n",
      "Fold: 1  Epoch: 34  Training loss = 2.9850  Validation loss = 2.9167  \n",
      "\n",
      "Fold: 1  Epoch: 35  Training loss = 2.9792  Validation loss = 2.9028  \n",
      "\n",
      "Fold: 1  Epoch: 36  Training loss = 2.9678  Validation loss = 2.8757  \n",
      "\n",
      "Fold: 1  Epoch: 37  Training loss = 2.9597  Validation loss = 2.8565  \n",
      "\n",
      "Fold: 1  Epoch: 38  Training loss = 2.9528  Validation loss = 2.8401  \n",
      "\n",
      "Fold: 1  Epoch: 39  Training loss = 2.9420  Validation loss = 2.8139  \n",
      "\n",
      "Fold: 1  Epoch: 40  Training loss = 2.9327  Validation loss = 2.7913  \n",
      "\n",
      "Fold: 1  Epoch: 41  Training loss = 2.9246  Validation loss = 2.7700  \n",
      "\n",
      "Fold: 1  Epoch: 42  Training loss = 2.9187  Validation loss = 2.7550  \n",
      "\n",
      "Fold: 1  Epoch: 43  Training loss = 2.9119  Validation loss = 2.7371  \n",
      "\n",
      "Fold: 1  Epoch: 44  Training loss = 2.9058  Validation loss = 2.7210  \n",
      "\n",
      "Fold: 1  Epoch: 45  Training loss = 2.9001  Validation loss = 2.7058  \n",
      "\n",
      "Fold: 1  Epoch: 46  Training loss = 2.8924  Validation loss = 2.6850  \n",
      "\n",
      "Fold: 1  Epoch: 47  Training loss = 2.8869  Validation loss = 2.6696  \n",
      "\n",
      "Fold: 1  Epoch: 48  Training loss = 2.8814  Validation loss = 2.6546  \n",
      "\n",
      "Fold: 1  Epoch: 49  Training loss = 2.8745  Validation loss = 2.6352  \n",
      "\n",
      "Fold: 1  Epoch: 50  Training loss = 2.8685  Validation loss = 2.6183  \n",
      "\n",
      "Fold: 1  Epoch: 51  Training loss = 2.8609  Validation loss = 2.5966  \n",
      "\n",
      "Fold: 1  Epoch: 52  Training loss = 2.8550  Validation loss = 2.5791  \n",
      "\n",
      "Fold: 1  Epoch: 53  Training loss = 2.8527  Validation loss = 2.5720  \n",
      "\n",
      "Fold: 1  Epoch: 54  Training loss = 2.8475  Validation loss = 2.5570  \n",
      "\n",
      "Fold: 1  Epoch: 55  Training loss = 2.8459  Validation loss = 2.5523  \n",
      "\n",
      "Fold: 1  Epoch: 56  Training loss = 2.8417  Validation loss = 2.5400  \n",
      "\n",
      "Fold: 1  Epoch: 57  Training loss = 2.8376  Validation loss = 2.5276  \n",
      "\n",
      "Fold: 1  Epoch: 58  Training loss = 2.8333  Validation loss = 2.5144  \n",
      "\n",
      "Fold: 1  Epoch: 59  Training loss = 2.8288  Validation loss = 2.4998  \n",
      "\n",
      "Fold: 1  Epoch: 60  Training loss = 2.8270  Validation loss = 2.4944  \n",
      "\n",
      "Fold: 1  Epoch: 61  Training loss = 2.8255  Validation loss = 2.4895  \n",
      "\n",
      "Fold: 1  Epoch: 62  Training loss = 2.8224  Validation loss = 2.4805  \n",
      "\n",
      "Fold: 1  Epoch: 63  Training loss = 2.8191  Validation loss = 2.4691  \n",
      "\n",
      "Fold: 1  Epoch: 64  Training loss = 2.8149  Validation loss = 2.4551  \n",
      "\n",
      "Fold: 1  Epoch: 65  Training loss = 2.8090  Validation loss = 2.4358  \n",
      "\n",
      "Fold: 1  Epoch: 66  Training loss = 2.8074  Validation loss = 2.4300  \n",
      "\n",
      "Fold: 1  Epoch: 67  Training loss = 2.8044  Validation loss = 2.4197  \n",
      "\n",
      "Fold: 1  Epoch: 68  Training loss = 2.7992  Validation loss = 2.4019  \n",
      "\n",
      "Fold: 1  Epoch: 69  Training loss = 2.7939  Validation loss = 2.3834  \n",
      "\n",
      "Fold: 1  Epoch: 70  Training loss = 2.7916  Validation loss = 2.3756  \n",
      "\n",
      "Fold: 1  Epoch: 71  Training loss = 2.7892  Validation loss = 2.3675  \n",
      "\n",
      "Fold: 1  Epoch: 72  Training loss = 2.7866  Validation loss = 2.3586  \n",
      "\n",
      "Fold: 1  Epoch: 73  Training loss = 2.7808  Validation loss = 2.3368  \n",
      "\n",
      "Fold: 1  Epoch: 74  Training loss = 2.7780  Validation loss = 2.3270  \n",
      "\n",
      "Fold: 1  Epoch: 75  Training loss = 2.7753  Validation loss = 2.3167  \n",
      "\n",
      "Fold: 1  Epoch: 76  Training loss = 2.7718  Validation loss = 2.3039  \n",
      "\n",
      "Fold: 1  Epoch: 77  Training loss = 2.7709  Validation loss = 2.3003  \n",
      "\n",
      "Fold: 1  Epoch: 78  Training loss = 2.7674  Validation loss = 2.2864  \n",
      "\n",
      "Fold: 1  Epoch: 79  Training loss = 2.7652  Validation loss = 2.2774  \n",
      "\n",
      "Fold: 1  Epoch: 80  Training loss = 2.7623  Validation loss = 2.2662  \n",
      "\n",
      "Fold: 1  Epoch: 81  Training loss = 2.7589  Validation loss = 2.2525  \n",
      "\n",
      "Fold: 1  Epoch: 82  Training loss = 2.7583  Validation loss = 2.2506  \n",
      "\n",
      "Fold: 1  Epoch: 83  Training loss = 2.7555  Validation loss = 2.2401  \n",
      "\n",
      "Fold: 1  Epoch: 84  Training loss = 2.7540  Validation loss = 2.2335  \n",
      "\n",
      "Fold: 1  Epoch: 85  Training loss = 2.7528  Validation loss = 2.2289  \n",
      "\n",
      "Fold: 1  Epoch: 86  Training loss = 2.7500  Validation loss = 2.2169  \n",
      "\n",
      "Fold: 1  Epoch: 87  Training loss = 2.7474  Validation loss = 2.2061  \n",
      "\n",
      "Fold: 1  Epoch: 88  Training loss = 2.7454  Validation loss = 2.1974  \n",
      "\n",
      "Fold: 1  Epoch: 89  Training loss = 2.7435  Validation loss = 2.1894  \n",
      "\n",
      "Fold: 1  Epoch: 90  Training loss = 2.7435  Validation loss = 2.1889  \n",
      "\n",
      "Fold: 1  Epoch: 91  Training loss = 2.7393  Validation loss = 2.1699  \n",
      "\n",
      "Fold: 1  Epoch: 92  Training loss = 2.7383  Validation loss = 2.1662  \n",
      "\n",
      "Fold: 1  Epoch: 93  Training loss = 2.7379  Validation loss = 2.1643  \n",
      "\n",
      "Fold: 1  Epoch: 94  Training loss = 2.7353  Validation loss = 2.1516  \n",
      "\n",
      "Fold: 1  Epoch: 95  Training loss = 2.7343  Validation loss = 2.1471  \n",
      "\n",
      "Fold: 1  Epoch: 96  Training loss = 2.7317  Validation loss = 2.1355  \n",
      "\n",
      "Fold: 1  Epoch: 97  Training loss = 2.7284  Validation loss = 2.1199  \n",
      "\n",
      "Fold: 1  Epoch: 98  Training loss = 2.7259  Validation loss = 2.1068  \n",
      "\n",
      "Fold: 1  Epoch: 99  Training loss = 2.7231  Validation loss = 2.0935  \n",
      "\n",
      "Fold: 1  Epoch: 100  Training loss = 2.7221  Validation loss = 2.0876  \n",
      "\n",
      "Fold: 1  Epoch: 101  Training loss = 2.7196  Validation loss = 2.0741  \n",
      "\n",
      "Fold: 1  Epoch: 102  Training loss = 2.7185  Validation loss = 2.0698  \n",
      "\n",
      "Fold: 1  Epoch: 103  Training loss = 2.7175  Validation loss = 2.0651  \n",
      "\n",
      "Fold: 1  Epoch: 104  Training loss = 2.7136  Validation loss = 2.0449  \n",
      "\n",
      "Fold: 1  Epoch: 105  Training loss = 2.7120  Validation loss = 2.0364  \n",
      "\n",
      "Fold: 1  Epoch: 106  Training loss = 2.7104  Validation loss = 2.0280  \n",
      "\n",
      "Fold: 1  Epoch: 107  Training loss = 2.7081  Validation loss = 2.0156  \n",
      "\n",
      "Fold: 1  Epoch: 108  Training loss = 2.7073  Validation loss = 2.0107  \n",
      "\n",
      "Fold: 1  Epoch: 109  Training loss = 2.7064  Validation loss = 2.0066  \n",
      "\n",
      "Fold: 1  Epoch: 110  Training loss = 2.7045  Validation loss = 1.9949  \n",
      "\n",
      "Fold: 1  Epoch: 111  Training loss = 2.7026  Validation loss = 1.9823  \n",
      "\n",
      "Fold: 1  Epoch: 112  Training loss = 2.7007  Validation loss = 1.9712  \n",
      "\n",
      "Fold: 1  Epoch: 113  Training loss = 2.6994  Validation loss = 1.9630  \n",
      "\n",
      "Fold: 1  Epoch: 114  Training loss = 2.6974  Validation loss = 1.9515  \n",
      "\n",
      "Fold: 1  Epoch: 115  Training loss = 2.6963  Validation loss = 1.9460  \n",
      "\n",
      "Fold: 1  Epoch: 116  Training loss = 2.6933  Validation loss = 1.9261  \n",
      "\n",
      "Fold: 1  Epoch: 117  Training loss = 2.6912  Validation loss = 1.9099  \n",
      "\n",
      "Fold: 1  Epoch: 118  Training loss = 2.6897  Validation loss = 1.8979  \n",
      "\n",
      "Fold: 1  Epoch: 119  Training loss = 2.6880  Validation loss = 1.8871  \n",
      "\n",
      "Fold: 1  Epoch: 120  Training loss = 2.6872  Validation loss = 1.8818  \n",
      "\n",
      "Fold: 1  Epoch: 121  Training loss = 2.6873  Validation loss = 1.8830  \n",
      "\n",
      "Fold: 1  Epoch: 122  Training loss = 2.6870  Validation loss = 1.8849  \n",
      "\n",
      "Fold: 1  Epoch: 123  Training loss = 2.6851  Validation loss = 1.8712  \n",
      "\n",
      "Fold: 1  Epoch: 124  Training loss = 2.6836  Validation loss = 1.8600  \n",
      "\n",
      "Fold: 1  Epoch: 125  Training loss = 2.6825  Validation loss = 1.8521  \n",
      "\n",
      "Fold: 1  Epoch: 126  Training loss = 2.6819  Validation loss = 1.8465  \n",
      "\n",
      "Fold: 1  Epoch: 127  Training loss = 2.6806  Validation loss = 1.8358  \n",
      "\n",
      "Fold: 1  Epoch: 128  Training loss = 2.6803  Validation loss = 1.8343  \n",
      "\n",
      "Fold: 1  Epoch: 129  Training loss = 2.6786  Validation loss = 1.8206  \n",
      "\n",
      "Fold: 1  Epoch: 130  Training loss = 2.6770  Validation loss = 1.8043  \n",
      "\n",
      "Fold: 1  Epoch: 131  Training loss = 2.6760  Validation loss = 1.7964  \n",
      "\n",
      "Fold: 1  Epoch: 132  Training loss = 2.6753  Validation loss = 1.7917  \n",
      "\n",
      "Fold: 1  Epoch: 133  Training loss = 2.6754  Validation loss = 1.7945  \n",
      "\n",
      "Fold: 1  Epoch: 134  Training loss = 2.6744  Validation loss = 1.7854  \n",
      "\n",
      "Fold: 1  Epoch: 135  Training loss = 2.6738  Validation loss = 1.7841  \n",
      "\n",
      "Fold: 1  Epoch: 136  Training loss = 2.6735  Validation loss = 1.7812  \n",
      "\n",
      "Fold: 1  Epoch: 137  Training loss = 2.6731  Validation loss = 1.7785  \n",
      "\n",
      "Fold: 1  Epoch: 138  Training loss = 2.6724  Validation loss = 1.7728  \n",
      "\n",
      "Fold: 1  Epoch: 139  Training loss = 2.6717  Validation loss = 1.7668  \n",
      "\n",
      "Fold: 1  Epoch: 140  Training loss = 2.6714  Validation loss = 1.7684  \n",
      "\n",
      "Fold: 1  Epoch: 141  Training loss = 2.6704  Validation loss = 1.7586  \n",
      "\n",
      "Fold: 1  Epoch: 142  Training loss = 2.6701  Validation loss = 1.7561  \n",
      "\n",
      "Fold: 1  Epoch: 143  Training loss = 2.6695  Validation loss = 1.7521  \n",
      "\n",
      "Fold: 1  Epoch: 144  Training loss = 2.6696  Validation loss = 1.7612  \n",
      "\n",
      "Fold: 1  Epoch: 145  Training loss = 2.6693  Validation loss = 1.7608  \n",
      "\n",
      "Fold: 1  Epoch: 146  Training loss = 2.6691  Validation loss = 1.7594  \n",
      "\n",
      "Fold: 1  Epoch: 147  Training loss = 2.6683  Validation loss = 1.7564  \n",
      "\n",
      "Fold: 1  Epoch: 148  Training loss = 2.6675  Validation loss = 1.7501  \n",
      "\n",
      "Fold: 1  Epoch: 149  Training loss = 2.6672  Validation loss = 1.7483  \n",
      "\n",
      "Fold: 1  Epoch: 150  Training loss = 2.6669  Validation loss = 1.7473  \n",
      "\n",
      "Fold: 1  Epoch: 151  Training loss = 2.6668  Validation loss = 1.7522  \n",
      "\n",
      "Fold: 1  Epoch: 152  Training loss = 2.6662  Validation loss = 1.7478  \n",
      "\n",
      "Fold: 1  Epoch: 153  Training loss = 2.6660  Validation loss = 1.7496  \n",
      "\n",
      "Fold: 1  Epoch: 154  Training loss = 2.6652  Validation loss = 1.7438  \n",
      "\n",
      "Fold: 1  Epoch: 155  Training loss = 2.6649  Validation loss = 1.7423  \n",
      "\n",
      "Fold: 1  Epoch: 156  Training loss = 2.6646  Validation loss = 1.7425  \n",
      "\n",
      "Fold: 1  Epoch: 157  Training loss = 2.6638  Validation loss = 1.7375  \n",
      "\n",
      "Fold: 1  Epoch: 158  Training loss = 2.6627  Validation loss = 1.7240  \n",
      "\n",
      "Fold: 1  Epoch: 159  Training loss = 2.6621  Validation loss = 1.7207  \n",
      "\n",
      "Fold: 1  Epoch: 160  Training loss = 2.6617  Validation loss = 1.7196  \n",
      "\n",
      "Fold: 1  Epoch: 161  Training loss = 2.6612  Validation loss = 1.7127  \n",
      "\n",
      "Fold: 1  Epoch: 162  Training loss = 2.6609  Validation loss = 1.7121  \n",
      "\n",
      "Fold: 1  Epoch: 163  Training loss = 2.6606  Validation loss = 1.7083  \n",
      "\n",
      "Fold: 1  Epoch: 164  Training loss = 2.6603  Validation loss = 1.7048  \n",
      "\n",
      "Fold: 1  Epoch: 165  Training loss = 2.6593  Validation loss = 1.6953  \n",
      "\n",
      "Fold: 1  Epoch: 166  Training loss = 2.6592  Validation loss = 1.6980  \n",
      "\n",
      "Fold: 1  Epoch: 167  Training loss = 2.6589  Validation loss = 1.6996  \n",
      "\n",
      "Fold: 1  Epoch: 168  Training loss = 2.6584  Validation loss = 1.6958  \n",
      "\n",
      "Fold: 1  Epoch: 169  Training loss = 2.6578  Validation loss = 1.6877  \n",
      "\n",
      "Fold: 1  Epoch: 170  Training loss = 2.6576  Validation loss = 1.6886  \n",
      "\n",
      "Fold: 1  Epoch: 171  Training loss = 2.6571  Validation loss = 1.6805  \n",
      "\n",
      "Fold: 1  Epoch: 172  Training loss = 2.6565  Validation loss = 1.6727  \n",
      "\n",
      "Fold: 1  Epoch: 173  Training loss = 2.6562  Validation loss = 1.6730  \n",
      "\n",
      "Fold: 1  Epoch: 174  Training loss = 2.6558  Validation loss = 1.6726  \n",
      "\n",
      "Fold: 1  Epoch: 175  Training loss = 2.6554  Validation loss = 1.6710  \n",
      "\n",
      "Fold: 1  Epoch: 176  Training loss = 2.6552  Validation loss = 1.6707  \n",
      "\n",
      "Fold: 1  Epoch: 177  Training loss = 2.6546  Validation loss = 1.6672  \n",
      "\n",
      "Fold: 1  Epoch: 178  Training loss = 2.6542  Validation loss = 1.6658  \n",
      "\n",
      "Fold: 1  Epoch: 179  Training loss = 2.6536  Validation loss = 1.6571  \n",
      "\n",
      "Fold: 1  Epoch: 180  Training loss = 2.6530  Validation loss = 1.6478  \n",
      "\n",
      "Fold: 1  Epoch: 181  Training loss = 2.6527  Validation loss = 1.6451  \n",
      "\n",
      "Fold: 1  Epoch: 182  Training loss = 2.6524  Validation loss = 1.6439  \n",
      "\n",
      "Fold: 1  Epoch: 183  Training loss = 2.6527  Validation loss = 1.6488  \n",
      "\n",
      "Fold: 1  Epoch: 184  Training loss = 2.6528  Validation loss = 1.6519  \n",
      "\n",
      "Fold: 1  Epoch: 185  Training loss = 2.6527  Validation loss = 1.6592  \n",
      "\n",
      "Fold: 1  Epoch: 186  Training loss = 2.6522  Validation loss = 1.6542  \n",
      "\n",
      "Fold: 1  Epoch: 187  Training loss = 2.6520  Validation loss = 1.6549  \n",
      "\n",
      "Fold: 1  Epoch: 188  Training loss = 2.6519  Validation loss = 1.6502  \n",
      "\n",
      "Fold: 1  Epoch: 189  Training loss = 2.6516  Validation loss = 1.6478  \n",
      "\n",
      "Fold: 1  Epoch: 190  Training loss = 2.6514  Validation loss = 1.6475  \n",
      "\n",
      "Fold: 1  Epoch: 191  Training loss = 2.6503  Validation loss = 1.6359  \n",
      "\n",
      "Fold: 1  Epoch: 192  Training loss = 2.6499  Validation loss = 1.6321  \n",
      "\n",
      "Fold: 1  Epoch: 193  Training loss = 2.6497  Validation loss = 1.6284  \n",
      "\n",
      "Fold: 1  Epoch: 194  Training loss = 2.6497  Validation loss = 1.6326  \n",
      "\n",
      "Fold: 1  Epoch: 195  Training loss = 2.6490  Validation loss = 1.6219  \n",
      "\n",
      "Fold: 1  Epoch: 196  Training loss = 2.6482  Validation loss = 1.6133  \n",
      "\n",
      "Fold: 1  Epoch: 197  Training loss = 2.6479  Validation loss = 1.6107  \n",
      "\n",
      "Fold: 1  Epoch: 198  Training loss = 2.6476  Validation loss = 1.6032  \n",
      "\n",
      "Fold: 1  Epoch: 199  Training loss = 2.6471  Validation loss = 1.6053  \n",
      "\n",
      "Fold: 1  Epoch: 200  Training loss = 2.6470  Validation loss = 1.6062  \n",
      "\n",
      "Fold: 1  Epoch: 201  Training loss = 2.6469  Validation loss = 1.6171  \n",
      "\n",
      "Fold: 1  Epoch: 202  Training loss = 2.6468  Validation loss = 1.6158  \n",
      "\n",
      "Fold: 1  Epoch: 203  Training loss = 2.6468  Validation loss = 1.6199  \n",
      "\n",
      "Fold: 1  Epoch: 204  Training loss = 2.6466  Validation loss = 1.6212  \n",
      "\n",
      "Fold: 1  Epoch: 205  Training loss = 2.6463  Validation loss = 1.6187  \n",
      "\n",
      "Fold: 1  Epoch: 206  Training loss = 2.6460  Validation loss = 1.6092  \n",
      "\n",
      "Fold: 1  Epoch: 207  Training loss = 2.6456  Validation loss = 1.6042  \n",
      "\n",
      "Fold: 1  Epoch: 208  Training loss = 2.6450  Validation loss = 1.6078  \n",
      "\n",
      "Fold: 1  Epoch: 209  Training loss = 2.6445  Validation loss = 1.6008  \n",
      "\n",
      "Fold: 1  Epoch: 210  Training loss = 2.6441  Validation loss = 1.5983  \n",
      "\n",
      "Fold: 1  Epoch: 211  Training loss = 2.6436  Validation loss = 1.5884  \n",
      "\n",
      "Fold: 1  Epoch: 212  Training loss = 2.6433  Validation loss = 1.5867  \n",
      "\n",
      "Fold: 1  Epoch: 213  Training loss = 2.6428  Validation loss = 1.5856  \n",
      "\n",
      "Fold: 1  Epoch: 214  Training loss = 2.6425  Validation loss = 1.5794  \n",
      "\n",
      "Fold: 1  Epoch: 215  Training loss = 2.6421  Validation loss = 1.5611  \n",
      "\n",
      "Fold: 1  Epoch: 216  Training loss = 2.6418  Validation loss = 1.5607  \n",
      "\n",
      "Fold: 1  Epoch: 217  Training loss = 2.6413  Validation loss = 1.5588  \n",
      "\n",
      "Fold: 1  Epoch: 218  Training loss = 2.6410  Validation loss = 1.5578  \n",
      "\n",
      "Fold: 1  Epoch: 219  Training loss = 2.6410  Validation loss = 1.5425  \n",
      "\n",
      "Fold: 1  Epoch: 220  Training loss = 2.6406  Validation loss = 1.5578  \n",
      "\n",
      "Fold: 1  Epoch: 221  Training loss = 2.6403  Validation loss = 1.5515  \n",
      "\n",
      "Fold: 1  Epoch: 222  Training loss = 2.6395  Validation loss = 1.5566  \n",
      "\n",
      "Fold: 1  Epoch: 223  Training loss = 2.6394  Validation loss = 1.5584  \n",
      "\n",
      "Fold: 1  Epoch: 224  Training loss = 2.6392  Validation loss = 1.5545  \n",
      "\n",
      "Fold: 1  Epoch: 225  Training loss = 2.6387  Validation loss = 1.5592  \n",
      "\n",
      "Fold: 1  Epoch: 226  Training loss = 2.6385  Validation loss = 1.5723  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 219  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.5333  Validation loss = 2.0627  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.5325  Validation loss = 2.0578  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.5322  Validation loss = 2.0632  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.5318  Validation loss = 2.0630  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.5314  Validation loss = 2.0634  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.5310  Validation loss = 2.0621  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.5300  Validation loss = 2.0583  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.5294  Validation loss = 2.0584  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.5290  Validation loss = 2.0554  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.5286  Validation loss = 2.0558  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.5278  Validation loss = 2.0571  \n",
      "\n",
      "Fold: 2  Epoch: 12  Training loss = 2.5273  Validation loss = 2.0631  \n",
      "\n",
      "Fold: 2  Epoch: 13  Training loss = 2.5271  Validation loss = 2.0629  \n",
      "\n",
      "Fold: 2  Epoch: 14  Training loss = 2.5266  Validation loss = 2.0593  \n",
      "\n",
      "Fold: 2  Epoch: 15  Training loss = 2.5261  Validation loss = 2.0628  \n",
      "\n",
      "Fold: 2  Epoch: 16  Training loss = 2.5255  Validation loss = 2.0568  \n",
      "\n",
      "Fold: 2  Epoch: 17  Training loss = 2.5251  Validation loss = 2.0574  \n",
      "\n",
      "Fold: 2  Epoch: 18  Training loss = 2.5241  Validation loss = 2.0636  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 9  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.5461  Validation loss = 3.0137  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.5445  Validation loss = 3.0132  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.5427  Validation loss = 3.0114  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.5410  Validation loss = 3.0179  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.5401  Validation loss = 3.0159  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.5387  Validation loss = 3.0157  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.5360  Validation loss = 3.0086  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.5342  Validation loss = 3.0057  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.5316  Validation loss = 3.0088  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.5308  Validation loss = 3.0092  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.5298  Validation loss = 3.0048  \n",
      "\n",
      "Fold: 3  Epoch: 12  Training loss = 1.5270  Validation loss = 3.0095  \n",
      "\n",
      "Fold: 3  Epoch: 13  Training loss = 1.5246  Validation loss = 3.0107  \n",
      "\n",
      "Fold: 3  Epoch: 14  Training loss = 1.5223  Validation loss = 3.0124  \n",
      "\n",
      "Fold: 3  Epoch: 15  Training loss = 1.5202  Validation loss = 3.0126  \n",
      "\n",
      "Fold: 3  Epoch: 16  Training loss = 1.5207  Validation loss = 3.0073  \n",
      "\n",
      "Fold: 3  Epoch: 17  Training loss = 1.5177  Validation loss = 3.0128  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 11  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.5937  Validation loss = 4.1321  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.5905  Validation loss = 4.1230  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.5883  Validation loss = 4.1214  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.5859  Validation loss = 4.1199  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.5840  Validation loss = 4.1110  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.5822  Validation loss = 4.1073  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.5797  Validation loss = 4.1060  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.5767  Validation loss = 4.1043  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.5733  Validation loss = 4.1073  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 1.5702  Validation loss = 4.1094  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.5681  Validation loss = 4.1047  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 1.5676  Validation loss = 4.1097  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.5654  Validation loss = 4.1021  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 1.5642  Validation loss = 4.0972  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.5623  Validation loss = 4.0888  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 1.5615  Validation loss = 4.0865  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 1.5602  Validation loss = 4.0864  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 1.5577  Validation loss = 4.0790  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 1.5563  Validation loss = 4.0689  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 1.5541  Validation loss = 4.0635  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 1.5533  Validation loss = 4.0552  \n",
      "\n",
      "Fold: 4  Epoch: 22  Training loss = 1.5515  Validation loss = 4.0544  \n",
      "\n",
      "Fold: 4  Epoch: 23  Training loss = 1.5499  Validation loss = 4.0524  \n",
      "\n",
      "Fold: 4  Epoch: 24  Training loss = 1.5483  Validation loss = 4.0463  \n",
      "\n",
      "Fold: 4  Epoch: 25  Training loss = 1.5463  Validation loss = 4.0384  \n",
      "\n",
      "Fold: 4  Epoch: 26  Training loss = 1.5444  Validation loss = 4.0268  \n",
      "\n",
      "Fold: 4  Epoch: 27  Training loss = 1.5431  Validation loss = 4.0251  \n",
      "\n",
      "Fold: 4  Epoch: 28  Training loss = 1.5414  Validation loss = 4.0251  \n",
      "\n",
      "Fold: 4  Epoch: 29  Training loss = 1.5403  Validation loss = 4.0303  \n",
      "\n",
      "Fold: 4  Epoch: 30  Training loss = 1.5387  Validation loss = 4.0252  \n",
      "\n",
      "Fold: 4  Epoch: 31  Training loss = 1.5373  Validation loss = 4.0226  \n",
      "\n",
      "Fold: 4  Epoch: 32  Training loss = 1.5361  Validation loss = 4.0185  \n",
      "\n",
      "Fold: 4  Epoch: 33  Training loss = 1.5346  Validation loss = 4.0088  \n",
      "\n",
      "Fold: 4  Epoch: 34  Training loss = 1.5331  Validation loss = 3.9999  \n",
      "\n",
      "Fold: 4  Epoch: 35  Training loss = 1.5318  Validation loss = 3.9948  \n",
      "\n",
      "Fold: 4  Epoch: 36  Training loss = 1.5303  Validation loss = 3.9896  \n",
      "\n",
      "Fold: 4  Epoch: 37  Training loss = 1.5285  Validation loss = 3.9897  \n",
      "\n",
      "Fold: 4  Epoch: 38  Training loss = 1.5276  Validation loss = 3.9900  \n",
      "\n",
      "Fold: 4  Epoch: 39  Training loss = 1.5264  Validation loss = 3.9850  \n",
      "\n",
      "Fold: 4  Epoch: 40  Training loss = 1.5247  Validation loss = 3.9787  \n",
      "\n",
      "Fold: 4  Epoch: 41  Training loss = 1.5235  Validation loss = 3.9705  \n",
      "\n",
      "Fold: 4  Epoch: 42  Training loss = 1.5224  Validation loss = 3.9630  \n",
      "\n",
      "Fold: 4  Epoch: 43  Training loss = 1.5209  Validation loss = 3.9627  \n",
      "\n",
      "Fold: 4  Epoch: 44  Training loss = 1.5199  Validation loss = 3.9510  \n",
      "\n",
      "Fold: 4  Epoch: 45  Training loss = 1.5185  Validation loss = 3.9500  \n",
      "\n",
      "Fold: 4  Epoch: 46  Training loss = 1.5174  Validation loss = 3.9481  \n",
      "\n",
      "Fold: 4  Epoch: 47  Training loss = 1.5160  Validation loss = 3.9472  \n",
      "\n",
      "Fold: 4  Epoch: 48  Training loss = 1.5150  Validation loss = 3.9434  \n",
      "\n",
      "Fold: 4  Epoch: 49  Training loss = 1.5136  Validation loss = 3.9378  \n",
      "\n",
      "Fold: 4  Epoch: 50  Training loss = 1.5122  Validation loss = 3.9335  \n",
      "\n",
      "Fold: 4  Epoch: 51  Training loss = 1.5111  Validation loss = 3.9289  \n",
      "\n",
      "Fold: 4  Epoch: 52  Training loss = 1.5100  Validation loss = 3.9218  \n",
      "\n",
      "Fold: 4  Epoch: 53  Training loss = 1.5090  Validation loss = 3.9178  \n",
      "\n",
      "Fold: 4  Epoch: 54  Training loss = 1.5076  Validation loss = 3.9138  \n",
      "\n",
      "Fold: 4  Epoch: 55  Training loss = 1.5061  Validation loss = 3.9076  \n",
      "\n",
      "Fold: 4  Epoch: 56  Training loss = 1.5046  Validation loss = 3.9003  \n",
      "\n",
      "Fold: 4  Epoch: 57  Training loss = 1.5037  Validation loss = 3.8939  \n",
      "\n",
      "Fold: 4  Epoch: 58  Training loss = 1.5023  Validation loss = 3.8833  \n",
      "\n",
      "Fold: 4  Epoch: 59  Training loss = 1.5007  Validation loss = 3.8768  \n",
      "\n",
      "Fold: 4  Epoch: 60  Training loss = 1.5000  Validation loss = 3.8691  \n",
      "\n",
      "Fold: 4  Epoch: 61  Training loss = 1.4989  Validation loss = 3.8696  \n",
      "\n",
      "Fold: 4  Epoch: 62  Training loss = 1.4981  Validation loss = 3.8668  \n",
      "\n",
      "Fold: 4  Epoch: 63  Training loss = 1.4965  Validation loss = 3.8616  \n",
      "\n",
      "Fold: 4  Epoch: 64  Training loss = 1.4958  Validation loss = 3.8600  \n",
      "\n",
      "Fold: 4  Epoch: 65  Training loss = 1.4943  Validation loss = 3.8531  \n",
      "\n",
      "Fold: 4  Epoch: 66  Training loss = 1.4929  Validation loss = 3.8475  \n",
      "\n",
      "Fold: 4  Epoch: 67  Training loss = 1.4916  Validation loss = 3.8348  \n",
      "\n",
      "Fold: 4  Epoch: 68  Training loss = 1.4901  Validation loss = 3.8314  \n",
      "\n",
      "Fold: 4  Epoch: 69  Training loss = 1.4889  Validation loss = 3.8244  \n",
      "\n",
      "Fold: 4  Epoch: 70  Training loss = 1.4876  Validation loss = 3.8179  \n",
      "\n",
      "Fold: 4  Epoch: 71  Training loss = 1.4854  Validation loss = 3.8198  \n",
      "\n",
      "Fold: 4  Epoch: 72  Training loss = 1.4842  Validation loss = 3.8118  \n",
      "\n",
      "Fold: 4  Epoch: 73  Training loss = 1.4828  Validation loss = 3.8122  \n",
      "\n",
      "Fold: 4  Epoch: 74  Training loss = 1.4828  Validation loss = 3.8080  \n",
      "\n",
      "Fold: 4  Epoch: 75  Training loss = 1.4815  Validation loss = 3.8094  \n",
      "\n",
      "Fold: 4  Epoch: 76  Training loss = 1.4809  Validation loss = 3.8059  \n",
      "\n",
      "Fold: 4  Epoch: 77  Training loss = 1.4800  Validation loss = 3.8063  \n",
      "\n",
      "Fold: 4  Epoch: 78  Training loss = 1.4789  Validation loss = 3.8072  \n",
      "\n",
      "Fold: 4  Epoch: 79  Training loss = 1.4777  Validation loss = 3.7921  \n",
      "\n",
      "Fold: 4  Epoch: 80  Training loss = 1.4765  Validation loss = 3.7894  \n",
      "\n",
      "Fold: 4  Epoch: 81  Training loss = 1.4756  Validation loss = 3.7868  \n",
      "\n",
      "Fold: 4  Epoch: 82  Training loss = 1.4743  Validation loss = 3.7796  \n",
      "\n",
      "Fold: 4  Epoch: 83  Training loss = 1.4729  Validation loss = 3.7715  \n",
      "\n",
      "Fold: 4  Epoch: 84  Training loss = 1.4713  Validation loss = 3.7688  \n",
      "\n",
      "Fold: 4  Epoch: 85  Training loss = 1.4702  Validation loss = 3.7693  \n",
      "\n",
      "Fold: 4  Epoch: 86  Training loss = 1.4694  Validation loss = 3.7676  \n",
      "\n",
      "Fold: 4  Epoch: 87  Training loss = 1.4687  Validation loss = 3.7662  \n",
      "\n",
      "Fold: 4  Epoch: 88  Training loss = 1.4672  Validation loss = 3.7603  \n",
      "\n",
      "Fold: 4  Epoch: 89  Training loss = 1.4663  Validation loss = 3.7559  \n",
      "\n",
      "Fold: 4  Epoch: 90  Training loss = 1.4655  Validation loss = 3.7511  \n",
      "\n",
      "Fold: 4  Epoch: 91  Training loss = 1.4644  Validation loss = 3.7518  \n",
      "\n",
      "Fold: 4  Epoch: 92  Training loss = 1.4634  Validation loss = 3.7479  \n",
      "\n",
      "Fold: 4  Epoch: 93  Training loss = 1.4620  Validation loss = 3.7406  \n",
      "\n",
      "Fold: 4  Epoch: 94  Training loss = 1.4611  Validation loss = 3.7322  \n",
      "\n",
      "Fold: 4  Epoch: 95  Training loss = 1.4595  Validation loss = 3.7336  \n",
      "\n",
      "Fold: 4  Epoch: 96  Training loss = 1.4586  Validation loss = 3.7352  \n",
      "\n",
      "Fold: 4  Epoch: 97  Training loss = 1.4575  Validation loss = 3.7317  \n",
      "\n",
      "Fold: 4  Epoch: 98  Training loss = 1.4568  Validation loss = 3.7310  \n",
      "\n",
      "Fold: 4  Epoch: 99  Training loss = 1.4557  Validation loss = 3.7282  \n",
      "\n",
      "Fold: 4  Epoch: 100  Training loss = 1.4555  Validation loss = 3.7299  \n",
      "\n",
      "Fold: 4  Epoch: 101  Training loss = 1.4543  Validation loss = 3.7249  \n",
      "\n",
      "Fold: 4  Epoch: 102  Training loss = 1.4533  Validation loss = 3.7214  \n",
      "\n",
      "Fold: 4  Epoch: 103  Training loss = 1.4528  Validation loss = 3.7212  \n",
      "\n",
      "Fold: 4  Epoch: 104  Training loss = 1.4510  Validation loss = 3.7142  \n",
      "\n",
      "Fold: 4  Epoch: 105  Training loss = 1.4513  Validation loss = 3.7195  \n",
      "\n",
      "Fold: 4  Epoch: 106  Training loss = 1.4485  Validation loss = 3.7041  \n",
      "\n",
      "Fold: 4  Epoch: 107  Training loss = 1.4480  Validation loss = 3.7043  \n",
      "\n",
      "Fold: 4  Epoch: 108  Training loss = 1.4477  Validation loss = 3.7052  \n",
      "\n",
      "Fold: 4  Epoch: 109  Training loss = 1.4473  Validation loss = 3.7051  \n",
      "\n",
      "Fold: 4  Epoch: 110  Training loss = 1.4464  Validation loss = 3.7015  \n",
      "\n",
      "Fold: 4  Epoch: 111  Training loss = 1.4442  Validation loss = 3.6943  \n",
      "\n",
      "Fold: 4  Epoch: 112  Training loss = 1.4434  Validation loss = 3.6918  \n",
      "\n",
      "Fold: 4  Epoch: 113  Training loss = 1.4423  Validation loss = 3.6892  \n",
      "\n",
      "Fold: 4  Epoch: 114  Training loss = 1.4412  Validation loss = 3.6843  \n",
      "\n",
      "Fold: 4  Epoch: 115  Training loss = 1.4399  Validation loss = 3.6785  \n",
      "\n",
      "Fold: 4  Epoch: 116  Training loss = 1.4387  Validation loss = 3.6727  \n",
      "\n",
      "Fold: 4  Epoch: 117  Training loss = 1.4379  Validation loss = 3.6718  \n",
      "\n",
      "Fold: 4  Epoch: 118  Training loss = 1.4377  Validation loss = 3.6738  \n",
      "\n",
      "Fold: 4  Epoch: 119  Training loss = 1.4367  Validation loss = 3.6697  \n",
      "\n",
      "Fold: 4  Epoch: 120  Training loss = 1.4363  Validation loss = 3.6695  \n",
      "\n",
      "Fold: 4  Epoch: 121  Training loss = 1.4362  Validation loss = 3.6716  \n",
      "\n",
      "Fold: 4  Epoch: 122  Training loss = 1.4355  Validation loss = 3.6704  \n",
      "\n",
      "Fold: 4  Epoch: 123  Training loss = 1.4341  Validation loss = 3.6665  \n",
      "\n",
      "Fold: 4  Epoch: 124  Training loss = 1.4323  Validation loss = 3.6594  \n",
      "\n",
      "Fold: 4  Epoch: 125  Training loss = 1.4313  Validation loss = 3.6571  \n",
      "\n",
      "Fold: 4  Epoch: 126  Training loss = 1.4305  Validation loss = 3.6530  \n",
      "\n",
      "Fold: 4  Epoch: 127  Training loss = 1.4304  Validation loss = 3.6540  \n",
      "\n",
      "Fold: 4  Epoch: 128  Training loss = 1.4298  Validation loss = 3.6515  \n",
      "\n",
      "Fold: 4  Epoch: 129  Training loss = 1.4285  Validation loss = 3.6480  \n",
      "\n",
      "Fold: 4  Epoch: 130  Training loss = 1.4271  Validation loss = 3.6422  \n",
      "\n",
      "Fold: 4  Epoch: 131  Training loss = 1.4255  Validation loss = 3.6347  \n",
      "\n",
      "Fold: 4  Epoch: 132  Training loss = 1.4244  Validation loss = 3.6313  \n",
      "\n",
      "Fold: 4  Epoch: 133  Training loss = 1.4244  Validation loss = 3.6331  \n",
      "\n",
      "Fold: 4  Epoch: 134  Training loss = 1.4230  Validation loss = 3.6284  \n",
      "\n",
      "Fold: 4  Epoch: 135  Training loss = 1.4219  Validation loss = 3.6254  \n",
      "\n",
      "Fold: 4  Epoch: 136  Training loss = 1.4218  Validation loss = 3.6250  \n",
      "\n",
      "Fold: 4  Epoch: 137  Training loss = 1.4209  Validation loss = 3.6228  \n",
      "\n",
      "Fold: 4  Epoch: 138  Training loss = 1.4208  Validation loss = 3.6235  \n",
      "\n",
      "Fold: 4  Epoch: 139  Training loss = 1.4201  Validation loss = 3.6213  \n",
      "\n",
      "Fold: 4  Epoch: 140  Training loss = 1.4209  Validation loss = 3.6231  \n",
      "\n",
      "Fold: 4  Epoch: 141  Training loss = 1.4181  Validation loss = 3.6116  \n",
      "\n",
      "Fold: 4  Epoch: 142  Training loss = 1.4158  Validation loss = 3.6012  \n",
      "\n",
      "Fold: 4  Epoch: 143  Training loss = 1.4144  Validation loss = 3.5952  \n",
      "\n",
      "Fold: 4  Epoch: 144  Training loss = 1.4135  Validation loss = 3.5919  \n",
      "\n",
      "Fold: 4  Epoch: 145  Training loss = 1.4123  Validation loss = 3.5854  \n",
      "\n",
      "Fold: 4  Epoch: 146  Training loss = 1.4118  Validation loss = 3.5862  \n",
      "\n",
      "Fold: 4  Epoch: 147  Training loss = 1.4110  Validation loss = 3.5817  \n",
      "\n",
      "Fold: 4  Epoch: 148  Training loss = 1.4094  Validation loss = 3.5703  \n",
      "\n",
      "Fold: 4  Epoch: 149  Training loss = 1.4085  Validation loss = 3.5643  \n",
      "\n",
      "Fold: 4  Epoch: 150  Training loss = 1.4077  Validation loss = 3.5611  \n",
      "\n",
      "Fold: 4  Epoch: 151  Training loss = 1.4070  Validation loss = 3.5622  \n",
      "\n",
      "Fold: 4  Epoch: 152  Training loss = 1.4061  Validation loss = 3.5554  \n",
      "\n",
      "Fold: 4  Epoch: 153  Training loss = 1.4049  Validation loss = 3.5475  \n",
      "\n",
      "Fold: 4  Epoch: 154  Training loss = 1.4043  Validation loss = 3.5465  \n",
      "\n",
      "Fold: 4  Epoch: 155  Training loss = 1.4034  Validation loss = 3.5431  \n",
      "\n",
      "Fold: 4  Epoch: 156  Training loss = 1.4026  Validation loss = 3.5376  \n",
      "\n",
      "Fold: 4  Epoch: 157  Training loss = 1.4014  Validation loss = 3.5357  \n",
      "\n",
      "Fold: 4  Epoch: 158  Training loss = 1.4005  Validation loss = 3.5372  \n",
      "\n",
      "Fold: 4  Epoch: 159  Training loss = 1.3998  Validation loss = 3.5349  \n",
      "\n",
      "Fold: 4  Epoch: 160  Training loss = 1.3990  Validation loss = 3.5339  \n",
      "\n",
      "Fold: 4  Epoch: 161  Training loss = 1.3979  Validation loss = 3.5280  \n",
      "\n",
      "Fold: 4  Epoch: 162  Training loss = 1.3971  Validation loss = 3.5174  \n",
      "\n",
      "Fold: 4  Epoch: 163  Training loss = 1.3957  Validation loss = 3.5187  \n",
      "\n",
      "Fold: 4  Epoch: 164  Training loss = 1.3951  Validation loss = 3.5154  \n",
      "\n",
      "Fold: 4  Epoch: 165  Training loss = 1.3946  Validation loss = 3.5145  \n",
      "\n",
      "Fold: 4  Epoch: 166  Training loss = 1.3938  Validation loss = 3.5102  \n",
      "\n",
      "Fold: 4  Epoch: 167  Training loss = 1.3929  Validation loss = 3.5062  \n",
      "\n",
      "Fold: 4  Epoch: 168  Training loss = 1.3924  Validation loss = 3.5027  \n",
      "\n",
      "Fold: 4  Epoch: 169  Training loss = 1.3915  Validation loss = 3.5010  \n",
      "\n",
      "Fold: 4  Epoch: 170  Training loss = 1.3902  Validation loss = 3.4948  \n",
      "\n",
      "Fold: 4  Epoch: 171  Training loss = 1.3895  Validation loss = 3.4951  \n",
      "\n",
      "Fold: 4  Epoch: 172  Training loss = 1.3883  Validation loss = 3.4879  \n",
      "\n",
      "Fold: 4  Epoch: 173  Training loss = 1.3877  Validation loss = 3.4853  \n",
      "\n",
      "Fold: 4  Epoch: 174  Training loss = 1.3877  Validation loss = 3.4903  \n",
      "\n",
      "Fold: 4  Epoch: 175  Training loss = 1.3873  Validation loss = 3.4893  \n",
      "\n",
      "Fold: 4  Epoch: 176  Training loss = 1.3863  Validation loss = 3.4879  \n",
      "\n",
      "Fold: 4  Epoch: 177  Training loss = 1.3858  Validation loss = 3.4854  \n",
      "\n",
      "Fold: 4  Epoch: 178  Training loss = 1.3851  Validation loss = 3.4813  \n",
      "\n",
      "Fold: 4  Epoch: 179  Training loss = 1.3842  Validation loss = 3.4739  \n",
      "\n",
      "Fold: 4  Epoch: 180  Training loss = 1.3835  Validation loss = 3.4728  \n",
      "\n",
      "Fold: 4  Epoch: 181  Training loss = 1.3830  Validation loss = 3.4729  \n",
      "\n",
      "Fold: 4  Epoch: 182  Training loss = 1.3823  Validation loss = 3.4685  \n",
      "\n",
      "Fold: 4  Epoch: 183  Training loss = 1.3819  Validation loss = 3.4655  \n",
      "\n",
      "Fold: 4  Epoch: 184  Training loss = 1.3809  Validation loss = 3.4628  \n",
      "\n",
      "Fold: 4  Epoch: 185  Training loss = 1.3806  Validation loss = 3.4599  \n",
      "\n",
      "Fold: 4  Epoch: 186  Training loss = 1.3795  Validation loss = 3.4534  \n",
      "\n",
      "Fold: 4  Epoch: 187  Training loss = 1.3789  Validation loss = 3.4521  \n",
      "\n",
      "Fold: 4  Epoch: 188  Training loss = 1.3782  Validation loss = 3.4489  \n",
      "\n",
      "Fold: 4  Epoch: 189  Training loss = 1.3779  Validation loss = 3.4513  \n",
      "\n",
      "Fold: 4  Epoch: 190  Training loss = 1.3762  Validation loss = 3.4434  \n",
      "\n",
      "Fold: 4  Epoch: 191  Training loss = 1.3752  Validation loss = 3.4384  \n",
      "\n",
      "Fold: 4  Epoch: 192  Training loss = 1.3745  Validation loss = 3.4375  \n",
      "\n",
      "Fold: 4  Epoch: 193  Training loss = 1.3738  Validation loss = 3.4299  \n",
      "\n",
      "Fold: 4  Epoch: 194  Training loss = 1.3731  Validation loss = 3.4260  \n",
      "\n",
      "Fold: 4  Epoch: 195  Training loss = 1.3727  Validation loss = 3.4219  \n",
      "\n",
      "Fold: 4  Epoch: 196  Training loss = 1.3721  Validation loss = 3.4215  \n",
      "\n",
      "Fold: 4  Epoch: 197  Training loss = 1.3712  Validation loss = 3.4202  \n",
      "\n",
      "Fold: 4  Epoch: 198  Training loss = 1.3708  Validation loss = 3.4206  \n",
      "\n",
      "Fold: 4  Epoch: 199  Training loss = 1.3699  Validation loss = 3.4142  \n",
      "\n",
      "Fold: 4  Epoch: 200  Training loss = 1.3690  Validation loss = 3.4165  \n",
      "\n",
      "Fold: 4  Epoch: 201  Training loss = 1.3687  Validation loss = 3.4171  \n",
      "\n",
      "Fold: 4  Epoch: 202  Training loss = 1.3678  Validation loss = 3.4128  \n",
      "\n",
      "Fold: 4  Epoch: 203  Training loss = 1.3663  Validation loss = 3.4045  \n",
      "\n",
      "Fold: 4  Epoch: 204  Training loss = 1.3656  Validation loss = 3.4009  \n",
      "\n",
      "Fold: 4  Epoch: 205  Training loss = 1.3645  Validation loss = 3.3947  \n",
      "\n",
      "Fold: 4  Epoch: 206  Training loss = 1.3639  Validation loss = 3.3891  \n",
      "\n",
      "Fold: 4  Epoch: 207  Training loss = 1.3626  Validation loss = 3.3873  \n",
      "\n",
      "Fold: 4  Epoch: 208  Training loss = 1.3625  Validation loss = 3.3847  \n",
      "\n",
      "Fold: 4  Epoch: 209  Training loss = 1.3623  Validation loss = 3.3871  \n",
      "\n",
      "Fold: 4  Epoch: 210  Training loss = 1.3617  Validation loss = 3.3882  \n",
      "\n",
      "Fold: 4  Epoch: 211  Training loss = 1.3612  Validation loss = 3.3860  \n",
      "\n",
      "Fold: 4  Epoch: 212  Training loss = 1.3602  Validation loss = 3.3788  \n",
      "\n",
      "Fold: 4  Epoch: 213  Training loss = 1.3596  Validation loss = 3.3757  \n",
      "\n",
      "Fold: 4  Epoch: 214  Training loss = 1.3589  Validation loss = 3.3672  \n",
      "\n",
      "Fold: 4  Epoch: 215  Training loss = 1.3578  Validation loss = 3.3677  \n",
      "\n",
      "Fold: 4  Epoch: 216  Training loss = 1.3570  Validation loss = 3.3570  \n",
      "\n",
      "Fold: 4  Epoch: 217  Training loss = 1.3565  Validation loss = 3.3554  \n",
      "\n",
      "Fold: 4  Epoch: 218  Training loss = 1.3558  Validation loss = 3.3523  \n",
      "\n",
      "Fold: 4  Epoch: 219  Training loss = 1.3553  Validation loss = 3.3476  \n",
      "\n",
      "Fold: 4  Epoch: 220  Training loss = 1.3544  Validation loss = 3.3443  \n",
      "\n",
      "Fold: 4  Epoch: 221  Training loss = 1.3536  Validation loss = 3.3418  \n",
      "\n",
      "Fold: 4  Epoch: 222  Training loss = 1.3530  Validation loss = 3.3410  \n",
      "\n",
      "Fold: 4  Epoch: 223  Training loss = 1.3529  Validation loss = 3.3343  \n",
      "\n",
      "Fold: 4  Epoch: 224  Training loss = 1.3520  Validation loss = 3.3319  \n",
      "\n",
      "Fold: 4  Epoch: 225  Training loss = 1.3512  Validation loss = 3.3326  \n",
      "\n",
      "Fold: 4  Epoch: 226  Training loss = 1.3508  Validation loss = 3.3285  \n",
      "\n",
      "Fold: 4  Epoch: 227  Training loss = 1.3506  Validation loss = 3.3225  \n",
      "\n",
      "Fold: 4  Epoch: 228  Training loss = 1.3495  Validation loss = 3.3204  \n",
      "\n",
      "Fold: 4  Epoch: 229  Training loss = 1.3488  Validation loss = 3.3178  \n",
      "\n",
      "Fold: 4  Epoch: 230  Training loss = 1.3479  Validation loss = 3.3176  \n",
      "\n",
      "Fold: 4  Epoch: 231  Training loss = 1.3472  Validation loss = 3.3172  \n",
      "\n",
      "Fold: 4  Epoch: 232  Training loss = 1.3465  Validation loss = 3.3121  \n",
      "\n",
      "Fold: 4  Epoch: 233  Training loss = 1.3455  Validation loss = 3.3093  \n",
      "\n",
      "Fold: 4  Epoch: 234  Training loss = 1.3450  Validation loss = 3.3076  \n",
      "\n",
      "Fold: 4  Epoch: 235  Training loss = 1.3447  Validation loss = 3.3072  \n",
      "\n",
      "Fold: 4  Epoch: 236  Training loss = 1.3444  Validation loss = 3.3025  \n",
      "\n",
      "Fold: 4  Epoch: 237  Training loss = 1.3441  Validation loss = 3.2936  \n",
      "\n",
      "Fold: 4  Epoch: 238  Training loss = 1.3433  Validation loss = 3.2892  \n",
      "\n",
      "Fold: 4  Epoch: 239  Training loss = 1.3418  Validation loss = 3.2894  \n",
      "\n",
      "Fold: 4  Epoch: 240  Training loss = 1.3422  Validation loss = 3.2805  \n",
      "\n",
      "Fold: 4  Epoch: 241  Training loss = 1.3408  Validation loss = 3.2812  \n",
      "\n",
      "Fold: 4  Epoch: 242  Training loss = 1.3397  Validation loss = 3.2828  \n",
      "\n",
      "Fold: 4  Epoch: 243  Training loss = 1.3393  Validation loss = 3.2785  \n",
      "\n",
      "Fold: 4  Epoch: 244  Training loss = 1.3387  Validation loss = 3.2742  \n",
      "\n",
      "Fold: 4  Epoch: 245  Training loss = 1.3383  Validation loss = 3.2706  \n",
      "\n",
      "Fold: 4  Epoch: 246  Training loss = 1.3384  Validation loss = 3.2642  \n",
      "\n",
      "Fold: 4  Epoch: 247  Training loss = 1.3376  Validation loss = 3.2586  \n",
      "\n",
      "Fold: 4  Epoch: 248  Training loss = 1.3365  Validation loss = 3.2567  \n",
      "\n",
      "Fold: 4  Epoch: 249  Training loss = 1.3359  Validation loss = 3.2595  \n",
      "\n",
      "Fold: 4  Epoch: 250  Training loss = 1.3353  Validation loss = 3.2579  \n",
      "\n",
      "Fold: 4  Epoch: 251  Training loss = 1.3350  Validation loss = 3.2494  \n",
      "\n",
      "Fold: 4  Epoch: 252  Training loss = 1.3337  Validation loss = 3.2530  \n",
      "\n",
      "Fold: 4  Epoch: 253  Training loss = 1.3329  Validation loss = 3.2495  \n",
      "\n",
      "Fold: 4  Epoch: 254  Training loss = 1.3329  Validation loss = 3.2479  \n",
      "\n",
      "Fold: 4  Epoch: 255  Training loss = 1.3323  Validation loss = 3.2444  \n",
      "\n",
      "Fold: 4  Epoch: 256  Training loss = 1.3320  Validation loss = 3.2434  \n",
      "\n",
      "Fold: 4  Epoch: 257  Training loss = 1.3314  Validation loss = 3.2424  \n",
      "\n",
      "Fold: 4  Epoch: 258  Training loss = 1.3305  Validation loss = 3.2439  \n",
      "\n",
      "Fold: 4  Epoch: 259  Training loss = 1.3301  Validation loss = 3.2409  \n",
      "\n",
      "Fold: 4  Epoch: 260  Training loss = 1.3290  Validation loss = 3.2360  \n",
      "\n",
      "Fold: 4  Epoch: 261  Training loss = 1.3286  Validation loss = 3.2300  \n",
      "\n",
      "Fold: 4  Epoch: 262  Training loss = 1.3276  Validation loss = 3.2340  \n",
      "\n",
      "Fold: 4  Epoch: 263  Training loss = 1.3271  Validation loss = 3.2358  \n",
      "\n",
      "Fold: 4  Epoch: 264  Training loss = 1.3266  Validation loss = 3.2346  \n",
      "\n",
      "Fold: 4  Epoch: 265  Training loss = 1.3262  Validation loss = 3.2353  \n",
      "\n",
      "Fold: 4  Epoch: 266  Training loss = 1.3254  Validation loss = 3.2315  \n",
      "\n",
      "Fold: 4  Epoch: 267  Training loss = 1.3253  Validation loss = 3.2318  \n",
      "\n",
      "Fold: 4  Epoch: 268  Training loss = 1.3245  Validation loss = 3.2227  \n",
      "\n",
      "Fold: 4  Epoch: 269  Training loss = 1.3245  Validation loss = 3.2201  \n",
      "\n",
      "Fold: 4  Epoch: 270  Training loss = 1.3237  Validation loss = 3.2142  \n",
      "\n",
      "Fold: 4  Epoch: 271  Training loss = 1.3235  Validation loss = 3.2117  \n",
      "\n",
      "Fold: 4  Epoch: 272  Training loss = 1.3232  Validation loss = 3.2110  \n",
      "\n",
      "Fold: 4  Epoch: 273  Training loss = 1.3230  Validation loss = 3.2045  \n",
      "\n",
      "Fold: 4  Epoch: 274  Training loss = 1.3225  Validation loss = 3.2023  \n",
      "\n",
      "Fold: 4  Epoch: 275  Training loss = 1.3210  Validation loss = 3.2029  \n",
      "\n",
      "Fold: 4  Epoch: 276  Training loss = 1.3209  Validation loss = 3.2033  \n",
      "\n",
      "Fold: 4  Epoch: 277  Training loss = 1.3198  Validation loss = 3.2017  \n",
      "\n",
      "Fold: 4  Epoch: 278  Training loss = 1.3192  Validation loss = 3.1985  \n",
      "\n",
      "Fold: 4  Epoch: 279  Training loss = 1.3187  Validation loss = 3.1948  \n",
      "\n",
      "Fold: 4  Epoch: 280  Training loss = 1.3180  Validation loss = 3.1899  \n",
      "\n",
      "Fold: 4  Epoch: 281  Training loss = 1.3177  Validation loss = 3.1910  \n",
      "\n",
      "Fold: 4  Epoch: 282  Training loss = 1.3174  Validation loss = 3.1922  \n",
      "\n",
      "Fold: 4  Epoch: 283  Training loss = 1.3169  Validation loss = 3.1902  \n",
      "\n",
      "Fold: 4  Epoch: 284  Training loss = 1.3161  Validation loss = 3.1844  \n",
      "\n",
      "Fold: 4  Epoch: 285  Training loss = 1.3160  Validation loss = 3.1867  \n",
      "\n",
      "Fold: 4  Epoch: 286  Training loss = 1.3155  Validation loss = 3.1834  \n",
      "\n",
      "Fold: 4  Epoch: 287  Training loss = 1.3151  Validation loss = 3.1812  \n",
      "\n",
      "Fold: 4  Epoch: 288  Training loss = 1.3145  Validation loss = 3.1796  \n",
      "\n",
      "Fold: 4  Epoch: 289  Training loss = 1.3142  Validation loss = 3.1750  \n",
      "\n",
      "Fold: 4  Epoch: 290  Training loss = 1.3136  Validation loss = 3.1720  \n",
      "\n",
      "Fold: 4  Epoch: 291  Training loss = 1.3131  Validation loss = 3.1674  \n",
      "\n",
      "Fold: 4  Epoch: 292  Training loss = 1.3127  Validation loss = 3.1669  \n",
      "\n",
      "Fold: 4  Epoch: 293  Training loss = 1.3122  Validation loss = 3.1656  \n",
      "\n",
      "Fold: 4  Epoch: 294  Training loss = 1.3116  Validation loss = 3.1623  \n",
      "\n",
      "Fold: 4  Epoch: 295  Training loss = 1.3112  Validation loss = 3.1553  \n",
      "\n",
      "Fold: 4  Epoch: 296  Training loss = 1.3109  Validation loss = 3.1531  \n",
      "\n",
      "Fold: 4  Epoch: 297  Training loss = 1.3107  Validation loss = 3.1508  \n",
      "\n",
      "Fold: 4  Epoch: 298  Training loss = 1.3113  Validation loss = 3.1447  \n",
      "\n",
      "Fold: 4  Epoch: 299  Training loss = 1.3100  Validation loss = 3.1463  \n",
      "\n",
      "Fold: 4  Epoch: 300  Training loss = 1.3094  Validation loss = 3.1376  \n",
      "\n",
      "Fold: 4  Epoch: 301  Training loss = 1.3086  Validation loss = 3.1351  \n",
      "\n",
      "Fold: 4  Epoch: 302  Training loss = 1.3078  Validation loss = 3.1379  \n",
      "\n",
      "Fold: 4  Epoch: 303  Training loss = 1.3078  Validation loss = 3.1294  \n",
      "\n",
      "Fold: 4  Epoch: 304  Training loss = 1.3069  Validation loss = 3.1311  \n",
      "\n",
      "Fold: 4  Epoch: 305  Training loss = 1.3066  Validation loss = 3.1324  \n",
      "\n",
      "Fold: 4  Epoch: 306  Training loss = 1.3062  Validation loss = 3.1310  \n",
      "\n",
      "Fold: 4  Epoch: 307  Training loss = 1.3059  Validation loss = 3.1280  \n",
      "\n",
      "Fold: 4  Epoch: 308  Training loss = 1.3057  Validation loss = 3.1308  \n",
      "\n",
      "Fold: 4  Epoch: 309  Training loss = 1.3050  Validation loss = 3.1250  \n",
      "\n",
      "Fold: 4  Epoch: 310  Training loss = 1.3043  Validation loss = 3.1254  \n",
      "\n",
      "Fold: 4  Epoch: 311  Training loss = 1.3038  Validation loss = 3.1194  \n",
      "\n",
      "Fold: 4  Epoch: 312  Training loss = 1.3034  Validation loss = 3.1171  \n",
      "\n",
      "Fold: 4  Epoch: 313  Training loss = 1.3030  Validation loss = 3.1149  \n",
      "\n",
      "Fold: 4  Epoch: 314  Training loss = 1.3024  Validation loss = 3.1127  \n",
      "\n",
      "Fold: 4  Epoch: 315  Training loss = 1.3024  Validation loss = 3.1170  \n",
      "\n",
      "Fold: 4  Epoch: 316  Training loss = 1.3032  Validation loss = 3.1261  \n",
      "\n",
      "Fold: 4  Epoch: 317  Training loss = 1.3022  Validation loss = 3.1224  \n",
      "\n",
      "Fold: 4  Epoch: 318  Training loss = 1.3015  Validation loss = 3.1189  \n",
      "\n",
      "Fold: 4  Epoch: 319  Training loss = 1.3015  Validation loss = 3.1192  \n",
      "\n",
      "Fold: 4  Epoch: 320  Training loss = 1.3010  Validation loss = 3.1180  \n",
      "\n",
      "Fold: 4  Epoch: 321  Training loss = 1.3000  Validation loss = 3.1109  \n",
      "\n",
      "Fold: 4  Epoch: 322  Training loss = 1.2998  Validation loss = 3.1110  \n",
      "\n",
      "Fold: 4  Epoch: 323  Training loss = 1.2998  Validation loss = 3.1102  \n",
      "\n",
      "Fold: 4  Epoch: 324  Training loss = 1.2991  Validation loss = 3.1060  \n",
      "\n",
      "Fold: 4  Epoch: 325  Training loss = 1.2993  Validation loss = 3.1084  \n",
      "\n",
      "Fold: 4  Epoch: 326  Training loss = 1.2984  Validation loss = 3.1035  \n",
      "\n",
      "Fold: 4  Epoch: 327  Training loss = 1.2978  Validation loss = 3.1009  \n",
      "\n",
      "Fold: 4  Epoch: 328  Training loss = 1.2973  Validation loss = 3.0918  \n",
      "\n",
      "Fold: 4  Epoch: 329  Training loss = 1.2969  Validation loss = 3.0924  \n",
      "\n",
      "Fold: 4  Epoch: 330  Training loss = 1.2967  Validation loss = 3.0883  \n",
      "\n",
      "Fold: 4  Epoch: 331  Training loss = 1.2963  Validation loss = 3.0887  \n",
      "\n",
      "Fold: 4  Epoch: 332  Training loss = 1.2959  Validation loss = 3.0881  \n",
      "\n",
      "Fold: 4  Epoch: 333  Training loss = 1.2958  Validation loss = 3.0873  \n",
      "\n",
      "Fold: 4  Epoch: 334  Training loss = 1.2952  Validation loss = 3.0812  \n",
      "\n",
      "Fold: 4  Epoch: 335  Training loss = 1.2949  Validation loss = 3.0810  \n",
      "\n",
      "Fold: 4  Epoch: 336  Training loss = 1.2945  Validation loss = 3.0793  \n",
      "\n",
      "Fold: 4  Epoch: 337  Training loss = 1.2943  Validation loss = 3.0722  \n",
      "\n",
      "Fold: 4  Epoch: 338  Training loss = 1.2940  Validation loss = 3.0741  \n",
      "\n",
      "Fold: 4  Epoch: 339  Training loss = 1.2937  Validation loss = 3.0726  \n",
      "\n",
      "Fold: 4  Epoch: 340  Training loss = 1.2935  Validation loss = 3.0657  \n",
      "\n",
      "Fold: 4  Epoch: 341  Training loss = 1.2930  Validation loss = 3.0662  \n",
      "\n",
      "Fold: 4  Epoch: 342  Training loss = 1.2932  Validation loss = 3.0603  \n",
      "\n",
      "Fold: 4  Epoch: 343  Training loss = 1.2923  Validation loss = 3.0589  \n",
      "\n",
      "Fold: 4  Epoch: 344  Training loss = 1.2919  Validation loss = 3.0547  \n",
      "\n",
      "Fold: 4  Epoch: 345  Training loss = 1.2914  Validation loss = 3.0617  \n",
      "\n",
      "Fold: 4  Epoch: 346  Training loss = 1.2912  Validation loss = 3.0648  \n",
      "\n",
      "Fold: 4  Epoch: 347  Training loss = 1.2908  Validation loss = 3.0603  \n",
      "\n",
      "Fold: 4  Epoch: 348  Training loss = 1.2904  Validation loss = 3.0559  \n",
      "\n",
      "Fold: 4  Epoch: 349  Training loss = 1.2901  Validation loss = 3.0547  \n",
      "\n",
      "Fold: 4  Epoch: 350  Training loss = 1.2901  Validation loss = 3.0582  \n",
      "\n",
      "Fold: 4  Epoch: 351  Training loss = 1.2893  Validation loss = 3.0520  \n",
      "\n",
      "Fold: 4  Epoch: 352  Training loss = 1.2892  Validation loss = 3.0451  \n",
      "\n",
      "Fold: 4  Epoch: 353  Training loss = 1.2888  Validation loss = 3.0430  \n",
      "\n",
      "Fold: 4  Epoch: 354  Training loss = 1.2884  Validation loss = 3.0432  \n",
      "\n",
      "Fold: 4  Epoch: 355  Training loss = 1.2879  Validation loss = 3.0425  \n",
      "\n",
      "Fold: 4  Epoch: 356  Training loss = 1.2880  Validation loss = 3.0474  \n",
      "\n",
      "Fold: 4  Epoch: 357  Training loss = 1.2875  Validation loss = 3.0438  \n",
      "\n",
      "Fold: 4  Epoch: 358  Training loss = 1.2871  Validation loss = 3.0421  \n",
      "\n",
      "Fold: 4  Epoch: 359  Training loss = 1.2869  Validation loss = 3.0366  \n",
      "\n",
      "Fold: 4  Epoch: 360  Training loss = 1.2865  Validation loss = 3.0375  \n",
      "\n",
      "Fold: 4  Epoch: 361  Training loss = 1.2862  Validation loss = 3.0315  \n",
      "\n",
      "Fold: 4  Epoch: 362  Training loss = 1.2859  Validation loss = 3.0315  \n",
      "\n",
      "Fold: 4  Epoch: 363  Training loss = 1.2853  Validation loss = 3.0261  \n",
      "\n",
      "Fold: 4  Epoch: 364  Training loss = 1.2850  Validation loss = 3.0198  \n",
      "\n",
      "Fold: 4  Epoch: 365  Training loss = 1.2853  Validation loss = 3.0171  \n",
      "\n",
      "Fold: 4  Epoch: 366  Training loss = 1.2846  Validation loss = 3.0207  \n",
      "\n",
      "Fold: 4  Epoch: 367  Training loss = 1.2843  Validation loss = 3.0149  \n",
      "\n",
      "Fold: 4  Epoch: 368  Training loss = 1.2839  Validation loss = 3.0177  \n",
      "\n",
      "Fold: 4  Epoch: 369  Training loss = 1.2838  Validation loss = 3.0123  \n",
      "\n",
      "Fold: 4  Epoch: 370  Training loss = 1.2831  Validation loss = 3.0103  \n",
      "\n",
      "Fold: 4  Epoch: 371  Training loss = 1.2831  Validation loss = 3.0118  \n",
      "\n",
      "Fold: 4  Epoch: 372  Training loss = 1.2826  Validation loss = 3.0117  \n",
      "\n",
      "Fold: 4  Epoch: 373  Training loss = 1.2826  Validation loss = 3.0139  \n",
      "\n",
      "Fold: 4  Epoch: 374  Training loss = 1.2824  Validation loss = 3.0132  \n",
      "\n",
      "Fold: 4  Epoch: 375  Training loss = 1.2822  Validation loss = 3.0155  \n",
      "\n",
      "Fold: 4  Epoch: 376  Training loss = 1.2821  Validation loss = 3.0144  \n",
      "\n",
      "Fold: 4  Epoch: 377  Training loss = 1.2815  Validation loss = 3.0087  \n",
      "\n",
      "Fold: 4  Epoch: 378  Training loss = 1.2806  Validation loss = 3.0027  \n",
      "\n",
      "Fold: 4  Epoch: 379  Training loss = 1.2803  Validation loss = 3.0024  \n",
      "\n",
      "Fold: 4  Epoch: 380  Training loss = 1.2800  Validation loss = 2.9957  \n",
      "\n",
      "Fold: 4  Epoch: 381  Training loss = 1.2802  Validation loss = 2.9928  \n",
      "\n",
      "Fold: 4  Epoch: 382  Training loss = 1.2800  Validation loss = 2.9882  \n",
      "\n",
      "Fold: 4  Epoch: 383  Training loss = 1.2797  Validation loss = 2.9876  \n",
      "\n",
      "Fold: 4  Epoch: 384  Training loss = 1.2789  Validation loss = 2.9862  \n",
      "\n",
      "Fold: 4  Epoch: 385  Training loss = 1.2786  Validation loss = 2.9917  \n",
      "\n",
      "Fold: 4  Epoch: 386  Training loss = 1.2782  Validation loss = 2.9884  \n",
      "\n",
      "Fold: 4  Epoch: 387  Training loss = 1.2778  Validation loss = 2.9860  \n",
      "\n",
      "Fold: 4  Epoch: 388  Training loss = 1.2775  Validation loss = 2.9871  \n",
      "\n",
      "Fold: 4  Epoch: 389  Training loss = 1.2771  Validation loss = 2.9821  \n",
      "\n",
      "Fold: 4  Epoch: 390  Training loss = 1.2768  Validation loss = 2.9809  \n",
      "\n",
      "Fold: 4  Epoch: 391  Training loss = 1.2772  Validation loss = 2.9857  \n",
      "\n",
      "Fold: 4  Epoch: 392  Training loss = 1.2770  Validation loss = 2.9851  \n",
      "\n",
      "Fold: 4  Epoch: 393  Training loss = 1.2762  Validation loss = 2.9790  \n",
      "\n",
      "Fold: 4  Epoch: 394  Training loss = 1.2760  Validation loss = 2.9757  \n",
      "\n",
      "Fold: 4  Epoch: 395  Training loss = 1.2759  Validation loss = 2.9776  \n",
      "\n",
      "Fold: 4  Epoch: 396  Training loss = 1.2757  Validation loss = 2.9724  \n",
      "\n",
      "Fold: 4  Epoch: 397  Training loss = 1.2755  Validation loss = 2.9651  \n",
      "\n",
      "Fold: 4  Epoch: 398  Training loss = 1.2754  Validation loss = 2.9626  \n",
      "\n",
      "Fold: 4  Epoch: 399  Training loss = 1.2749  Validation loss = 2.9646  \n",
      "\n",
      "Fold: 4  Epoch: 400  Training loss = 1.2746  Validation loss = 2.9610  \n",
      "\n",
      "Fold: 4  Epoch: 401  Training loss = 1.2745  Validation loss = 2.9552  \n",
      "\n",
      "Fold: 4  Epoch: 402  Training loss = 1.2740  Validation loss = 2.9583  \n",
      "\n",
      "Fold: 4  Epoch: 403  Training loss = 1.2736  Validation loss = 2.9549  \n",
      "\n",
      "Fold: 4  Epoch: 404  Training loss = 1.2737  Validation loss = 2.9563  \n",
      "\n",
      "Fold: 4  Epoch: 405  Training loss = 1.2735  Validation loss = 2.9575  \n",
      "\n",
      "Fold: 4  Epoch: 406  Training loss = 1.2730  Validation loss = 2.9560  \n",
      "\n",
      "Fold: 4  Epoch: 407  Training loss = 1.2727  Validation loss = 2.9533  \n",
      "\n",
      "Fold: 4  Epoch: 408  Training loss = 1.2723  Validation loss = 2.9473  \n",
      "\n",
      "Fold: 4  Epoch: 409  Training loss = 1.2721  Validation loss = 2.9483  \n",
      "\n",
      "Fold: 4  Epoch: 410  Training loss = 1.2720  Validation loss = 2.9443  \n",
      "\n",
      "Fold: 4  Epoch: 411  Training loss = 1.2715  Validation loss = 2.9471  \n",
      "\n",
      "Fold: 4  Epoch: 412  Training loss = 1.2712  Validation loss = 2.9469  \n",
      "\n",
      "Fold: 4  Epoch: 413  Training loss = 1.2711  Validation loss = 2.9443  \n",
      "\n",
      "Fold: 4  Epoch: 414  Training loss = 1.2711  Validation loss = 2.9451  \n",
      "\n",
      "Fold: 4  Epoch: 415  Training loss = 1.2704  Validation loss = 2.9413  \n",
      "\n",
      "Fold: 4  Epoch: 416  Training loss = 1.2701  Validation loss = 2.9435  \n",
      "\n",
      "Fold: 4  Epoch: 417  Training loss = 1.2700  Validation loss = 2.9403  \n",
      "\n",
      "Fold: 4  Epoch: 418  Training loss = 1.2696  Validation loss = 2.9387  \n",
      "\n",
      "Fold: 4  Epoch: 419  Training loss = 1.2693  Validation loss = 2.9358  \n",
      "\n",
      "Fold: 4  Epoch: 420  Training loss = 1.2691  Validation loss = 2.9329  \n",
      "\n",
      "Fold: 4  Epoch: 421  Training loss = 1.2690  Validation loss = 2.9340  \n",
      "\n",
      "Fold: 4  Epoch: 422  Training loss = 1.2690  Validation loss = 2.9329  \n",
      "\n",
      "Fold: 4  Epoch: 423  Training loss = 1.2691  Validation loss = 2.9286  \n",
      "\n",
      "Fold: 4  Epoch: 424  Training loss = 1.2688  Validation loss = 2.9289  \n",
      "\n",
      "Fold: 4  Epoch: 425  Training loss = 1.2685  Validation loss = 2.9247  \n",
      "\n",
      "Fold: 4  Epoch: 426  Training loss = 1.2680  Validation loss = 2.9284  \n",
      "\n",
      "Fold: 4  Epoch: 427  Training loss = 1.2678  Validation loss = 2.9250  \n",
      "\n",
      "Fold: 4  Epoch: 428  Training loss = 1.2676  Validation loss = 2.9204  \n",
      "\n",
      "Fold: 4  Epoch: 429  Training loss = 1.2676  Validation loss = 2.9197  \n",
      "\n",
      "Fold: 4  Epoch: 430  Training loss = 1.2670  Validation loss = 2.9204  \n",
      "\n",
      "Fold: 4  Epoch: 431  Training loss = 1.2667  Validation loss = 2.9214  \n",
      "\n",
      "Fold: 4  Epoch: 432  Training loss = 1.2665  Validation loss = 2.9163  \n",
      "\n",
      "Fold: 4  Epoch: 433  Training loss = 1.2663  Validation loss = 2.9176  \n",
      "\n",
      "Fold: 4  Epoch: 434  Training loss = 1.2659  Validation loss = 2.9136  \n",
      "\n",
      "Fold: 4  Epoch: 435  Training loss = 1.2656  Validation loss = 2.9162  \n",
      "\n",
      "Fold: 4  Epoch: 436  Training loss = 1.2656  Validation loss = 2.9187  \n",
      "\n",
      "Fold: 4  Epoch: 437  Training loss = 1.2659  Validation loss = 2.9249  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 434  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.3644  Validation loss = 2.7492  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.3622  Validation loss = 2.7398  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.3600  Validation loss = 2.7280  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.3585  Validation loss = 2.7203  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.3557  Validation loss = 2.7068  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 1.3544  Validation loss = 2.7079  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 1.3527  Validation loss = 2.6998  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.3516  Validation loss = 2.6961  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 1.3499  Validation loss = 2.6935  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.3481  Validation loss = 2.6897  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 1.3461  Validation loss = 2.6789  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 1.3452  Validation loss = 2.6766  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 1.3435  Validation loss = 2.6654  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.3423  Validation loss = 2.6559  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 1.3409  Validation loss = 2.6475  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 1.3401  Validation loss = 2.6449  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 1.3388  Validation loss = 2.6344  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 1.3383  Validation loss = 2.6364  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 1.3363  Validation loss = 2.6273  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 1.3349  Validation loss = 2.6204  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 1.3342  Validation loss = 2.6180  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 1.3321  Validation loss = 2.6079  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 1.3305  Validation loss = 2.5924  \n",
      "\n",
      "Fold: 5  Epoch: 24  Training loss = 1.3291  Validation loss = 2.5853  \n",
      "\n",
      "Fold: 5  Epoch: 25  Training loss = 1.3276  Validation loss = 2.5744  \n",
      "\n",
      "Fold: 5  Epoch: 26  Training loss = 1.3263  Validation loss = 2.5723  \n",
      "\n",
      "Fold: 5  Epoch: 27  Training loss = 1.3249  Validation loss = 2.5671  \n",
      "\n",
      "Fold: 5  Epoch: 28  Training loss = 1.3244  Validation loss = 2.5695  \n",
      "\n",
      "Fold: 5  Epoch: 29  Training loss = 1.3232  Validation loss = 2.5656  \n",
      "\n",
      "Fold: 5  Epoch: 30  Training loss = 1.3224  Validation loss = 2.5649  \n",
      "\n",
      "Fold: 5  Epoch: 31  Training loss = 1.3210  Validation loss = 2.5506  \n",
      "\n",
      "Fold: 5  Epoch: 32  Training loss = 1.3202  Validation loss = 2.5431  \n",
      "\n",
      "Fold: 5  Epoch: 33  Training loss = 1.3195  Validation loss = 2.5462  \n",
      "\n",
      "Fold: 5  Epoch: 34  Training loss = 1.3183  Validation loss = 2.5416  \n",
      "\n",
      "Fold: 5  Epoch: 35  Training loss = 1.3174  Validation loss = 2.5368  \n",
      "\n",
      "Fold: 5  Epoch: 36  Training loss = 1.3167  Validation loss = 2.5361  \n",
      "\n",
      "Fold: 5  Epoch: 37  Training loss = 1.3156  Validation loss = 2.5322  \n",
      "\n",
      "Fold: 5  Epoch: 38  Training loss = 1.3145  Validation loss = 2.5295  \n",
      "\n",
      "Fold: 5  Epoch: 39  Training loss = 1.3133  Validation loss = 2.5234  \n",
      "\n",
      "Fold: 5  Epoch: 40  Training loss = 1.3125  Validation loss = 2.5220  \n",
      "\n",
      "Fold: 5  Epoch: 41  Training loss = 1.3119  Validation loss = 2.5185  \n",
      "\n",
      "Fold: 5  Epoch: 42  Training loss = 1.3112  Validation loss = 2.5058  \n",
      "\n",
      "Fold: 5  Epoch: 43  Training loss = 1.3107  Validation loss = 2.5103  \n",
      "\n",
      "Fold: 5  Epoch: 44  Training loss = 1.3110  Validation loss = 2.5072  \n",
      "\n",
      "Fold: 5  Epoch: 45  Training loss = 1.3099  Validation loss = 2.5061  \n",
      "\n",
      "Fold: 5  Epoch: 46  Training loss = 1.3091  Validation loss = 2.4979  \n",
      "\n",
      "Fold: 5  Epoch: 47  Training loss = 1.3067  Validation loss = 2.4817  \n",
      "\n",
      "Fold: 5  Epoch: 48  Training loss = 1.3062  Validation loss = 2.4829  \n",
      "\n",
      "Fold: 5  Epoch: 49  Training loss = 1.3049  Validation loss = 2.4824  \n",
      "\n",
      "Fold: 5  Epoch: 50  Training loss = 1.3044  Validation loss = 2.4817  \n",
      "\n",
      "Fold: 5  Epoch: 51  Training loss = 1.3032  Validation loss = 2.4704  \n",
      "\n",
      "Fold: 5  Epoch: 52  Training loss = 1.3022  Validation loss = 2.4616  \n",
      "\n",
      "Fold: 5  Epoch: 53  Training loss = 1.3014  Validation loss = 2.4526  \n",
      "\n",
      "Fold: 5  Epoch: 54  Training loss = 1.3000  Validation loss = 2.4477  \n",
      "\n",
      "Fold: 5  Epoch: 55  Training loss = 1.2992  Validation loss = 2.4404  \n",
      "\n",
      "Fold: 5  Epoch: 56  Training loss = 1.2976  Validation loss = 2.4311  \n",
      "\n",
      "Fold: 5  Epoch: 57  Training loss = 1.2964  Validation loss = 2.4157  \n",
      "\n",
      "Fold: 5  Epoch: 58  Training loss = 1.2958  Validation loss = 2.4116  \n",
      "\n",
      "Fold: 5  Epoch: 59  Training loss = 1.2957  Validation loss = 2.4061  \n",
      "\n",
      "Fold: 5  Epoch: 60  Training loss = 1.2947  Validation loss = 2.3996  \n",
      "\n",
      "Fold: 5  Epoch: 61  Training loss = 1.2932  Validation loss = 2.4041  \n",
      "\n",
      "Fold: 5  Epoch: 62  Training loss = 1.2924  Validation loss = 2.4076  \n",
      "\n",
      "Fold: 5  Epoch: 63  Training loss = 1.2916  Validation loss = 2.4044  \n",
      "\n",
      "Fold: 5  Epoch: 64  Training loss = 1.2912  Validation loss = 2.4065  \n",
      "\n",
      "Fold: 5  Epoch: 65  Training loss = 1.2901  Validation loss = 2.4008  \n",
      "\n",
      "Fold: 5  Epoch: 66  Training loss = 1.2892  Validation loss = 2.3920  \n",
      "\n",
      "Fold: 5  Epoch: 67  Training loss = 1.2887  Validation loss = 2.3881  \n",
      "\n",
      "Fold: 5  Epoch: 68  Training loss = 1.2879  Validation loss = 2.3897  \n",
      "\n",
      "Fold: 5  Epoch: 69  Training loss = 1.2871  Validation loss = 2.3853  \n",
      "\n",
      "Fold: 5  Epoch: 70  Training loss = 1.2864  Validation loss = 2.3865  \n",
      "\n",
      "Fold: 5  Epoch: 71  Training loss = 1.2851  Validation loss = 2.3788  \n",
      "\n",
      "Fold: 5  Epoch: 72  Training loss = 1.2846  Validation loss = 2.3783  \n",
      "\n",
      "Fold: 5  Epoch: 73  Training loss = 1.2835  Validation loss = 2.3720  \n",
      "\n",
      "Fold: 5  Epoch: 74  Training loss = 1.2831  Validation loss = 2.3710  \n",
      "\n",
      "Fold: 5  Epoch: 75  Training loss = 1.2827  Validation loss = 2.3752  \n",
      "\n",
      "Fold: 5  Epoch: 76  Training loss = 1.2817  Validation loss = 2.3691  \n",
      "\n",
      "Fold: 5  Epoch: 77  Training loss = 1.2812  Validation loss = 2.3650  \n",
      "\n",
      "Fold: 5  Epoch: 78  Training loss = 1.2812  Validation loss = 2.3639  \n",
      "\n",
      "Fold: 5  Epoch: 79  Training loss = 1.2805  Validation loss = 2.3611  \n",
      "\n",
      "Fold: 5  Epoch: 80  Training loss = 1.2783  Validation loss = 2.3488  \n",
      "\n",
      "Fold: 5  Epoch: 81  Training loss = 1.2770  Validation loss = 2.3422  \n",
      "\n",
      "Fold: 5  Epoch: 82  Training loss = 1.2766  Validation loss = 2.3403  \n",
      "\n",
      "Fold: 5  Epoch: 83  Training loss = 1.2760  Validation loss = 2.3350  \n",
      "\n",
      "Fold: 5  Epoch: 84  Training loss = 1.2750  Validation loss = 2.3282  \n",
      "\n",
      "Fold: 5  Epoch: 85  Training loss = 1.2744  Validation loss = 2.3236  \n",
      "\n",
      "Fold: 5  Epoch: 86  Training loss = 1.2739  Validation loss = 2.3179  \n",
      "\n",
      "Fold: 5  Epoch: 87  Training loss = 1.2731  Validation loss = 2.3191  \n",
      "\n",
      "Fold: 5  Epoch: 88  Training loss = 1.2723  Validation loss = 2.3149  \n",
      "\n",
      "Fold: 5  Epoch: 89  Training loss = 1.2717  Validation loss = 2.2994  \n",
      "\n",
      "Fold: 5  Epoch: 90  Training loss = 1.2710  Validation loss = 2.2999  \n",
      "\n",
      "Fold: 5  Epoch: 91  Training loss = 1.2706  Validation loss = 2.2918  \n",
      "\n",
      "Fold: 5  Epoch: 92  Training loss = 1.2700  Validation loss = 2.2859  \n",
      "\n",
      "Fold: 5  Epoch: 93  Training loss = 1.2685  Validation loss = 2.2863  \n",
      "\n",
      "Fold: 5  Epoch: 94  Training loss = 1.2683  Validation loss = 2.2828  \n",
      "\n",
      "Fold: 5  Epoch: 95  Training loss = 1.2674  Validation loss = 2.2846  \n",
      "\n",
      "Fold: 5  Epoch: 96  Training loss = 1.2666  Validation loss = 2.2836  \n",
      "\n",
      "Fold: 5  Epoch: 97  Training loss = 1.2665  Validation loss = 2.2711  \n",
      "\n",
      "Fold: 5  Epoch: 98  Training loss = 1.2655  Validation loss = 2.2657  \n",
      "\n",
      "Fold: 5  Epoch: 99  Training loss = 1.2648  Validation loss = 2.2668  \n",
      "\n",
      "Fold: 5  Epoch: 100  Training loss = 1.2640  Validation loss = 2.2612  \n",
      "\n",
      "Fold: 5  Epoch: 101  Training loss = 1.2631  Validation loss = 2.2567  \n",
      "\n",
      "Fold: 5  Epoch: 102  Training loss = 1.2630  Validation loss = 2.2487  \n",
      "\n",
      "Fold: 5  Epoch: 103  Training loss = 1.2629  Validation loss = 2.2455  \n",
      "\n",
      "Fold: 5  Epoch: 104  Training loss = 1.2626  Validation loss = 2.2458  \n",
      "\n",
      "Fold: 5  Epoch: 105  Training loss = 1.2612  Validation loss = 2.2535  \n",
      "\n",
      "Fold: 5  Epoch: 106  Training loss = 1.2610  Validation loss = 2.2529  \n",
      "\n",
      "Fold: 5  Epoch: 107  Training loss = 1.2606  Validation loss = 2.2501  \n",
      "\n",
      "Fold: 5  Epoch: 108  Training loss = 1.2602  Validation loss = 2.2470  \n",
      "\n",
      "Fold: 5  Epoch: 109  Training loss = 1.2596  Validation loss = 2.2417  \n",
      "\n",
      "Fold: 5  Epoch: 110  Training loss = 1.2587  Validation loss = 2.2388  \n",
      "\n",
      "Fold: 5  Epoch: 111  Training loss = 1.2582  Validation loss = 2.2284  \n",
      "\n",
      "Fold: 5  Epoch: 112  Training loss = 1.2579  Validation loss = 2.2229  \n",
      "\n",
      "Fold: 5  Epoch: 113  Training loss = 1.2568  Validation loss = 2.2178  \n",
      "\n",
      "Fold: 5  Epoch: 114  Training loss = 1.2567  Validation loss = 2.2140  \n",
      "\n",
      "Fold: 5  Epoch: 115  Training loss = 1.2560  Validation loss = 2.2175  \n",
      "\n",
      "Fold: 5  Epoch: 116  Training loss = 1.2552  Validation loss = 2.2215  \n",
      "\n",
      "Fold: 5  Epoch: 117  Training loss = 1.2550  Validation loss = 2.2171  \n",
      "\n",
      "Fold: 5  Epoch: 118  Training loss = 1.2547  Validation loss = 2.2220  \n",
      "\n",
      "Fold: 5  Epoch: 119  Training loss = 1.2550  Validation loss = 2.2255  \n",
      "\n",
      "Fold: 5  Epoch: 120  Training loss = 1.2576  Validation loss = 2.2323  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 114  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 1.3482  Validation loss = 0.5513  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 1.3471  Validation loss = 0.5521  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 1.3400  Validation loss = 0.5473  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 1.3366  Validation loss = 0.5494  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 1.3341  Validation loss = 0.5527  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 1.3328  Validation loss = 0.5553  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 1.3304  Validation loss = 0.5597  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 1.3277  Validation loss = 0.5607  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 1.3236  Validation loss = 0.5670  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 1.3225  Validation loss = 0.5643  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 1.3201  Validation loss = 0.5679  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 3  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 1.2875  Validation loss = 0.7343  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 1.2860  Validation loss = 0.7566  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 1.2847  Validation loss = 0.7510  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.2839  Validation loss = 0.7531  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 1.2819  Validation loss = 0.7640  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 1.2808  Validation loss = 0.7494  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 1.2801  Validation loss = 0.7552  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 1.2787  Validation loss = 0.7574  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 1.2774  Validation loss = 0.7312  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 1.2758  Validation loss = 0.7353  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 1.2752  Validation loss = 0.7327  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 1.2731  Validation loss = 0.7420  \n",
      "\n",
      "Fold: 7  Epoch: 13  Training loss = 1.2722  Validation loss = 0.7428  \n",
      "\n",
      "Fold: 7  Epoch: 14  Training loss = 1.2713  Validation loss = 0.7465  \n",
      "\n",
      "Fold: 7  Epoch: 15  Training loss = 1.2707  Validation loss = 0.7367  \n",
      "\n",
      "Fold: 7  Epoch: 16  Training loss = 1.2696  Validation loss = 0.7490  \n",
      "\n",
      "Fold: 7  Epoch: 17  Training loss = 1.2680  Validation loss = 0.7962  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 9  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 1.2031  Validation loss = 4.8734  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 1.2030  Validation loss = 4.8751  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 1.2025  Validation loss = 4.8804  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 1.2008  Validation loss = 4.8593  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 1.1993  Validation loss = 4.8496  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 1.1983  Validation loss = 4.8451  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 1.1978  Validation loss = 4.8251  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 1.1957  Validation loss = 4.8265  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 1.1951  Validation loss = 4.8325  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 1.1941  Validation loss = 4.8404  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 1.1942  Validation loss = 4.8554  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 1.1929  Validation loss = 4.8464  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 1.1916  Validation loss = 4.8287  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 1.1913  Validation loss = 4.8406  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 1.1903  Validation loss = 4.8308  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 1.1886  Validation loss = 4.8216  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 1.1878  Validation loss = 4.8119  \n",
      "\n",
      "Fold: 8  Epoch: 18  Training loss = 1.1871  Validation loss = 4.8156  \n",
      "\n",
      "Fold: 8  Epoch: 19  Training loss = 1.1861  Validation loss = 4.8121  \n",
      "\n",
      "Fold: 8  Epoch: 20  Training loss = 1.1852  Validation loss = 4.7902  \n",
      "\n",
      "Fold: 8  Epoch: 21  Training loss = 1.1850  Validation loss = 4.8015  \n",
      "\n",
      "Fold: 8  Epoch: 22  Training loss = 1.1836  Validation loss = 4.7927  \n",
      "\n",
      "Fold: 8  Epoch: 23  Training loss = 1.1829  Validation loss = 4.7881  \n",
      "\n",
      "Fold: 8  Epoch: 24  Training loss = 1.1823  Validation loss = 4.7788  \n",
      "\n",
      "Fold: 8  Epoch: 25  Training loss = 1.1821  Validation loss = 4.7580  \n",
      "\n",
      "Fold: 8  Epoch: 26  Training loss = 1.1810  Validation loss = 4.7683  \n",
      "\n",
      "Fold: 8  Epoch: 27  Training loss = 1.1801  Validation loss = 4.7662  \n",
      "\n",
      "Fold: 8  Epoch: 28  Training loss = 1.1791  Validation loss = 4.7780  \n",
      "\n",
      "Fold: 8  Epoch: 29  Training loss = 1.1784  Validation loss = 4.7707  \n",
      "\n",
      "Fold: 8  Epoch: 30  Training loss = 1.1773  Validation loss = 4.7631  \n",
      "\n",
      "Fold: 8  Epoch: 31  Training loss = 1.1764  Validation loss = 4.7545  \n",
      "\n",
      "Fold: 8  Epoch: 32  Training loss = 1.1750  Validation loss = 4.7705  \n",
      "\n",
      "Fold: 8  Epoch: 33  Training loss = 1.1748  Validation loss = 4.7849  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 31  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 1.6576  Validation loss = 8.2638  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 1.6464  Validation loss = 8.2355  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 1.6430  Validation loss = 8.2214  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 1.6419  Validation loss = 8.2163  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 1.6375  Validation loss = 8.2024  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 1.6364  Validation loss = 8.1932  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 1.6405  Validation loss = 8.1940  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 1.6391  Validation loss = 8.1821  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 1.6340  Validation loss = 8.1780  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 1.6270  Validation loss = 8.1266  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 1.6279  Validation loss = 8.1253  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 1.6231  Validation loss = 8.1111  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 1.6215  Validation loss = 8.1077  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 1.6170  Validation loss = 8.0895  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 1.6127  Validation loss = 8.0587  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 1.6092  Validation loss = 8.0461  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 1.6070  Validation loss = 8.0437  \n",
      "\n",
      "Fold: 9  Epoch: 18  Training loss = 1.6051  Validation loss = 8.0122  \n",
      "\n",
      "Fold: 9  Epoch: 19  Training loss = 1.6038  Validation loss = 8.0052  \n",
      "\n",
      "Fold: 9  Epoch: 20  Training loss = 1.6029  Validation loss = 8.0036  \n",
      "\n",
      "Fold: 9  Epoch: 21  Training loss = 1.5995  Validation loss = 7.9710  \n",
      "\n",
      "Fold: 9  Epoch: 22  Training loss = 1.5978  Validation loss = 7.9663  \n",
      "\n",
      "Fold: 9  Epoch: 23  Training loss = 1.5952  Validation loss = 7.9403  \n",
      "\n",
      "Fold: 9  Epoch: 24  Training loss = 1.5930  Validation loss = 7.9331  \n",
      "\n",
      "Fold: 9  Epoch: 25  Training loss = 1.5912  Validation loss = 7.9300  \n",
      "\n",
      "Fold: 9  Epoch: 26  Training loss = 1.5895  Validation loss = 7.9209  \n",
      "\n",
      "Fold: 9  Epoch: 27  Training loss = 1.5892  Validation loss = 7.9067  \n",
      "\n",
      "Fold: 9  Epoch: 28  Training loss = 1.5874  Validation loss = 7.8975  \n",
      "\n",
      "Fold: 9  Epoch: 29  Training loss = 1.5816  Validation loss = 7.8829  \n",
      "\n",
      "Fold: 9  Epoch: 30  Training loss = 1.5799  Validation loss = 7.8725  \n",
      "\n",
      "Fold: 9  Epoch: 31  Training loss = 1.5780  Validation loss = 7.8721  \n",
      "\n",
      "Fold: 9  Epoch: 32  Training loss = 1.5767  Validation loss = 7.8881  \n",
      "\n",
      "Fold: 9  Epoch: 33  Training loss = 1.5755  Validation loss = 7.9065  \n",
      "\n",
      "Fold: 9  Epoch: 34  Training loss = 1.5736  Validation loss = 7.9172  \n",
      "\n",
      "Fold: 9  Epoch: 35  Training loss = 1.5722  Validation loss = 7.9004  \n",
      "\n",
      "Fold: 9  Epoch: 36  Training loss = 1.5711  Validation loss = 7.8995  \n",
      "\n",
      "Fold: 9  Epoch: 37  Training loss = 1.5707  Validation loss = 7.8714  \n",
      "\n",
      "Fold: 9  Epoch: 38  Training loss = 1.5679  Validation loss = 7.8849  \n",
      "\n",
      "Fold: 9  Epoch: 39  Training loss = 1.5655  Validation loss = 7.8938  \n",
      "\n",
      "Fold: 9  Epoch: 40  Training loss = 1.5658  Validation loss = 7.8377  \n",
      "\n",
      "Fold: 9  Epoch: 41  Training loss = 1.5630  Validation loss = 7.8172  \n",
      "\n",
      "Fold: 9  Epoch: 42  Training loss = 1.5610  Validation loss = 7.8227  \n",
      "\n",
      "Fold: 9  Epoch: 43  Training loss = 1.5576  Validation loss = 7.8880  \n",
      "\n",
      "Fold: 9  Epoch: 44  Training loss = 1.5559  Validation loss = 7.8360  \n",
      "\n",
      "Fold: 9  Epoch: 45  Training loss = 1.5545  Validation loss = 7.8539  \n",
      "\n",
      "Fold: 9  Epoch: 46  Training loss = 1.5526  Validation loss = 7.8413  \n",
      "\n",
      "Fold: 9  Epoch: 47  Training loss = 1.5544  Validation loss = 7.9357  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 41  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 2.4092  Validation loss = 3.4853  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 2.4028  Validation loss = 3.4724  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 2.3968  Validation loss = 3.4599  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 2.3906  Validation loss = 3.4453  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 2.3848  Validation loss = 3.4309  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 2.3767  Validation loss = 3.4075  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 2.3729  Validation loss = 3.3999  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 2.3679  Validation loss = 3.3598  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 2.3606  Validation loss = 3.2701  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 2.3510  Validation loss = 3.2377  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 2.3464  Validation loss = 3.2248  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 2.3417  Validation loss = 3.2143  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 2.3359  Validation loss = 3.2032  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 2.3293  Validation loss = 3.1876  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 2.3202  Validation loss = 3.1642  \n",
      "\n",
      "Fold: 10  Epoch: 16  Training loss = 2.3177  Validation loss = 3.1639  \n",
      "\n",
      "Fold: 10  Epoch: 17  Training loss = 2.3100  Validation loss = 3.1456  \n",
      "\n",
      "Fold: 10  Epoch: 18  Training loss = 2.3053  Validation loss = 3.1350  \n",
      "\n",
      "Fold: 10  Epoch: 19  Training loss = 2.3015  Validation loss = 3.1198  \n",
      "\n",
      "Fold: 10  Epoch: 20  Training loss = 2.2974  Validation loss = 3.1125  \n",
      "\n",
      "Fold: 10  Epoch: 21  Training loss = 2.2913  Validation loss = 3.0985  \n",
      "\n",
      "Fold: 10  Epoch: 22  Training loss = 2.2848  Validation loss = 3.0846  \n",
      "\n",
      "Fold: 10  Epoch: 23  Training loss = 2.2813  Validation loss = 3.0741  \n",
      "\n",
      "Fold: 10  Epoch: 24  Training loss = 2.2732  Validation loss = 3.0573  \n",
      "\n",
      "Fold: 10  Epoch: 25  Training loss = 2.2623  Validation loss = 3.0171  \n",
      "\n",
      "Fold: 10  Epoch: 26  Training loss = 2.2565  Validation loss = 2.9993  \n",
      "\n",
      "Fold: 10  Epoch: 27  Training loss = 2.2509  Validation loss = 2.9886  \n",
      "\n",
      "Fold: 10  Epoch: 28  Training loss = 2.2425  Validation loss = 2.9668  \n",
      "\n",
      "Fold: 10  Epoch: 29  Training loss = 2.2363  Validation loss = 2.9457  \n",
      "\n",
      "Fold: 10  Epoch: 30  Training loss = 2.2316  Validation loss = 2.8510  \n",
      "\n",
      "Fold: 10  Epoch: 31  Training loss = 2.2263  Validation loss = 2.8434  \n",
      "\n",
      "Fold: 10  Epoch: 32  Training loss = 2.2207  Validation loss = 2.8293  \n",
      "\n",
      "Fold: 10  Epoch: 33  Training loss = 2.2184  Validation loss = 2.8232  \n",
      "\n",
      "Fold: 10  Epoch: 34  Training loss = 2.2134  Validation loss = 2.8106  \n",
      "\n",
      "Fold: 10  Epoch: 35  Training loss = 2.2109  Validation loss = 2.8036  \n",
      "\n",
      "Fold: 10  Epoch: 36  Training loss = 2.2032  Validation loss = 2.7832  \n",
      "\n",
      "Fold: 10  Epoch: 37  Training loss = 2.1998  Validation loss = 2.7757  \n",
      "\n",
      "Fold: 10  Epoch: 38  Training loss = 2.1993  Validation loss = 2.7733  \n",
      "\n",
      "Fold: 10  Epoch: 39  Training loss = 2.1959  Validation loss = 2.7657  \n",
      "\n",
      "Fold: 10  Epoch: 40  Training loss = 2.1909  Validation loss = 2.7170  \n",
      "\n",
      "Fold: 10  Epoch: 41  Training loss = 2.1876  Validation loss = 2.7084  \n",
      "\n",
      "Fold: 10  Epoch: 42  Training loss = 2.1850  Validation loss = 2.7011  \n",
      "\n",
      "Fold: 10  Epoch: 43  Training loss = 2.1817  Validation loss = 2.6930  \n",
      "\n",
      "Fold: 10  Epoch: 44  Training loss = 2.1730  Validation loss = 2.6691  \n",
      "\n",
      "Fold: 10  Epoch: 45  Training loss = 2.1714  Validation loss = 2.6655  \n",
      "\n",
      "Fold: 10  Epoch: 46  Training loss = 2.1656  Validation loss = 2.6488  \n",
      "\n",
      "Fold: 10  Epoch: 47  Training loss = 2.1605  Validation loss = 2.6321  \n",
      "\n",
      "Fold: 10  Epoch: 48  Training loss = 2.1552  Validation loss = 2.6180  \n",
      "\n",
      "Fold: 10  Epoch: 49  Training loss = 2.1499  Validation loss = 2.5998  \n",
      "\n",
      "Fold: 10  Epoch: 50  Training loss = 2.1459  Validation loss = 2.5904  \n",
      "\n",
      "Fold: 10  Epoch: 51  Training loss = 2.1457  Validation loss = 2.5975  \n",
      "\n",
      "Fold: 10  Epoch: 52  Training loss = 2.1378  Validation loss = 2.4113  \n",
      "\n",
      "Fold: 10  Epoch: 53  Training loss = 2.1346  Validation loss = 2.4032  \n",
      "\n",
      "Fold: 10  Epoch: 54  Training loss = 2.1292  Validation loss = 2.3854  \n",
      "\n",
      "Fold: 10  Epoch: 55  Training loss = 2.1248  Validation loss = 2.3665  \n",
      "\n",
      "Fold: 10  Epoch: 56  Training loss = 2.1205  Validation loss = 2.3540  \n",
      "\n",
      "Fold: 10  Epoch: 57  Training loss = 2.1171  Validation loss = 2.3526  \n",
      "\n",
      "Fold: 10  Epoch: 58  Training loss = 2.1101  Validation loss = 2.3274  \n",
      "\n",
      "Fold: 10  Epoch: 59  Training loss = 2.1047  Validation loss = 2.3123  \n",
      "\n",
      "Fold: 10  Epoch: 60  Training loss = 2.1003  Validation loss = 2.2745  \n",
      "\n",
      "Fold: 10  Epoch: 61  Training loss = 2.0969  Validation loss = 2.2693  \n",
      "\n",
      "Fold: 10  Epoch: 62  Training loss = 2.0944  Validation loss = 2.2700  \n",
      "\n",
      "Fold: 10  Epoch: 63  Training loss = 2.0940  Validation loss = 2.2721  \n",
      "\n",
      "Fold: 10  Epoch: 64  Training loss = 2.0916  Validation loss = 2.2682  \n",
      "\n",
      "Fold: 10  Epoch: 65  Training loss = 2.0845  Validation loss = 2.2437  \n",
      "\n",
      "Fold: 10  Epoch: 66  Training loss = 2.0810  Validation loss = 2.2303  \n",
      "\n",
      "Fold: 10  Epoch: 67  Training loss = 2.0800  Validation loss = 2.2318  \n",
      "\n",
      "Fold: 10  Epoch: 68  Training loss = 2.0766  Validation loss = 2.2166  \n",
      "\n",
      "Fold: 10  Epoch: 69  Training loss = 2.0745  Validation loss = 2.2134  \n",
      "\n",
      "Fold: 10  Epoch: 70  Training loss = 2.0729  Validation loss = 2.2109  \n",
      "\n",
      "Fold: 10  Epoch: 71  Training loss = 2.0683  Validation loss = 2.1977  \n",
      "\n",
      "Fold: 10  Epoch: 72  Training loss = 2.0654  Validation loss = 2.1911  \n",
      "\n",
      "Fold: 10  Epoch: 73  Training loss = 2.0633  Validation loss = 2.1810  \n",
      "\n",
      "Fold: 10  Epoch: 74  Training loss = 2.0623  Validation loss = 2.1875  \n",
      "\n",
      "Fold: 10  Epoch: 75  Training loss = 2.0596  Validation loss = 2.1774  \n",
      "\n",
      "Fold: 10  Epoch: 76  Training loss = 2.0549  Validation loss = 2.1574  \n",
      "\n",
      "Fold: 10  Epoch: 77  Training loss = 2.0532  Validation loss = 2.1398  \n",
      "\n",
      "Fold: 10  Epoch: 78  Training loss = 2.0506  Validation loss = 2.1315  \n",
      "\n",
      "Fold: 10  Epoch: 79  Training loss = 2.0475  Validation loss = 2.1324  \n",
      "\n",
      "Fold: 10  Epoch: 80  Training loss = 2.0442  Validation loss = 2.1098  \n",
      "\n",
      "Fold: 10  Epoch: 81  Training loss = 2.0439  Validation loss = 2.0970  \n",
      "\n",
      "Fold: 10  Epoch: 82  Training loss = 2.0378  Validation loss = 2.1036  \n",
      "\n",
      "Fold: 10  Epoch: 83  Training loss = 2.0348  Validation loss = 2.0773  \n",
      "\n",
      "Fold: 10  Epoch: 84  Training loss = 2.0326  Validation loss = 2.0780  \n",
      "\n",
      "Fold: 10  Epoch: 85  Training loss = 2.0302  Validation loss = 2.0613  \n",
      "\n",
      "Fold: 10  Epoch: 86  Training loss = 2.0256  Validation loss = 2.0665  \n",
      "\n",
      "Fold: 10  Epoch: 87  Training loss = 2.0215  Validation loss = 2.0577  \n",
      "\n",
      "Fold: 10  Epoch: 88  Training loss = 2.0198  Validation loss = 2.0461  \n",
      "\n",
      "Fold: 10  Epoch: 89  Training loss = 2.0189  Validation loss = 2.0460  \n",
      "\n",
      "Fold: 10  Epoch: 90  Training loss = 2.0172  Validation loss = 2.0445  \n",
      "\n",
      "Fold: 10  Epoch: 91  Training loss = 2.0166  Validation loss = 2.0507  \n",
      "\n",
      "Fold: 10  Epoch: 92  Training loss = 2.0117  Validation loss = 2.0411  \n",
      "\n",
      "Fold: 10  Epoch: 93  Training loss = 2.0103  Validation loss = 2.0384  \n",
      "\n",
      "Fold: 10  Epoch: 94  Training loss = 2.0078  Validation loss = 2.0284  \n",
      "\n",
      "Fold: 10  Epoch: 95  Training loss = 2.0050  Validation loss = 2.0238  \n",
      "\n",
      "Fold: 10  Epoch: 96  Training loss = 2.0044  Validation loss = 1.9576  \n",
      "\n",
      "Fold: 10  Epoch: 97  Training loss = 2.0006  Validation loss = 1.9460  \n",
      "\n",
      "Fold: 10  Epoch: 98  Training loss = 1.9978  Validation loss = 1.9359  \n",
      "\n",
      "Fold: 10  Epoch: 99  Training loss = 1.9981  Validation loss = 1.9438  \n",
      "\n",
      "Fold: 10  Epoch: 100  Training loss = 1.9963  Validation loss = 1.9474  \n",
      "\n",
      "Fold: 10  Epoch: 101  Training loss = 1.9939  Validation loss = 1.9427  \n",
      "\n",
      "Fold: 10  Epoch: 102  Training loss = 1.9907  Validation loss = 1.9268  \n",
      "\n",
      "Fold: 10  Epoch: 103  Training loss = 1.9896  Validation loss = 1.9512  \n",
      "\n",
      "Fold: 10  Epoch: 104  Training loss = 1.9862  Validation loss = 1.9255  \n",
      "\n",
      "Fold: 10  Epoch: 105  Training loss = 1.9831  Validation loss = 1.9295  \n",
      "\n",
      "Fold: 10  Epoch: 106  Training loss = 1.9832  Validation loss = 1.9350  \n",
      "\n",
      "Fold: 10  Epoch: 107  Training loss = 1.9795  Validation loss = 1.9092  \n",
      "\n",
      "Fold: 10  Epoch: 108  Training loss = 1.9784  Validation loss = 1.8840  \n",
      "\n",
      "Fold: 10  Epoch: 109  Training loss = 1.9768  Validation loss = 1.8792  \n",
      "\n",
      "Fold: 10  Epoch: 110  Training loss = 1.9753  Validation loss = 1.8666  \n",
      "\n",
      "Fold: 10  Epoch: 111  Training loss = 1.9723  Validation loss = 1.8891  \n",
      "\n",
      "Fold: 10  Epoch: 112  Training loss = 1.9705  Validation loss = 1.8905  \n",
      "\n",
      "Fold: 10  Epoch: 113  Training loss = 1.9724  Validation loss = 1.8557  \n",
      "\n",
      "Fold: 10  Epoch: 114  Training loss = 1.9692  Validation loss = 1.8564  \n",
      "\n",
      "Fold: 10  Epoch: 115  Training loss = 1.9651  Validation loss = 1.8650  \n",
      "\n",
      "Fold: 10  Epoch: 116  Training loss = 1.9641  Validation loss = 1.8541  \n",
      "\n",
      "Fold: 10  Epoch: 117  Training loss = 1.9656  Validation loss = 1.8238  \n",
      "\n",
      "Fold: 10  Epoch: 118  Training loss = 1.9628  Validation loss = 1.8157  \n",
      "\n",
      "Fold: 10  Epoch: 119  Training loss = 1.9601  Validation loss = 1.8018  \n",
      "\n",
      "Fold: 10  Epoch: 120  Training loss = 1.9567  Validation loss = 1.7902  \n",
      "\n",
      "Fold: 10  Epoch: 121  Training loss = 1.9607  Validation loss = 1.7519  \n",
      "\n",
      "Fold: 10  Epoch: 122  Training loss = 1.9575  Validation loss = 1.7559  \n",
      "\n",
      "Fold: 10  Epoch: 123  Training loss = 1.9597  Validation loss = 1.7358  \n",
      "\n",
      "Fold: 10  Epoch: 124  Training loss = 1.9530  Validation loss = 1.7487  \n",
      "\n",
      "Fold: 10  Epoch: 125  Training loss = 1.9508  Validation loss = 1.7658  \n",
      "\n",
      "Fold: 10  Epoch: 126  Training loss = 1.9494  Validation loss = 1.7690  \n",
      "\n",
      "Fold: 10  Epoch: 127  Training loss = 1.9487  Validation loss = 1.7987  \n",
      "\n",
      "Fold: 10  Epoch: 128  Training loss = 1.9455  Validation loss = 1.8259  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 123  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 1.9639  Validation loss = 1.7336  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 1.9561  Validation loss = 1.6646  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 1.9472  Validation loss = 1.6158  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 1.9587  Validation loss = 1.6071  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 1.9534  Validation loss = 1.6757  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 1.9520  Validation loss = 1.7555  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 1.9542  Validation loss = 1.7169  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 1.9524  Validation loss = 1.7794  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 1.9500  Validation loss = 1.7569  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 1.9480  Validation loss = 1.6891  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 1.9455  Validation loss = 1.7048  \n",
      "\n",
      "Fold: 11  Epoch: 12  Training loss = 1.9459  Validation loss = 1.6698  \n",
      "\n",
      "Fold: 11  Epoch: 13  Training loss = 1.9451  Validation loss = 1.6797  \n",
      "\n",
      "Fold: 11  Epoch: 14  Training loss = 1.9413  Validation loss = 1.6887  \n",
      "\n",
      "Fold: 11  Epoch: 15  Training loss = 1.9404  Validation loss = 1.6679  \n",
      "\n",
      "Fold: 11  Epoch: 16  Training loss = 1.9388  Validation loss = 1.6288  \n",
      "\n",
      "Fold: 11  Epoch: 17  Training loss = 1.9271  Validation loss = 1.6918  \n",
      "\n",
      "Fold: 11  Epoch: 18  Training loss = 1.9325  Validation loss = 1.6739  \n",
      "\n",
      "Fold: 11  Epoch: 19  Training loss = 1.9226  Validation loss = 1.6215  \n",
      "\n",
      "Fold: 11  Epoch: 20  Training loss = 1.9187  Validation loss = 1.6324  \n",
      "\n",
      "Fold: 11  Epoch: 21  Training loss = 1.9302  Validation loss = 1.5861  \n",
      "\n",
      "Fold: 11  Epoch: 22  Training loss = 1.9327  Validation loss = 1.5645  \n",
      "\n",
      "Fold: 11  Epoch: 23  Training loss = 1.9321  Validation loss = 1.5565  \n",
      "\n",
      "Fold: 11  Epoch: 24  Training loss = 1.9143  Validation loss = 1.5338  \n",
      "\n",
      "Fold: 11  Epoch: 25  Training loss = 1.9059  Validation loss = 1.5147  \n",
      "\n",
      "Fold: 11  Epoch: 26  Training loss = 1.9223  Validation loss = 1.5006  \n",
      "\n",
      "Fold: 11  Epoch: 27  Training loss = 1.9090  Validation loss = 1.5206  \n",
      "\n",
      "Fold: 11  Epoch: 28  Training loss = 1.9010  Validation loss = 1.5033  \n",
      "\n",
      "Fold: 11  Epoch: 29  Training loss = 1.9182  Validation loss = 1.4937  \n",
      "\n",
      "Fold: 11  Epoch: 30  Training loss = 1.8948  Validation loss = 1.4735  \n",
      "\n",
      "Fold: 11  Epoch: 31  Training loss = 1.8927  Validation loss = 1.4647  \n",
      "\n",
      "Fold: 11  Epoch: 32  Training loss = 1.9004  Validation loss = 1.4535  \n",
      "\n",
      "Fold: 11  Epoch: 33  Training loss = 1.8985  Validation loss = 1.4443  \n",
      "\n",
      "Fold: 11  Epoch: 34  Training loss = 1.8902  Validation loss = 1.4609  \n",
      "\n",
      "Fold: 11  Epoch: 35  Training loss = 1.8820  Validation loss = 1.4576  \n",
      "\n",
      "Fold: 11  Epoch: 36  Training loss = 1.8793  Validation loss = 1.4572  \n",
      "\n",
      "Fold: 11  Epoch: 37  Training loss = 1.8741  Validation loss = 1.4546  \n",
      "\n",
      "Fold: 11  Epoch: 38  Training loss = 1.8724  Validation loss = 1.4487  \n",
      "\n",
      "Fold: 11  Epoch: 39  Training loss = 1.8704  Validation loss = 1.4524  \n",
      "\n",
      "Fold: 11  Epoch: 40  Training loss = 1.8679  Validation loss = 1.4381  \n",
      "\n",
      "Fold: 11  Epoch: 41  Training loss = 1.8651  Validation loss = 1.4258  \n",
      "\n",
      "Fold: 11  Epoch: 42  Training loss = 1.8645  Validation loss = 1.4210  \n",
      "\n",
      "Fold: 11  Epoch: 43  Training loss = 1.8619  Validation loss = 1.4053  \n",
      "\n",
      "Fold: 11  Epoch: 44  Training loss = 1.8595  Validation loss = 1.3871  \n",
      "\n",
      "Fold: 11  Epoch: 45  Training loss = 1.8564  Validation loss = 1.3885  \n",
      "\n",
      "Fold: 11  Epoch: 46  Training loss = 1.8561  Validation loss = 1.3857  \n",
      "\n",
      "Fold: 11  Epoch: 47  Training loss = 1.8556  Validation loss = 1.3711  \n",
      "\n",
      "Fold: 11  Epoch: 48  Training loss = 1.8537  Validation loss = 1.3452  \n",
      "\n",
      "Fold: 11  Epoch: 49  Training loss = 1.8506  Validation loss = 1.3488  \n",
      "\n",
      "Fold: 11  Epoch: 50  Training loss = 1.8498  Validation loss = 1.3622  \n",
      "\n",
      "Fold: 11  Epoch: 51  Training loss = 1.8484  Validation loss = 1.3723  \n",
      "\n",
      "Fold: 11  Epoch: 52  Training loss = 1.8521  Validation loss = 1.3894  \n",
      "\n",
      "Fold: 11  Epoch: 53  Training loss = 1.8487  Validation loss = 1.3847  \n",
      "\n",
      "Fold: 11  Epoch: 54  Training loss = 1.8445  Validation loss = 1.4031  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 48  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 1.8445  Validation loss = 2.6662  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 1.8416  Validation loss = 2.6586  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 1.8386  Validation loss = 2.6184  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 1.8309  Validation loss = 2.5598  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 1.8300  Validation loss = 2.5530  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 1.8281  Validation loss = 2.5400  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 1.8277  Validation loss = 2.5305  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 1.8228  Validation loss = 2.4628  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 1.8237  Validation loss = 2.5007  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 1.8219  Validation loss = 2.5185  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 1.8188  Validation loss = 2.4859  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 1.8168  Validation loss = 2.4506  \n",
      "\n",
      "Fold: 12  Epoch: 13  Training loss = 1.8157  Validation loss = 2.4457  \n",
      "\n",
      "Fold: 12  Epoch: 14  Training loss = 1.8138  Validation loss = 2.4558  \n",
      "\n",
      "Fold: 12  Epoch: 15  Training loss = 1.8129  Validation loss = 2.4469  \n",
      "\n",
      "Fold: 12  Epoch: 16  Training loss = 1.8119  Validation loss = 2.3951  \n",
      "\n",
      "Fold: 12  Epoch: 17  Training loss = 1.8084  Validation loss = 2.3302  \n",
      "\n",
      "Fold: 12  Epoch: 18  Training loss = 1.8054  Validation loss = 2.2999  \n",
      "\n",
      "Fold: 12  Epoch: 19  Training loss = 1.8030  Validation loss = 2.2685  \n",
      "\n",
      "Fold: 12  Epoch: 20  Training loss = 1.8032  Validation loss = 2.2665  \n",
      "\n",
      "Fold: 12  Epoch: 21  Training loss = 1.7998  Validation loss = 2.3290  \n",
      "\n",
      "Fold: 12  Epoch: 22  Training loss = 1.7976  Validation loss = 2.3122  \n",
      "\n",
      "Fold: 12  Epoch: 23  Training loss = 1.7945  Validation loss = 2.3319  \n",
      "\n",
      "Fold: 12  Epoch: 24  Training loss = 1.7929  Validation loss = 2.3172  \n",
      "\n",
      "Fold: 12  Epoch: 25  Training loss = 1.7919  Validation loss = 2.3024  \n",
      "\n",
      "Fold: 12  Epoch: 26  Training loss = 1.7914  Validation loss = 2.3249  \n",
      "\n",
      "Fold: 12  Epoch: 27  Training loss = 1.7937  Validation loss = 2.2243  \n",
      "\n",
      "Fold: 12  Epoch: 28  Training loss = 1.7906  Validation loss = 2.2443  \n",
      "\n",
      "Fold: 12  Epoch: 29  Training loss = 1.7882  Validation loss = 2.2161  \n",
      "\n",
      "Fold: 12  Epoch: 30  Training loss = 1.7849  Validation loss = 2.2446  \n",
      "\n",
      "Fold: 12  Epoch: 31  Training loss = 1.7849  Validation loss = 2.2436  \n",
      "\n",
      "Fold: 12  Epoch: 32  Training loss = 1.7833  Validation loss = 2.2542  \n",
      "\n",
      "Fold: 12  Epoch: 33  Training loss = 1.7843  Validation loss = 2.1703  \n",
      "\n",
      "Fold: 12  Epoch: 34  Training loss = 1.7800  Validation loss = 2.1058  \n",
      "\n",
      "Fold: 12  Epoch: 35  Training loss = 1.7793  Validation loss = 2.1738  \n",
      "\n",
      "Fold: 12  Epoch: 36  Training loss = 1.7773  Validation loss = 2.1495  \n",
      "\n",
      "Fold: 12  Epoch: 37  Training loss = 1.7766  Validation loss = 2.1380  \n",
      "\n",
      "Fold: 12  Epoch: 38  Training loss = 1.7760  Validation loss = 2.1359  \n",
      "\n",
      "Fold: 12  Epoch: 39  Training loss = 1.7745  Validation loss = 2.1193  \n",
      "\n",
      "Fold: 12  Epoch: 40  Training loss = 1.7726  Validation loss = 2.0420  \n",
      "\n",
      "Fold: 12  Epoch: 41  Training loss = 1.7735  Validation loss = 2.0605  \n",
      "\n",
      "Fold: 12  Epoch: 42  Training loss = 1.7689  Validation loss = 2.0694  \n",
      "\n",
      "Fold: 12  Epoch: 43  Training loss = 1.7665  Validation loss = 2.1396  \n",
      "\n",
      "Fold: 12  Epoch: 44  Training loss = 1.7651  Validation loss = 2.1137  \n",
      "\n",
      "Fold: 12  Epoch: 45  Training loss = 1.7642  Validation loss = 2.1227  \n",
      "\n",
      "Fold: 12  Epoch: 46  Training loss = 1.7633  Validation loss = 2.1006  \n",
      "\n",
      "Fold: 12  Epoch: 47  Training loss = 1.7627  Validation loss = 2.1553  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 40  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 1.8095  Validation loss = 2.7392  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 1.7863  Validation loss = 2.6880  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 1.7858  Validation loss = 2.7191  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 1.7797  Validation loss = 2.8373  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 1.7756  Validation loss = 2.7865  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 1.7756  Validation loss = 2.6846  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 1.7713  Validation loss = 2.7183  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 1.7663  Validation loss = 2.8827  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 1.7652  Validation loss = 2.9337  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 1.7639  Validation loss = 2.8499  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 1.7606  Validation loss = 2.8964  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 1.7593  Validation loss = 2.8728  \n",
      "\n",
      "Fold: 13  Epoch: 13  Training loss = 1.7580  Validation loss = 2.8274  \n",
      "\n",
      "Fold: 13  Epoch: 14  Training loss = 1.7616  Validation loss = 2.6295  \n",
      "\n",
      "Fold: 13  Epoch: 15  Training loss = 1.7721  Validation loss = 2.4216  \n",
      "\n",
      "Fold: 13  Epoch: 16  Training loss = 1.7658  Validation loss = 2.4648  \n",
      "\n",
      "Fold: 13  Epoch: 17  Training loss = 1.7565  Validation loss = 2.6080  \n",
      "\n",
      "Fold: 13  Epoch: 18  Training loss = 1.7534  Validation loss = 2.6944  \n",
      "\n",
      "Fold: 13  Epoch: 19  Training loss = 1.7515  Validation loss = 2.8121  \n",
      "\n",
      "Fold: 13  Epoch: 20  Training loss = 1.7500  Validation loss = 2.9953  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 15  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 1.8724  Validation loss = 5.0515  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 1.8582  Validation loss = 5.0221  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 1.8721  Validation loss = 5.0193  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 1.8810  Validation loss = 5.0303  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 1.8598  Validation loss = 4.9977  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 1.8586  Validation loss = 4.9891  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 1.8472  Validation loss = 4.9744  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 1.8442  Validation loss = 4.9527  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 1.8416  Validation loss = 4.9204  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 1.8386  Validation loss = 4.9205  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 1.8564  Validation loss = 4.9435  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 1.8530  Validation loss = 4.9381  \n",
      "\n",
      "Fold: 14  Epoch: 13  Training loss = 1.8337  Validation loss = 4.9044  \n",
      "\n",
      "Fold: 14  Epoch: 14  Training loss = 1.8483  Validation loss = 4.8857  \n",
      "\n",
      "Fold: 14  Epoch: 15  Training loss = 1.8319  Validation loss = 4.8910  \n",
      "\n",
      "Fold: 14  Epoch: 16  Training loss = 1.8325  Validation loss = 4.8842  \n",
      "\n",
      "Fold: 14  Epoch: 17  Training loss = 1.8288  Validation loss = 4.8859  \n",
      "\n",
      "Fold: 14  Epoch: 18  Training loss = 1.8320  Validation loss = 4.8676  \n",
      "\n",
      "Fold: 14  Epoch: 19  Training loss = 1.8272  Validation loss = 4.8847  \n",
      "\n",
      "Fold: 14  Epoch: 20  Training loss = 1.8258  Validation loss = 4.8857  \n",
      "\n",
      "Fold: 14  Epoch: 21  Training loss = 1.8266  Validation loss = 4.8575  \n",
      "\n",
      "Fold: 14  Epoch: 22  Training loss = 1.8227  Validation loss = 4.8707  \n",
      "\n",
      "Fold: 14  Epoch: 23  Training loss = 1.8211  Validation loss = 4.8722  \n",
      "\n",
      "Fold: 14  Epoch: 24  Training loss = 1.8210  Validation loss = 4.8718  \n",
      "\n",
      "Fold: 14  Epoch: 25  Training loss = 1.8198  Validation loss = 4.8644  \n",
      "\n",
      "Fold: 14  Epoch: 26  Training loss = 1.8191  Validation loss = 4.8705  \n",
      "\n",
      "Fold: 14  Epoch: 27  Training loss = 1.8153  Validation loss = 4.8689  \n",
      "\n",
      "Fold: 14  Epoch: 28  Training loss = 1.8123  Validation loss = 4.8652  \n",
      "\n",
      "Fold: 14  Epoch: 29  Training loss = 1.8109  Validation loss = 4.8636  \n",
      "\n",
      "Fold: 14  Epoch: 30  Training loss = 1.8085  Validation loss = 4.8572  \n",
      "\n",
      "Fold: 14  Epoch: 31  Training loss = 1.8075  Validation loss = 4.8578  \n",
      "\n",
      "Fold: 14  Epoch: 32  Training loss = 1.8045  Validation loss = 4.8518  \n",
      "\n",
      "Fold: 14  Epoch: 33  Training loss = 1.8039  Validation loss = 4.8460  \n",
      "\n",
      "Fold: 14  Epoch: 34  Training loss = 1.8026  Validation loss = 4.8346  \n",
      "\n",
      "Fold: 14  Epoch: 35  Training loss = 1.8059  Validation loss = 4.8469  \n",
      "\n",
      "Fold: 14  Epoch: 36  Training loss = 1.7985  Validation loss = 4.8426  \n",
      "\n",
      "Fold: 14  Epoch: 37  Training loss = 1.8048  Validation loss = 4.8242  \n",
      "\n",
      "Fold: 14  Epoch: 38  Training loss = 1.8022  Validation loss = 4.8559  \n",
      "\n",
      "Fold: 14  Epoch: 39  Training loss = 1.7988  Validation loss = 4.8561  \n",
      "\n",
      "Fold: 14  Epoch: 40  Training loss = 1.7923  Validation loss = 4.8206  \n",
      "\n",
      "Fold: 14  Epoch: 41  Training loss = 1.7902  Validation loss = 4.8220  \n",
      "\n",
      "Fold: 14  Epoch: 42  Training loss = 1.7842  Validation loss = 4.8110  \n",
      "\n",
      "Fold: 14  Epoch: 43  Training loss = 1.7884  Validation loss = 4.7941  \n",
      "\n",
      "Fold: 14  Epoch: 44  Training loss = 1.7843  Validation loss = 4.7802  \n",
      "\n",
      "Fold: 14  Epoch: 45  Training loss = 1.8080  Validation loss = 4.7995  \n",
      "\n",
      "Fold: 14  Epoch: 46  Training loss = 1.7828  Validation loss = 4.7674  \n",
      "\n",
      "Fold: 14  Epoch: 47  Training loss = 1.7812  Validation loss = 4.7735  \n",
      "\n",
      "Fold: 14  Epoch: 48  Training loss = 1.7827  Validation loss = 4.7729  \n",
      "\n",
      "Fold: 14  Epoch: 49  Training loss = 1.7832  Validation loss = 4.7693  \n",
      "\n",
      "Fold: 14  Epoch: 50  Training loss = 1.7755  Validation loss = 4.7736  \n",
      "\n",
      "Fold: 14  Epoch: 51  Training loss = 1.7738  Validation loss = 4.7727  \n",
      "\n",
      "Fold: 14  Epoch: 52  Training loss = 1.7893  Validation loss = 4.7801  \n",
      "\n",
      "Fold: 14  Epoch: 53  Training loss = 1.7973  Validation loss = 4.7906  \n",
      "\n",
      "Fold: 14  Epoch: 54  Training loss = 1.7744  Validation loss = 4.7805  \n",
      "\n",
      "Fold: 14  Epoch: 55  Training loss = 1.7772  Validation loss = 4.7640  \n",
      "\n",
      "Fold: 14  Epoch: 56  Training loss = 1.7702  Validation loss = 4.7691  \n",
      "\n",
      "Fold: 14  Epoch: 57  Training loss = 1.7713  Validation loss = 4.7546  \n",
      "\n",
      "Fold: 14  Epoch: 58  Training loss = 1.7642  Validation loss = 4.7620  \n",
      "\n",
      "Fold: 14  Epoch: 59  Training loss = 1.7634  Validation loss = 4.7497  \n",
      "\n",
      "Fold: 14  Epoch: 60  Training loss = 1.7677  Validation loss = 4.7385  \n",
      "\n",
      "Fold: 14  Epoch: 61  Training loss = 1.7796  Validation loss = 4.7452  \n",
      "\n",
      "Fold: 14  Epoch: 62  Training loss = 1.7606  Validation loss = 4.7119  \n",
      "\n",
      "Fold: 14  Epoch: 63  Training loss = 1.7705  Validation loss = 4.7166  \n",
      "\n",
      "Fold: 14  Epoch: 64  Training loss = 1.7853  Validation loss = 4.6941  \n",
      "\n",
      "Fold: 14  Epoch: 65  Training loss = 1.7869  Validation loss = 4.6986  \n",
      "\n",
      "Fold: 14  Epoch: 66  Training loss = 1.7842  Validation loss = 4.6934  \n",
      "\n",
      "Fold: 14  Epoch: 67  Training loss = 1.7856  Validation loss = 4.6842  \n",
      "\n",
      "Fold: 14  Epoch: 68  Training loss = 1.7975  Validation loss = 4.6762  \n",
      "\n",
      "Fold: 14  Epoch: 69  Training loss = 1.7850  Validation loss = 4.6923  \n",
      "\n",
      "Fold: 14  Epoch: 70  Training loss = 1.7799  Validation loss = 4.6952  \n",
      "\n",
      "Fold: 14  Epoch: 71  Training loss = 1.7782  Validation loss = 4.7058  \n",
      "\n",
      "Fold: 14  Epoch: 72  Training loss = 1.7778  Validation loss = 4.6997  \n",
      "\n",
      "Fold: 14  Epoch: 73  Training loss = 1.7872  Validation loss = 4.6761  \n",
      "\n",
      "Fold: 14  Epoch: 74  Training loss = 1.7784  Validation loss = 4.6831  \n",
      "\n",
      "Fold: 14  Epoch: 75  Training loss = 1.7931  Validation loss = 4.6604  \n",
      "\n",
      "Fold: 14  Epoch: 76  Training loss = 1.7842  Validation loss = 4.6905  \n",
      "\n",
      "Fold: 14  Epoch: 77  Training loss = 1.7778  Validation loss = 4.6901  \n",
      "\n",
      "Fold: 14  Epoch: 78  Training loss = 1.7708  Validation loss = 4.6886  \n",
      "\n",
      "Fold: 14  Epoch: 79  Training loss = 1.7778  Validation loss = 4.6619  \n",
      "\n",
      "Fold: 14  Epoch: 80  Training loss = 1.7710  Validation loss = 4.6545  \n",
      "\n",
      "Fold: 14  Epoch: 81  Training loss = 1.7722  Validation loss = 4.6631  \n",
      "\n",
      "Fold: 14  Epoch: 82  Training loss = 1.7694  Validation loss = 4.6701  \n",
      "\n",
      "Fold: 14  Epoch: 83  Training loss = 1.7680  Validation loss = 4.6741  \n",
      "\n",
      "Fold: 14  Epoch: 84  Training loss = 1.7674  Validation loss = 4.6594  \n",
      "\n",
      "Fold: 14  Epoch: 85  Training loss = 1.7663  Validation loss = 4.6665  \n",
      "\n",
      "Fold: 14  Epoch: 86  Training loss = 1.7658  Validation loss = 4.6384  \n",
      "\n",
      "Fold: 14  Epoch: 87  Training loss = 1.7707  Validation loss = 4.6345  \n",
      "\n",
      "Fold: 14  Epoch: 88  Training loss = 1.7724  Validation loss = 4.6569  \n",
      "\n",
      "Fold: 14  Epoch: 89  Training loss = 1.7636  Validation loss = 4.6379  \n",
      "\n",
      "Fold: 14  Epoch: 90  Training loss = 1.7750  Validation loss = 4.6689  \n",
      "\n",
      "Fold: 14  Epoch: 91  Training loss = 1.7621  Validation loss = 4.6546  \n",
      "\n",
      "Fold: 14  Epoch: 92  Training loss = 1.7584  Validation loss = 4.6318  \n",
      "\n",
      "Fold: 14  Epoch: 93  Training loss = 1.8034  Validation loss = 4.6224  \n",
      "\n",
      "Fold: 14  Epoch: 94  Training loss = 1.8053  Validation loss = 4.6185  \n",
      "\n",
      "Fold: 14  Epoch: 95  Training loss = 1.7539  Validation loss = 4.6362  \n",
      "\n",
      "Fold: 14  Epoch: 96  Training loss = 1.7539  Validation loss = 4.6503  \n",
      "\n",
      "Fold: 14  Epoch: 97  Training loss = 1.7563  Validation loss = 4.6442  \n",
      "\n",
      "Fold: 14  Epoch: 98  Training loss = 1.7520  Validation loss = 4.6294  \n",
      "\n",
      "Fold: 14  Epoch: 99  Training loss = 1.7578  Validation loss = 4.6444  \n",
      "\n",
      "Fold: 14  Epoch: 100  Training loss = 1.7606  Validation loss = 4.6626  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 94  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 2.0801  Validation loss = 5.2845  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 2.0773  Validation loss = 5.2717  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 2.0749  Validation loss = 5.2735  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 2.0781  Validation loss = 5.2667  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 2.0659  Validation loss = 5.2506  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 2.0624  Validation loss = 5.2399  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 2.0967  Validation loss = 5.2277  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 2.0565  Validation loss = 5.2163  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 2.0572  Validation loss = 5.2038  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 2.0608  Validation loss = 5.1816  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 2.0488  Validation loss = 5.1733  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 2.0403  Validation loss = 5.1608  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 2.0372  Validation loss = 5.1448  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 2.0334  Validation loss = 5.1363  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 2.0301  Validation loss = 5.1315  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 2.0435  Validation loss = 5.1186  \n",
      "\n",
      "Fold: 15  Epoch: 17  Training loss = 2.0251  Validation loss = 5.1238  \n",
      "\n",
      "Fold: 15  Epoch: 18  Training loss = 2.0246  Validation loss = 5.1225  \n",
      "\n",
      "Fold: 15  Epoch: 19  Training loss = 2.0227  Validation loss = 5.1194  \n",
      "\n",
      "Fold: 15  Epoch: 20  Training loss = 2.0323  Validation loss = 5.1028  \n",
      "\n",
      "Fold: 15  Epoch: 21  Training loss = 2.0191  Validation loss = 5.1001  \n",
      "\n",
      "Fold: 15  Epoch: 22  Training loss = 2.0168  Validation loss = 5.1041  \n",
      "\n",
      "Fold: 15  Epoch: 23  Training loss = 2.0307  Validation loss = 5.1034  \n",
      "\n",
      "Fold: 15  Epoch: 24  Training loss = 2.0332  Validation loss = 5.0975  \n",
      "\n",
      "Fold: 15  Epoch: 25  Training loss = 2.0095  Validation loss = 5.0801  \n",
      "\n",
      "Fold: 15  Epoch: 26  Training loss = 2.0073  Validation loss = 5.0778  \n",
      "\n",
      "Fold: 15  Epoch: 27  Training loss = 2.0128  Validation loss = 5.0810  \n",
      "\n",
      "Fold: 15  Epoch: 28  Training loss = 2.0048  Validation loss = 5.0619  \n",
      "\n",
      "Fold: 15  Epoch: 29  Training loss = 2.0028  Validation loss = 5.0597  \n",
      "\n",
      "Fold: 15  Epoch: 30  Training loss = 2.0020  Validation loss = 5.0603  \n",
      "\n",
      "Fold: 15  Epoch: 31  Training loss = 1.9998  Validation loss = 5.0494  \n",
      "\n",
      "Fold: 15  Epoch: 32  Training loss = 1.9959  Validation loss = 5.0376  \n",
      "\n",
      "Fold: 15  Epoch: 33  Training loss = 1.9924  Validation loss = 5.0237  \n",
      "\n",
      "Fold: 15  Epoch: 34  Training loss = 1.9884  Validation loss = 5.0127  \n",
      "\n",
      "Fold: 15  Epoch: 35  Training loss = 1.9867  Validation loss = 5.0037  \n",
      "\n",
      "Fold: 15  Epoch: 36  Training loss = 1.9856  Validation loss = 5.0006  \n",
      "\n",
      "Fold: 15  Epoch: 37  Training loss = 1.9850  Validation loss = 4.9962  \n",
      "\n",
      "Fold: 15  Epoch: 38  Training loss = 1.9845  Validation loss = 4.9964  \n",
      "\n",
      "Fold: 15  Epoch: 39  Training loss = 1.9817  Validation loss = 4.9842  \n",
      "\n",
      "Fold: 15  Epoch: 40  Training loss = 1.9785  Validation loss = 4.9792  \n",
      "\n",
      "Fold: 15  Epoch: 41  Training loss = 2.0080  Validation loss = 4.9726  \n",
      "\n",
      "Fold: 15  Epoch: 42  Training loss = 1.9831  Validation loss = 4.9683  \n",
      "\n",
      "Fold: 15  Epoch: 43  Training loss = 1.9742  Validation loss = 4.9647  \n",
      "\n",
      "Fold: 15  Epoch: 44  Training loss = 1.9730  Validation loss = 4.9571  \n",
      "\n",
      "Fold: 15  Epoch: 45  Training loss = 1.9734  Validation loss = 4.9471  \n",
      "\n",
      "Fold: 15  Epoch: 46  Training loss = 1.9660  Validation loss = 4.9359  \n",
      "\n",
      "Fold: 15  Epoch: 47  Training loss = 1.9649  Validation loss = 4.9230  \n",
      "\n",
      "Fold: 15  Epoch: 48  Training loss = 1.9663  Validation loss = 4.9223  \n",
      "\n",
      "Fold: 15  Epoch: 49  Training loss = 1.9687  Validation loss = 4.9181  \n",
      "\n",
      "Fold: 15  Epoch: 50  Training loss = 1.9627  Validation loss = 4.9221  \n",
      "\n",
      "Fold: 15  Epoch: 51  Training loss = 1.9599  Validation loss = 4.9017  \n",
      "\n",
      "Fold: 15  Epoch: 52  Training loss = 1.9606  Validation loss = 4.8960  \n",
      "\n",
      "Fold: 15  Epoch: 53  Training loss = 1.9578  Validation loss = 4.8841  \n",
      "\n",
      "Fold: 15  Epoch: 54  Training loss = 1.9542  Validation loss = 4.8846  \n",
      "\n",
      "Fold: 15  Epoch: 55  Training loss = 1.9559  Validation loss = 4.8773  \n",
      "\n",
      "Fold: 15  Epoch: 56  Training loss = 1.9532  Validation loss = 4.8779  \n",
      "\n",
      "Fold: 15  Epoch: 57  Training loss = 1.9513  Validation loss = 4.8779  \n",
      "\n",
      "Fold: 15  Epoch: 58  Training loss = 1.9491  Validation loss = 4.8774  \n",
      "\n",
      "Fold: 15  Epoch: 59  Training loss = 1.9502  Validation loss = 4.8718  \n",
      "\n",
      "Fold: 15  Epoch: 60  Training loss = 1.9536  Validation loss = 4.8740  \n",
      "\n",
      "Fold: 15  Epoch: 61  Training loss = 1.9450  Validation loss = 4.8651  \n",
      "\n",
      "Fold: 15  Epoch: 62  Training loss = 1.9466  Validation loss = 4.8586  \n",
      "\n",
      "Fold: 15  Epoch: 63  Training loss = 1.9439  Validation loss = 4.8489  \n",
      "\n",
      "Fold: 15  Epoch: 64  Training loss = 1.9403  Validation loss = 4.8452  \n",
      "\n",
      "Fold: 15  Epoch: 65  Training loss = 1.9509  Validation loss = 4.8376  \n",
      "\n",
      "Fold: 15  Epoch: 66  Training loss = 1.9528  Validation loss = 4.8405  \n",
      "\n",
      "Fold: 15  Epoch: 67  Training loss = 1.9398  Validation loss = 4.8381  \n",
      "\n",
      "Fold: 15  Epoch: 68  Training loss = 1.9358  Validation loss = 4.8323  \n",
      "\n",
      "Fold: 15  Epoch: 69  Training loss = 1.9377  Validation loss = 4.8308  \n",
      "\n",
      "Fold: 15  Epoch: 70  Training loss = 1.9341  Validation loss = 4.8221  \n",
      "\n",
      "Fold: 15  Epoch: 71  Training loss = 1.9258  Validation loss = 4.8140  \n",
      "\n",
      "Fold: 15  Epoch: 72  Training loss = 1.9217  Validation loss = 4.7973  \n",
      "\n",
      "Fold: 15  Epoch: 73  Training loss = 1.9244  Validation loss = 4.7887  \n",
      "\n",
      "Fold: 15  Epoch: 74  Training loss = 1.9187  Validation loss = 4.7791  \n",
      "\n",
      "Fold: 15  Epoch: 75  Training loss = 1.9155  Validation loss = 4.7740  \n",
      "\n",
      "Fold: 15  Epoch: 76  Training loss = 1.9190  Validation loss = 4.7759  \n",
      "\n",
      "Fold: 15  Epoch: 77  Training loss = 1.9160  Validation loss = 4.7699  \n",
      "\n",
      "Fold: 15  Epoch: 78  Training loss = 1.9165  Validation loss = 4.7723  \n",
      "\n",
      "Fold: 15  Epoch: 79  Training loss = 1.9267  Validation loss = 4.7695  \n",
      "\n",
      "Fold: 15  Epoch: 80  Training loss = 1.9208  Validation loss = 4.7654  \n",
      "\n",
      "Fold: 15  Epoch: 81  Training loss = 1.9148  Validation loss = 4.7592  \n",
      "\n",
      "Fold: 15  Epoch: 82  Training loss = 1.9100  Validation loss = 4.7576  \n",
      "\n",
      "Fold: 15  Epoch: 83  Training loss = 1.9083  Validation loss = 4.7554  \n",
      "\n",
      "Fold: 15  Epoch: 84  Training loss = 1.9139  Validation loss = 4.7568  \n",
      "\n",
      "Fold: 15  Epoch: 85  Training loss = 1.9384  Validation loss = 4.7571  \n",
      "\n",
      "Fold: 15  Epoch: 86  Training loss = 1.9175  Validation loss = 4.7465  \n",
      "\n",
      "Fold: 15  Epoch: 87  Training loss = 1.9051  Validation loss = 4.7380  \n",
      "\n",
      "Fold: 15  Epoch: 88  Training loss = 1.9092  Validation loss = 4.7352  \n",
      "\n",
      "Fold: 15  Epoch: 89  Training loss = 1.9527  Validation loss = 5.2755  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 88  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 2.2317  Validation loss = 3.7109  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 2.2463  Validation loss = 3.5208  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 2.2198  Validation loss = 3.7034  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 2.2229  Validation loss = 3.7565  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 2.2098  Validation loss = 3.7083  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 2.2153  Validation loss = 3.6828  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 2.2096  Validation loss = 3.6048  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 2.2033  Validation loss = 3.5487  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 2.2037  Validation loss = 3.6210  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 2.1969  Validation loss = 3.5291  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 2.1916  Validation loss = 3.7194  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 2.1919  Validation loss = 3.8929  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 2  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 2.3514  Validation loss = 2.4625  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 2.3251  Validation loss = 2.6748  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 2.3232  Validation loss = 2.8426  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 2.3050  Validation loss = 2.7872  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 2.3098  Validation loss = 2.8814  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 2.2914  Validation loss = 2.8806  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 2.2988  Validation loss = 2.8413  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 2.2950  Validation loss = 2.8283  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 2.3028  Validation loss = 2.7589  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 2.2795  Validation loss = 2.6164  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 2.2835  Validation loss = 2.5287  \n",
      "\n",
      "Fold: 17  Epoch: 12  Training loss = 2.2784  Validation loss = 2.6639  \n",
      "\n",
      "Fold: 17  Epoch: 13  Training loss = 2.2782  Validation loss = 2.6049  \n",
      "\n",
      "Fold: 17  Epoch: 14  Training loss = 2.2916  Validation loss = 2.4826  \n",
      "\n",
      "Fold: 17  Epoch: 15  Training loss = 2.2646  Validation loss = 2.6714  \n",
      "\n",
      "Fold: 17  Epoch: 16  Training loss = 2.2567  Validation loss = 2.6455  \n",
      "\n",
      "Fold: 17  Epoch: 17  Training loss = 2.2531  Validation loss = 2.8612  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 1  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 2.3423  Validation loss = 1.7332  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 2.3365  Validation loss = 1.7395  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 2.3293  Validation loss = 1.7376  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 2.3267  Validation loss = 1.7718  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 2.3214  Validation loss = 1.7759  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 2.3142  Validation loss = 1.8145  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 2.3192  Validation loss = 1.8077  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 2.3067  Validation loss = 1.8149  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 2.3116  Validation loss = 1.8087  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 2.3096  Validation loss = 1.7824  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 2.3128  Validation loss = 1.7612  \n",
      "\n",
      "Fold: 18  Epoch: 12  Training loss = 2.2976  Validation loss = 1.7556  \n",
      "\n",
      "Fold: 18  Epoch: 13  Training loss = 2.2924  Validation loss = 1.7623  \n",
      "\n",
      "Fold: 18  Epoch: 14  Training loss = 2.3165  Validation loss = 1.7283  \n",
      "\n",
      "Fold: 18  Epoch: 15  Training loss = 2.2980  Validation loss = 1.7511  \n",
      "\n",
      "Fold: 18  Epoch: 16  Training loss = 2.2972  Validation loss = 1.7285  \n",
      "\n",
      "Fold: 18  Epoch: 17  Training loss = 2.3103  Validation loss = 1.7322  \n",
      "\n",
      "Fold: 18  Epoch: 18  Training loss = 2.2936  Validation loss = 1.7288  \n",
      "\n",
      "Fold: 18  Epoch: 19  Training loss = 2.2845  Validation loss = 1.7407  \n",
      "\n",
      "Fold: 18  Epoch: 20  Training loss = 2.2833  Validation loss = 1.7106  \n",
      "\n",
      "Fold: 18  Epoch: 21  Training loss = 2.2745  Validation loss = 1.7516  \n",
      "\n",
      "Fold: 18  Epoch: 22  Training loss = 2.2857  Validation loss = 1.7296  \n",
      "\n",
      "Fold: 18  Epoch: 23  Training loss = 2.2705  Validation loss = 1.7152  \n",
      "\n",
      "Fold: 18  Epoch: 24  Training loss = 2.2690  Validation loss = 1.7287  \n",
      "\n",
      "Fold: 18  Epoch: 25  Training loss = 2.2612  Validation loss = 1.7436  \n",
      "\n",
      "Fold: 18  Epoch: 26  Training loss = 2.2623  Validation loss = 1.7427  \n",
      "\n",
      "Fold: 18  Epoch: 27  Training loss = 2.2801  Validation loss = 1.6792  \n",
      "\n",
      "Fold: 18  Epoch: 28  Training loss = 2.2714  Validation loss = 1.6880  \n",
      "\n",
      "Fold: 18  Epoch: 29  Training loss = 2.2642  Validation loss = 1.7206  \n",
      "\n",
      "Fold: 18  Epoch: 30  Training loss = 2.2845  Validation loss = 1.7372  \n",
      "\n",
      "Fold: 18  Epoch: 31  Training loss = 2.2680  Validation loss = 1.7225  \n",
      "\n",
      "Fold: 18  Epoch: 32  Training loss = 2.2783  Validation loss = 1.7441  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 27  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 2.2591  Validation loss = 2.0555  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 2.2656  Validation loss = 2.0550  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 2.3675  Validation loss = 2.0496  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 2.3872  Validation loss = 2.0417  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 2.3544  Validation loss = 2.0564  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 2.2774  Validation loss = 2.0756  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 2.2213  Validation loss = 2.0655  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 2.2132  Validation loss = 2.0686  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 2.2227  Validation loss = 2.0674  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 2.2699  Validation loss = 2.0749  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 2.2086  Validation loss = 2.0657  \n",
      "\n",
      "Fold: 19  Epoch: 12  Training loss = 2.1934  Validation loss = 2.0710  \n",
      "\n",
      "Fold: 19  Epoch: 13  Training loss = 2.1901  Validation loss = 2.0712  \n",
      "\n",
      "Fold: 19  Epoch: 14  Training loss = 2.1903  Validation loss = 2.0843  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 4  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 2.2396  Validation loss = 0.8469  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 2.2474  Validation loss = 0.8468  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 2.2766  Validation loss = 0.8462  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 2.2541  Validation loss = 0.8492  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 2.2586  Validation loss = 0.8169  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 2.2378  Validation loss = 0.7960  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 2.2585  Validation loss = 0.8494  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 2.2303  Validation loss = 0.8969  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 2.2264  Validation loss = 0.8758  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 2.2688  Validation loss = 0.8264  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 2.2128  Validation loss = 0.8184  \n",
      "\n",
      "Fold: 20  Epoch: 12  Training loss = 2.2220  Validation loss = 0.8507  \n",
      "\n",
      "Fold: 20  Epoch: 13  Training loss = 2.2233  Validation loss = 0.8120  \n",
      "\n",
      "Fold: 20  Epoch: 14  Training loss = 2.2312  Validation loss = 0.8311  \n",
      "\n",
      "Fold: 20  Epoch: 15  Training loss = 2.2228  Validation loss = 0.8447  \n",
      "\n",
      "Fold: 20  Epoch: 16  Training loss = 2.2280  Validation loss = 0.8075  \n",
      "\n",
      "Fold: 20  Epoch: 17  Training loss = 2.2266  Validation loss = 0.7753  \n",
      "\n",
      "Fold: 20  Epoch: 18  Training loss = 2.2159  Validation loss = 0.7764  \n",
      "\n",
      "Fold: 20  Epoch: 19  Training loss = 2.2275  Validation loss = 0.7193  \n",
      "\n",
      "Fold: 20  Epoch: 20  Training loss = 2.2219  Validation loss = 0.7389  \n",
      "\n",
      "Fold: 20  Epoch: 21  Training loss = 2.2161  Validation loss = 0.7772  \n",
      "\n",
      "Fold: 20  Epoch: 22  Training loss = 2.2227  Validation loss = 0.7967  \n",
      "\n",
      "Fold: 20  Epoch: 23  Training loss = 2.2181  Validation loss = 0.7307  \n",
      "\n",
      "Fold: 20  Epoch: 24  Training loss = 2.2151  Validation loss = 0.7697  \n",
      "\n",
      "Fold: 20  Epoch: 25  Training loss = 2.2259  Validation loss = 0.7182  \n",
      "\n",
      "Fold: 20  Epoch: 26  Training loss = 2.1849  Validation loss = 0.7497  \n",
      "\n",
      "Fold: 20  Epoch: 27  Training loss = 2.1790  Validation loss = 0.7359  \n",
      "\n",
      "Fold: 20  Epoch: 28  Training loss = 2.1926  Validation loss = 0.7161  \n",
      "\n",
      "Fold: 20  Epoch: 29  Training loss = 2.2187  Validation loss = 0.7357  \n",
      "\n",
      "Fold: 20  Epoch: 30  Training loss = 2.1887  Validation loss = 0.7773  \n",
      "\n",
      "Fold: 20  Epoch: 31  Training loss = 2.1880  Validation loss = 0.7036  \n",
      "\n",
      "Fold: 20  Epoch: 32  Training loss = 2.1637  Validation loss = 0.6827  \n",
      "\n",
      "Fold: 20  Epoch: 33  Training loss = 2.1708  Validation loss = 0.6860  \n",
      "\n",
      "Fold: 20  Epoch: 34  Training loss = 2.1475  Validation loss = 0.6734  \n",
      "\n",
      "Fold: 20  Epoch: 35  Training loss = 2.1441  Validation loss = 0.6798  \n",
      "\n",
      "Fold: 20  Epoch: 36  Training loss = 2.1452  Validation loss = 0.6892  \n",
      "\n",
      "Fold: 20  Epoch: 37  Training loss = 2.1426  Validation loss = 0.7025  \n",
      "\n",
      "Fold: 20  Epoch: 38  Training loss = 2.1518  Validation loss = 0.7093  \n",
      "\n",
      "Fold: 20  Epoch: 39  Training loss = 2.1949  Validation loss = 0.7263  \n",
      "\n",
      "Fold: 20  Epoch: 40  Training loss = 2.1827  Validation loss = 0.7335  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 34  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 2.1575  Validation loss = 3.5448  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 2.0961  Validation loss = 3.4234  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 2.1182  Validation loss = 3.4158  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 2.1013  Validation loss = 3.5056  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 2.0968  Validation loss = 3.5599  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 2.0864  Validation loss = 3.5747  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 2.0941  Validation loss = 3.5773  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 2.0864  Validation loss = 3.5318  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 2.1386  Validation loss = 3.4873  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 2.1388  Validation loss = 3.9597  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 2.1316  Validation loss = 3.8014  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 2.1344  Validation loss = 3.6844  \n",
      "\n",
      "Fold: 21  Epoch: 13  Training loss = 2.1104  Validation loss = 3.6890  \n",
      "\n",
      "Fold: 21  Epoch: 14  Training loss = 2.1158  Validation loss = 3.5834  \n",
      "\n",
      "Fold: 21  Epoch: 15  Training loss = 2.1228  Validation loss = 3.7110  \n",
      "\n",
      "Fold: 21  Epoch: 16  Training loss = 2.1050  Validation loss = 3.6618  \n",
      "\n",
      "Fold: 21  Epoch: 17  Training loss = 2.1055  Validation loss = 3.5268  \n",
      "\n",
      "Fold: 21  Epoch: 18  Training loss = 2.0825  Validation loss = 3.6271  \n",
      "\n",
      "Fold: 21  Epoch: 19  Training loss = 2.0724  Validation loss = 3.5646  \n",
      "\n",
      "Fold: 21  Epoch: 20  Training loss = 2.0636  Validation loss = 3.6135  \n",
      "\n",
      "Fold: 21  Epoch: 21  Training loss = 2.0949  Validation loss = 3.6492  \n",
      "\n",
      "Fold: 21  Epoch: 22  Training loss = 2.0802  Validation loss = 3.6070  \n",
      "\n",
      "Fold: 21  Epoch: 23  Training loss = 2.0673  Validation loss = 3.5470  \n",
      "\n",
      "Fold: 21  Epoch: 24  Training loss = 2.0471  Validation loss = 3.5913  \n",
      "\n",
      "Fold: 21  Epoch: 25  Training loss = 2.0493  Validation loss = 3.5554  \n",
      "\n",
      "Fold: 21  Epoch: 26  Training loss = 2.1224  Validation loss = 3.6997  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 3  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 2.1404  Validation loss = 3.4335  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 2.1273  Validation loss = 3.2844  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 2.1275  Validation loss = 3.3860  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 2.1028  Validation loss = 3.0041  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 2.1009  Validation loss = 2.5620  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 2.0863  Validation loss = 3.1274  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 2.0736  Validation loss = 3.8497  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 2.1162  Validation loss = 3.9120  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 2.0706  Validation loss = 1.7996  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 2.0726  Validation loss = 3.0495  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 2.1185  Validation loss = 4.0965  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 9  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 2.1366  Validation loss = 1.3440  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 2.1040  Validation loss = 2.0276  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 2.0762  Validation loss = 1.8705  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 2.0891  Validation loss = 1.9581  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 2.1943  Validation loss = 1.5079  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 2.1159  Validation loss = 1.9156  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 2.1564  Validation loss = 2.3812  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 2.0446  Validation loss = 1.8324  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 2.0636  Validation loss = 2.1142  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 2.0116  Validation loss = 1.4597  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 2.0346  Validation loss = 1.6340  \n",
      "\n",
      "Fold: 23  Epoch: 12  Training loss = 2.0195  Validation loss = 1.7544  \n",
      "\n",
      "Fold: 23  Epoch: 13  Training loss = 2.0136  Validation loss = 1.7635  \n",
      "\n",
      "Fold: 23  Epoch: 14  Training loss = 2.0137  Validation loss = 1.5079  \n",
      "\n",
      "Fold: 23  Epoch: 15  Training loss = 2.0110  Validation loss = 1.6779  \n",
      "\n",
      "Fold: 23  Epoch: 16  Training loss = 1.9979  Validation loss = 1.3400  \n",
      "\n",
      "Fold: 23  Epoch: 17  Training loss = 1.9970  Validation loss = 1.7954  \n",
      "\n",
      "Fold: 23  Epoch: 18  Training loss = 2.0202  Validation loss = 1.9633  \n",
      "\n",
      "Fold: 23  Epoch: 19  Training loss = 1.9969  Validation loss = 1.8123  \n",
      "\n",
      "Fold: 23  Epoch: 20  Training loss = 2.1436  Validation loss = 2.8141  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 16  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 1.9955  Validation loss = 1.7284  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 2.0522  Validation loss = 1.3826  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 1.9915  Validation loss = 1.8264  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 2.0039  Validation loss = 1.3706  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 1.9668  Validation loss = 1.7588  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 1.9687  Validation loss = 1.7096  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 2.0462  Validation loss = 1.3595  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 2.0406  Validation loss = 1.8053  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 2.0389  Validation loss = 1.1895  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 2.0610  Validation loss = 1.3862  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 2.0191  Validation loss = 1.7816  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 2.0247  Validation loss = 1.2035  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 2.0060  Validation loss = 1.1594  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 2.0076  Validation loss = 1.1700  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 1.9787  Validation loss = 1.3953  \n",
      "\n",
      "Fold: 24  Epoch: 16  Training loss = 1.9650  Validation loss = 1.4660  \n",
      "\n",
      "Fold: 24  Epoch: 17  Training loss = 1.9669  Validation loss = 1.2515  \n",
      "\n",
      "Fold: 24  Epoch: 18  Training loss = 1.9670  Validation loss = 1.7215  \n",
      "\n",
      "Fold: 24  Epoch: 19  Training loss = 1.9563  Validation loss = 1.6651  \n",
      "\n",
      "Fold: 24  Epoch: 20  Training loss = 1.9566  Validation loss = 1.6350  \n",
      "\n",
      "Fold: 24  Epoch: 21  Training loss = 1.9821  Validation loss = 1.7348  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 13  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 2.2528  Validation loss = 2.2553  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 1.9148  Validation loss = 2.8158  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 1.9289  Validation loss = 3.0692  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 1.8983  Validation loss = 2.4307  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 1.8957  Validation loss = 2.5202  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 1.8717  Validation loss = 2.7103  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 1.8786  Validation loss = 2.8840  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 1.8715  Validation loss = 2.6412  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 1.8768  Validation loss = 2.8379  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 1.8875  Validation loss = 2.9151  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 1.8798  Validation loss = 2.4780  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 1.8670  Validation loss = 2.9823  \n",
      "\n",
      "Fold: 25  Epoch: 13  Training loss = 1.8672  Validation loss = 2.4356  \n",
      "\n",
      "Fold: 25  Epoch: 14  Training loss = 1.8473  Validation loss = 2.5146  \n",
      "\n",
      "Fold: 25  Epoch: 15  Training loss = 1.8611  Validation loss = 2.9339  \n",
      "\n",
      "Fold: 25  Epoch: 16  Training loss = 1.8555  Validation loss = 2.8811  \n",
      "\n",
      "Fold: 25  Epoch: 17  Training loss = 1.8560  Validation loss = 2.8363  \n",
      "\n",
      "Fold: 25  Epoch: 18  Training loss = 1.8502  Validation loss = 2.7830  \n",
      "\n",
      "Fold: 25  Epoch: 19  Training loss = 1.8444  Validation loss = 2.8599  \n",
      "\n",
      "Fold: 25  Epoch: 20  Training loss = 1.8437  Validation loss = 2.4279  \n",
      "\n",
      "Fold: 25  Epoch: 21  Training loss = 1.8635  Validation loss = 2.6765  \n",
      "\n",
      "Fold: 25  Epoch: 22  Training loss = 1.9138  Validation loss = 2.4853  \n",
      "\n",
      "Fold: 25  Epoch: 23  Training loss = 1.8768  Validation loss = 2.6100  \n",
      "\n",
      "Fold: 25  Epoch: 24  Training loss = 1.8410  Validation loss = 2.7852  \n",
      "\n",
      "Fold: 25  Epoch: 25  Training loss = 1.8287  Validation loss = 2.7842  \n",
      "\n",
      "Fold: 25  Epoch: 26  Training loss = 1.8320  Validation loss = 2.6517  \n",
      "\n",
      "Fold: 25  Epoch: 27  Training loss = 1.8248  Validation loss = 2.5825  \n",
      "\n",
      "Fold: 25  Epoch: 28  Training loss = 1.8457  Validation loss = 3.1254  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 1  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 1.9667  Validation loss = 2.9342  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 1.9522  Validation loss = 2.7553  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 1.9533  Validation loss = 1.3925  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 1.9238  Validation loss = 1.4230  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 1.9086  Validation loss = 1.5201  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 1.9350  Validation loss = 2.1796  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 1.9561  Validation loss = 1.7961  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 2.7761  Validation loss = 3.1461  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 2.1575  Validation loss = 2.9367  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 1.9502  Validation loss = 2.6965  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 2.0449  Validation loss = 2.9016  \n",
      "\n",
      "Fold: 26  Epoch: 12  Training loss = 2.2557  Validation loss = 3.2606  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 3  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 1.9392  Validation loss = 0.9572  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 1.8378  Validation loss = 0.9700  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 1.9421  Validation loss = 0.9874  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 1.8772  Validation loss = 0.9866  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 1.9562  Validation loss = 1.1085  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 1.8147  Validation loss = 1.0005  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 1.7672  Validation loss = 0.9588  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 1.8342  Validation loss = 0.9648  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 1.8782  Validation loss = 0.9484  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 2.0583  Validation loss = 0.9554  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 1.7346  Validation loss = 0.9603  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 1.7834  Validation loss = 0.9617  \n",
      "\n",
      "Fold: 27  Epoch: 13  Training loss = 2.2848  Validation loss = 1.1911  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 9  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 1.8973  Validation loss = 0.4886  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 1.8284  Validation loss = 0.6781  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 1.9093  Validation loss = 0.6868  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 1.8782  Validation loss = 0.6572  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 1.8666  Validation loss = 0.6891  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 1.8119  Validation loss = 0.6672  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 1.8982  Validation loss = 0.4737  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 1.7644  Validation loss = 0.6038  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 1.7462  Validation loss = 0.6518  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 1.8496  Validation loss = 0.5451  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 1.8103  Validation loss = 0.6019  \n",
      "\n",
      "Fold: 28  Epoch: 12  Training loss = 1.7931  Validation loss = 0.7371  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 7  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 1.7340  Validation loss = 0.9895  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 1.8123  Validation loss = 1.0243  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 1.9198  Validation loss = 0.9134  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 1.8168  Validation loss = 0.9289  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 1.8161  Validation loss = 0.9502  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 1.8556  Validation loss = 0.9338  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 1.7441  Validation loss = 0.5216  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 1.7199  Validation loss = 0.5555  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 1.7259  Validation loss = 0.5390  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 1.7098  Validation loss = 0.6585  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 1.7845  Validation loss = 0.5619  \n",
      "\n",
      "Fold: 29  Epoch: 12  Training loss = 1.8957  Validation loss = 0.5851  \n",
      "\n",
      "Fold: 29  Epoch: 13  Training loss = 1.6662  Validation loss = 0.6089  \n",
      "\n",
      "Fold: 29  Epoch: 14  Training loss = 1.6965  Validation loss = 0.6029  \n",
      "\n",
      "Fold: 29  Epoch: 15  Training loss = 1.7387  Validation loss = 0.6429  \n",
      "\n",
      "Fold: 29  Epoch: 16  Training loss = 1.7896  Validation loss = 0.6034  \n",
      "\n",
      "Fold: 29  Epoch: 17  Training loss = 1.7437  Validation loss = 0.6591  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 7  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 1.7040  Validation loss = 1.4150  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 1.8988  Validation loss = 2.8961  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 1.7948  Validation loss = 2.5873  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 1.7913  Validation loss = 2.0636  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 1.7569  Validation loss = 1.9674  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 1.8252  Validation loss = 1.7257  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 1.7268  Validation loss = 1.5261  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 1.7049  Validation loss = 1.3088  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 1.7068  Validation loss = 1.7504  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 1.6859  Validation loss = 1.2197  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 1.6958  Validation loss = 1.1232  \n",
      "\n",
      "Fold: 30  Epoch: 12  Training loss = 1.6710  Validation loss = 0.9933  \n",
      "\n",
      "Fold: 30  Epoch: 13  Training loss = 1.7159  Validation loss = 0.9718  \n",
      "\n",
      "Fold: 30  Epoch: 14  Training loss = 1.7867  Validation loss = 1.0735  \n",
      "\n",
      "Fold: 30  Epoch: 15  Training loss = 1.7715  Validation loss = 1.4000  \n",
      "\n",
      "Fold: 30  Epoch: 16  Training loss = 1.8008  Validation loss = 0.8904  \n",
      "\n",
      "Fold: 30  Epoch: 17  Training loss = 1.6785  Validation loss = 0.9793  \n",
      "\n",
      "Fold: 30  Epoch: 18  Training loss = 1.6467  Validation loss = 1.0673  \n",
      "\n",
      "Fold: 30  Epoch: 19  Training loss = 1.7536  Validation loss = 1.0634  \n",
      "\n",
      "Fold: 30  Epoch: 20  Training loss = 1.6534  Validation loss = 1.1126  \n",
      "\n",
      "Fold: 30  Epoch: 21  Training loss = 1.6416  Validation loss = 1.1255  \n",
      "\n",
      "Fold: 30  Epoch: 22  Training loss = 1.6699  Validation loss = 1.0198  \n",
      "\n",
      "Fold: 30  Epoch: 23  Training loss = 1.6321  Validation loss = 1.1342  \n",
      "\n",
      "Fold: 30  Epoch: 24  Training loss = 1.6357  Validation loss = 1.1496  \n",
      "\n",
      "Fold: 30  Epoch: 25  Training loss = 1.6625  Validation loss = 1.1159  \n",
      "\n",
      "Fold: 30  Epoch: 26  Training loss = 1.6174  Validation loss = 1.0775  \n",
      "\n",
      "Fold: 30  Epoch: 27  Training loss = 1.6283  Validation loss = 1.0476  \n",
      "\n",
      "Fold: 30  Epoch: 28  Training loss = 1.6946  Validation loss = 1.3880  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 16  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 1.6525  Validation loss = 1.1165  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 1.6604  Validation loss = 1.1629  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 1.6080  Validation loss = 1.0689  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 1.6437  Validation loss = 1.1844  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 1.7205  Validation loss = 1.2388  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 1.6169  Validation loss = 1.0239  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 1.6019  Validation loss = 1.0856  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 1.5922  Validation loss = 1.0997  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 1.6309  Validation loss = 1.1776  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 1.5834  Validation loss = 1.0575  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 1.6053  Validation loss = 1.4460  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 6  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 1.3762  Validation loss = 2.3310  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 1.3796  Validation loss = 2.3551  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 1.3671  Validation loss = 2.1683  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 1.3640  Validation loss = 1.4749  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 1.3537  Validation loss = 2.0531  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 1.3523  Validation loss = 2.0445  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 1.3743  Validation loss = 1.9291  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 1.3746  Validation loss = 2.1482  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 1.3547  Validation loss = 2.2077  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 1.3366  Validation loss = 2.0810  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 1.3517  Validation loss = 1.9368  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 1.3414  Validation loss = 2.3220  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 4  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    # Load model with lowest validation error for each fold\n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 45\n",
      "Average validation error: 2.56246\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 1.3448  Test loss = 2.6202  \n",
      "\n",
      "Epoch: 2  Training loss = 1.3214  Test loss = 2.5940  \n",
      "\n",
      "Epoch: 3  Training loss = 1.2998  Test loss = 2.5788  \n",
      "\n",
      "Epoch: 4  Training loss = 1.2773  Test loss = 2.5715  \n",
      "\n",
      "Epoch: 5  Training loss = 1.2607  Test loss = 2.5708  \n",
      "\n",
      "Epoch: 6  Training loss = 1.2572  Test loss = 2.5768  \n",
      "\n",
      "Epoch: 7  Training loss = 1.2552  Test loss = 2.5853  \n",
      "\n",
      "Epoch: 8  Training loss = 1.2537  Test loss = 2.5922  \n",
      "\n",
      "Epoch: 9  Training loss = 1.2526  Test loss = 2.5970  \n",
      "\n",
      "Epoch: 10  Training loss = 1.2517  Test loss = 2.6000  \n",
      "\n",
      "Epoch: 11  Training loss = 1.2509  Test loss = 2.6017  \n",
      "\n",
      "Epoch: 12  Training loss = 1.2502  Test loss = 2.6028  \n",
      "\n",
      "Epoch: 13  Training loss = 1.2495  Test loss = 2.6035  \n",
      "\n",
      "Epoch: 14  Training loss = 1.2488  Test loss = 2.6041  \n",
      "\n",
      "Epoch: 15  Training loss = 1.2482  Test loss = 2.6046  \n",
      "\n",
      "Epoch: 16  Training loss = 1.2475  Test loss = 2.6051  \n",
      "\n",
      "Epoch: 17  Training loss = 1.2469  Test loss = 2.6056  \n",
      "\n",
      "Epoch: 18  Training loss = 1.2463  Test loss = 2.6060  \n",
      "\n",
      "Epoch: 19  Training loss = 1.2457  Test loss = 2.6064  \n",
      "\n",
      "Epoch: 20  Training loss = 1.2451  Test loss = 2.6068  \n",
      "\n",
      "Epoch: 21  Training loss = 1.2445  Test loss = 2.6073  \n",
      "\n",
      "Epoch: 22  Training loss = 1.2440  Test loss = 2.6077  \n",
      "\n",
      "Epoch: 23  Training loss = 1.2434  Test loss = 2.6081  \n",
      "\n",
      "Epoch: 24  Training loss = 1.2429  Test loss = 2.6085  \n",
      "\n",
      "Epoch: 25  Training loss = 1.2423  Test loss = 2.6089  \n",
      "\n",
      "Epoch: 26  Training loss = 1.2418  Test loss = 2.6093  \n",
      "\n",
      "Epoch: 27  Training loss = 1.2413  Test loss = 2.6097  \n",
      "\n",
      "Epoch: 28  Training loss = 1.2408  Test loss = 2.6101  \n",
      "\n",
      "Epoch: 29  Training loss = 1.2403  Test loss = 2.6105  \n",
      "\n",
      "Epoch: 30  Training loss = 1.2398  Test loss = 2.6109  \n",
      "\n",
      "Epoch: 31  Training loss = 1.2393  Test loss = 2.6112  \n",
      "\n",
      "Epoch: 32  Training loss = 1.2388  Test loss = 2.6116  \n",
      "\n",
      "Epoch: 33  Training loss = 1.2383  Test loss = 2.6120  \n",
      "\n",
      "Epoch: 34  Training loss = 1.2378  Test loss = 2.6124  \n",
      "\n",
      "Epoch: 35  Training loss = 1.2373  Test loss = 2.6127  \n",
      "\n",
      "Epoch: 36  Training loss = 1.2369  Test loss = 2.6131  \n",
      "\n",
      "Epoch: 37  Training loss = 1.2364  Test loss = 2.6135  \n",
      "\n",
      "Epoch: 38  Training loss = 1.2360  Test loss = 2.6138  \n",
      "\n",
      "Epoch: 39  Training loss = 1.2355  Test loss = 2.6142  \n",
      "\n",
      "Epoch: 40  Training loss = 1.2351  Test loss = 2.6145  \n",
      "\n",
      "Epoch: 41  Training loss = 1.2346  Test loss = 2.6149  \n",
      "\n",
      "Epoch: 42  Training loss = 1.2342  Test loss = 2.6152  \n",
      "\n",
      "Epoch: 43  Training loss = 1.2338  Test loss = 2.6156  \n",
      "\n",
      "Epoch: 44  Training loss = 1.2333  Test loss = 2.6159  \n",
      "\n",
      "Epoch: 45  Training loss = 1.2329  Test loss = 2.6162  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc1f9e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiQZMv4BRkie8FVdo/sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc8b3be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 2.61623185423 \n",
      "Fixed scheme MAE:  1.65733582738\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 1.2329  Test loss = 3.0142  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 1.2600  Test loss = 2.5311  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 1.2830  Test loss = 0.2688  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 1.2816  Test loss = 0.2126  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 1.1582  Test loss = 1.6697  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 1.1463  Test loss = 0.0991  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 1.1237  Test loss = 1.1750  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 1.1080  Test loss = 0.1223  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 1.0426  Test loss = 0.2385  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 1.0365  Test loss = 1.6010  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 1.0232  Test loss = 1.6448  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 1.0402  Test loss = 2.6210  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 1.0010  Test loss = 1.0502  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 1.0094  Test loss = 1.1824  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 1.0170  Test loss = 2.0648  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 1.0444  Test loss = 2.9597  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 1.0270  Test loss = 1.0696  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 1.0280  Test loss = 0.4764  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 1.0283  Test loss = 1.2326  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 0.9782  Test loss = 0.4696  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 0.9018  Test loss = 2.3125  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 0.9462  Test loss = 4.1849  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 1.0681  Test loss = 1.0538  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 1.0677  Test loss = 1.0761  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 0.9485  Test loss = 2.0749  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 0.9825  Test loss = 0.6252  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 0.9856  Test loss = 0.4149  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 0.9839  Test loss = 2.0676  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 0.9833  Test loss = 0.6985  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 0.9819  Test loss = 0.6266  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 0.9806  Test loss = 4.1707  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 1.1016  Test loss = 0.0267  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 0.9969  Test loss = 1.4804  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 1.0135  Test loss = 0.4904  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 0.9680  Test loss = 0.3869  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 0.9488  Test loss = 4.8331  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 1.0838  Test loss = 1.1740  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 1.0792  Test loss = 2.2359  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 1.1053  Test loss = 2.2584  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 1.1390  Test loss = 1.9085  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 1.0754  Test loss = 0.7393  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 1.0790  Test loss = 1.2087  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 1.0893  Test loss = 2.1997  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 1.1199  Test loss = 13.2116  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 1.9698  Test loss = 7.3116  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.1679  Test loss = 1.6206  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.1762  Test loss = 0.6710  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.1778  Test loss = 0.5512  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 1.8893  Test loss = 2.3837  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 1.9075  Test loss = 3.0877  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 1.9455  Test loss = 2.1987  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 1.9644  Test loss = 1.2278  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 1.8808  Test loss = 2.8314  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 1.9133  Test loss = 2.2595  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 1.9322  Test loss = 0.3328  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 1.9276  Test loss = 0.2625  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 1.8370  Test loss = 0.8664  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 1.8401  Test loss = 1.7958  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 1.8533  Test loss = 0.2688  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 1.8505  Test loss = 0.8942  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.7754  Test loss = 0.4461  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.7760  Test loss = 2.6841  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 1.8053  Test loss = 0.4352  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 1.8038  Test loss = 0.3879  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 1.7529  Test loss = 0.1986  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 1.7521  Test loss = 0.7785  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.7520  Test loss = 1.6552  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 1.7622  Test loss = 2.0792  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 1.7367  Test loss = 5.5597  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 1.8686  Test loss = 0.1863  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 1.8678  Test loss = 1.5863  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 1.8708  Test loss = 3.7769  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 1.8699  Test loss = 3.6529  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 1.9239  Test loss = 0.5106  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 1.9203  Test loss = 2.0089  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 1.9249  Test loss = 0.0229  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 1.8209  Test loss = 1.3267  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nauJIarOXVAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd299b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.56033718433 \n",
      "Updating scheme MAE:  1.72756560977\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_test_fixed.csv\", yhat_test_fixed)\n",
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_update.csv\", yhat_update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
