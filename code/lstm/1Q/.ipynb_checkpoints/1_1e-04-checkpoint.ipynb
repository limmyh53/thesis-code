{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"1Q/1_cell/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 750\n",
    "learning_rate = 1e-4\n",
    "batch_size = 5\n",
    "early_stop_iters = 15\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 12 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    stacked_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell]*3, state_is_tuple = True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag1',\n",
    "                                       'inflation.lag2',\n",
    "                                       'inflation.lag3',\n",
    "                                       'inflation.lag4']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag1',\n",
    "                                   'unemp.lag2',\n",
    "                                   'unemp.lag3',\n",
    "                                   'unemp.lag4']])\n",
    "train_4lag_oil = np.array(train[['oil.lag1',\n",
    "                                 'oil.lag2',\n",
    "                                 'oil.lag3',\n",
    "                                 'oil.lag4']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag1',\n",
    "                                     'inflation.lag2',\n",
    "                                     'inflation.lag3',\n",
    "                                     'inflation.lag4']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag1',\n",
    "                                 'unemp.lag2',\n",
    "                                 'unemp.lag3',\n",
    "                                 'unemp.lag4']])\n",
    "test_4lag_oil = np.array(test[['oil.lag1',\n",
    "                               'oil.lag2',\n",
    "                               'oil.lag3',\n",
    "                               'oil.lag4']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 12 \n",
      "Learning rate = 0.0001 \n",
      "Epochs = 750 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 15 \n",
      "Learning rate = 0.0001\n",
      "Fold: 1  Epoch: 1  Training loss = 3.1004  Validation loss = 3.1089  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 3.0969  Validation loss = 3.1012  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 3.0907  Validation loss = 3.0884  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 3.0864  Validation loss = 3.0792  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 3.0797  Validation loss = 3.0651  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 3.0743  Validation loss = 3.0534  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 3.0688  Validation loss = 3.0416  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 3.0623  Validation loss = 3.0280  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 3.0589  Validation loss = 3.0206  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 3.0509  Validation loss = 3.0028  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 3.0472  Validation loss = 2.9946  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 3.0411  Validation loss = 2.9808  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 3.0371  Validation loss = 2.9718  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 3.0325  Validation loss = 2.9614  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 3.0270  Validation loss = 2.9489  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 3.0230  Validation loss = 2.9397  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 3.0182  Validation loss = 2.9285  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 3.0120  Validation loss = 2.9142  \n",
      "\n",
      "Fold: 1  Epoch: 19  Training loss = 3.0075  Validation loss = 2.9037  \n",
      "\n",
      "Fold: 1  Epoch: 20  Training loss = 3.0039  Validation loss = 2.8952  \n",
      "\n",
      "Fold: 1  Epoch: 21  Training loss = 3.0013  Validation loss = 2.8891  \n",
      "\n",
      "Fold: 1  Epoch: 22  Training loss = 2.9982  Validation loss = 2.8817  \n",
      "\n",
      "Fold: 1  Epoch: 23  Training loss = 2.9947  Validation loss = 2.8735  \n",
      "\n",
      "Fold: 1  Epoch: 24  Training loss = 2.9898  Validation loss = 2.8618  \n",
      "\n",
      "Fold: 1  Epoch: 25  Training loss = 2.9867  Validation loss = 2.8543  \n",
      "\n",
      "Fold: 1  Epoch: 26  Training loss = 2.9825  Validation loss = 2.8441  \n",
      "\n",
      "Fold: 1  Epoch: 27  Training loss = 2.9797  Validation loss = 2.8372  \n",
      "\n",
      "Fold: 1  Epoch: 28  Training loss = 2.9748  Validation loss = 2.8254  \n",
      "\n",
      "Fold: 1  Epoch: 29  Training loss = 2.9708  Validation loss = 2.8154  \n",
      "\n",
      "Fold: 1  Epoch: 30  Training loss = 2.9660  Validation loss = 2.8042  \n",
      "\n",
      "Fold: 1  Epoch: 31  Training loss = 2.9606  Validation loss = 2.7907  \n",
      "\n",
      "Fold: 1  Epoch: 32  Training loss = 2.9586  Validation loss = 2.7854  \n",
      "\n",
      "Fold: 1  Epoch: 33  Training loss = 2.9544  Validation loss = 2.7749  \n",
      "\n",
      "Fold: 1  Epoch: 34  Training loss = 2.9526  Validation loss = 2.7698  \n",
      "\n",
      "Fold: 1  Epoch: 35  Training loss = 2.9500  Validation loss = 2.7631  \n",
      "\n",
      "Fold: 1  Epoch: 36  Training loss = 2.9445  Validation loss = 2.7490  \n",
      "\n",
      "Fold: 1  Epoch: 37  Training loss = 2.9409  Validation loss = 2.7395  \n",
      "\n",
      "Fold: 1  Epoch: 38  Training loss = 2.9375  Validation loss = 2.7305  \n",
      "\n",
      "Fold: 1  Epoch: 39  Training loss = 2.9351  Validation loss = 2.7242  \n",
      "\n",
      "Fold: 1  Epoch: 40  Training loss = 2.9323  Validation loss = 2.7166  \n",
      "\n",
      "Fold: 1  Epoch: 41  Training loss = 2.9263  Validation loss = 2.7008  \n",
      "\n",
      "Fold: 1  Epoch: 42  Training loss = 2.9231  Validation loss = 2.6922  \n",
      "\n",
      "Fold: 1  Epoch: 43  Training loss = 2.9181  Validation loss = 2.6788  \n",
      "\n",
      "Fold: 1  Epoch: 44  Training loss = 2.9156  Validation loss = 2.6714  \n",
      "\n",
      "Fold: 1  Epoch: 45  Training loss = 2.9118  Validation loss = 2.6610  \n",
      "\n",
      "Fold: 1  Epoch: 46  Training loss = 2.9095  Validation loss = 2.6544  \n",
      "\n",
      "Fold: 1  Epoch: 47  Training loss = 2.9067  Validation loss = 2.6467  \n",
      "\n",
      "Fold: 1  Epoch: 48  Training loss = 2.9036  Validation loss = 2.6380  \n",
      "\n",
      "Fold: 1  Epoch: 49  Training loss = 2.9005  Validation loss = 2.6290  \n",
      "\n",
      "Fold: 1  Epoch: 50  Training loss = 2.8970  Validation loss = 2.6189  \n",
      "\n",
      "Fold: 1  Epoch: 51  Training loss = 2.8940  Validation loss = 2.6101  \n",
      "\n",
      "Fold: 1  Epoch: 52  Training loss = 2.8915  Validation loss = 2.6028  \n",
      "\n",
      "Fold: 1  Epoch: 53  Training loss = 2.8885  Validation loss = 2.5945  \n",
      "\n",
      "Fold: 1  Epoch: 54  Training loss = 2.8855  Validation loss = 2.5859  \n",
      "\n",
      "Fold: 1  Epoch: 55  Training loss = 2.8832  Validation loss = 2.5793  \n",
      "\n",
      "Fold: 1  Epoch: 56  Training loss = 2.8810  Validation loss = 2.5726  \n",
      "\n",
      "Fold: 1  Epoch: 57  Training loss = 2.8796  Validation loss = 2.5685  \n",
      "\n",
      "Fold: 1  Epoch: 58  Training loss = 2.8766  Validation loss = 2.5593  \n",
      "\n",
      "Fold: 1  Epoch: 59  Training loss = 2.8742  Validation loss = 2.5518  \n",
      "\n",
      "Fold: 1  Epoch: 60  Training loss = 2.8719  Validation loss = 2.5449  \n",
      "\n",
      "Fold: 1  Epoch: 61  Training loss = 2.8689  Validation loss = 2.5359  \n",
      "\n",
      "Fold: 1  Epoch: 62  Training loss = 2.8667  Validation loss = 2.5292  \n",
      "\n",
      "Fold: 1  Epoch: 63  Training loss = 2.8649  Validation loss = 2.5235  \n",
      "\n",
      "Fold: 1  Epoch: 64  Training loss = 2.8628  Validation loss = 2.5172  \n",
      "\n",
      "Fold: 1  Epoch: 65  Training loss = 2.8592  Validation loss = 2.5057  \n",
      "\n",
      "Fold: 1  Epoch: 66  Training loss = 2.8569  Validation loss = 2.4988  \n",
      "\n",
      "Fold: 1  Epoch: 67  Training loss = 2.8550  Validation loss = 2.4929  \n",
      "\n",
      "Fold: 1  Epoch: 68  Training loss = 2.8525  Validation loss = 2.4849  \n",
      "\n",
      "Fold: 1  Epoch: 69  Training loss = 2.8497  Validation loss = 2.4762  \n",
      "\n",
      "Fold: 1  Epoch: 70  Training loss = 2.8460  Validation loss = 2.4642  \n",
      "\n",
      "Fold: 1  Epoch: 71  Training loss = 2.8450  Validation loss = 2.4606  \n",
      "\n",
      "Fold: 1  Epoch: 72  Training loss = 2.8415  Validation loss = 2.4494  \n",
      "\n",
      "Fold: 1  Epoch: 73  Training loss = 2.8383  Validation loss = 2.4388  \n",
      "\n",
      "Fold: 1  Epoch: 74  Training loss = 2.8360  Validation loss = 2.4311  \n",
      "\n",
      "Fold: 1  Epoch: 75  Training loss = 2.8332  Validation loss = 2.4217  \n",
      "\n",
      "Fold: 1  Epoch: 76  Training loss = 2.8305  Validation loss = 2.4126  \n",
      "\n",
      "Fold: 1  Epoch: 77  Training loss = 2.8272  Validation loss = 2.4010  \n",
      "\n",
      "Fold: 1  Epoch: 78  Training loss = 2.8259  Validation loss = 2.3969  \n",
      "\n",
      "Fold: 1  Epoch: 79  Training loss = 2.8236  Validation loss = 2.3890  \n",
      "\n",
      "Fold: 1  Epoch: 80  Training loss = 2.8222  Validation loss = 2.3839  \n",
      "\n",
      "Fold: 1  Epoch: 81  Training loss = 2.8201  Validation loss = 2.3765  \n",
      "\n",
      "Fold: 1  Epoch: 82  Training loss = 2.8182  Validation loss = 2.3699  \n",
      "\n",
      "Fold: 1  Epoch: 83  Training loss = 2.8169  Validation loss = 2.3651  \n",
      "\n",
      "Fold: 1  Epoch: 84  Training loss = 2.8154  Validation loss = 2.3599  \n",
      "\n",
      "Fold: 1  Epoch: 85  Training loss = 2.8129  Validation loss = 2.3513  \n",
      "\n",
      "Fold: 1  Epoch: 86  Training loss = 2.8117  Validation loss = 2.3465  \n",
      "\n",
      "Fold: 1  Epoch: 87  Training loss = 2.8104  Validation loss = 2.3421  \n",
      "\n",
      "Fold: 1  Epoch: 88  Training loss = 2.8094  Validation loss = 2.3387  \n",
      "\n",
      "Fold: 1  Epoch: 89  Training loss = 2.8079  Validation loss = 2.3332  \n",
      "\n",
      "Fold: 1  Epoch: 90  Training loss = 2.8066  Validation loss = 2.3288  \n",
      "\n",
      "Fold: 1  Epoch: 91  Training loss = 2.8054  Validation loss = 2.3244  \n",
      "\n",
      "Fold: 1  Epoch: 92  Training loss = 2.8028  Validation loss = 2.3147  \n",
      "\n",
      "Fold: 1  Epoch: 93  Training loss = 2.8010  Validation loss = 2.3084  \n",
      "\n",
      "Fold: 1  Epoch: 94  Training loss = 2.7989  Validation loss = 2.3001  \n",
      "\n",
      "Fold: 1  Epoch: 95  Training loss = 2.7977  Validation loss = 2.2959  \n",
      "\n",
      "Fold: 1  Epoch: 96  Training loss = 2.7953  Validation loss = 2.2870  \n",
      "\n",
      "Fold: 1  Epoch: 97  Training loss = 2.7939  Validation loss = 2.2813  \n",
      "\n",
      "Fold: 1  Epoch: 98  Training loss = 2.7928  Validation loss = 2.2770  \n",
      "\n",
      "Fold: 1  Epoch: 99  Training loss = 2.7926  Validation loss = 2.2763  \n",
      "\n",
      "Fold: 1  Epoch: 100  Training loss = 2.7923  Validation loss = 2.2755  \n",
      "\n",
      "Fold: 1  Epoch: 101  Training loss = 2.7909  Validation loss = 2.2702  \n",
      "\n",
      "Fold: 1  Epoch: 102  Training loss = 2.7891  Validation loss = 2.2634  \n",
      "\n",
      "Fold: 1  Epoch: 103  Training loss = 2.7888  Validation loss = 2.2620  \n",
      "\n",
      "Fold: 1  Epoch: 104  Training loss = 2.7867  Validation loss = 2.2542  \n",
      "\n",
      "Fold: 1  Epoch: 105  Training loss = 2.7863  Validation loss = 2.2526  \n",
      "\n",
      "Fold: 1  Epoch: 106  Training loss = 2.7855  Validation loss = 2.2495  \n",
      "\n",
      "Fold: 1  Epoch: 107  Training loss = 2.7839  Validation loss = 2.2434  \n",
      "\n",
      "Fold: 1  Epoch: 108  Training loss = 2.7824  Validation loss = 2.2373  \n",
      "\n",
      "Fold: 1  Epoch: 109  Training loss = 2.7800  Validation loss = 2.2278  \n",
      "\n",
      "Fold: 1  Epoch: 110  Training loss = 2.7790  Validation loss = 2.2235  \n",
      "\n",
      "Fold: 1  Epoch: 111  Training loss = 2.7776  Validation loss = 2.2181  \n",
      "\n",
      "Fold: 1  Epoch: 112  Training loss = 2.7744  Validation loss = 2.2052  \n",
      "\n",
      "Fold: 1  Epoch: 113  Training loss = 2.7728  Validation loss = 2.1980  \n",
      "\n",
      "Fold: 1  Epoch: 114  Training loss = 2.7723  Validation loss = 2.1962  \n",
      "\n",
      "Fold: 1  Epoch: 115  Training loss = 2.7708  Validation loss = 2.1900  \n",
      "\n",
      "Fold: 1  Epoch: 116  Training loss = 2.7695  Validation loss = 2.1839  \n",
      "\n",
      "Fold: 1  Epoch: 117  Training loss = 2.7686  Validation loss = 2.1801  \n",
      "\n",
      "Fold: 1  Epoch: 118  Training loss = 2.7668  Validation loss = 2.1719  \n",
      "\n",
      "Fold: 1  Epoch: 119  Training loss = 2.7651  Validation loss = 2.1639  \n",
      "\n",
      "Fold: 1  Epoch: 120  Training loss = 2.7641  Validation loss = 2.1595  \n",
      "\n",
      "Fold: 1  Epoch: 121  Training loss = 2.7636  Validation loss = 2.1572  \n",
      "\n",
      "Fold: 1  Epoch: 122  Training loss = 2.7624  Validation loss = 2.1521  \n",
      "\n",
      "Fold: 1  Epoch: 123  Training loss = 2.7609  Validation loss = 2.1457  \n",
      "\n",
      "Fold: 1  Epoch: 124  Training loss = 2.7597  Validation loss = 2.1402  \n",
      "\n",
      "Fold: 1  Epoch: 125  Training loss = 2.7582  Validation loss = 2.1334  \n",
      "\n",
      "Fold: 1  Epoch: 126  Training loss = 2.7570  Validation loss = 2.1279  \n",
      "\n",
      "Fold: 1  Epoch: 127  Training loss = 2.7564  Validation loss = 2.1250  \n",
      "\n",
      "Fold: 1  Epoch: 128  Training loss = 2.7552  Validation loss = 2.1196  \n",
      "\n",
      "Fold: 1  Epoch: 129  Training loss = 2.7535  Validation loss = 2.1117  \n",
      "\n",
      "Fold: 1  Epoch: 130  Training loss = 2.7524  Validation loss = 2.1066  \n",
      "\n",
      "Fold: 1  Epoch: 131  Training loss = 2.7514  Validation loss = 2.1020  \n",
      "\n",
      "Fold: 1  Epoch: 132  Training loss = 2.7511  Validation loss = 2.1005  \n",
      "\n",
      "Fold: 1  Epoch: 133  Training loss = 2.7501  Validation loss = 2.0963  \n",
      "\n",
      "Fold: 1  Epoch: 134  Training loss = 2.7490  Validation loss = 2.0907  \n",
      "\n",
      "Fold: 1  Epoch: 135  Training loss = 2.7489  Validation loss = 2.0899  \n",
      "\n",
      "Fold: 1  Epoch: 136  Training loss = 2.7483  Validation loss = 2.0870  \n",
      "\n",
      "Fold: 1  Epoch: 137  Training loss = 2.7481  Validation loss = 2.0864  \n",
      "\n",
      "Fold: 1  Epoch: 138  Training loss = 2.7478  Validation loss = 2.0855  \n",
      "\n",
      "Fold: 1  Epoch: 139  Training loss = 2.7477  Validation loss = 2.0847  \n",
      "\n",
      "Fold: 1  Epoch: 140  Training loss = 2.7472  Validation loss = 2.0826  \n",
      "\n",
      "Fold: 1  Epoch: 141  Training loss = 2.7464  Validation loss = 2.0786  \n",
      "\n",
      "Fold: 1  Epoch: 142  Training loss = 2.7451  Validation loss = 2.0716  \n",
      "\n",
      "Fold: 1  Epoch: 143  Training loss = 2.7437  Validation loss = 2.0645  \n",
      "\n",
      "Fold: 1  Epoch: 144  Training loss = 2.7434  Validation loss = 2.0630  \n",
      "\n",
      "Fold: 1  Epoch: 145  Training loss = 2.7415  Validation loss = 2.0535  \n",
      "\n",
      "Fold: 1  Epoch: 146  Training loss = 2.7409  Validation loss = 2.0506  \n",
      "\n",
      "Fold: 1  Epoch: 147  Training loss = 2.7399  Validation loss = 2.0458  \n",
      "\n",
      "Fold: 1  Epoch: 148  Training loss = 2.7396  Validation loss = 2.0440  \n",
      "\n",
      "Fold: 1  Epoch: 149  Training loss = 2.7387  Validation loss = 2.0396  \n",
      "\n",
      "Fold: 1  Epoch: 150  Training loss = 2.7390  Validation loss = 2.0419  \n",
      "\n",
      "Fold: 1  Epoch: 151  Training loss = 2.7385  Validation loss = 2.0398  \n",
      "\n",
      "Fold: 1  Epoch: 152  Training loss = 2.7382  Validation loss = 2.0383  \n",
      "\n",
      "Fold: 1  Epoch: 153  Training loss = 2.7372  Validation loss = 2.0331  \n",
      "\n",
      "Fold: 1  Epoch: 154  Training loss = 2.7360  Validation loss = 2.0273  \n",
      "\n",
      "Fold: 1  Epoch: 155  Training loss = 2.7359  Validation loss = 2.0270  \n",
      "\n",
      "Fold: 1  Epoch: 156  Training loss = 2.7346  Validation loss = 2.0206  \n",
      "\n",
      "Fold: 1  Epoch: 157  Training loss = 2.7338  Validation loss = 2.0167  \n",
      "\n",
      "Fold: 1  Epoch: 158  Training loss = 2.7335  Validation loss = 2.0152  \n",
      "\n",
      "Fold: 1  Epoch: 159  Training loss = 2.7331  Validation loss = 2.0132  \n",
      "\n",
      "Fold: 1  Epoch: 160  Training loss = 2.7318  Validation loss = 2.0061  \n",
      "\n",
      "Fold: 1  Epoch: 161  Training loss = 2.7312  Validation loss = 2.0036  \n",
      "\n",
      "Fold: 1  Epoch: 162  Training loss = 2.7303  Validation loss = 1.9989  \n",
      "\n",
      "Fold: 1  Epoch: 163  Training loss = 2.7290  Validation loss = 1.9915  \n",
      "\n",
      "Fold: 1  Epoch: 164  Training loss = 2.7280  Validation loss = 1.9863  \n",
      "\n",
      "Fold: 1  Epoch: 165  Training loss = 2.7279  Validation loss = 1.9859  \n",
      "\n",
      "Fold: 1  Epoch: 166  Training loss = 2.7270  Validation loss = 1.9811  \n",
      "\n",
      "Fold: 1  Epoch: 167  Training loss = 2.7257  Validation loss = 1.9740  \n",
      "\n",
      "Fold: 1  Epoch: 168  Training loss = 2.7254  Validation loss = 1.9725  \n",
      "\n",
      "Fold: 1  Epoch: 169  Training loss = 2.7248  Validation loss = 1.9691  \n",
      "\n",
      "Fold: 1  Epoch: 170  Training loss = 2.7241  Validation loss = 1.9656  \n",
      "\n",
      "Fold: 1  Epoch: 171  Training loss = 2.7237  Validation loss = 1.9637  \n",
      "\n",
      "Fold: 1  Epoch: 172  Training loss = 2.7230  Validation loss = 1.9591  \n",
      "\n",
      "Fold: 1  Epoch: 173  Training loss = 2.7216  Validation loss = 1.9513  \n",
      "\n",
      "Fold: 1  Epoch: 174  Training loss = 2.7211  Validation loss = 1.9484  \n",
      "\n",
      "Fold: 1  Epoch: 175  Training loss = 2.7208  Validation loss = 1.9468  \n",
      "\n",
      "Fold: 1  Epoch: 176  Training loss = 2.7201  Validation loss = 1.9432  \n",
      "\n",
      "Fold: 1  Epoch: 177  Training loss = 2.7190  Validation loss = 1.9366  \n",
      "\n",
      "Fold: 1  Epoch: 178  Training loss = 2.7179  Validation loss = 1.9299  \n",
      "\n",
      "Fold: 1  Epoch: 179  Training loss = 2.7167  Validation loss = 1.9225  \n",
      "\n",
      "Fold: 1  Epoch: 180  Training loss = 2.7164  Validation loss = 1.9211  \n",
      "\n",
      "Fold: 1  Epoch: 181  Training loss = 2.7159  Validation loss = 1.9175  \n",
      "\n",
      "Fold: 1  Epoch: 182  Training loss = 2.7146  Validation loss = 1.9093  \n",
      "\n",
      "Fold: 1  Epoch: 183  Training loss = 2.7144  Validation loss = 1.9094  \n",
      "\n",
      "Fold: 1  Epoch: 184  Training loss = 2.7140  Validation loss = 1.9066  \n",
      "\n",
      "Fold: 1  Epoch: 185  Training loss = 2.7136  Validation loss = 1.9050  \n",
      "\n",
      "Fold: 1  Epoch: 186  Training loss = 2.7134  Validation loss = 1.9046  \n",
      "\n",
      "Fold: 1  Epoch: 187  Training loss = 2.7125  Validation loss = 1.8982  \n",
      "\n",
      "Fold: 1  Epoch: 188  Training loss = 2.7121  Validation loss = 1.8963  \n",
      "\n",
      "Fold: 1  Epoch: 189  Training loss = 2.7116  Validation loss = 1.8942  \n",
      "\n",
      "Fold: 1  Epoch: 190  Training loss = 2.7113  Validation loss = 1.8929  \n",
      "\n",
      "Fold: 1  Epoch: 191  Training loss = 2.7104  Validation loss = 1.8867  \n",
      "\n",
      "Fold: 1  Epoch: 192  Training loss = 2.7098  Validation loss = 1.8827  \n",
      "\n",
      "Fold: 1  Epoch: 193  Training loss = 2.7097  Validation loss = 1.8831  \n",
      "\n",
      "Fold: 1  Epoch: 194  Training loss = 2.7094  Validation loss = 1.8820  \n",
      "\n",
      "Fold: 1  Epoch: 195  Training loss = 2.7092  Validation loss = 1.8810  \n",
      "\n",
      "Fold: 1  Epoch: 196  Training loss = 2.7079  Validation loss = 1.8714  \n",
      "\n",
      "Fold: 1  Epoch: 197  Training loss = 2.7070  Validation loss = 1.8659  \n",
      "\n",
      "Fold: 1  Epoch: 198  Training loss = 2.7063  Validation loss = 1.8611  \n",
      "\n",
      "Fold: 1  Epoch: 199  Training loss = 2.7053  Validation loss = 1.8534  \n",
      "\n",
      "Fold: 1  Epoch: 200  Training loss = 2.7054  Validation loss = 1.8548  \n",
      "\n",
      "Fold: 1  Epoch: 201  Training loss = 2.7043  Validation loss = 1.8468  \n",
      "\n",
      "Fold: 1  Epoch: 202  Training loss = 2.7037  Validation loss = 1.8421  \n",
      "\n",
      "Fold: 1  Epoch: 203  Training loss = 2.7031  Validation loss = 1.8390  \n",
      "\n",
      "Fold: 1  Epoch: 204  Training loss = 2.7025  Validation loss = 1.8356  \n",
      "\n",
      "Fold: 1  Epoch: 205  Training loss = 2.7024  Validation loss = 1.8357  \n",
      "\n",
      "Fold: 1  Epoch: 206  Training loss = 2.7021  Validation loss = 1.8327  \n",
      "\n",
      "Fold: 1  Epoch: 207  Training loss = 2.7016  Validation loss = 1.8299  \n",
      "\n",
      "Fold: 1  Epoch: 208  Training loss = 2.7011  Validation loss = 1.8267  \n",
      "\n",
      "Fold: 1  Epoch: 209  Training loss = 2.7007  Validation loss = 1.8251  \n",
      "\n",
      "Fold: 1  Epoch: 210  Training loss = 2.7003  Validation loss = 1.8231  \n",
      "\n",
      "Fold: 1  Epoch: 211  Training loss = 2.7000  Validation loss = 1.8220  \n",
      "\n",
      "Fold: 1  Epoch: 212  Training loss = 2.6987  Validation loss = 1.8121  \n",
      "\n",
      "Fold: 1  Epoch: 213  Training loss = 2.6981  Validation loss = 1.8064  \n",
      "\n",
      "Fold: 1  Epoch: 214  Training loss = 2.6979  Validation loss = 1.8070  \n",
      "\n",
      "Fold: 1  Epoch: 215  Training loss = 2.6976  Validation loss = 1.8051  \n",
      "\n",
      "Fold: 1  Epoch: 216  Training loss = 2.6973  Validation loss = 1.8028  \n",
      "\n",
      "Fold: 1  Epoch: 217  Training loss = 2.6970  Validation loss = 1.8008  \n",
      "\n",
      "Fold: 1  Epoch: 218  Training loss = 2.6963  Validation loss = 1.7952  \n",
      "\n",
      "Fold: 1  Epoch: 219  Training loss = 2.6962  Validation loss = 1.7957  \n",
      "\n",
      "Fold: 1  Epoch: 220  Training loss = 2.6960  Validation loss = 1.7959  \n",
      "\n",
      "Fold: 1  Epoch: 221  Training loss = 2.6957  Validation loss = 1.7943  \n",
      "\n",
      "Fold: 1  Epoch: 222  Training loss = 2.6953  Validation loss = 1.7911  \n",
      "\n",
      "Fold: 1  Epoch: 223  Training loss = 2.6949  Validation loss = 1.7882  \n",
      "\n",
      "Fold: 1  Epoch: 224  Training loss = 2.6945  Validation loss = 1.7858  \n",
      "\n",
      "Fold: 1  Epoch: 225  Training loss = 2.6945  Validation loss = 1.7883  \n",
      "\n",
      "Fold: 1  Epoch: 226  Training loss = 2.6934  Validation loss = 1.7781  \n",
      "\n",
      "Fold: 1  Epoch: 227  Training loss = 2.6935  Validation loss = 1.7805  \n",
      "\n",
      "Fold: 1  Epoch: 228  Training loss = 2.6931  Validation loss = 1.7776  \n",
      "\n",
      "Fold: 1  Epoch: 229  Training loss = 2.6924  Validation loss = 1.7713  \n",
      "\n",
      "Fold: 1  Epoch: 230  Training loss = 2.6922  Validation loss = 1.7704  \n",
      "\n",
      "Fold: 1  Epoch: 231  Training loss = 2.6915  Validation loss = 1.7652  \n",
      "\n",
      "Fold: 1  Epoch: 232  Training loss = 2.6916  Validation loss = 1.7665  \n",
      "\n",
      "Fold: 1  Epoch: 233  Training loss = 2.6915  Validation loss = 1.7670  \n",
      "\n",
      "Fold: 1  Epoch: 234  Training loss = 2.6910  Validation loss = 1.7638  \n",
      "\n",
      "Fold: 1  Epoch: 235  Training loss = 2.6906  Validation loss = 1.7607  \n",
      "\n",
      "Fold: 1  Epoch: 236  Training loss = 2.6902  Validation loss = 1.7581  \n",
      "\n",
      "Fold: 1  Epoch: 237  Training loss = 2.6897  Validation loss = 1.7532  \n",
      "\n",
      "Fold: 1  Epoch: 238  Training loss = 2.6891  Validation loss = 1.7479  \n",
      "\n",
      "Fold: 1  Epoch: 239  Training loss = 2.6888  Validation loss = 1.7470  \n",
      "\n",
      "Fold: 1  Epoch: 240  Training loss = 2.6882  Validation loss = 1.7413  \n",
      "\n",
      "Fold: 1  Epoch: 241  Training loss = 2.6880  Validation loss = 1.7401  \n",
      "\n",
      "Fold: 1  Epoch: 242  Training loss = 2.6877  Validation loss = 1.7389  \n",
      "\n",
      "Fold: 1  Epoch: 243  Training loss = 2.6875  Validation loss = 1.7376  \n",
      "\n",
      "Fold: 1  Epoch: 244  Training loss = 2.6874  Validation loss = 1.7377  \n",
      "\n",
      "Fold: 1  Epoch: 245  Training loss = 2.6868  Validation loss = 1.7327  \n",
      "\n",
      "Fold: 1  Epoch: 246  Training loss = 2.6869  Validation loss = 1.7359  \n",
      "\n",
      "Fold: 1  Epoch: 247  Training loss = 2.6871  Validation loss = 1.7397  \n",
      "\n",
      "Fold: 1  Epoch: 248  Training loss = 2.6867  Validation loss = 1.7358  \n",
      "\n",
      "Fold: 1  Epoch: 249  Training loss = 2.6865  Validation loss = 1.7344  \n",
      "\n",
      "Fold: 1  Epoch: 250  Training loss = 2.6860  Validation loss = 1.7301  \n",
      "\n",
      "Fold: 1  Epoch: 251  Training loss = 2.6853  Validation loss = 1.7227  \n",
      "\n",
      "Fold: 1  Epoch: 252  Training loss = 2.6850  Validation loss = 1.7215  \n",
      "\n",
      "Fold: 1  Epoch: 253  Training loss = 2.6843  Validation loss = 1.7133  \n",
      "\n",
      "Fold: 1  Epoch: 254  Training loss = 2.6839  Validation loss = 1.7092  \n",
      "\n",
      "Fold: 1  Epoch: 255  Training loss = 2.6837  Validation loss = 1.7086  \n",
      "\n",
      "Fold: 1  Epoch: 256  Training loss = 2.6835  Validation loss = 1.7063  \n",
      "\n",
      "Fold: 1  Epoch: 257  Training loss = 2.6831  Validation loss = 1.7030  \n",
      "\n",
      "Fold: 1  Epoch: 258  Training loss = 2.6826  Validation loss = 1.6993  \n",
      "\n",
      "Fold: 1  Epoch: 259  Training loss = 2.6822  Validation loss = 1.6938  \n",
      "\n",
      "Fold: 1  Epoch: 260  Training loss = 2.6821  Validation loss = 1.6954  \n",
      "\n",
      "Fold: 1  Epoch: 261  Training loss = 2.6819  Validation loss = 1.6927  \n",
      "\n",
      "Fold: 1  Epoch: 262  Training loss = 2.6812  Validation loss = 1.6861  \n",
      "\n",
      "Fold: 1  Epoch: 263  Training loss = 2.6808  Validation loss = 1.6812  \n",
      "\n",
      "Fold: 1  Epoch: 264  Training loss = 2.6808  Validation loss = 1.6841  \n",
      "\n",
      "Fold: 1  Epoch: 265  Training loss = 2.6804  Validation loss = 1.6807  \n",
      "\n",
      "Fold: 1  Epoch: 266  Training loss = 2.6802  Validation loss = 1.6791  \n",
      "\n",
      "Fold: 1  Epoch: 267  Training loss = 2.6796  Validation loss = 1.6725  \n",
      "\n",
      "Fold: 1  Epoch: 268  Training loss = 2.6797  Validation loss = 1.6764  \n",
      "\n",
      "Fold: 1  Epoch: 269  Training loss = 2.6797  Validation loss = 1.6776  \n",
      "\n",
      "Fold: 1  Epoch: 270  Training loss = 2.6793  Validation loss = 1.6750  \n",
      "\n",
      "Fold: 1  Epoch: 271  Training loss = 2.6786  Validation loss = 1.6651  \n",
      "\n",
      "Fold: 1  Epoch: 272  Training loss = 2.6781  Validation loss = 1.6594  \n",
      "\n",
      "Fold: 1  Epoch: 273  Training loss = 2.6781  Validation loss = 1.6595  \n",
      "\n",
      "Fold: 1  Epoch: 274  Training loss = 2.6781  Validation loss = 1.6630  \n",
      "\n",
      "Fold: 1  Epoch: 275  Training loss = 2.6780  Validation loss = 1.6638  \n",
      "\n",
      "Fold: 1  Epoch: 276  Training loss = 2.6780  Validation loss = 1.6668  \n",
      "\n",
      "Fold: 1  Epoch: 277  Training loss = 2.6780  Validation loss = 1.6682  \n",
      "\n",
      "Fold: 1  Epoch: 278  Training loss = 2.6779  Validation loss = 1.6682  \n",
      "\n",
      "Fold: 1  Epoch: 279  Training loss = 2.6777  Validation loss = 1.6665  \n",
      "\n",
      "Fold: 1  Epoch: 280  Training loss = 2.6773  Validation loss = 1.6623  \n",
      "\n",
      "Fold: 1  Epoch: 281  Training loss = 2.6770  Validation loss = 1.6582  \n",
      "\n",
      "Fold: 1  Epoch: 282  Training loss = 2.6770  Validation loss = 1.6602  \n",
      "\n",
      "Fold: 1  Epoch: 283  Training loss = 2.6771  Validation loss = 1.6627  \n",
      "\n",
      "Fold: 1  Epoch: 284  Training loss = 2.6769  Validation loss = 1.6632  \n",
      "\n",
      "Fold: 1  Epoch: 285  Training loss = 2.6770  Validation loss = 1.6654  \n",
      "\n",
      "Fold: 1  Epoch: 286  Training loss = 2.6768  Validation loss = 1.6655  \n",
      "\n",
      "Fold: 1  Epoch: 287  Training loss = 2.6763  Validation loss = 1.6615  \n",
      "\n",
      "Fold: 1  Epoch: 288  Training loss = 2.6762  Validation loss = 1.6610  \n",
      "\n",
      "Fold: 1  Epoch: 289  Training loss = 2.6759  Validation loss = 1.6581  \n",
      "\n",
      "Fold: 1  Epoch: 290  Training loss = 2.6755  Validation loss = 1.6554  \n",
      "\n",
      "Fold: 1  Epoch: 291  Training loss = 2.6749  Validation loss = 1.6471  \n",
      "\n",
      "Fold: 1  Epoch: 292  Training loss = 2.6748  Validation loss = 1.6461  \n",
      "\n",
      "Fold: 1  Epoch: 293  Training loss = 2.6748  Validation loss = 1.6489  \n",
      "\n",
      "Fold: 1  Epoch: 294  Training loss = 2.6747  Validation loss = 1.6488  \n",
      "\n",
      "Fold: 1  Epoch: 295  Training loss = 2.6744  Validation loss = 1.6458  \n",
      "\n",
      "Fold: 1  Epoch: 296  Training loss = 2.6744  Validation loss = 1.6448  \n",
      "\n",
      "Fold: 1  Epoch: 297  Training loss = 2.6743  Validation loss = 1.6451  \n",
      "\n",
      "Fold: 1  Epoch: 298  Training loss = 2.6741  Validation loss = 1.6450  \n",
      "\n",
      "Fold: 1  Epoch: 299  Training loss = 2.6743  Validation loss = 1.6520  \n",
      "\n",
      "Fold: 1  Epoch: 300  Training loss = 2.6742  Validation loss = 1.6522  \n",
      "\n",
      "Fold: 1  Epoch: 301  Training loss = 2.6740  Validation loss = 1.6511  \n",
      "\n",
      "Fold: 1  Epoch: 302  Training loss = 2.6739  Validation loss = 1.6550  \n",
      "\n",
      "Fold: 1  Epoch: 303  Training loss = 2.6737  Validation loss = 1.6527  \n",
      "\n",
      "Fold: 1  Epoch: 304  Training loss = 2.6733  Validation loss = 1.6478  \n",
      "\n",
      "Fold: 1  Epoch: 305  Training loss = 2.6732  Validation loss = 1.6475  \n",
      "\n",
      "Fold: 1  Epoch: 306  Training loss = 2.6728  Validation loss = 1.6416  \n",
      "\n",
      "Fold: 1  Epoch: 307  Training loss = 2.6724  Validation loss = 1.6364  \n",
      "\n",
      "Fold: 1  Epoch: 308  Training loss = 2.6721  Validation loss = 1.6347  \n",
      "\n",
      "Fold: 1  Epoch: 309  Training loss = 2.6721  Validation loss = 1.6382  \n",
      "\n",
      "Fold: 1  Epoch: 310  Training loss = 2.6717  Validation loss = 1.6336  \n",
      "\n",
      "Fold: 1  Epoch: 311  Training loss = 2.6712  Validation loss = 1.6258  \n",
      "\n",
      "Fold: 1  Epoch: 312  Training loss = 2.6711  Validation loss = 1.6256  \n",
      "\n",
      "Fold: 1  Epoch: 313  Training loss = 2.6706  Validation loss = 1.6186  \n",
      "\n",
      "Fold: 1  Epoch: 314  Training loss = 2.6704  Validation loss = 1.6172  \n",
      "\n",
      "Fold: 1  Epoch: 315  Training loss = 2.6703  Validation loss = 1.6147  \n",
      "\n",
      "Fold: 1  Epoch: 316  Training loss = 2.6701  Validation loss = 1.6143  \n",
      "\n",
      "Fold: 1  Epoch: 317  Training loss = 2.6697  Validation loss = 1.6085  \n",
      "\n",
      "Fold: 1  Epoch: 318  Training loss = 2.6697  Validation loss = 1.6139  \n",
      "\n",
      "Fold: 1  Epoch: 319  Training loss = 2.6693  Validation loss = 1.6081  \n",
      "\n",
      "Fold: 1  Epoch: 320  Training loss = 2.6690  Validation loss = 1.6049  \n",
      "\n",
      "Fold: 1  Epoch: 321  Training loss = 2.6689  Validation loss = 1.6065  \n",
      "\n",
      "Fold: 1  Epoch: 322  Training loss = 2.6689  Validation loss = 1.6074  \n",
      "\n",
      "Fold: 1  Epoch: 323  Training loss = 2.6686  Validation loss = 1.6052  \n",
      "\n",
      "Fold: 1  Epoch: 324  Training loss = 2.6681  Validation loss = 1.5937  \n",
      "\n",
      "Fold: 1  Epoch: 325  Training loss = 2.6679  Validation loss = 1.5943  \n",
      "\n",
      "Fold: 1  Epoch: 326  Training loss = 2.6676  Validation loss = 1.5930  \n",
      "\n",
      "Fold: 1  Epoch: 327  Training loss = 2.6674  Validation loss = 1.5898  \n",
      "\n",
      "Fold: 1  Epoch: 328  Training loss = 2.6674  Validation loss = 1.5937  \n",
      "\n",
      "Fold: 1  Epoch: 329  Training loss = 2.6674  Validation loss = 1.5945  \n",
      "\n",
      "Fold: 1  Epoch: 330  Training loss = 2.6672  Validation loss = 1.5934  \n",
      "\n",
      "Fold: 1  Epoch: 331  Training loss = 2.6670  Validation loss = 1.5917  \n",
      "\n",
      "Fold: 1  Epoch: 332  Training loss = 2.6670  Validation loss = 1.5952  \n",
      "\n",
      "Fold: 1  Epoch: 333  Training loss = 2.6669  Validation loss = 1.5928  \n",
      "\n",
      "Fold: 1  Epoch: 334  Training loss = 2.6667  Validation loss = 1.5936  \n",
      "\n",
      "Fold: 1  Epoch: 335  Training loss = 2.6663  Validation loss = 1.5859  \n",
      "\n",
      "Fold: 1  Epoch: 336  Training loss = 2.6661  Validation loss = 1.5877  \n",
      "\n",
      "Fold: 1  Epoch: 337  Training loss = 2.6660  Validation loss = 1.5865  \n",
      "\n",
      "Fold: 1  Epoch: 338  Training loss = 2.6656  Validation loss = 1.5804  \n",
      "\n",
      "Fold: 1  Epoch: 339  Training loss = 2.6654  Validation loss = 1.5814  \n",
      "\n",
      "Fold: 1  Epoch: 340  Training loss = 2.6654  Validation loss = 1.5835  \n",
      "\n",
      "Fold: 1  Epoch: 341  Training loss = 2.6651  Validation loss = 1.5818  \n",
      "\n",
      "Fold: 1  Epoch: 342  Training loss = 2.6651  Validation loss = 1.5897  \n",
      "\n",
      "Fold: 1  Epoch: 343  Training loss = 2.6650  Validation loss = 1.5911  \n",
      "\n",
      "Fold: 1  Epoch: 344  Training loss = 2.6648  Validation loss = 1.5898  \n",
      "\n",
      "Fold: 1  Epoch: 345  Training loss = 2.6646  Validation loss = 1.5885  \n",
      "\n",
      "Fold: 1  Epoch: 346  Training loss = 2.6643  Validation loss = 1.5860  \n",
      "\n",
      "Fold: 1  Epoch: 347  Training loss = 2.6642  Validation loss = 1.5860  \n",
      "\n",
      "Fold: 1  Epoch: 348  Training loss = 2.6638  Validation loss = 1.5765  \n",
      "\n",
      "Fold: 1  Epoch: 349  Training loss = 2.6636  Validation loss = 1.5747  \n",
      "\n",
      "Fold: 1  Epoch: 350  Training loss = 2.6633  Validation loss = 1.5692  \n",
      "\n",
      "Fold: 1  Epoch: 351  Training loss = 2.6630  Validation loss = 1.5666  \n",
      "\n",
      "Fold: 1  Epoch: 352  Training loss = 2.6627  Validation loss = 1.5664  \n",
      "\n",
      "Fold: 1  Epoch: 353  Training loss = 2.6623  Validation loss = 1.5585  \n",
      "\n",
      "Fold: 1  Epoch: 354  Training loss = 2.6623  Validation loss = 1.5581  \n",
      "\n",
      "Fold: 1  Epoch: 355  Training loss = 2.6621  Validation loss = 1.5572  \n",
      "\n",
      "Fold: 1  Epoch: 356  Training loss = 2.6620  Validation loss = 1.5591  \n",
      "\n",
      "Fold: 1  Epoch: 357  Training loss = 2.6618  Validation loss = 1.5568  \n",
      "\n",
      "Fold: 1  Epoch: 358  Training loss = 2.6617  Validation loss = 1.5627  \n",
      "\n",
      "Fold: 1  Epoch: 359  Training loss = 2.6615  Validation loss = 1.5604  \n",
      "\n",
      "Fold: 1  Epoch: 360  Training loss = 2.6611  Validation loss = 1.5565  \n",
      "\n",
      "Fold: 1  Epoch: 361  Training loss = 2.6610  Validation loss = 1.5554  \n",
      "\n",
      "Fold: 1  Epoch: 362  Training loss = 2.6608  Validation loss = 1.5589  \n",
      "\n",
      "Fold: 1  Epoch: 363  Training loss = 2.6605  Validation loss = 1.5548  \n",
      "\n",
      "Fold: 1  Epoch: 364  Training loss = 2.6604  Validation loss = 1.5534  \n",
      "\n",
      "Fold: 1  Epoch: 365  Training loss = 2.6602  Validation loss = 1.5541  \n",
      "\n",
      "Fold: 1  Epoch: 366  Training loss = 2.6601  Validation loss = 1.5573  \n",
      "\n",
      "Fold: 1  Epoch: 367  Training loss = 2.6599  Validation loss = 1.5590  \n",
      "\n",
      "Fold: 1  Epoch: 368  Training loss = 2.6597  Validation loss = 1.5580  \n",
      "\n",
      "Fold: 1  Epoch: 369  Training loss = 2.6596  Validation loss = 1.5607  \n",
      "\n",
      "Fold: 1  Epoch: 370  Training loss = 2.6594  Validation loss = 1.5620  \n",
      "\n",
      "Fold: 1  Epoch: 371  Training loss = 2.6592  Validation loss = 1.5603  \n",
      "\n",
      "Fold: 1  Epoch: 372  Training loss = 2.6590  Validation loss = 1.5612  \n",
      "\n",
      "Fold: 1  Epoch: 373  Training loss = 2.6590  Validation loss = 1.5660  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 364  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.5578  Validation loss = 2.0147  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.5579  Validation loss = 2.0164  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.5578  Validation loss = 2.0166  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.5576  Validation loss = 2.0170  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.5576  Validation loss = 2.0176  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.5575  Validation loss = 2.0168  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.5575  Validation loss = 2.0183  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.5569  Validation loss = 2.0139  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.5566  Validation loss = 2.0110  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.5566  Validation loss = 2.0131  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.5565  Validation loss = 2.0123  \n",
      "\n",
      "Fold: 2  Epoch: 12  Training loss = 2.5560  Validation loss = 2.0087  \n",
      "\n",
      "Fold: 2  Epoch: 13  Training loss = 2.5557  Validation loss = 2.0061  \n",
      "\n",
      "Fold: 2  Epoch: 14  Training loss = 2.5556  Validation loss = 2.0053  \n",
      "\n",
      "Fold: 2  Epoch: 15  Training loss = 2.5552  Validation loss = 2.0020  \n",
      "\n",
      "Fold: 2  Epoch: 16  Training loss = 2.5547  Validation loss = 1.9949  \n",
      "\n",
      "Fold: 2  Epoch: 17  Training loss = 2.5543  Validation loss = 1.9907  \n",
      "\n",
      "Fold: 2  Epoch: 18  Training loss = 2.5542  Validation loss = 1.9902  \n",
      "\n",
      "Fold: 2  Epoch: 19  Training loss = 2.5543  Validation loss = 1.9939  \n",
      "\n",
      "Fold: 2  Epoch: 20  Training loss = 2.5541  Validation loss = 1.9929  \n",
      "\n",
      "Fold: 2  Epoch: 21  Training loss = 2.5536  Validation loss = 1.9891  \n",
      "\n",
      "Fold: 2  Epoch: 22  Training loss = 2.5534  Validation loss = 1.9892  \n",
      "\n",
      "Fold: 2  Epoch: 23  Training loss = 2.5533  Validation loss = 1.9891  \n",
      "\n",
      "Fold: 2  Epoch: 24  Training loss = 2.5532  Validation loss = 1.9890  \n",
      "\n",
      "Fold: 2  Epoch: 25  Training loss = 2.5528  Validation loss = 1.9838  \n",
      "\n",
      "Fold: 2  Epoch: 26  Training loss = 2.5528  Validation loss = 1.9860  \n",
      "\n",
      "Fold: 2  Epoch: 27  Training loss = 2.5527  Validation loss = 1.9878  \n",
      "\n",
      "Fold: 2  Epoch: 28  Training loss = 2.5525  Validation loss = 1.9885  \n",
      "\n",
      "Fold: 2  Epoch: 29  Training loss = 2.5523  Validation loss = 1.9870  \n",
      "\n",
      "Fold: 2  Epoch: 30  Training loss = 2.5521  Validation loss = 1.9888  \n",
      "\n",
      "Fold: 2  Epoch: 31  Training loss = 2.5521  Validation loss = 1.9916  \n",
      "\n",
      "Fold: 2  Epoch: 32  Training loss = 2.5519  Validation loss = 1.9894  \n",
      "\n",
      "Fold: 2  Epoch: 33  Training loss = 2.5518  Validation loss = 1.9906  \n",
      "\n",
      "Fold: 2  Epoch: 34  Training loss = 2.5514  Validation loss = 1.9854  \n",
      "\n",
      "Fold: 2  Epoch: 35  Training loss = 2.5514  Validation loss = 1.9870  \n",
      "\n",
      "Fold: 2  Epoch: 36  Training loss = 2.5512  Validation loss = 1.9878  \n",
      "\n",
      "Fold: 2  Epoch: 37  Training loss = 2.5510  Validation loss = 1.9856  \n",
      "\n",
      "Fold: 2  Epoch: 38  Training loss = 2.5510  Validation loss = 1.9883  \n",
      "\n",
      "Fold: 2  Epoch: 39  Training loss = 2.5508  Validation loss = 1.9885  \n",
      "\n",
      "Fold: 2  Epoch: 40  Training loss = 2.5511  Validation loss = 1.9937  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 25  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.5567  Validation loss = 2.8073  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.5556  Validation loss = 2.8081  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.5544  Validation loss = 2.8069  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.5534  Validation loss = 2.8061  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.5523  Validation loss = 2.8047  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.5514  Validation loss = 2.8092  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.5500  Validation loss = 2.8126  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.5492  Validation loss = 2.8127  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.5485  Validation loss = 2.8141  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.5477  Validation loss = 2.8163  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.5470  Validation loss = 2.8189  \n",
      "\n",
      "Fold: 3  Epoch: 12  Training loss = 1.5463  Validation loss = 2.8226  \n",
      "\n",
      "Fold: 3  Epoch: 13  Training loss = 1.5456  Validation loss = 2.8220  \n",
      "\n",
      "Fold: 3  Epoch: 14  Training loss = 1.5448  Validation loss = 2.8206  \n",
      "\n",
      "Fold: 3  Epoch: 15  Training loss = 1.5439  Validation loss = 2.8173  \n",
      "\n",
      "Fold: 3  Epoch: 16  Training loss = 1.5436  Validation loss = 2.8165  \n",
      "\n",
      "Fold: 3  Epoch: 17  Training loss = 1.5436  Validation loss = 2.8119  \n",
      "\n",
      "Fold: 3  Epoch: 18  Training loss = 1.5432  Validation loss = 2.8079  \n",
      "\n",
      "Fold: 3  Epoch: 19  Training loss = 1.5427  Validation loss = 2.8087  \n",
      "\n",
      "Fold: 3  Epoch: 20  Training loss = 1.5421  Validation loss = 2.8084  \n",
      "\n",
      "Fold: 3  Epoch: 21  Training loss = 1.5411  Validation loss = 2.8150  \n",
      "\n",
      "Fold: 3  Epoch: 22  Training loss = 1.5408  Validation loss = 2.8116  \n",
      "\n",
      "Fold: 3  Epoch: 23  Training loss = 1.5407  Validation loss = 2.8107  \n",
      "\n",
      "Fold: 3  Epoch: 24  Training loss = 1.5401  Validation loss = 2.8086  \n",
      "\n",
      "Fold: 3  Epoch: 25  Training loss = 1.5392  Validation loss = 2.8094  \n",
      "\n",
      "Fold: 3  Epoch: 26  Training loss = 1.5385  Validation loss = 2.8127  \n",
      "\n",
      "Fold: 3  Epoch: 27  Training loss = 1.5377  Validation loss = 2.8115  \n",
      "\n",
      "Fold: 3  Epoch: 28  Training loss = 1.5371  Validation loss = 2.8109  \n",
      "\n",
      "Fold: 3  Epoch: 29  Training loss = 1.5364  Validation loss = 2.8108  \n",
      "\n",
      "Fold: 3  Epoch: 30  Training loss = 1.5356  Validation loss = 2.8151  \n",
      "\n",
      "Fold: 3  Epoch: 31  Training loss = 1.5348  Validation loss = 2.8178  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 5  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.5936  Validation loss = 4.0447  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.5923  Validation loss = 4.0393  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.5914  Validation loss = 4.0375  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.5910  Validation loss = 4.0388  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.5901  Validation loss = 4.0367  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.5892  Validation loss = 4.0323  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.5881  Validation loss = 4.0282  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.5870  Validation loss = 4.0277  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.5861  Validation loss = 4.0237  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 1.5849  Validation loss = 4.0224  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.5842  Validation loss = 4.0204  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 1.5837  Validation loss = 4.0182  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.5827  Validation loss = 4.0162  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 1.5815  Validation loss = 4.0132  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.5808  Validation loss = 4.0063  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 1.5797  Validation loss = 4.0034  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 1.5790  Validation loss = 4.0017  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 1.5780  Validation loss = 4.0014  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 1.5770  Validation loss = 4.0004  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 1.5760  Validation loss = 4.0018  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 1.5752  Validation loss = 4.0025  \n",
      "\n",
      "Fold: 4  Epoch: 22  Training loss = 1.5749  Validation loss = 4.0022  \n",
      "\n",
      "Fold: 4  Epoch: 23  Training loss = 1.5741  Validation loss = 3.9977  \n",
      "\n",
      "Fold: 4  Epoch: 24  Training loss = 1.5732  Validation loss = 3.9968  \n",
      "\n",
      "Fold: 4  Epoch: 25  Training loss = 1.5724  Validation loss = 3.9954  \n",
      "\n",
      "Fold: 4  Epoch: 26  Training loss = 1.5720  Validation loss = 3.9924  \n",
      "\n",
      "Fold: 4  Epoch: 27  Training loss = 1.5713  Validation loss = 3.9882  \n",
      "\n",
      "Fold: 4  Epoch: 28  Training loss = 1.5705  Validation loss = 3.9891  \n",
      "\n",
      "Fold: 4  Epoch: 29  Training loss = 1.5701  Validation loss = 3.9913  \n",
      "\n",
      "Fold: 4  Epoch: 30  Training loss = 1.5695  Validation loss = 3.9927  \n",
      "\n",
      "Fold: 4  Epoch: 31  Training loss = 1.5691  Validation loss = 3.9922  \n",
      "\n",
      "Fold: 4  Epoch: 32  Training loss = 1.5682  Validation loss = 3.9878  \n",
      "\n",
      "Fold: 4  Epoch: 33  Training loss = 1.5673  Validation loss = 3.9868  \n",
      "\n",
      "Fold: 4  Epoch: 34  Training loss = 1.5667  Validation loss = 3.9872  \n",
      "\n",
      "Fold: 4  Epoch: 35  Training loss = 1.5662  Validation loss = 3.9880  \n",
      "\n",
      "Fold: 4  Epoch: 36  Training loss = 1.5648  Validation loss = 3.9832  \n",
      "\n",
      "Fold: 4  Epoch: 37  Training loss = 1.5642  Validation loss = 3.9831  \n",
      "\n",
      "Fold: 4  Epoch: 38  Training loss = 1.5639  Validation loss = 3.9823  \n",
      "\n",
      "Fold: 4  Epoch: 39  Training loss = 1.5632  Validation loss = 3.9778  \n",
      "\n",
      "Fold: 4  Epoch: 40  Training loss = 1.5624  Validation loss = 3.9738  \n",
      "\n",
      "Fold: 4  Epoch: 41  Training loss = 1.5620  Validation loss = 3.9741  \n",
      "\n",
      "Fold: 4  Epoch: 42  Training loss = 1.5615  Validation loss = 3.9716  \n",
      "\n",
      "Fold: 4  Epoch: 43  Training loss = 1.5604  Validation loss = 3.9665  \n",
      "\n",
      "Fold: 4  Epoch: 44  Training loss = 1.5598  Validation loss = 3.9710  \n",
      "\n",
      "Fold: 4  Epoch: 45  Training loss = 1.5592  Validation loss = 3.9714  \n",
      "\n",
      "Fold: 4  Epoch: 46  Training loss = 1.5585  Validation loss = 3.9676  \n",
      "\n",
      "Fold: 4  Epoch: 47  Training loss = 1.5582  Validation loss = 3.9663  \n",
      "\n",
      "Fold: 4  Epoch: 48  Training loss = 1.5578  Validation loss = 3.9670  \n",
      "\n",
      "Fold: 4  Epoch: 49  Training loss = 1.5571  Validation loss = 3.9641  \n",
      "\n",
      "Fold: 4  Epoch: 50  Training loss = 1.5565  Validation loss = 3.9651  \n",
      "\n",
      "Fold: 4  Epoch: 51  Training loss = 1.5555  Validation loss = 3.9606  \n",
      "\n",
      "Fold: 4  Epoch: 52  Training loss = 1.5551  Validation loss = 3.9609  \n",
      "\n",
      "Fold: 4  Epoch: 53  Training loss = 1.5546  Validation loss = 3.9648  \n",
      "\n",
      "Fold: 4  Epoch: 54  Training loss = 1.5536  Validation loss = 3.9628  \n",
      "\n",
      "Fold: 4  Epoch: 55  Training loss = 1.5530  Validation loss = 3.9627  \n",
      "\n",
      "Fold: 4  Epoch: 56  Training loss = 1.5526  Validation loss = 3.9645  \n",
      "\n",
      "Fold: 4  Epoch: 57  Training loss = 1.5521  Validation loss = 3.9655  \n",
      "\n",
      "Fold: 4  Epoch: 58  Training loss = 1.5518  Validation loss = 3.9657  \n",
      "\n",
      "Fold: 4  Epoch: 59  Training loss = 1.5509  Validation loss = 3.9622  \n",
      "\n",
      "Fold: 4  Epoch: 60  Training loss = 1.5502  Validation loss = 3.9625  \n",
      "\n",
      "Fold: 4  Epoch: 61  Training loss = 1.5496  Validation loss = 3.9614  \n",
      "\n",
      "Fold: 4  Epoch: 62  Training loss = 1.5490  Validation loss = 3.9603  \n",
      "\n",
      "Fold: 4  Epoch: 63  Training loss = 1.5484  Validation loss = 3.9593  \n",
      "\n",
      "Fold: 4  Epoch: 64  Training loss = 1.5477  Validation loss = 3.9585  \n",
      "\n",
      "Fold: 4  Epoch: 65  Training loss = 1.5471  Validation loss = 3.9554  \n",
      "\n",
      "Fold: 4  Epoch: 66  Training loss = 1.5467  Validation loss = 3.9546  \n",
      "\n",
      "Fold: 4  Epoch: 67  Training loss = 1.5463  Validation loss = 3.9539  \n",
      "\n",
      "Fold: 4  Epoch: 68  Training loss = 1.5456  Validation loss = 3.9533  \n",
      "\n",
      "Fold: 4  Epoch: 69  Training loss = 1.5452  Validation loss = 3.9535  \n",
      "\n",
      "Fold: 4  Epoch: 70  Training loss = 1.5448  Validation loss = 3.9559  \n",
      "\n",
      "Fold: 4  Epoch: 71  Training loss = 1.5439  Validation loss = 3.9558  \n",
      "\n",
      "Fold: 4  Epoch: 72  Training loss = 1.5434  Validation loss = 3.9542  \n",
      "\n",
      "Fold: 4  Epoch: 73  Training loss = 1.5428  Validation loss = 3.9495  \n",
      "\n",
      "Fold: 4  Epoch: 74  Training loss = 1.5425  Validation loss = 3.9490  \n",
      "\n",
      "Fold: 4  Epoch: 75  Training loss = 1.5419  Validation loss = 3.9464  \n",
      "\n",
      "Fold: 4  Epoch: 76  Training loss = 1.5413  Validation loss = 3.9405  \n",
      "\n",
      "Fold: 4  Epoch: 77  Training loss = 1.5406  Validation loss = 3.9414  \n",
      "\n",
      "Fold: 4  Epoch: 78  Training loss = 1.5401  Validation loss = 3.9437  \n",
      "\n",
      "Fold: 4  Epoch: 79  Training loss = 1.5395  Validation loss = 3.9456  \n",
      "\n",
      "Fold: 4  Epoch: 80  Training loss = 1.5390  Validation loss = 3.9477  \n",
      "\n",
      "Fold: 4  Epoch: 81  Training loss = 1.5381  Validation loss = 3.9477  \n",
      "\n",
      "Fold: 4  Epoch: 82  Training loss = 1.5375  Validation loss = 3.9450  \n",
      "\n",
      "Fold: 4  Epoch: 83  Training loss = 1.5374  Validation loss = 3.9425  \n",
      "\n",
      "Fold: 4  Epoch: 84  Training loss = 1.5369  Validation loss = 3.9433  \n",
      "\n",
      "Fold: 4  Epoch: 85  Training loss = 1.5362  Validation loss = 3.9414  \n",
      "\n",
      "Fold: 4  Epoch: 86  Training loss = 1.5356  Validation loss = 3.9408  \n",
      "\n",
      "Fold: 4  Epoch: 87  Training loss = 1.5349  Validation loss = 3.9408  \n",
      "\n",
      "Fold: 4  Epoch: 88  Training loss = 1.5346  Validation loss = 3.9414  \n",
      "\n",
      "Fold: 4  Epoch: 89  Training loss = 1.5332  Validation loss = 3.9401  \n",
      "\n",
      "Fold: 4  Epoch: 90  Training loss = 1.5328  Validation loss = 3.9384  \n",
      "\n",
      "Fold: 4  Epoch: 91  Training loss = 1.5323  Validation loss = 3.9376  \n",
      "\n",
      "Fold: 4  Epoch: 92  Training loss = 1.5315  Validation loss = 3.9363  \n",
      "\n",
      "Fold: 4  Epoch: 93  Training loss = 1.5311  Validation loss = 3.9351  \n",
      "\n",
      "Fold: 4  Epoch: 94  Training loss = 1.5303  Validation loss = 3.9279  \n",
      "\n",
      "Fold: 4  Epoch: 95  Training loss = 1.5298  Validation loss = 3.9254  \n",
      "\n",
      "Fold: 4  Epoch: 96  Training loss = 1.5292  Validation loss = 3.9288  \n",
      "\n",
      "Fold: 4  Epoch: 97  Training loss = 1.5289  Validation loss = 3.9310  \n",
      "\n",
      "Fold: 4  Epoch: 98  Training loss = 1.5281  Validation loss = 3.9282  \n",
      "\n",
      "Fold: 4  Epoch: 99  Training loss = 1.5272  Validation loss = 3.9272  \n",
      "\n",
      "Fold: 4  Epoch: 100  Training loss = 1.5265  Validation loss = 3.9278  \n",
      "\n",
      "Fold: 4  Epoch: 101  Training loss = 1.5259  Validation loss = 3.9275  \n",
      "\n",
      "Fold: 4  Epoch: 102  Training loss = 1.5252  Validation loss = 3.9301  \n",
      "\n",
      "Fold: 4  Epoch: 103  Training loss = 1.5244  Validation loss = 3.9299  \n",
      "\n",
      "Fold: 4  Epoch: 104  Training loss = 1.5238  Validation loss = 3.9229  \n",
      "\n",
      "Fold: 4  Epoch: 105  Training loss = 1.5231  Validation loss = 3.9232  \n",
      "\n",
      "Fold: 4  Epoch: 106  Training loss = 1.5225  Validation loss = 3.9227  \n",
      "\n",
      "Fold: 4  Epoch: 107  Training loss = 1.5218  Validation loss = 3.9255  \n",
      "\n",
      "Fold: 4  Epoch: 108  Training loss = 1.5210  Validation loss = 3.9243  \n",
      "\n",
      "Fold: 4  Epoch: 109  Training loss = 1.5204  Validation loss = 3.9226  \n",
      "\n",
      "Fold: 4  Epoch: 110  Training loss = 1.5196  Validation loss = 3.9271  \n",
      "\n",
      "Fold: 4  Epoch: 111  Training loss = 1.5192  Validation loss = 3.9272  \n",
      "\n",
      "Fold: 4  Epoch: 112  Training loss = 1.5186  Validation loss = 3.9237  \n",
      "\n",
      "Fold: 4  Epoch: 113  Training loss = 1.5178  Validation loss = 3.9173  \n",
      "\n",
      "Fold: 4  Epoch: 114  Training loss = 1.5169  Validation loss = 3.9141  \n",
      "\n",
      "Fold: 4  Epoch: 115  Training loss = 1.5161  Validation loss = 3.9116  \n",
      "\n",
      "Fold: 4  Epoch: 116  Training loss = 1.5154  Validation loss = 3.9121  \n",
      "\n",
      "Fold: 4  Epoch: 117  Training loss = 1.5147  Validation loss = 3.9114  \n",
      "\n",
      "Fold: 4  Epoch: 118  Training loss = 1.5141  Validation loss = 3.9095  \n",
      "\n",
      "Fold: 4  Epoch: 119  Training loss = 1.5135  Validation loss = 3.9071  \n",
      "\n",
      "Fold: 4  Epoch: 120  Training loss = 1.5129  Validation loss = 3.9078  \n",
      "\n",
      "Fold: 4  Epoch: 121  Training loss = 1.5124  Validation loss = 3.9065  \n",
      "\n",
      "Fold: 4  Epoch: 122  Training loss = 1.5118  Validation loss = 3.9085  \n",
      "\n",
      "Fold: 4  Epoch: 123  Training loss = 1.5112  Validation loss = 3.9082  \n",
      "\n",
      "Fold: 4  Epoch: 124  Training loss = 1.5103  Validation loss = 3.9018  \n",
      "\n",
      "Fold: 4  Epoch: 125  Training loss = 1.5098  Validation loss = 3.9008  \n",
      "\n",
      "Fold: 4  Epoch: 126  Training loss = 1.5092  Validation loss = 3.8981  \n",
      "\n",
      "Fold: 4  Epoch: 127  Training loss = 1.5084  Validation loss = 3.8953  \n",
      "\n",
      "Fold: 4  Epoch: 128  Training loss = 1.5080  Validation loss = 3.8987  \n",
      "\n",
      "Fold: 4  Epoch: 129  Training loss = 1.5075  Validation loss = 3.8979  \n",
      "\n",
      "Fold: 4  Epoch: 130  Training loss = 1.5068  Validation loss = 3.8900  \n",
      "\n",
      "Fold: 4  Epoch: 131  Training loss = 1.5065  Validation loss = 3.8868  \n",
      "\n",
      "Fold: 4  Epoch: 132  Training loss = 1.5058  Validation loss = 3.8861  \n",
      "\n",
      "Fold: 4  Epoch: 133  Training loss = 1.5056  Validation loss = 3.8825  \n",
      "\n",
      "Fold: 4  Epoch: 134  Training loss = 1.5052  Validation loss = 3.8838  \n",
      "\n",
      "Fold: 4  Epoch: 135  Training loss = 1.5046  Validation loss = 3.8802  \n",
      "\n",
      "Fold: 4  Epoch: 136  Training loss = 1.5037  Validation loss = 3.8793  \n",
      "\n",
      "Fold: 4  Epoch: 137  Training loss = 1.5029  Validation loss = 3.8785  \n",
      "\n",
      "Fold: 4  Epoch: 138  Training loss = 1.5021  Validation loss = 3.8786  \n",
      "\n",
      "Fold: 4  Epoch: 139  Training loss = 1.5018  Validation loss = 3.8736  \n",
      "\n",
      "Fold: 4  Epoch: 140  Training loss = 1.5011  Validation loss = 3.8751  \n",
      "\n",
      "Fold: 4  Epoch: 141  Training loss = 1.5001  Validation loss = 3.8766  \n",
      "\n",
      "Fold: 4  Epoch: 142  Training loss = 1.4996  Validation loss = 3.8774  \n",
      "\n",
      "Fold: 4  Epoch: 143  Training loss = 1.4989  Validation loss = 3.8772  \n",
      "\n",
      "Fold: 4  Epoch: 144  Training loss = 1.4980  Validation loss = 3.8789  \n",
      "\n",
      "Fold: 4  Epoch: 145  Training loss = 1.4975  Validation loss = 3.8783  \n",
      "\n",
      "Fold: 4  Epoch: 146  Training loss = 1.4969  Validation loss = 3.8773  \n",
      "\n",
      "Fold: 4  Epoch: 147  Training loss = 1.4964  Validation loss = 3.8760  \n",
      "\n",
      "Fold: 4  Epoch: 148  Training loss = 1.4958  Validation loss = 3.8794  \n",
      "\n",
      "Fold: 4  Epoch: 149  Training loss = 1.4954  Validation loss = 3.8782  \n",
      "\n",
      "Fold: 4  Epoch: 150  Training loss = 1.4948  Validation loss = 3.8708  \n",
      "\n",
      "Fold: 4  Epoch: 151  Training loss = 1.4942  Validation loss = 3.8641  \n",
      "\n",
      "Fold: 4  Epoch: 152  Training loss = 1.4934  Validation loss = 3.8647  \n",
      "\n",
      "Fold: 4  Epoch: 153  Training loss = 1.4926  Validation loss = 3.8614  \n",
      "\n",
      "Fold: 4  Epoch: 154  Training loss = 1.4919  Validation loss = 3.8610  \n",
      "\n",
      "Fold: 4  Epoch: 155  Training loss = 1.4915  Validation loss = 3.8599  \n",
      "\n",
      "Fold: 4  Epoch: 156  Training loss = 1.4907  Validation loss = 3.8529  \n",
      "\n",
      "Fold: 4  Epoch: 157  Training loss = 1.4900  Validation loss = 3.8494  \n",
      "\n",
      "Fold: 4  Epoch: 158  Training loss = 1.4894  Validation loss = 3.8501  \n",
      "\n",
      "Fold: 4  Epoch: 159  Training loss = 1.4889  Validation loss = 3.8488  \n",
      "\n",
      "Fold: 4  Epoch: 160  Training loss = 1.4880  Validation loss = 3.8430  \n",
      "\n",
      "Fold: 4  Epoch: 161  Training loss = 1.4874  Validation loss = 3.8441  \n",
      "\n",
      "Fold: 4  Epoch: 162  Training loss = 1.4867  Validation loss = 3.8444  \n",
      "\n",
      "Fold: 4  Epoch: 163  Training loss = 1.4858  Validation loss = 3.8375  \n",
      "\n",
      "Fold: 4  Epoch: 164  Training loss = 1.4851  Validation loss = 3.8301  \n",
      "\n",
      "Fold: 4  Epoch: 165  Training loss = 1.4845  Validation loss = 3.8289  \n",
      "\n",
      "Fold: 4  Epoch: 166  Training loss = 1.4839  Validation loss = 3.8291  \n",
      "\n",
      "Fold: 4  Epoch: 167  Training loss = 1.4832  Validation loss = 3.8274  \n",
      "\n",
      "Fold: 4  Epoch: 168  Training loss = 1.4824  Validation loss = 3.8233  \n",
      "\n",
      "Fold: 4  Epoch: 169  Training loss = 1.4817  Validation loss = 3.8205  \n",
      "\n",
      "Fold: 4  Epoch: 170  Training loss = 1.4809  Validation loss = 3.8215  \n",
      "\n",
      "Fold: 4  Epoch: 171  Training loss = 1.4804  Validation loss = 3.8211  \n",
      "\n",
      "Fold: 4  Epoch: 172  Training loss = 1.4795  Validation loss = 3.8161  \n",
      "\n",
      "Fold: 4  Epoch: 173  Training loss = 1.4789  Validation loss = 3.8137  \n",
      "\n",
      "Fold: 4  Epoch: 174  Training loss = 1.4783  Validation loss = 3.8111  \n",
      "\n",
      "Fold: 4  Epoch: 175  Training loss = 1.4780  Validation loss = 3.8098  \n",
      "\n",
      "Fold: 4  Epoch: 176  Training loss = 1.4775  Validation loss = 3.8071  \n",
      "\n",
      "Fold: 4  Epoch: 177  Training loss = 1.4769  Validation loss = 3.8052  \n",
      "\n",
      "Fold: 4  Epoch: 178  Training loss = 1.4764  Validation loss = 3.8037  \n",
      "\n",
      "Fold: 4  Epoch: 179  Training loss = 1.4753  Validation loss = 3.8025  \n",
      "\n",
      "Fold: 4  Epoch: 180  Training loss = 1.4750  Validation loss = 3.8005  \n",
      "\n",
      "Fold: 4  Epoch: 181  Training loss = 1.4743  Validation loss = 3.7960  \n",
      "\n",
      "Fold: 4  Epoch: 182  Training loss = 1.4739  Validation loss = 3.7954  \n",
      "\n",
      "Fold: 4  Epoch: 183  Training loss = 1.4732  Validation loss = 3.7940  \n",
      "\n",
      "Fold: 4  Epoch: 184  Training loss = 1.4724  Validation loss = 3.7878  \n",
      "\n",
      "Fold: 4  Epoch: 185  Training loss = 1.4719  Validation loss = 3.7888  \n",
      "\n",
      "Fold: 4  Epoch: 186  Training loss = 1.4713  Validation loss = 3.7901  \n",
      "\n",
      "Fold: 4  Epoch: 187  Training loss = 1.4708  Validation loss = 3.7890  \n",
      "\n",
      "Fold: 4  Epoch: 188  Training loss = 1.4703  Validation loss = 3.7879  \n",
      "\n",
      "Fold: 4  Epoch: 189  Training loss = 1.4698  Validation loss = 3.7902  \n",
      "\n",
      "Fold: 4  Epoch: 190  Training loss = 1.4691  Validation loss = 3.7866  \n",
      "\n",
      "Fold: 4  Epoch: 191  Training loss = 1.4684  Validation loss = 3.7850  \n",
      "\n",
      "Fold: 4  Epoch: 192  Training loss = 1.4677  Validation loss = 3.7841  \n",
      "\n",
      "Fold: 4  Epoch: 193  Training loss = 1.4673  Validation loss = 3.7838  \n",
      "\n",
      "Fold: 4  Epoch: 194  Training loss = 1.4669  Validation loss = 3.7805  \n",
      "\n",
      "Fold: 4  Epoch: 195  Training loss = 1.4664  Validation loss = 3.7774  \n",
      "\n",
      "Fold: 4  Epoch: 196  Training loss = 1.4658  Validation loss = 3.7761  \n",
      "\n",
      "Fold: 4  Epoch: 197  Training loss = 1.4654  Validation loss = 3.7739  \n",
      "\n",
      "Fold: 4  Epoch: 198  Training loss = 1.4649  Validation loss = 3.7747  \n",
      "\n",
      "Fold: 4  Epoch: 199  Training loss = 1.4641  Validation loss = 3.7698  \n",
      "\n",
      "Fold: 4  Epoch: 200  Training loss = 1.4635  Validation loss = 3.7676  \n",
      "\n",
      "Fold: 4  Epoch: 201  Training loss = 1.4628  Validation loss = 3.7629  \n",
      "\n",
      "Fold: 4  Epoch: 202  Training loss = 1.4620  Validation loss = 3.7614  \n",
      "\n",
      "Fold: 4  Epoch: 203  Training loss = 1.4614  Validation loss = 3.7590  \n",
      "\n",
      "Fold: 4  Epoch: 204  Training loss = 1.4608  Validation loss = 3.7554  \n",
      "\n",
      "Fold: 4  Epoch: 205  Training loss = 1.4603  Validation loss = 3.7544  \n",
      "\n",
      "Fold: 4  Epoch: 206  Training loss = 1.4596  Validation loss = 3.7536  \n",
      "\n",
      "Fold: 4  Epoch: 207  Training loss = 1.4590  Validation loss = 3.7530  \n",
      "\n",
      "Fold: 4  Epoch: 208  Training loss = 1.4589  Validation loss = 3.7532  \n",
      "\n",
      "Fold: 4  Epoch: 209  Training loss = 1.4582  Validation loss = 3.7519  \n",
      "\n",
      "Fold: 4  Epoch: 210  Training loss = 1.4579  Validation loss = 3.7567  \n",
      "\n",
      "Fold: 4  Epoch: 211  Training loss = 1.4576  Validation loss = 3.7574  \n",
      "\n",
      "Fold: 4  Epoch: 212  Training loss = 1.4571  Validation loss = 3.7552  \n",
      "\n",
      "Fold: 4  Epoch: 213  Training loss = 1.4563  Validation loss = 3.7525  \n",
      "\n",
      "Fold: 4  Epoch: 214  Training loss = 1.4555  Validation loss = 3.7486  \n",
      "\n",
      "Fold: 4  Epoch: 215  Training loss = 1.4546  Validation loss = 3.7441  \n",
      "\n",
      "Fold: 4  Epoch: 216  Training loss = 1.4538  Validation loss = 3.7370  \n",
      "\n",
      "Fold: 4  Epoch: 217  Training loss = 1.4531  Validation loss = 3.7354  \n",
      "\n",
      "Fold: 4  Epoch: 218  Training loss = 1.4528  Validation loss = 3.7372  \n",
      "\n",
      "Fold: 4  Epoch: 219  Training loss = 1.4522  Validation loss = 3.7356  \n",
      "\n",
      "Fold: 4  Epoch: 220  Training loss = 1.4513  Validation loss = 3.7283  \n",
      "\n",
      "Fold: 4  Epoch: 221  Training loss = 1.4507  Validation loss = 3.7292  \n",
      "\n",
      "Fold: 4  Epoch: 222  Training loss = 1.4504  Validation loss = 3.7300  \n",
      "\n",
      "Fold: 4  Epoch: 223  Training loss = 1.4499  Validation loss = 3.7274  \n",
      "\n",
      "Fold: 4  Epoch: 224  Training loss = 1.4494  Validation loss = 3.7255  \n",
      "\n",
      "Fold: 4  Epoch: 225  Training loss = 1.4487  Validation loss = 3.7220  \n",
      "\n",
      "Fold: 4  Epoch: 226  Training loss = 1.4484  Validation loss = 3.7216  \n",
      "\n",
      "Fold: 4  Epoch: 227  Training loss = 1.4481  Validation loss = 3.7222  \n",
      "\n",
      "Fold: 4  Epoch: 228  Training loss = 1.4473  Validation loss = 3.7160  \n",
      "\n",
      "Fold: 4  Epoch: 229  Training loss = 1.4469  Validation loss = 3.7148  \n",
      "\n",
      "Fold: 4  Epoch: 230  Training loss = 1.4467  Validation loss = 3.7137  \n",
      "\n",
      "Fold: 4  Epoch: 231  Training loss = 1.4462  Validation loss = 3.7122  \n",
      "\n",
      "Fold: 4  Epoch: 232  Training loss = 1.4458  Validation loss = 3.7096  \n",
      "\n",
      "Fold: 4  Epoch: 233  Training loss = 1.4452  Validation loss = 3.7068  \n",
      "\n",
      "Fold: 4  Epoch: 234  Training loss = 1.4448  Validation loss = 3.7075  \n",
      "\n",
      "Fold: 4  Epoch: 235  Training loss = 1.4442  Validation loss = 3.7034  \n",
      "\n",
      "Fold: 4  Epoch: 236  Training loss = 1.4438  Validation loss = 3.7026  \n",
      "\n",
      "Fold: 4  Epoch: 237  Training loss = 1.4434  Validation loss = 3.7033  \n",
      "\n",
      "Fold: 4  Epoch: 238  Training loss = 1.4429  Validation loss = 3.7006  \n",
      "\n",
      "Fold: 4  Epoch: 239  Training loss = 1.4424  Validation loss = 3.6985  \n",
      "\n",
      "Fold: 4  Epoch: 240  Training loss = 1.4416  Validation loss = 3.6966  \n",
      "\n",
      "Fold: 4  Epoch: 241  Training loss = 1.4408  Validation loss = 3.6925  \n",
      "\n",
      "Fold: 4  Epoch: 242  Training loss = 1.4401  Validation loss = 3.6905  \n",
      "\n",
      "Fold: 4  Epoch: 243  Training loss = 1.4392  Validation loss = 3.6857  \n",
      "\n",
      "Fold: 4  Epoch: 244  Training loss = 1.4388  Validation loss = 3.6847  \n",
      "\n",
      "Fold: 4  Epoch: 245  Training loss = 1.4386  Validation loss = 3.6864  \n",
      "\n",
      "Fold: 4  Epoch: 246  Training loss = 1.4378  Validation loss = 3.6838  \n",
      "\n",
      "Fold: 4  Epoch: 247  Training loss = 1.4372  Validation loss = 3.6813  \n",
      "\n",
      "Fold: 4  Epoch: 248  Training loss = 1.4363  Validation loss = 3.6779  \n",
      "\n",
      "Fold: 4  Epoch: 249  Training loss = 1.4358  Validation loss = 3.6758  \n",
      "\n",
      "Fold: 4  Epoch: 250  Training loss = 1.4353  Validation loss = 3.6724  \n",
      "\n",
      "Fold: 4  Epoch: 251  Training loss = 1.4350  Validation loss = 3.6725  \n",
      "\n",
      "Fold: 4  Epoch: 252  Training loss = 1.4342  Validation loss = 3.6704  \n",
      "\n",
      "Fold: 4  Epoch: 253  Training loss = 1.4338  Validation loss = 3.6709  \n",
      "\n",
      "Fold: 4  Epoch: 254  Training loss = 1.4333  Validation loss = 3.6691  \n",
      "\n",
      "Fold: 4  Epoch: 255  Training loss = 1.4323  Validation loss = 3.6632  \n",
      "\n",
      "Fold: 4  Epoch: 256  Training loss = 1.4320  Validation loss = 3.6630  \n",
      "\n",
      "Fold: 4  Epoch: 257  Training loss = 1.4311  Validation loss = 3.6580  \n",
      "\n",
      "Fold: 4  Epoch: 258  Training loss = 1.4311  Validation loss = 3.6621  \n",
      "\n",
      "Fold: 4  Epoch: 259  Training loss = 1.4303  Validation loss = 3.6550  \n",
      "\n",
      "Fold: 4  Epoch: 260  Training loss = 1.4298  Validation loss = 3.6534  \n",
      "\n",
      "Fold: 4  Epoch: 261  Training loss = 1.4294  Validation loss = 3.6537  \n",
      "\n",
      "Fold: 4  Epoch: 262  Training loss = 1.4286  Validation loss = 3.6490  \n",
      "\n",
      "Fold: 4  Epoch: 263  Training loss = 1.4282  Validation loss = 3.6486  \n",
      "\n",
      "Fold: 4  Epoch: 264  Training loss = 1.4277  Validation loss = 3.6484  \n",
      "\n",
      "Fold: 4  Epoch: 265  Training loss = 1.4270  Validation loss = 3.6436  \n",
      "\n",
      "Fold: 4  Epoch: 266  Training loss = 1.4262  Validation loss = 3.6394  \n",
      "\n",
      "Fold: 4  Epoch: 267  Training loss = 1.4257  Validation loss = 3.6384  \n",
      "\n",
      "Fold: 4  Epoch: 268  Training loss = 1.4255  Validation loss = 3.6355  \n",
      "\n",
      "Fold: 4  Epoch: 269  Training loss = 1.4252  Validation loss = 3.6375  \n",
      "\n",
      "Fold: 4  Epoch: 270  Training loss = 1.4248  Validation loss = 3.6351  \n",
      "\n",
      "Fold: 4  Epoch: 271  Training loss = 1.4245  Validation loss = 3.6327  \n",
      "\n",
      "Fold: 4  Epoch: 272  Training loss = 1.4237  Validation loss = 3.6283  \n",
      "\n",
      "Fold: 4  Epoch: 273  Training loss = 1.4234  Validation loss = 3.6266  \n",
      "\n",
      "Fold: 4  Epoch: 274  Training loss = 1.4229  Validation loss = 3.6230  \n",
      "\n",
      "Fold: 4  Epoch: 275  Training loss = 1.4224  Validation loss = 3.6205  \n",
      "\n",
      "Fold: 4  Epoch: 276  Training loss = 1.4220  Validation loss = 3.6185  \n",
      "\n",
      "Fold: 4  Epoch: 277  Training loss = 1.4214  Validation loss = 3.6128  \n",
      "\n",
      "Fold: 4  Epoch: 278  Training loss = 1.4208  Validation loss = 3.6070  \n",
      "\n",
      "Fold: 4  Epoch: 279  Training loss = 1.4205  Validation loss = 3.6035  \n",
      "\n",
      "Fold: 4  Epoch: 280  Training loss = 1.4203  Validation loss = 3.6045  \n",
      "\n",
      "Fold: 4  Epoch: 281  Training loss = 1.4195  Validation loss = 3.6039  \n",
      "\n",
      "Fold: 4  Epoch: 282  Training loss = 1.4193  Validation loss = 3.5996  \n",
      "\n",
      "Fold: 4  Epoch: 283  Training loss = 1.4187  Validation loss = 3.6001  \n",
      "\n",
      "Fold: 4  Epoch: 284  Training loss = 1.4185  Validation loss = 3.5979  \n",
      "\n",
      "Fold: 4  Epoch: 285  Training loss = 1.4177  Validation loss = 3.5955  \n",
      "\n",
      "Fold: 4  Epoch: 286  Training loss = 1.4174  Validation loss = 3.5961  \n",
      "\n",
      "Fold: 4  Epoch: 287  Training loss = 1.4168  Validation loss = 3.5937  \n",
      "\n",
      "Fold: 4  Epoch: 288  Training loss = 1.4161  Validation loss = 3.5908  \n",
      "\n",
      "Fold: 4  Epoch: 289  Training loss = 1.4156  Validation loss = 3.5905  \n",
      "\n",
      "Fold: 4  Epoch: 290  Training loss = 1.4153  Validation loss = 3.5886  \n",
      "\n",
      "Fold: 4  Epoch: 291  Training loss = 1.4149  Validation loss = 3.5866  \n",
      "\n",
      "Fold: 4  Epoch: 292  Training loss = 1.4141  Validation loss = 3.5861  \n",
      "\n",
      "Fold: 4  Epoch: 293  Training loss = 1.4135  Validation loss = 3.5867  \n",
      "\n",
      "Fold: 4  Epoch: 294  Training loss = 1.4130  Validation loss = 3.5858  \n",
      "\n",
      "Fold: 4  Epoch: 295  Training loss = 1.4127  Validation loss = 3.5845  \n",
      "\n",
      "Fold: 4  Epoch: 296  Training loss = 1.4120  Validation loss = 3.5804  \n",
      "\n",
      "Fold: 4  Epoch: 297  Training loss = 1.4117  Validation loss = 3.5828  \n",
      "\n",
      "Fold: 4  Epoch: 298  Training loss = 1.4111  Validation loss = 3.5805  \n",
      "\n",
      "Fold: 4  Epoch: 299  Training loss = 1.4106  Validation loss = 3.5801  \n",
      "\n",
      "Fold: 4  Epoch: 300  Training loss = 1.4103  Validation loss = 3.5792  \n",
      "\n",
      "Fold: 4  Epoch: 301  Training loss = 1.4099  Validation loss = 3.5800  \n",
      "\n",
      "Fold: 4  Epoch: 302  Training loss = 1.4091  Validation loss = 3.5743  \n",
      "\n",
      "Fold: 4  Epoch: 303  Training loss = 1.4081  Validation loss = 3.5690  \n",
      "\n",
      "Fold: 4  Epoch: 304  Training loss = 1.4074  Validation loss = 3.5655  \n",
      "\n",
      "Fold: 4  Epoch: 305  Training loss = 1.4070  Validation loss = 3.5637  \n",
      "\n",
      "Fold: 4  Epoch: 306  Training loss = 1.4063  Validation loss = 3.5592  \n",
      "\n",
      "Fold: 4  Epoch: 307  Training loss = 1.4058  Validation loss = 3.5601  \n",
      "\n",
      "Fold: 4  Epoch: 308  Training loss = 1.4056  Validation loss = 3.5616  \n",
      "\n",
      "Fold: 4  Epoch: 309  Training loss = 1.4049  Validation loss = 3.5579  \n",
      "\n",
      "Fold: 4  Epoch: 310  Training loss = 1.4043  Validation loss = 3.5554  \n",
      "\n",
      "Fold: 4  Epoch: 311  Training loss = 1.4038  Validation loss = 3.5513  \n",
      "\n",
      "Fold: 4  Epoch: 312  Training loss = 1.4031  Validation loss = 3.5485  \n",
      "\n",
      "Fold: 4  Epoch: 313  Training loss = 1.4023  Validation loss = 3.5423  \n",
      "\n",
      "Fold: 4  Epoch: 314  Training loss = 1.4017  Validation loss = 3.5400  \n",
      "\n",
      "Fold: 4  Epoch: 315  Training loss = 1.4010  Validation loss = 3.5370  \n",
      "\n",
      "Fold: 4  Epoch: 316  Training loss = 1.4003  Validation loss = 3.5389  \n",
      "\n",
      "Fold: 4  Epoch: 317  Training loss = 1.3996  Validation loss = 3.5362  \n",
      "\n",
      "Fold: 4  Epoch: 318  Training loss = 1.3993  Validation loss = 3.5327  \n",
      "\n",
      "Fold: 4  Epoch: 319  Training loss = 1.3993  Validation loss = 3.5290  \n",
      "\n",
      "Fold: 4  Epoch: 320  Training loss = 1.3985  Validation loss = 3.5224  \n",
      "\n",
      "Fold: 4  Epoch: 321  Training loss = 1.3977  Validation loss = 3.5192  \n",
      "\n",
      "Fold: 4  Epoch: 322  Training loss = 1.3973  Validation loss = 3.5193  \n",
      "\n",
      "Fold: 4  Epoch: 323  Training loss = 1.3968  Validation loss = 3.5195  \n",
      "\n",
      "Fold: 4  Epoch: 324  Training loss = 1.3962  Validation loss = 3.5163  \n",
      "\n",
      "Fold: 4  Epoch: 325  Training loss = 1.3958  Validation loss = 3.5165  \n",
      "\n",
      "Fold: 4  Epoch: 326  Training loss = 1.3954  Validation loss = 3.5156  \n",
      "\n",
      "Fold: 4  Epoch: 327  Training loss = 1.3950  Validation loss = 3.5162  \n",
      "\n",
      "Fold: 4  Epoch: 328  Training loss = 1.3943  Validation loss = 3.5160  \n",
      "\n",
      "Fold: 4  Epoch: 329  Training loss = 1.3936  Validation loss = 3.5131  \n",
      "\n",
      "Fold: 4  Epoch: 330  Training loss = 1.3930  Validation loss = 3.5129  \n",
      "\n",
      "Fold: 4  Epoch: 331  Training loss = 1.3925  Validation loss = 3.5109  \n",
      "\n",
      "Fold: 4  Epoch: 332  Training loss = 1.3922  Validation loss = 3.5097  \n",
      "\n",
      "Fold: 4  Epoch: 333  Training loss = 1.3916  Validation loss = 3.5105  \n",
      "\n",
      "Fold: 4  Epoch: 334  Training loss = 1.3910  Validation loss = 3.5074  \n",
      "\n",
      "Fold: 4  Epoch: 335  Training loss = 1.3905  Validation loss = 3.5055  \n",
      "\n",
      "Fold: 4  Epoch: 336  Training loss = 1.3899  Validation loss = 3.5012  \n",
      "\n",
      "Fold: 4  Epoch: 337  Training loss = 1.3896  Validation loss = 3.5004  \n",
      "\n",
      "Fold: 4  Epoch: 338  Training loss = 1.3891  Validation loss = 3.4998  \n",
      "\n",
      "Fold: 4  Epoch: 339  Training loss = 1.3884  Validation loss = 3.4945  \n",
      "\n",
      "Fold: 4  Epoch: 340  Training loss = 1.3878  Validation loss = 3.4932  \n",
      "\n",
      "Fold: 4  Epoch: 341  Training loss = 1.3874  Validation loss = 3.4913  \n",
      "\n",
      "Fold: 4  Epoch: 342  Training loss = 1.3871  Validation loss = 3.4927  \n",
      "\n",
      "Fold: 4  Epoch: 343  Training loss = 1.3867  Validation loss = 3.4914  \n",
      "\n",
      "Fold: 4  Epoch: 344  Training loss = 1.3860  Validation loss = 3.4863  \n",
      "\n",
      "Fold: 4  Epoch: 345  Training loss = 1.3855  Validation loss = 3.4845  \n",
      "\n",
      "Fold: 4  Epoch: 346  Training loss = 1.3848  Validation loss = 3.4795  \n",
      "\n",
      "Fold: 4  Epoch: 347  Training loss = 1.3844  Validation loss = 3.4774  \n",
      "\n",
      "Fold: 4  Epoch: 348  Training loss = 1.3836  Validation loss = 3.4743  \n",
      "\n",
      "Fold: 4  Epoch: 349  Training loss = 1.3829  Validation loss = 3.4722  \n",
      "\n",
      "Fold: 4  Epoch: 350  Training loss = 1.3826  Validation loss = 3.4715  \n",
      "\n",
      "Fold: 4  Epoch: 351  Training loss = 1.3822  Validation loss = 3.4681  \n",
      "\n",
      "Fold: 4  Epoch: 352  Training loss = 1.3822  Validation loss = 3.4699  \n",
      "\n",
      "Fold: 4  Epoch: 353  Training loss = 1.3819  Validation loss = 3.4706  \n",
      "\n",
      "Fold: 4  Epoch: 354  Training loss = 1.3805  Validation loss = 3.4611  \n",
      "\n",
      "Fold: 4  Epoch: 355  Training loss = 1.3803  Validation loss = 3.4614  \n",
      "\n",
      "Fold: 4  Epoch: 356  Training loss = 1.3795  Validation loss = 3.4558  \n",
      "\n",
      "Fold: 4  Epoch: 357  Training loss = 1.3791  Validation loss = 3.4558  \n",
      "\n",
      "Fold: 4  Epoch: 358  Training loss = 1.3786  Validation loss = 3.4568  \n",
      "\n",
      "Fold: 4  Epoch: 359  Training loss = 1.3781  Validation loss = 3.4525  \n",
      "\n",
      "Fold: 4  Epoch: 360  Training loss = 1.3779  Validation loss = 3.4508  \n",
      "\n",
      "Fold: 4  Epoch: 361  Training loss = 1.3775  Validation loss = 3.4478  \n",
      "\n",
      "Fold: 4  Epoch: 362  Training loss = 1.3767  Validation loss = 3.4427  \n",
      "\n",
      "Fold: 4  Epoch: 363  Training loss = 1.3764  Validation loss = 3.4444  \n",
      "\n",
      "Fold: 4  Epoch: 364  Training loss = 1.3759  Validation loss = 3.4423  \n",
      "\n",
      "Fold: 4  Epoch: 365  Training loss = 1.3754  Validation loss = 3.4422  \n",
      "\n",
      "Fold: 4  Epoch: 366  Training loss = 1.3749  Validation loss = 3.4388  \n",
      "\n",
      "Fold: 4  Epoch: 367  Training loss = 1.3743  Validation loss = 3.4362  \n",
      "\n",
      "Fold: 4  Epoch: 368  Training loss = 1.3737  Validation loss = 3.4310  \n",
      "\n",
      "Fold: 4  Epoch: 369  Training loss = 1.3734  Validation loss = 3.4287  \n",
      "\n",
      "Fold: 4  Epoch: 370  Training loss = 1.3731  Validation loss = 3.4282  \n",
      "\n",
      "Fold: 4  Epoch: 371  Training loss = 1.3725  Validation loss = 3.4271  \n",
      "\n",
      "Fold: 4  Epoch: 372  Training loss = 1.3720  Validation loss = 3.4246  \n",
      "\n",
      "Fold: 4  Epoch: 373  Training loss = 1.3713  Validation loss = 3.4253  \n",
      "\n",
      "Fold: 4  Epoch: 374  Training loss = 1.3713  Validation loss = 3.4254  \n",
      "\n",
      "Fold: 4  Epoch: 375  Training loss = 1.3705  Validation loss = 3.4234  \n",
      "\n",
      "Fold: 4  Epoch: 376  Training loss = 1.3700  Validation loss = 3.4207  \n",
      "\n",
      "Fold: 4  Epoch: 377  Training loss = 1.3693  Validation loss = 3.4170  \n",
      "\n",
      "Fold: 4  Epoch: 378  Training loss = 1.3691  Validation loss = 3.4168  \n",
      "\n",
      "Fold: 4  Epoch: 379  Training loss = 1.3686  Validation loss = 3.4159  \n",
      "\n",
      "Fold: 4  Epoch: 380  Training loss = 1.3684  Validation loss = 3.4175  \n",
      "\n",
      "Fold: 4  Epoch: 381  Training loss = 1.3678  Validation loss = 3.4161  \n",
      "\n",
      "Fold: 4  Epoch: 382  Training loss = 1.3670  Validation loss = 3.4143  \n",
      "\n",
      "Fold: 4  Epoch: 383  Training loss = 1.3660  Validation loss = 3.4081  \n",
      "\n",
      "Fold: 4  Epoch: 384  Training loss = 1.3653  Validation loss = 3.4050  \n",
      "\n",
      "Fold: 4  Epoch: 385  Training loss = 1.3651  Validation loss = 3.4030  \n",
      "\n",
      "Fold: 4  Epoch: 386  Training loss = 1.3647  Validation loss = 3.3990  \n",
      "\n",
      "Fold: 4  Epoch: 387  Training loss = 1.3642  Validation loss = 3.3972  \n",
      "\n",
      "Fold: 4  Epoch: 388  Training loss = 1.3640  Validation loss = 3.3967  \n",
      "\n",
      "Fold: 4  Epoch: 389  Training loss = 1.3637  Validation loss = 3.3952  \n",
      "\n",
      "Fold: 4  Epoch: 390  Training loss = 1.3631  Validation loss = 3.3930  \n",
      "\n",
      "Fold: 4  Epoch: 391  Training loss = 1.3625  Validation loss = 3.3887  \n",
      "\n",
      "Fold: 4  Epoch: 392  Training loss = 1.3617  Validation loss = 3.3865  \n",
      "\n",
      "Fold: 4  Epoch: 393  Training loss = 1.3614  Validation loss = 3.3885  \n",
      "\n",
      "Fold: 4  Epoch: 394  Training loss = 1.3606  Validation loss = 3.3862  \n",
      "\n",
      "Fold: 4  Epoch: 395  Training loss = 1.3603  Validation loss = 3.3881  \n",
      "\n",
      "Fold: 4  Epoch: 396  Training loss = 1.3597  Validation loss = 3.3853  \n",
      "\n",
      "Fold: 4  Epoch: 397  Training loss = 1.3591  Validation loss = 3.3836  \n",
      "\n",
      "Fold: 4  Epoch: 398  Training loss = 1.3585  Validation loss = 3.3814  \n",
      "\n",
      "Fold: 4  Epoch: 399  Training loss = 1.3582  Validation loss = 3.3810  \n",
      "\n",
      "Fold: 4  Epoch: 400  Training loss = 1.3579  Validation loss = 3.3793  \n",
      "\n",
      "Fold: 4  Epoch: 401  Training loss = 1.3573  Validation loss = 3.3753  \n",
      "\n",
      "Fold: 4  Epoch: 402  Training loss = 1.3570  Validation loss = 3.3721  \n",
      "\n",
      "Fold: 4  Epoch: 403  Training loss = 1.3563  Validation loss = 3.3708  \n",
      "\n",
      "Fold: 4  Epoch: 404  Training loss = 1.3559  Validation loss = 3.3704  \n",
      "\n",
      "Fold: 4  Epoch: 405  Training loss = 1.3553  Validation loss = 3.3669  \n",
      "\n",
      "Fold: 4  Epoch: 406  Training loss = 1.3549  Validation loss = 3.3664  \n",
      "\n",
      "Fold: 4  Epoch: 407  Training loss = 1.3549  Validation loss = 3.3696  \n",
      "\n",
      "Fold: 4  Epoch: 408  Training loss = 1.3543  Validation loss = 3.3681  \n",
      "\n",
      "Fold: 4  Epoch: 409  Training loss = 1.3537  Validation loss = 3.3650  \n",
      "\n",
      "Fold: 4  Epoch: 410  Training loss = 1.3531  Validation loss = 3.3619  \n",
      "\n",
      "Fold: 4  Epoch: 411  Training loss = 1.3528  Validation loss = 3.3620  \n",
      "\n",
      "Fold: 4  Epoch: 412  Training loss = 1.3522  Validation loss = 3.3604  \n",
      "\n",
      "Fold: 4  Epoch: 413  Training loss = 1.3519  Validation loss = 3.3576  \n",
      "\n",
      "Fold: 4  Epoch: 414  Training loss = 1.3518  Validation loss = 3.3578  \n",
      "\n",
      "Fold: 4  Epoch: 415  Training loss = 1.3509  Validation loss = 3.3551  \n",
      "\n",
      "Fold: 4  Epoch: 416  Training loss = 1.3508  Validation loss = 3.3570  \n",
      "\n",
      "Fold: 4  Epoch: 417  Training loss = 1.3494  Validation loss = 3.3510  \n",
      "\n",
      "Fold: 4  Epoch: 418  Training loss = 1.3488  Validation loss = 3.3483  \n",
      "\n",
      "Fold: 4  Epoch: 419  Training loss = 1.3482  Validation loss = 3.3478  \n",
      "\n",
      "Fold: 4  Epoch: 420  Training loss = 1.3480  Validation loss = 3.3481  \n",
      "\n",
      "Fold: 4  Epoch: 421  Training loss = 1.3475  Validation loss = 3.3449  \n",
      "\n",
      "Fold: 4  Epoch: 422  Training loss = 1.3470  Validation loss = 3.3439  \n",
      "\n",
      "Fold: 4  Epoch: 423  Training loss = 1.3465  Validation loss = 3.3398  \n",
      "\n",
      "Fold: 4  Epoch: 424  Training loss = 1.3458  Validation loss = 3.3368  \n",
      "\n",
      "Fold: 4  Epoch: 425  Training loss = 1.3455  Validation loss = 3.3348  \n",
      "\n",
      "Fold: 4  Epoch: 426  Training loss = 1.3449  Validation loss = 3.3311  \n",
      "\n",
      "Fold: 4  Epoch: 427  Training loss = 1.3445  Validation loss = 3.3294  \n",
      "\n",
      "Fold: 4  Epoch: 428  Training loss = 1.3439  Validation loss = 3.3276  \n",
      "\n",
      "Fold: 4  Epoch: 429  Training loss = 1.3434  Validation loss = 3.3281  \n",
      "\n",
      "Fold: 4  Epoch: 430  Training loss = 1.3431  Validation loss = 3.3260  \n",
      "\n",
      "Fold: 4  Epoch: 431  Training loss = 1.3429  Validation loss = 3.3251  \n",
      "\n",
      "Fold: 4  Epoch: 432  Training loss = 1.3422  Validation loss = 3.3242  \n",
      "\n",
      "Fold: 4  Epoch: 433  Training loss = 1.3421  Validation loss = 3.3273  \n",
      "\n",
      "Fold: 4  Epoch: 434  Training loss = 1.3415  Validation loss = 3.3223  \n",
      "\n",
      "Fold: 4  Epoch: 435  Training loss = 1.3411  Validation loss = 3.3233  \n",
      "\n",
      "Fold: 4  Epoch: 436  Training loss = 1.3409  Validation loss = 3.3233  \n",
      "\n",
      "Fold: 4  Epoch: 437  Training loss = 1.3405  Validation loss = 3.3220  \n",
      "\n",
      "Fold: 4  Epoch: 438  Training loss = 1.3399  Validation loss = 3.3191  \n",
      "\n",
      "Fold: 4  Epoch: 439  Training loss = 1.3398  Validation loss = 3.3195  \n",
      "\n",
      "Fold: 4  Epoch: 440  Training loss = 1.3390  Validation loss = 3.3144  \n",
      "\n",
      "Fold: 4  Epoch: 441  Training loss = 1.3386  Validation loss = 3.3138  \n",
      "\n",
      "Fold: 4  Epoch: 442  Training loss = 1.3382  Validation loss = 3.3107  \n",
      "\n",
      "Fold: 4  Epoch: 443  Training loss = 1.3381  Validation loss = 3.3141  \n",
      "\n",
      "Fold: 4  Epoch: 444  Training loss = 1.3378  Validation loss = 3.3153  \n",
      "\n",
      "Fold: 4  Epoch: 445  Training loss = 1.3372  Validation loss = 3.3128  \n",
      "\n",
      "Fold: 4  Epoch: 446  Training loss = 1.3365  Validation loss = 3.3099  \n",
      "\n",
      "Fold: 4  Epoch: 447  Training loss = 1.3361  Validation loss = 3.3088  \n",
      "\n",
      "Fold: 4  Epoch: 448  Training loss = 1.3361  Validation loss = 3.3108  \n",
      "\n",
      "Fold: 4  Epoch: 449  Training loss = 1.3353  Validation loss = 3.3058  \n",
      "\n",
      "Fold: 4  Epoch: 450  Training loss = 1.3348  Validation loss = 3.3037  \n",
      "\n",
      "Fold: 4  Epoch: 451  Training loss = 1.3346  Validation loss = 3.3032  \n",
      "\n",
      "Fold: 4  Epoch: 452  Training loss = 1.3342  Validation loss = 3.3009  \n",
      "\n",
      "Fold: 4  Epoch: 453  Training loss = 1.3337  Validation loss = 3.2996  \n",
      "\n",
      "Fold: 4  Epoch: 454  Training loss = 1.3333  Validation loss = 3.2980  \n",
      "\n",
      "Fold: 4  Epoch: 455  Training loss = 1.3327  Validation loss = 3.2957  \n",
      "\n",
      "Fold: 4  Epoch: 456  Training loss = 1.3317  Validation loss = 3.2900  \n",
      "\n",
      "Fold: 4  Epoch: 457  Training loss = 1.3314  Validation loss = 3.2901  \n",
      "\n",
      "Fold: 4  Epoch: 458  Training loss = 1.3307  Validation loss = 3.2866  \n",
      "\n",
      "Fold: 4  Epoch: 459  Training loss = 1.3304  Validation loss = 3.2838  \n",
      "\n",
      "Fold: 4  Epoch: 460  Training loss = 1.3297  Validation loss = 3.2848  \n",
      "\n",
      "Fold: 4  Epoch: 461  Training loss = 1.3292  Validation loss = 3.2829  \n",
      "\n",
      "Fold: 4  Epoch: 462  Training loss = 1.3293  Validation loss = 3.2845  \n",
      "\n",
      "Fold: 4  Epoch: 463  Training loss = 1.3286  Validation loss = 3.2805  \n",
      "\n",
      "Fold: 4  Epoch: 464  Training loss = 1.3283  Validation loss = 3.2785  \n",
      "\n",
      "Fold: 4  Epoch: 465  Training loss = 1.3281  Validation loss = 3.2801  \n",
      "\n",
      "Fold: 4  Epoch: 466  Training loss = 1.3276  Validation loss = 3.2779  \n",
      "\n",
      "Fold: 4  Epoch: 467  Training loss = 1.3273  Validation loss = 3.2762  \n",
      "\n",
      "Fold: 4  Epoch: 468  Training loss = 1.3270  Validation loss = 3.2771  \n",
      "\n",
      "Fold: 4  Epoch: 469  Training loss = 1.3269  Validation loss = 3.2787  \n",
      "\n",
      "Fold: 4  Epoch: 470  Training loss = 1.3266  Validation loss = 3.2776  \n",
      "\n",
      "Fold: 4  Epoch: 471  Training loss = 1.3256  Validation loss = 3.2729  \n",
      "\n",
      "Fold: 4  Epoch: 472  Training loss = 1.3257  Validation loss = 3.2759  \n",
      "\n",
      "Fold: 4  Epoch: 473  Training loss = 1.3250  Validation loss = 3.2696  \n",
      "\n",
      "Fold: 4  Epoch: 474  Training loss = 1.3246  Validation loss = 3.2673  \n",
      "\n",
      "Fold: 4  Epoch: 475  Training loss = 1.3240  Validation loss = 3.2644  \n",
      "\n",
      "Fold: 4  Epoch: 476  Training loss = 1.3235  Validation loss = 3.2623  \n",
      "\n",
      "Fold: 4  Epoch: 477  Training loss = 1.3231  Validation loss = 3.2610  \n",
      "\n",
      "Fold: 4  Epoch: 478  Training loss = 1.3229  Validation loss = 3.2611  \n",
      "\n",
      "Fold: 4  Epoch: 479  Training loss = 1.3227  Validation loss = 3.2596  \n",
      "\n",
      "Fold: 4  Epoch: 480  Training loss = 1.3223  Validation loss = 3.2582  \n",
      "\n",
      "Fold: 4  Epoch: 481  Training loss = 1.3220  Validation loss = 3.2575  \n",
      "\n",
      "Fold: 4  Epoch: 482  Training loss = 1.3214  Validation loss = 3.2542  \n",
      "\n",
      "Fold: 4  Epoch: 483  Training loss = 1.3211  Validation loss = 3.2517  \n",
      "\n",
      "Fold: 4  Epoch: 484  Training loss = 1.3204  Validation loss = 3.2474  \n",
      "\n",
      "Fold: 4  Epoch: 485  Training loss = 1.3202  Validation loss = 3.2496  \n",
      "\n",
      "Fold: 4  Epoch: 486  Training loss = 1.3198  Validation loss = 3.2484  \n",
      "\n",
      "Fold: 4  Epoch: 487  Training loss = 1.3194  Validation loss = 3.2473  \n",
      "\n",
      "Fold: 4  Epoch: 488  Training loss = 1.3189  Validation loss = 3.2441  \n",
      "\n",
      "Fold: 4  Epoch: 489  Training loss = 1.3186  Validation loss = 3.2429  \n",
      "\n",
      "Fold: 4  Epoch: 490  Training loss = 1.3184  Validation loss = 3.2431  \n",
      "\n",
      "Fold: 4  Epoch: 491  Training loss = 1.3181  Validation loss = 3.2417  \n",
      "\n",
      "Fold: 4  Epoch: 492  Training loss = 1.3175  Validation loss = 3.2359  \n",
      "\n",
      "Fold: 4  Epoch: 493  Training loss = 1.3170  Validation loss = 3.2378  \n",
      "\n",
      "Fold: 4  Epoch: 494  Training loss = 1.3172  Validation loss = 3.2401  \n",
      "\n",
      "Fold: 4  Epoch: 495  Training loss = 1.3169  Validation loss = 3.2378  \n",
      "\n",
      "Fold: 4  Epoch: 496  Training loss = 1.3165  Validation loss = 3.2359  \n",
      "\n",
      "Fold: 4  Epoch: 497  Training loss = 1.3163  Validation loss = 3.2350  \n",
      "\n",
      "Fold: 4  Epoch: 498  Training loss = 1.3165  Validation loss = 3.2390  \n",
      "\n",
      "Fold: 4  Epoch: 499  Training loss = 1.3148  Validation loss = 3.2310  \n",
      "\n",
      "Fold: 4  Epoch: 500  Training loss = 1.3145  Validation loss = 3.2304  \n",
      "\n",
      "Fold: 4  Epoch: 501  Training loss = 1.3141  Validation loss = 3.2286  \n",
      "\n",
      "Fold: 4  Epoch: 502  Training loss = 1.3142  Validation loss = 3.2296  \n",
      "\n",
      "Fold: 4  Epoch: 503  Training loss = 1.3136  Validation loss = 3.2263  \n",
      "\n",
      "Fold: 4  Epoch: 504  Training loss = 1.3132  Validation loss = 3.2254  \n",
      "\n",
      "Fold: 4  Epoch: 505  Training loss = 1.3125  Validation loss = 3.2223  \n",
      "\n",
      "Fold: 4  Epoch: 506  Training loss = 1.3120  Validation loss = 3.2208  \n",
      "\n",
      "Fold: 4  Epoch: 507  Training loss = 1.3117  Validation loss = 3.2208  \n",
      "\n",
      "Fold: 4  Epoch: 508  Training loss = 1.3112  Validation loss = 3.2176  \n",
      "\n",
      "Fold: 4  Epoch: 509  Training loss = 1.3108  Validation loss = 3.2135  \n",
      "\n",
      "Fold: 4  Epoch: 510  Training loss = 1.3106  Validation loss = 3.2112  \n",
      "\n",
      "Fold: 4  Epoch: 511  Training loss = 1.3103  Validation loss = 3.2121  \n",
      "\n",
      "Fold: 4  Epoch: 512  Training loss = 1.3102  Validation loss = 3.2114  \n",
      "\n",
      "Fold: 4  Epoch: 513  Training loss = 1.3099  Validation loss = 3.2121  \n",
      "\n",
      "Fold: 4  Epoch: 514  Training loss = 1.3091  Validation loss = 3.2058  \n",
      "\n",
      "Fold: 4  Epoch: 515  Training loss = 1.3093  Validation loss = 3.2071  \n",
      "\n",
      "Fold: 4  Epoch: 516  Training loss = 1.3079  Validation loss = 3.2012  \n",
      "\n",
      "Fold: 4  Epoch: 517  Training loss = 1.3073  Validation loss = 3.1972  \n",
      "\n",
      "Fold: 4  Epoch: 518  Training loss = 1.3070  Validation loss = 3.1970  \n",
      "\n",
      "Fold: 4  Epoch: 519  Training loss = 1.3067  Validation loss = 3.1983  \n",
      "\n",
      "Fold: 4  Epoch: 520  Training loss = 1.3062  Validation loss = 3.1941  \n",
      "\n",
      "Fold: 4  Epoch: 521  Training loss = 1.3057  Validation loss = 3.1932  \n",
      "\n",
      "Fold: 4  Epoch: 522  Training loss = 1.3054  Validation loss = 3.1914  \n",
      "\n",
      "Fold: 4  Epoch: 523  Training loss = 1.3051  Validation loss = 3.1898  \n",
      "\n",
      "Fold: 4  Epoch: 524  Training loss = 1.3044  Validation loss = 3.1839  \n",
      "\n",
      "Fold: 4  Epoch: 525  Training loss = 1.3037  Validation loss = 3.1808  \n",
      "\n",
      "Fold: 4  Epoch: 526  Training loss = 1.3034  Validation loss = 3.1807  \n",
      "\n",
      "Fold: 4  Epoch: 527  Training loss = 1.3031  Validation loss = 3.1779  \n",
      "\n",
      "Fold: 4  Epoch: 528  Training loss = 1.3025  Validation loss = 3.1740  \n",
      "\n",
      "Fold: 4  Epoch: 529  Training loss = 1.3023  Validation loss = 3.1744  \n",
      "\n",
      "Fold: 4  Epoch: 530  Training loss = 1.3021  Validation loss = 3.1747  \n",
      "\n",
      "Fold: 4  Epoch: 531  Training loss = 1.3017  Validation loss = 3.1741  \n",
      "\n",
      "Fold: 4  Epoch: 532  Training loss = 1.3007  Validation loss = 3.1709  \n",
      "\n",
      "Fold: 4  Epoch: 533  Training loss = 1.3003  Validation loss = 3.1686  \n",
      "\n",
      "Fold: 4  Epoch: 534  Training loss = 1.3000  Validation loss = 3.1641  \n",
      "\n",
      "Fold: 4  Epoch: 535  Training loss = 1.2996  Validation loss = 3.1593  \n",
      "\n",
      "Fold: 4  Epoch: 536  Training loss = 1.2995  Validation loss = 3.1574  \n",
      "\n",
      "Fold: 4  Epoch: 537  Training loss = 1.2990  Validation loss = 3.1579  \n",
      "\n",
      "Fold: 4  Epoch: 538  Training loss = 1.2984  Validation loss = 3.1575  \n",
      "\n",
      "Fold: 4  Epoch: 539  Training loss = 1.2982  Validation loss = 3.1544  \n",
      "\n",
      "Fold: 4  Epoch: 540  Training loss = 1.2981  Validation loss = 3.1516  \n",
      "\n",
      "Fold: 4  Epoch: 541  Training loss = 1.2980  Validation loss = 3.1515  \n",
      "\n",
      "Fold: 4  Epoch: 542  Training loss = 1.2976  Validation loss = 3.1473  \n",
      "\n",
      "Fold: 4  Epoch: 543  Training loss = 1.2974  Validation loss = 3.1444  \n",
      "\n",
      "Fold: 4  Epoch: 544  Training loss = 1.2977  Validation loss = 3.1391  \n",
      "\n",
      "Fold: 4  Epoch: 545  Training loss = 1.2978  Validation loss = 3.1340  \n",
      "\n",
      "Fold: 4  Epoch: 546  Training loss = 1.2963  Validation loss = 3.1356  \n",
      "\n",
      "Fold: 4  Epoch: 547  Training loss = 1.2955  Validation loss = 3.1362  \n",
      "\n",
      "Fold: 4  Epoch: 548  Training loss = 1.2950  Validation loss = 3.1328  \n",
      "\n",
      "Fold: 4  Epoch: 549  Training loss = 1.2942  Validation loss = 3.1313  \n",
      "\n",
      "Fold: 4  Epoch: 550  Training loss = 1.2937  Validation loss = 3.1277  \n",
      "\n",
      "Fold: 4  Epoch: 551  Training loss = 1.2931  Validation loss = 3.1243  \n",
      "\n",
      "Fold: 4  Epoch: 552  Training loss = 1.2928  Validation loss = 3.1248  \n",
      "\n",
      "Fold: 4  Epoch: 553  Training loss = 1.2920  Validation loss = 3.1249  \n",
      "\n",
      "Fold: 4  Epoch: 554  Training loss = 1.2914  Validation loss = 3.1273  \n",
      "\n",
      "Fold: 4  Epoch: 555  Training loss = 1.2910  Validation loss = 3.1260  \n",
      "\n",
      "Fold: 4  Epoch: 556  Training loss = 1.2907  Validation loss = 3.1268  \n",
      "\n",
      "Fold: 4  Epoch: 557  Training loss = 1.2906  Validation loss = 3.1284  \n",
      "\n",
      "Fold: 4  Epoch: 558  Training loss = 1.2904  Validation loss = 3.1290  \n",
      "\n",
      "Fold: 4  Epoch: 559  Training loss = 1.2903  Validation loss = 3.1286  \n",
      "\n",
      "Fold: 4  Epoch: 560  Training loss = 1.2902  Validation loss = 3.1291  \n",
      "\n",
      "Fold: 4  Epoch: 561  Training loss = 1.2897  Validation loss = 3.1259  \n",
      "\n",
      "Fold: 4  Epoch: 562  Training loss = 1.2894  Validation loss = 3.1244  \n",
      "\n",
      "Fold: 4  Epoch: 563  Training loss = 1.2890  Validation loss = 3.1222  \n",
      "\n",
      "Fold: 4  Epoch: 564  Training loss = 1.2884  Validation loss = 3.1181  \n",
      "\n",
      "Fold: 4  Epoch: 565  Training loss = 1.2884  Validation loss = 3.1190  \n",
      "\n",
      "Fold: 4  Epoch: 566  Training loss = 1.2886  Validation loss = 3.1205  \n",
      "\n",
      "Fold: 4  Epoch: 567  Training loss = 1.2887  Validation loss = 3.1209  \n",
      "\n",
      "Fold: 4  Epoch: 568  Training loss = 1.2882  Validation loss = 3.1205  \n",
      "\n",
      "Fold: 4  Epoch: 569  Training loss = 1.2876  Validation loss = 3.1180  \n",
      "\n",
      "Fold: 4  Epoch: 570  Training loss = 1.2867  Validation loss = 3.1136  \n",
      "\n",
      "Fold: 4  Epoch: 571  Training loss = 1.2860  Validation loss = 3.1097  \n",
      "\n",
      "Fold: 4  Epoch: 572  Training loss = 1.2856  Validation loss = 3.1086  \n",
      "\n",
      "Fold: 4  Epoch: 573  Training loss = 1.2850  Validation loss = 3.1052  \n",
      "\n",
      "Fold: 4  Epoch: 574  Training loss = 1.2846  Validation loss = 3.1032  \n",
      "\n",
      "Fold: 4  Epoch: 575  Training loss = 1.2845  Validation loss = 3.1017  \n",
      "\n",
      "Fold: 4  Epoch: 576  Training loss = 1.2841  Validation loss = 3.1009  \n",
      "\n",
      "Fold: 4  Epoch: 577  Training loss = 1.2841  Validation loss = 3.1023  \n",
      "\n",
      "Fold: 4  Epoch: 578  Training loss = 1.2837  Validation loss = 3.0999  \n",
      "\n",
      "Fold: 4  Epoch: 579  Training loss = 1.2837  Validation loss = 3.1003  \n",
      "\n",
      "Fold: 4  Epoch: 580  Training loss = 1.2836  Validation loss = 3.1003  \n",
      "\n",
      "Fold: 4  Epoch: 581  Training loss = 1.2838  Validation loss = 3.1011  \n",
      "\n",
      "Fold: 4  Epoch: 582  Training loss = 1.2832  Validation loss = 3.0980  \n",
      "\n",
      "Fold: 4  Epoch: 583  Training loss = 1.2832  Validation loss = 3.0987  \n",
      "\n",
      "Fold: 4  Epoch: 584  Training loss = 1.2826  Validation loss = 3.0959  \n",
      "\n",
      "Fold: 4  Epoch: 585  Training loss = 1.2817  Validation loss = 3.0907  \n",
      "\n",
      "Fold: 4  Epoch: 586  Training loss = 1.2816  Validation loss = 3.0912  \n",
      "\n",
      "Fold: 4  Epoch: 587  Training loss = 1.2810  Validation loss = 3.0880  \n",
      "\n",
      "Fold: 4  Epoch: 588  Training loss = 1.2805  Validation loss = 3.0861  \n",
      "\n",
      "Fold: 4  Epoch: 589  Training loss = 1.2801  Validation loss = 3.0837  \n",
      "\n",
      "Fold: 4  Epoch: 590  Training loss = 1.2799  Validation loss = 3.0821  \n",
      "\n",
      "Fold: 4  Epoch: 591  Training loss = 1.2795  Validation loss = 3.0788  \n",
      "\n",
      "Fold: 4  Epoch: 592  Training loss = 1.2792  Validation loss = 3.0767  \n",
      "\n",
      "Fold: 4  Epoch: 593  Training loss = 1.2789  Validation loss = 3.0758  \n",
      "\n",
      "Fold: 4  Epoch: 594  Training loss = 1.2789  Validation loss = 3.0756  \n",
      "\n",
      "Fold: 4  Epoch: 595  Training loss = 1.2788  Validation loss = 3.0771  \n",
      "\n",
      "Fold: 4  Epoch: 596  Training loss = 1.2782  Validation loss = 3.0753  \n",
      "\n",
      "Fold: 4  Epoch: 597  Training loss = 1.2784  Validation loss = 3.0778  \n",
      "\n",
      "Fold: 4  Epoch: 598  Training loss = 1.2777  Validation loss = 3.0741  \n",
      "\n",
      "Fold: 4  Epoch: 599  Training loss = 1.2771  Validation loss = 3.0706  \n",
      "\n",
      "Fold: 4  Epoch: 600  Training loss = 1.2770  Validation loss = 3.0714  \n",
      "\n",
      "Fold: 4  Epoch: 601  Training loss = 1.2761  Validation loss = 3.0667  \n",
      "\n",
      "Fold: 4  Epoch: 602  Training loss = 1.2758  Validation loss = 3.0634  \n",
      "\n",
      "Fold: 4  Epoch: 603  Training loss = 1.2753  Validation loss = 3.0595  \n",
      "\n",
      "Fold: 4  Epoch: 604  Training loss = 1.2751  Validation loss = 3.0627  \n",
      "\n",
      "Fold: 4  Epoch: 605  Training loss = 1.2753  Validation loss = 3.0652  \n",
      "\n",
      "Fold: 4  Epoch: 606  Training loss = 1.2748  Validation loss = 3.0606  \n",
      "\n",
      "Fold: 4  Epoch: 607  Training loss = 1.2742  Validation loss = 3.0563  \n",
      "\n",
      "Fold: 4  Epoch: 608  Training loss = 1.2742  Validation loss = 3.0594  \n",
      "\n",
      "Fold: 4  Epoch: 609  Training loss = 1.2736  Validation loss = 3.0562  \n",
      "\n",
      "Fold: 4  Epoch: 610  Training loss = 1.2731  Validation loss = 3.0503  \n",
      "\n",
      "Fold: 4  Epoch: 611  Training loss = 1.2730  Validation loss = 3.0500  \n",
      "\n",
      "Fold: 4  Epoch: 612  Training loss = 1.2726  Validation loss = 3.0465  \n",
      "\n",
      "Fold: 4  Epoch: 613  Training loss = 1.2725  Validation loss = 3.0463  \n",
      "\n",
      "Fold: 4  Epoch: 614  Training loss = 1.2720  Validation loss = 3.0463  \n",
      "\n",
      "Fold: 4  Epoch: 615  Training loss = 1.2717  Validation loss = 3.0462  \n",
      "\n",
      "Fold: 4  Epoch: 616  Training loss = 1.2716  Validation loss = 3.0490  \n",
      "\n",
      "Fold: 4  Epoch: 617  Training loss = 1.2714  Validation loss = 3.0486  \n",
      "\n",
      "Fold: 4  Epoch: 618  Training loss = 1.2710  Validation loss = 3.0446  \n",
      "\n",
      "Fold: 4  Epoch: 619  Training loss = 1.2706  Validation loss = 3.0406  \n",
      "\n",
      "Fold: 4  Epoch: 620  Training loss = 1.2704  Validation loss = 3.0409  \n",
      "\n",
      "Fold: 4  Epoch: 621  Training loss = 1.2699  Validation loss = 3.0374  \n",
      "\n",
      "Fold: 4  Epoch: 622  Training loss = 1.2697  Validation loss = 3.0342  \n",
      "\n",
      "Fold: 4  Epoch: 623  Training loss = 1.2697  Validation loss = 3.0302  \n",
      "\n",
      "Fold: 4  Epoch: 624  Training loss = 1.2695  Validation loss = 3.0271  \n",
      "\n",
      "Fold: 4  Epoch: 625  Training loss = 1.2693  Validation loss = 3.0259  \n",
      "\n",
      "Fold: 4  Epoch: 626  Training loss = 1.2690  Validation loss = 3.0279  \n",
      "\n",
      "Fold: 4  Epoch: 627  Training loss = 1.2684  Validation loss = 3.0270  \n",
      "\n",
      "Fold: 4  Epoch: 628  Training loss = 1.2680  Validation loss = 3.0257  \n",
      "\n",
      "Fold: 4  Epoch: 629  Training loss = 1.2678  Validation loss = 3.0289  \n",
      "\n",
      "Fold: 4  Epoch: 630  Training loss = 1.2678  Validation loss = 3.0304  \n",
      "\n",
      "Fold: 4  Epoch: 631  Training loss = 1.2677  Validation loss = 3.0284  \n",
      "\n",
      "Fold: 4  Epoch: 632  Training loss = 1.2672  Validation loss = 3.0279  \n",
      "\n",
      "Fold: 4  Epoch: 633  Training loss = 1.2673  Validation loss = 3.0276  \n",
      "\n",
      "Fold: 4  Epoch: 634  Training loss = 1.2670  Validation loss = 3.0249  \n",
      "\n",
      "Fold: 4  Epoch: 635  Training loss = 1.2666  Validation loss = 3.0214  \n",
      "\n",
      "Fold: 4  Epoch: 636  Training loss = 1.2664  Validation loss = 3.0228  \n",
      "\n",
      "Fold: 4  Epoch: 637  Training loss = 1.2656  Validation loss = 3.0162  \n",
      "\n",
      "Fold: 4  Epoch: 638  Training loss = 1.2654  Validation loss = 3.0155  \n",
      "\n",
      "Fold: 4  Epoch: 639  Training loss = 1.2650  Validation loss = 3.0154  \n",
      "\n",
      "Fold: 4  Epoch: 640  Training loss = 1.2647  Validation loss = 3.0173  \n",
      "\n",
      "Fold: 4  Epoch: 641  Training loss = 1.2646  Validation loss = 3.0181  \n",
      "\n",
      "Fold: 4  Epoch: 642  Training loss = 1.2643  Validation loss = 3.0162  \n",
      "\n",
      "Fold: 4  Epoch: 643  Training loss = 1.2640  Validation loss = 3.0159  \n",
      "\n",
      "Fold: 4  Epoch: 644  Training loss = 1.2639  Validation loss = 3.0166  \n",
      "\n",
      "Fold: 4  Epoch: 645  Training loss = 1.2637  Validation loss = 3.0160  \n",
      "\n",
      "Fold: 4  Epoch: 646  Training loss = 1.2633  Validation loss = 3.0121  \n",
      "\n",
      "Fold: 4  Epoch: 647  Training loss = 1.2628  Validation loss = 3.0101  \n",
      "\n",
      "Fold: 4  Epoch: 648  Training loss = 1.2625  Validation loss = 3.0053  \n",
      "\n",
      "Fold: 4  Epoch: 649  Training loss = 1.2621  Validation loss = 3.0050  \n",
      "\n",
      "Fold: 4  Epoch: 650  Training loss = 1.2621  Validation loss = 3.0058  \n",
      "\n",
      "Fold: 4  Epoch: 651  Training loss = 1.2618  Validation loss = 3.0056  \n",
      "\n",
      "Fold: 4  Epoch: 652  Training loss = 1.2618  Validation loss = 3.0062  \n",
      "\n",
      "Fold: 4  Epoch: 653  Training loss = 1.2615  Validation loss = 3.0024  \n",
      "\n",
      "Fold: 4  Epoch: 654  Training loss = 1.2611  Validation loss = 2.9996  \n",
      "\n",
      "Fold: 4  Epoch: 655  Training loss = 1.2608  Validation loss = 2.9986  \n",
      "\n",
      "Fold: 4  Epoch: 656  Training loss = 1.2604  Validation loss = 2.9964  \n",
      "\n",
      "Fold: 4  Epoch: 657  Training loss = 1.2603  Validation loss = 2.9977  \n",
      "\n",
      "Fold: 4  Epoch: 658  Training loss = 1.2601  Validation loss = 2.9966  \n",
      "\n",
      "Fold: 4  Epoch: 659  Training loss = 1.2599  Validation loss = 2.9945  \n",
      "\n",
      "Fold: 4  Epoch: 660  Training loss = 1.2599  Validation loss = 2.9949  \n",
      "\n",
      "Fold: 4  Epoch: 661  Training loss = 1.2597  Validation loss = 2.9941  \n",
      "\n",
      "Fold: 4  Epoch: 662  Training loss = 1.2593  Validation loss = 2.9933  \n",
      "\n",
      "Fold: 4  Epoch: 663  Training loss = 1.2591  Validation loss = 2.9889  \n",
      "\n",
      "Fold: 4  Epoch: 664  Training loss = 1.2589  Validation loss = 2.9897  \n",
      "\n",
      "Fold: 4  Epoch: 665  Training loss = 1.2585  Validation loss = 2.9867  \n",
      "\n",
      "Fold: 4  Epoch: 666  Training loss = 1.2580  Validation loss = 2.9829  \n",
      "\n",
      "Fold: 4  Epoch: 667  Training loss = 1.2577  Validation loss = 2.9834  \n",
      "\n",
      "Fold: 4  Epoch: 668  Training loss = 1.2576  Validation loss = 2.9821  \n",
      "\n",
      "Fold: 4  Epoch: 669  Training loss = 1.2575  Validation loss = 2.9850  \n",
      "\n",
      "Fold: 4  Epoch: 670  Training loss = 1.2572  Validation loss = 2.9830  \n",
      "\n",
      "Fold: 4  Epoch: 671  Training loss = 1.2568  Validation loss = 2.9789  \n",
      "\n",
      "Fold: 4  Epoch: 672  Training loss = 1.2567  Validation loss = 2.9787  \n",
      "\n",
      "Fold: 4  Epoch: 673  Training loss = 1.2562  Validation loss = 2.9724  \n",
      "\n",
      "Fold: 4  Epoch: 674  Training loss = 1.2561  Validation loss = 2.9714  \n",
      "\n",
      "Fold: 4  Epoch: 675  Training loss = 1.2558  Validation loss = 2.9691  \n",
      "\n",
      "Fold: 4  Epoch: 676  Training loss = 1.2555  Validation loss = 2.9664  \n",
      "\n",
      "Fold: 4  Epoch: 677  Training loss = 1.2552  Validation loss = 2.9678  \n",
      "\n",
      "Fold: 4  Epoch: 678  Training loss = 1.2551  Validation loss = 2.9691  \n",
      "\n",
      "Fold: 4  Epoch: 679  Training loss = 1.2548  Validation loss = 2.9637  \n",
      "\n",
      "Fold: 4  Epoch: 680  Training loss = 1.2547  Validation loss = 2.9628  \n",
      "\n",
      "Fold: 4  Epoch: 681  Training loss = 1.2544  Validation loss = 2.9618  \n",
      "\n",
      "Fold: 4  Epoch: 682  Training loss = 1.2539  Validation loss = 2.9607  \n",
      "\n",
      "Fold: 4  Epoch: 683  Training loss = 1.2538  Validation loss = 2.9631  \n",
      "\n",
      "Fold: 4  Epoch: 684  Training loss = 1.2533  Validation loss = 2.9570  \n",
      "\n",
      "Fold: 4  Epoch: 685  Training loss = 1.2531  Validation loss = 2.9589  \n",
      "\n",
      "Fold: 4  Epoch: 686  Training loss = 1.2531  Validation loss = 2.9602  \n",
      "\n",
      "Fold: 4  Epoch: 687  Training loss = 1.2530  Validation loss = 2.9595  \n",
      "\n",
      "Fold: 4  Epoch: 688  Training loss = 1.2528  Validation loss = 2.9603  \n",
      "\n",
      "Fold: 4  Epoch: 689  Training loss = 1.2526  Validation loss = 2.9562  \n",
      "\n",
      "Fold: 4  Epoch: 690  Training loss = 1.2526  Validation loss = 2.9548  \n",
      "\n",
      "Fold: 4  Epoch: 691  Training loss = 1.2530  Validation loss = 2.9586  \n",
      "\n",
      "Fold: 4  Epoch: 692  Training loss = 1.2524  Validation loss = 2.9569  \n",
      "\n",
      "Fold: 4  Epoch: 693  Training loss = 1.2523  Validation loss = 2.9518  \n",
      "\n",
      "Fold: 4  Epoch: 694  Training loss = 1.2525  Validation loss = 2.9538  \n",
      "\n",
      "Fold: 4  Epoch: 695  Training loss = 1.2512  Validation loss = 2.9518  \n",
      "\n",
      "Fold: 4  Epoch: 696  Training loss = 1.2513  Validation loss = 2.9530  \n",
      "\n",
      "Fold: 4  Epoch: 697  Training loss = 1.2510  Validation loss = 2.9527  \n",
      "\n",
      "Fold: 4  Epoch: 698  Training loss = 1.2506  Validation loss = 2.9516  \n",
      "\n",
      "Fold: 4  Epoch: 699  Training loss = 1.2498  Validation loss = 2.9465  \n",
      "\n",
      "Fold: 4  Epoch: 700  Training loss = 1.2497  Validation loss = 2.9479  \n",
      "\n",
      "Fold: 4  Epoch: 701  Training loss = 1.2499  Validation loss = 2.9501  \n",
      "\n",
      "Fold: 4  Epoch: 702  Training loss = 1.2495  Validation loss = 2.9505  \n",
      "\n",
      "Fold: 4  Epoch: 703  Training loss = 1.2497  Validation loss = 2.9511  \n",
      "\n",
      "Fold: 4  Epoch: 704  Training loss = 1.2495  Validation loss = 2.9503  \n",
      "\n",
      "Fold: 4  Epoch: 705  Training loss = 1.2497  Validation loss = 2.9509  \n",
      "\n",
      "Fold: 4  Epoch: 706  Training loss = 1.2492  Validation loss = 2.9483  \n",
      "\n",
      "Fold: 4  Epoch: 707  Training loss = 1.2484  Validation loss = 2.9452  \n",
      "\n",
      "Fold: 4  Epoch: 708  Training loss = 1.2486  Validation loss = 2.9484  \n",
      "\n",
      "Fold: 4  Epoch: 709  Training loss = 1.2484  Validation loss = 2.9486  \n",
      "\n",
      "Fold: 4  Epoch: 710  Training loss = 1.2482  Validation loss = 2.9470  \n",
      "\n",
      "Fold: 4  Epoch: 711  Training loss = 1.2480  Validation loss = 2.9462  \n",
      "\n",
      "Fold: 4  Epoch: 712  Training loss = 1.2482  Validation loss = 2.9466  \n",
      "\n",
      "Fold: 4  Epoch: 713  Training loss = 1.2485  Validation loss = 2.9465  \n",
      "\n",
      "Fold: 4  Epoch: 714  Training loss = 1.2478  Validation loss = 2.9431  \n",
      "\n",
      "Fold: 4  Epoch: 715  Training loss = 1.2479  Validation loss = 2.9447  \n",
      "\n",
      "Fold: 4  Epoch: 716  Training loss = 1.2481  Validation loss = 2.9445  \n",
      "\n",
      "Fold: 4  Epoch: 717  Training loss = 1.2482  Validation loss = 2.9437  \n",
      "\n",
      "Fold: 4  Epoch: 718  Training loss = 1.2477  Validation loss = 2.9420  \n",
      "\n",
      "Fold: 4  Epoch: 719  Training loss = 1.2468  Validation loss = 2.9399  \n",
      "\n",
      "Fold: 4  Epoch: 720  Training loss = 1.2462  Validation loss = 2.9374  \n",
      "\n",
      "Fold: 4  Epoch: 721  Training loss = 1.2458  Validation loss = 2.9354  \n",
      "\n",
      "Fold: 4  Epoch: 722  Training loss = 1.2457  Validation loss = 2.9351  \n",
      "\n",
      "Fold: 4  Epoch: 723  Training loss = 1.2455  Validation loss = 2.9339  \n",
      "\n",
      "Fold: 4  Epoch: 724  Training loss = 1.2457  Validation loss = 2.9346  \n",
      "\n",
      "Fold: 4  Epoch: 725  Training loss = 1.2453  Validation loss = 2.9318  \n",
      "\n",
      "Fold: 4  Epoch: 726  Training loss = 1.2451  Validation loss = 2.9315  \n",
      "\n",
      "Fold: 4  Epoch: 727  Training loss = 1.2446  Validation loss = 2.9287  \n",
      "\n",
      "Fold: 4  Epoch: 728  Training loss = 1.2449  Validation loss = 2.9304  \n",
      "\n",
      "Fold: 4  Epoch: 729  Training loss = 1.2447  Validation loss = 2.9298  \n",
      "\n",
      "Fold: 4  Epoch: 730  Training loss = 1.2439  Validation loss = 2.9241  \n",
      "\n",
      "Fold: 4  Epoch: 731  Training loss = 1.2436  Validation loss = 2.9217  \n",
      "\n",
      "Fold: 4  Epoch: 732  Training loss = 1.2433  Validation loss = 2.9167  \n",
      "\n",
      "Fold: 4  Epoch: 733  Training loss = 1.2433  Validation loss = 2.9148  \n",
      "\n",
      "Fold: 4  Epoch: 734  Training loss = 1.2431  Validation loss = 2.9132  \n",
      "\n",
      "Fold: 4  Epoch: 735  Training loss = 1.2428  Validation loss = 2.9134  \n",
      "\n",
      "Fold: 4  Epoch: 736  Training loss = 1.2426  Validation loss = 2.9123  \n",
      "\n",
      "Fold: 4  Epoch: 737  Training loss = 1.2423  Validation loss = 2.9133  \n",
      "\n",
      "Fold: 4  Epoch: 738  Training loss = 1.2420  Validation loss = 2.9102  \n",
      "\n",
      "Fold: 4  Epoch: 739  Training loss = 1.2418  Validation loss = 2.9093  \n",
      "\n",
      "Fold: 4  Epoch: 740  Training loss = 1.2416  Validation loss = 2.9070  \n",
      "\n",
      "Fold: 4  Epoch: 741  Training loss = 1.2412  Validation loss = 2.9062  \n",
      "\n",
      "Fold: 4  Epoch: 742  Training loss = 1.2411  Validation loss = 2.9039  \n",
      "\n",
      "Fold: 4  Epoch: 743  Training loss = 1.2408  Validation loss = 2.9038  \n",
      "\n",
      "Fold: 4  Epoch: 744  Training loss = 1.2408  Validation loss = 2.9055  \n",
      "\n",
      "Fold: 4  Epoch: 745  Training loss = 1.2406  Validation loss = 2.9055  \n",
      "\n",
      "Fold: 4  Epoch: 746  Training loss = 1.2404  Validation loss = 2.9051  \n",
      "\n",
      "Fold: 4  Epoch: 747  Training loss = 1.2401  Validation loss = 2.9012  \n",
      "\n",
      "Fold: 4  Epoch: 748  Training loss = 1.2399  Validation loss = 2.8977  \n",
      "\n",
      "Fold: 4  Epoch: 749  Training loss = 1.2397  Validation loss = 2.8993  \n",
      "\n",
      "Fold: 4  Epoch: 750  Training loss = 1.2395  Validation loss = 2.8988  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 748  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.3352  Validation loss = 2.4342  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.3337  Validation loss = 2.4304  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.3316  Validation loss = 2.4193  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.3306  Validation loss = 2.4207  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.3288  Validation loss = 2.4131  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 1.3276  Validation loss = 2.4112  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 1.3265  Validation loss = 2.4053  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.3252  Validation loss = 2.4047  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 1.3243  Validation loss = 2.4044  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.3228  Validation loss = 2.3846  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 1.3214  Validation loss = 2.3780  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 1.3204  Validation loss = 2.3681  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 1.3197  Validation loss = 2.3715  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.3195  Validation loss = 2.3695  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 1.3184  Validation loss = 2.3613  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 1.3170  Validation loss = 2.3564  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 1.3171  Validation loss = 2.3626  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 1.3157  Validation loss = 2.3494  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 1.3148  Validation loss = 2.3410  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 1.3138  Validation loss = 2.3424  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 1.3131  Validation loss = 2.3353  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 1.3117  Validation loss = 2.3329  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 1.3106  Validation loss = 2.3295  \n",
      "\n",
      "Fold: 5  Epoch: 24  Training loss = 1.3100  Validation loss = 2.3288  \n",
      "\n",
      "Fold: 5  Epoch: 25  Training loss = 1.3096  Validation loss = 2.3285  \n",
      "\n",
      "Fold: 5  Epoch: 26  Training loss = 1.3084  Validation loss = 2.3259  \n",
      "\n",
      "Fold: 5  Epoch: 27  Training loss = 1.3076  Validation loss = 2.3117  \n",
      "\n",
      "Fold: 5  Epoch: 28  Training loss = 1.3063  Validation loss = 2.3148  \n",
      "\n",
      "Fold: 5  Epoch: 29  Training loss = 1.3053  Validation loss = 2.3109  \n",
      "\n",
      "Fold: 5  Epoch: 30  Training loss = 1.3045  Validation loss = 2.3094  \n",
      "\n",
      "Fold: 5  Epoch: 31  Training loss = 1.3030  Validation loss = 2.2950  \n",
      "\n",
      "Fold: 5  Epoch: 32  Training loss = 1.3020  Validation loss = 2.2899  \n",
      "\n",
      "Fold: 5  Epoch: 33  Training loss = 1.3015  Validation loss = 2.2831  \n",
      "\n",
      "Fold: 5  Epoch: 34  Training loss = 1.3006  Validation loss = 2.2815  \n",
      "\n",
      "Fold: 5  Epoch: 35  Training loss = 1.2996  Validation loss = 2.2740  \n",
      "\n",
      "Fold: 5  Epoch: 36  Training loss = 1.2992  Validation loss = 2.2752  \n",
      "\n",
      "Fold: 5  Epoch: 37  Training loss = 1.2985  Validation loss = 2.2702  \n",
      "\n",
      "Fold: 5  Epoch: 38  Training loss = 1.2973  Validation loss = 2.2652  \n",
      "\n",
      "Fold: 5  Epoch: 39  Training loss = 1.2968  Validation loss = 2.2651  \n",
      "\n",
      "Fold: 5  Epoch: 40  Training loss = 1.2957  Validation loss = 2.2619  \n",
      "\n",
      "Fold: 5  Epoch: 41  Training loss = 1.2948  Validation loss = 2.2477  \n",
      "\n",
      "Fold: 5  Epoch: 42  Training loss = 1.2936  Validation loss = 2.2352  \n",
      "\n",
      "Fold: 5  Epoch: 43  Training loss = 1.2927  Validation loss = 2.2332  \n",
      "\n",
      "Fold: 5  Epoch: 44  Training loss = 1.2914  Validation loss = 2.2231  \n",
      "\n",
      "Fold: 5  Epoch: 45  Training loss = 1.2908  Validation loss = 2.2208  \n",
      "\n",
      "Fold: 5  Epoch: 46  Training loss = 1.2901  Validation loss = 2.2214  \n",
      "\n",
      "Fold: 5  Epoch: 47  Training loss = 1.2897  Validation loss = 2.2244  \n",
      "\n",
      "Fold: 5  Epoch: 48  Training loss = 1.2885  Validation loss = 2.2203  \n",
      "\n",
      "Fold: 5  Epoch: 49  Training loss = 1.2877  Validation loss = 2.2151  \n",
      "\n",
      "Fold: 5  Epoch: 50  Training loss = 1.2869  Validation loss = 2.2092  \n",
      "\n",
      "Fold: 5  Epoch: 51  Training loss = 1.2863  Validation loss = 2.2056  \n",
      "\n",
      "Fold: 5  Epoch: 52  Training loss = 1.2858  Validation loss = 2.2015  \n",
      "\n",
      "Fold: 5  Epoch: 53  Training loss = 1.2850  Validation loss = 2.2013  \n",
      "\n",
      "Fold: 5  Epoch: 54  Training loss = 1.2841  Validation loss = 2.2035  \n",
      "\n",
      "Fold: 5  Epoch: 55  Training loss = 1.2834  Validation loss = 2.1968  \n",
      "\n",
      "Fold: 5  Epoch: 56  Training loss = 1.2829  Validation loss = 2.1927  \n",
      "\n",
      "Fold: 5  Epoch: 57  Training loss = 1.2823  Validation loss = 2.1889  \n",
      "\n",
      "Fold: 5  Epoch: 58  Training loss = 1.2822  Validation loss = 2.1897  \n",
      "\n",
      "Fold: 5  Epoch: 59  Training loss = 1.2817  Validation loss = 2.1888  \n",
      "\n",
      "Fold: 5  Epoch: 60  Training loss = 1.2804  Validation loss = 2.1801  \n",
      "\n",
      "Fold: 5  Epoch: 61  Training loss = 1.2797  Validation loss = 2.1808  \n",
      "\n",
      "Fold: 5  Epoch: 62  Training loss = 1.2790  Validation loss = 2.1737  \n",
      "\n",
      "Fold: 5  Epoch: 63  Training loss = 1.2782  Validation loss = 2.1636  \n",
      "\n",
      "Fold: 5  Epoch: 64  Training loss = 1.2773  Validation loss = 2.1669  \n",
      "\n",
      "Fold: 5  Epoch: 65  Training loss = 1.2773  Validation loss = 2.1529  \n",
      "\n",
      "Fold: 5  Epoch: 66  Training loss = 1.2761  Validation loss = 2.1467  \n",
      "\n",
      "Fold: 5  Epoch: 67  Training loss = 1.2759  Validation loss = 2.1379  \n",
      "\n",
      "Fold: 5  Epoch: 68  Training loss = 1.2747  Validation loss = 2.1347  \n",
      "\n",
      "Fold: 5  Epoch: 69  Training loss = 1.2745  Validation loss = 2.1308  \n",
      "\n",
      "Fold: 5  Epoch: 70  Training loss = 1.2733  Validation loss = 2.1297  \n",
      "\n",
      "Fold: 5  Epoch: 71  Training loss = 1.2727  Validation loss = 2.1313  \n",
      "\n",
      "Fold: 5  Epoch: 72  Training loss = 1.2724  Validation loss = 2.1320  \n",
      "\n",
      "Fold: 5  Epoch: 73  Training loss = 1.2715  Validation loss = 2.1284  \n",
      "\n",
      "Fold: 5  Epoch: 74  Training loss = 1.2707  Validation loss = 2.1305  \n",
      "\n",
      "Fold: 5  Epoch: 75  Training loss = 1.2702  Validation loss = 2.1311  \n",
      "\n",
      "Fold: 5  Epoch: 76  Training loss = 1.2698  Validation loss = 2.1303  \n",
      "\n",
      "Fold: 5  Epoch: 77  Training loss = 1.2699  Validation loss = 2.1356  \n",
      "\n",
      "Fold: 5  Epoch: 78  Training loss = 1.2693  Validation loss = 2.1313  \n",
      "\n",
      "Fold: 5  Epoch: 79  Training loss = 1.2688  Validation loss = 2.1272  \n",
      "\n",
      "Fold: 5  Epoch: 80  Training loss = 1.2681  Validation loss = 2.1245  \n",
      "\n",
      "Fold: 5  Epoch: 81  Training loss = 1.2676  Validation loss = 2.1232  \n",
      "\n",
      "Fold: 5  Epoch: 82  Training loss = 1.2671  Validation loss = 2.1184  \n",
      "\n",
      "Fold: 5  Epoch: 83  Training loss = 1.2667  Validation loss = 2.1181  \n",
      "\n",
      "Fold: 5  Epoch: 84  Training loss = 1.2661  Validation loss = 2.1136  \n",
      "\n",
      "Fold: 5  Epoch: 85  Training loss = 1.2653  Validation loss = 2.1107  \n",
      "\n",
      "Fold: 5  Epoch: 86  Training loss = 1.2642  Validation loss = 2.1044  \n",
      "\n",
      "Fold: 5  Epoch: 87  Training loss = 1.2634  Validation loss = 2.0956  \n",
      "\n",
      "Fold: 5  Epoch: 88  Training loss = 1.2630  Validation loss = 2.0966  \n",
      "\n",
      "Fold: 5  Epoch: 89  Training loss = 1.2627  Validation loss = 2.0962  \n",
      "\n",
      "Fold: 5  Epoch: 90  Training loss = 1.2616  Validation loss = 2.0860  \n",
      "\n",
      "Fold: 5  Epoch: 91  Training loss = 1.2612  Validation loss = 2.0844  \n",
      "\n",
      "Fold: 5  Epoch: 92  Training loss = 1.2609  Validation loss = 2.0848  \n",
      "\n",
      "Fold: 5  Epoch: 93  Training loss = 1.2603  Validation loss = 2.0831  \n",
      "\n",
      "Fold: 5  Epoch: 94  Training loss = 1.2600  Validation loss = 2.0791  \n",
      "\n",
      "Fold: 5  Epoch: 95  Training loss = 1.2590  Validation loss = 2.0683  \n",
      "\n",
      "Fold: 5  Epoch: 96  Training loss = 1.2579  Validation loss = 2.0601  \n",
      "\n",
      "Fold: 5  Epoch: 97  Training loss = 1.2579  Validation loss = 2.0595  \n",
      "\n",
      "Fold: 5  Epoch: 98  Training loss = 1.2572  Validation loss = 2.0590  \n",
      "\n",
      "Fold: 5  Epoch: 99  Training loss = 1.2567  Validation loss = 2.0618  \n",
      "\n",
      "Fold: 5  Epoch: 100  Training loss = 1.2560  Validation loss = 2.0514  \n",
      "\n",
      "Fold: 5  Epoch: 101  Training loss = 1.2556  Validation loss = 2.0515  \n",
      "\n",
      "Fold: 5  Epoch: 102  Training loss = 1.2548  Validation loss = 2.0448  \n",
      "\n",
      "Fold: 5  Epoch: 103  Training loss = 1.2543  Validation loss = 2.0443  \n",
      "\n",
      "Fold: 5  Epoch: 104  Training loss = 1.2537  Validation loss = 2.0437  \n",
      "\n",
      "Fold: 5  Epoch: 105  Training loss = 1.2531  Validation loss = 2.0433  \n",
      "\n",
      "Fold: 5  Epoch: 106  Training loss = 1.2522  Validation loss = 2.0393  \n",
      "\n",
      "Fold: 5  Epoch: 107  Training loss = 1.2519  Validation loss = 2.0396  \n",
      "\n",
      "Fold: 5  Epoch: 108  Training loss = 1.2514  Validation loss = 2.0390  \n",
      "\n",
      "Fold: 5  Epoch: 109  Training loss = 1.2510  Validation loss = 2.0392  \n",
      "\n",
      "Fold: 5  Epoch: 110  Training loss = 1.2501  Validation loss = 2.0352  \n",
      "\n",
      "Fold: 5  Epoch: 111  Training loss = 1.2492  Validation loss = 2.0263  \n",
      "\n",
      "Fold: 5  Epoch: 112  Training loss = 1.2493  Validation loss = 2.0292  \n",
      "\n",
      "Fold: 5  Epoch: 113  Training loss = 1.2492  Validation loss = 2.0275  \n",
      "\n",
      "Fold: 5  Epoch: 114  Training loss = 1.2485  Validation loss = 2.0232  \n",
      "\n",
      "Fold: 5  Epoch: 115  Training loss = 1.2474  Validation loss = 2.0124  \n",
      "\n",
      "Fold: 5  Epoch: 116  Training loss = 1.2471  Validation loss = 2.0113  \n",
      "\n",
      "Fold: 5  Epoch: 117  Training loss = 1.2465  Validation loss = 2.0047  \n",
      "\n",
      "Fold: 5  Epoch: 118  Training loss = 1.2459  Validation loss = 2.0044  \n",
      "\n",
      "Fold: 5  Epoch: 119  Training loss = 1.2455  Validation loss = 2.0037  \n",
      "\n",
      "Fold: 5  Epoch: 120  Training loss = 1.2453  Validation loss = 1.9981  \n",
      "\n",
      "Fold: 5  Epoch: 121  Training loss = 1.2451  Validation loss = 1.9982  \n",
      "\n",
      "Fold: 5  Epoch: 122  Training loss = 1.2446  Validation loss = 2.0008  \n",
      "\n",
      "Fold: 5  Epoch: 123  Training loss = 1.2441  Validation loss = 1.9890  \n",
      "\n",
      "Fold: 5  Epoch: 124  Training loss = 1.2434  Validation loss = 1.9847  \n",
      "\n",
      "Fold: 5  Epoch: 125  Training loss = 1.2432  Validation loss = 1.9847  \n",
      "\n",
      "Fold: 5  Epoch: 126  Training loss = 1.2429  Validation loss = 1.9788  \n",
      "\n",
      "Fold: 5  Epoch: 127  Training loss = 1.2422  Validation loss = 1.9731  \n",
      "\n",
      "Fold: 5  Epoch: 128  Training loss = 1.2418  Validation loss = 1.9672  \n",
      "\n",
      "Fold: 5  Epoch: 129  Training loss = 1.2414  Validation loss = 1.9631  \n",
      "\n",
      "Fold: 5  Epoch: 130  Training loss = 1.2411  Validation loss = 1.9606  \n",
      "\n",
      "Fold: 5  Epoch: 131  Training loss = 1.2410  Validation loss = 1.9686  \n",
      "\n",
      "Fold: 5  Epoch: 132  Training loss = 1.2403  Validation loss = 1.9663  \n",
      "\n",
      "Fold: 5  Epoch: 133  Training loss = 1.2400  Validation loss = 1.9627  \n",
      "\n",
      "Fold: 5  Epoch: 134  Training loss = 1.2397  Validation loss = 1.9636  \n",
      "\n",
      "Fold: 5  Epoch: 135  Training loss = 1.2395  Validation loss = 1.9655  \n",
      "\n",
      "Fold: 5  Epoch: 136  Training loss = 1.2393  Validation loss = 1.9631  \n",
      "\n",
      "Fold: 5  Epoch: 137  Training loss = 1.2387  Validation loss = 1.9608  \n",
      "\n",
      "Fold: 5  Epoch: 138  Training loss = 1.2380  Validation loss = 1.9530  \n",
      "\n",
      "Fold: 5  Epoch: 139  Training loss = 1.2376  Validation loss = 1.9523  \n",
      "\n",
      "Fold: 5  Epoch: 140  Training loss = 1.2369  Validation loss = 1.9442  \n",
      "\n",
      "Fold: 5  Epoch: 141  Training loss = 1.2364  Validation loss = 1.9438  \n",
      "\n",
      "Fold: 5  Epoch: 142  Training loss = 1.2358  Validation loss = 1.9442  \n",
      "\n",
      "Fold: 5  Epoch: 143  Training loss = 1.2357  Validation loss = 1.9424  \n",
      "\n",
      "Fold: 5  Epoch: 144  Training loss = 1.2353  Validation loss = 1.9397  \n",
      "\n",
      "Fold: 5  Epoch: 145  Training loss = 1.2346  Validation loss = 1.9343  \n",
      "\n",
      "Fold: 5  Epoch: 146  Training loss = 1.2345  Validation loss = 1.9345  \n",
      "\n",
      "Fold: 5  Epoch: 147  Training loss = 1.2343  Validation loss = 1.9319  \n",
      "\n",
      "Fold: 5  Epoch: 148  Training loss = 1.2349  Validation loss = 1.9321  \n",
      "\n",
      "Fold: 5  Epoch: 149  Training loss = 1.2343  Validation loss = 1.9255  \n",
      "\n",
      "Fold: 5  Epoch: 150  Training loss = 1.2330  Validation loss = 1.9233  \n",
      "\n",
      "Fold: 5  Epoch: 151  Training loss = 1.2331  Validation loss = 1.9269  \n",
      "\n",
      "Fold: 5  Epoch: 152  Training loss = 1.2324  Validation loss = 1.9213  \n",
      "\n",
      "Fold: 5  Epoch: 153  Training loss = 1.2321  Validation loss = 1.9175  \n",
      "\n",
      "Fold: 5  Epoch: 154  Training loss = 1.2316  Validation loss = 1.9175  \n",
      "\n",
      "Fold: 5  Epoch: 155  Training loss = 1.2312  Validation loss = 1.9098  \n",
      "\n",
      "Fold: 5  Epoch: 156  Training loss = 1.2307  Validation loss = 1.9073  \n",
      "\n",
      "Fold: 5  Epoch: 157  Training loss = 1.2305  Validation loss = 1.9095  \n",
      "\n",
      "Fold: 5  Epoch: 158  Training loss = 1.2298  Validation loss = 1.9028  \n",
      "\n",
      "Fold: 5  Epoch: 159  Training loss = 1.2293  Validation loss = 1.9003  \n",
      "\n",
      "Fold: 5  Epoch: 160  Training loss = 1.2287  Validation loss = 1.8959  \n",
      "\n",
      "Fold: 5  Epoch: 161  Training loss = 1.2286  Validation loss = 1.8943  \n",
      "\n",
      "Fold: 5  Epoch: 162  Training loss = 1.2283  Validation loss = 1.8893  \n",
      "\n",
      "Fold: 5  Epoch: 163  Training loss = 1.2285  Validation loss = 1.8920  \n",
      "\n",
      "Fold: 5  Epoch: 164  Training loss = 1.2283  Validation loss = 1.8850  \n",
      "\n",
      "Fold: 5  Epoch: 165  Training loss = 1.2285  Validation loss = 1.8851  \n",
      "\n",
      "Fold: 5  Epoch: 166  Training loss = 1.2282  Validation loss = 1.8828  \n",
      "\n",
      "Fold: 5  Epoch: 167  Training loss = 1.2280  Validation loss = 1.8817  \n",
      "\n",
      "Fold: 5  Epoch: 168  Training loss = 1.2276  Validation loss = 1.8804  \n",
      "\n",
      "Fold: 5  Epoch: 169  Training loss = 1.2278  Validation loss = 1.8814  \n",
      "\n",
      "Fold: 5  Epoch: 170  Training loss = 1.2261  Validation loss = 1.8668  \n",
      "\n",
      "Fold: 5  Epoch: 171  Training loss = 1.2266  Validation loss = 1.8664  \n",
      "\n",
      "Fold: 5  Epoch: 172  Training loss = 1.2270  Validation loss = 1.8685  \n",
      "\n",
      "Fold: 5  Epoch: 173  Training loss = 1.2266  Validation loss = 1.8652  \n",
      "\n",
      "Fold: 5  Epoch: 174  Training loss = 1.2254  Validation loss = 1.8545  \n",
      "\n",
      "Fold: 5  Epoch: 175  Training loss = 1.2245  Validation loss = 1.8496  \n",
      "\n",
      "Fold: 5  Epoch: 176  Training loss = 1.2239  Validation loss = 1.8483  \n",
      "\n",
      "Fold: 5  Epoch: 177  Training loss = 1.2237  Validation loss = 1.8495  \n",
      "\n",
      "Fold: 5  Epoch: 178  Training loss = 1.2230  Validation loss = 1.8443  \n",
      "\n",
      "Fold: 5  Epoch: 179  Training loss = 1.2229  Validation loss = 1.8378  \n",
      "\n",
      "Fold: 5  Epoch: 180  Training loss = 1.2224  Validation loss = 1.8382  \n",
      "\n",
      "Fold: 5  Epoch: 181  Training loss = 1.2224  Validation loss = 1.8412  \n",
      "\n",
      "Fold: 5  Epoch: 182  Training loss = 1.2220  Validation loss = 1.8377  \n",
      "\n",
      "Fold: 5  Epoch: 183  Training loss = 1.2217  Validation loss = 1.8337  \n",
      "\n",
      "Fold: 5  Epoch: 184  Training loss = 1.2212  Validation loss = 1.8357  \n",
      "\n",
      "Fold: 5  Epoch: 185  Training loss = 1.2208  Validation loss = 1.8329  \n",
      "\n",
      "Fold: 5  Epoch: 186  Training loss = 1.2206  Validation loss = 1.8308  \n",
      "\n",
      "Fold: 5  Epoch: 187  Training loss = 1.2202  Validation loss = 1.8257  \n",
      "\n",
      "Fold: 5  Epoch: 188  Training loss = 1.2199  Validation loss = 1.8284  \n",
      "\n",
      "Fold: 5  Epoch: 189  Training loss = 1.2192  Validation loss = 1.8269  \n",
      "\n",
      "Fold: 5  Epoch: 190  Training loss = 1.2193  Validation loss = 1.8276  \n",
      "\n",
      "Fold: 5  Epoch: 191  Training loss = 1.2193  Validation loss = 1.8285  \n",
      "\n",
      "Fold: 5  Epoch: 192  Training loss = 1.2192  Validation loss = 1.8240  \n",
      "\n",
      "Fold: 5  Epoch: 193  Training loss = 1.2193  Validation loss = 1.8260  \n",
      "\n",
      "Fold: 5  Epoch: 194  Training loss = 1.2192  Validation loss = 1.8274  \n",
      "\n",
      "Fold: 5  Epoch: 195  Training loss = 1.2183  Validation loss = 1.8278  \n",
      "\n",
      "Fold: 5  Epoch: 196  Training loss = 1.2178  Validation loss = 1.8242  \n",
      "\n",
      "Fold: 5  Epoch: 197  Training loss = 1.2172  Validation loss = 1.8236  \n",
      "\n",
      "Fold: 5  Epoch: 198  Training loss = 1.2169  Validation loss = 1.8230  \n",
      "\n",
      "Fold: 5  Epoch: 199  Training loss = 1.2164  Validation loss = 1.8203  \n",
      "\n",
      "Fold: 5  Epoch: 200  Training loss = 1.2162  Validation loss = 1.8200  \n",
      "\n",
      "Fold: 5  Epoch: 201  Training loss = 1.2164  Validation loss = 1.8177  \n",
      "\n",
      "Fold: 5  Epoch: 202  Training loss = 1.2160  Validation loss = 1.8132  \n",
      "\n",
      "Fold: 5  Epoch: 203  Training loss = 1.2157  Validation loss = 1.8094  \n",
      "\n",
      "Fold: 5  Epoch: 204  Training loss = 1.2154  Validation loss = 1.8034  \n",
      "\n",
      "Fold: 5  Epoch: 205  Training loss = 1.2149  Validation loss = 1.8027  \n",
      "\n",
      "Fold: 5  Epoch: 206  Training loss = 1.2146  Validation loss = 1.8015  \n",
      "\n",
      "Fold: 5  Epoch: 207  Training loss = 1.2148  Validation loss = 1.8031  \n",
      "\n",
      "Fold: 5  Epoch: 208  Training loss = 1.2145  Validation loss = 1.8028  \n",
      "\n",
      "Fold: 5  Epoch: 209  Training loss = 1.2132  Validation loss = 1.7881  \n",
      "\n",
      "Fold: 5  Epoch: 210  Training loss = 1.2130  Validation loss = 1.7842  \n",
      "\n",
      "Fold: 5  Epoch: 211  Training loss = 1.2128  Validation loss = 1.7809  \n",
      "\n",
      "Fold: 5  Epoch: 212  Training loss = 1.2121  Validation loss = 1.7759  \n",
      "\n",
      "Fold: 5  Epoch: 213  Training loss = 1.2118  Validation loss = 1.7681  \n",
      "\n",
      "Fold: 5  Epoch: 214  Training loss = 1.2114  Validation loss = 1.7712  \n",
      "\n",
      "Fold: 5  Epoch: 215  Training loss = 1.2114  Validation loss = 1.7756  \n",
      "\n",
      "Fold: 5  Epoch: 216  Training loss = 1.2110  Validation loss = 1.7725  \n",
      "\n",
      "Fold: 5  Epoch: 217  Training loss = 1.2109  Validation loss = 1.7682  \n",
      "\n",
      "Fold: 5  Epoch: 218  Training loss = 1.2106  Validation loss = 1.7686  \n",
      "\n",
      "Fold: 5  Epoch: 219  Training loss = 1.2104  Validation loss = 1.7681  \n",
      "\n",
      "Fold: 5  Epoch: 220  Training loss = 1.2104  Validation loss = 1.7656  \n",
      "\n",
      "Fold: 5  Epoch: 221  Training loss = 1.2092  Validation loss = 1.7575  \n",
      "\n",
      "Fold: 5  Epoch: 222  Training loss = 1.2086  Validation loss = 1.7572  \n",
      "\n",
      "Fold: 5  Epoch: 223  Training loss = 1.2084  Validation loss = 1.7547  \n",
      "\n",
      "Fold: 5  Epoch: 224  Training loss = 1.2082  Validation loss = 1.7479  \n",
      "\n",
      "Fold: 5  Epoch: 225  Training loss = 1.2080  Validation loss = 1.7444  \n",
      "\n",
      "Fold: 5  Epoch: 226  Training loss = 1.2074  Validation loss = 1.7393  \n",
      "\n",
      "Fold: 5  Epoch: 227  Training loss = 1.2071  Validation loss = 1.7385  \n",
      "\n",
      "Fold: 5  Epoch: 228  Training loss = 1.2070  Validation loss = 1.7350  \n",
      "\n",
      "Fold: 5  Epoch: 229  Training loss = 1.2065  Validation loss = 1.7319  \n",
      "\n",
      "Fold: 5  Epoch: 230  Training loss = 1.2061  Validation loss = 1.7297  \n",
      "\n",
      "Fold: 5  Epoch: 231  Training loss = 1.2055  Validation loss = 1.7234  \n",
      "\n",
      "Fold: 5  Epoch: 232  Training loss = 1.2052  Validation loss = 1.7225  \n",
      "\n",
      "Fold: 5  Epoch: 233  Training loss = 1.2045  Validation loss = 1.7063  \n",
      "\n",
      "Fold: 5  Epoch: 234  Training loss = 1.2042  Validation loss = 1.7077  \n",
      "\n",
      "Fold: 5  Epoch: 235  Training loss = 1.2042  Validation loss = 1.7067  \n",
      "\n",
      "Fold: 5  Epoch: 236  Training loss = 1.2037  Validation loss = 1.7061  \n",
      "\n",
      "Fold: 5  Epoch: 237  Training loss = 1.2034  Validation loss = 1.7032  \n",
      "\n",
      "Fold: 5  Epoch: 238  Training loss = 1.2030  Validation loss = 1.7017  \n",
      "\n",
      "Fold: 5  Epoch: 239  Training loss = 1.2025  Validation loss = 1.6981  \n",
      "\n",
      "Fold: 5  Epoch: 240  Training loss = 1.2028  Validation loss = 1.7040  \n",
      "\n",
      "Fold: 5  Epoch: 241  Training loss = 1.2028  Validation loss = 1.7032  \n",
      "\n",
      "Fold: 5  Epoch: 242  Training loss = 1.2021  Validation loss = 1.7005  \n",
      "\n",
      "Fold: 5  Epoch: 243  Training loss = 1.2018  Validation loss = 1.6970  \n",
      "\n",
      "Fold: 5  Epoch: 244  Training loss = 1.2016  Validation loss = 1.6970  \n",
      "\n",
      "Fold: 5  Epoch: 245  Training loss = 1.2014  Validation loss = 1.6990  \n",
      "\n",
      "Fold: 5  Epoch: 246  Training loss = 1.2010  Validation loss = 1.6957  \n",
      "\n",
      "Fold: 5  Epoch: 247  Training loss = 1.2014  Validation loss = 1.6989  \n",
      "\n",
      "Fold: 5  Epoch: 248  Training loss = 1.2019  Validation loss = 1.7008  \n",
      "\n",
      "Fold: 5  Epoch: 249  Training loss = 1.2015  Validation loss = 1.6987  \n",
      "\n",
      "Fold: 5  Epoch: 250  Training loss = 1.2010  Validation loss = 1.6881  \n",
      "\n",
      "Fold: 5  Epoch: 251  Training loss = 1.2006  Validation loss = 1.6865  \n",
      "\n",
      "Fold: 5  Epoch: 252  Training loss = 1.2007  Validation loss = 1.6850  \n",
      "\n",
      "Fold: 5  Epoch: 253  Training loss = 1.2002  Validation loss = 1.6861  \n",
      "\n",
      "Fold: 5  Epoch: 254  Training loss = 1.2000  Validation loss = 1.6874  \n",
      "\n",
      "Fold: 5  Epoch: 255  Training loss = 1.1996  Validation loss = 1.6829  \n",
      "\n",
      "Fold: 5  Epoch: 256  Training loss = 1.1994  Validation loss = 1.6823  \n",
      "\n",
      "Fold: 5  Epoch: 257  Training loss = 1.1999  Validation loss = 1.6853  \n",
      "\n",
      "Fold: 5  Epoch: 258  Training loss = 1.1994  Validation loss = 1.6806  \n",
      "\n",
      "Fold: 5  Epoch: 259  Training loss = 1.1991  Validation loss = 1.6796  \n",
      "\n",
      "Fold: 5  Epoch: 260  Training loss = 1.1984  Validation loss = 1.6735  \n",
      "\n",
      "Fold: 5  Epoch: 261  Training loss = 1.1979  Validation loss = 1.6652  \n",
      "\n",
      "Fold: 5  Epoch: 262  Training loss = 1.1975  Validation loss = 1.6628  \n",
      "\n",
      "Fold: 5  Epoch: 263  Training loss = 1.1971  Validation loss = 1.6635  \n",
      "\n",
      "Fold: 5  Epoch: 264  Training loss = 1.1968  Validation loss = 1.6635  \n",
      "\n",
      "Fold: 5  Epoch: 265  Training loss = 1.1966  Validation loss = 1.6614  \n",
      "\n",
      "Fold: 5  Epoch: 266  Training loss = 1.1961  Validation loss = 1.6605  \n",
      "\n",
      "Fold: 5  Epoch: 267  Training loss = 1.1958  Validation loss = 1.6547  \n",
      "\n",
      "Fold: 5  Epoch: 268  Training loss = 1.1956  Validation loss = 1.6510  \n",
      "\n",
      "Fold: 5  Epoch: 269  Training loss = 1.1953  Validation loss = 1.6502  \n",
      "\n",
      "Fold: 5  Epoch: 270  Training loss = 1.1955  Validation loss = 1.6427  \n",
      "\n",
      "Fold: 5  Epoch: 271  Training loss = 1.1950  Validation loss = 1.6434  \n",
      "\n",
      "Fold: 5  Epoch: 272  Training loss = 1.1950  Validation loss = 1.6382  \n",
      "\n",
      "Fold: 5  Epoch: 273  Training loss = 1.1948  Validation loss = 1.6362  \n",
      "\n",
      "Fold: 5  Epoch: 274  Training loss = 1.1943  Validation loss = 1.6435  \n",
      "\n",
      "Fold: 5  Epoch: 275  Training loss = 1.1941  Validation loss = 1.6393  \n",
      "\n",
      "Fold: 5  Epoch: 276  Training loss = 1.1937  Validation loss = 1.6419  \n",
      "\n",
      "Fold: 5  Epoch: 277  Training loss = 1.1934  Validation loss = 1.6406  \n",
      "\n",
      "Fold: 5  Epoch: 278  Training loss = 1.1932  Validation loss = 1.6364  \n",
      "\n",
      "Fold: 5  Epoch: 279  Training loss = 1.1929  Validation loss = 1.6309  \n",
      "\n",
      "Fold: 5  Epoch: 280  Training loss = 1.1926  Validation loss = 1.6320  \n",
      "\n",
      "Fold: 5  Epoch: 281  Training loss = 1.1923  Validation loss = 1.6311  \n",
      "\n",
      "Fold: 5  Epoch: 282  Training loss = 1.1923  Validation loss = 1.6308  \n",
      "\n",
      "Fold: 5  Epoch: 283  Training loss = 1.1937  Validation loss = 1.6385  \n",
      "\n",
      "Fold: 5  Epoch: 284  Training loss = 1.1931  Validation loss = 1.6367  \n",
      "\n",
      "Fold: 5  Epoch: 285  Training loss = 1.1926  Validation loss = 1.6338  \n",
      "\n",
      "Fold: 5  Epoch: 286  Training loss = 1.1919  Validation loss = 1.6305  \n",
      "\n",
      "Fold: 5  Epoch: 287  Training loss = 1.1910  Validation loss = 1.6263  \n",
      "\n",
      "Fold: 5  Epoch: 288  Training loss = 1.1906  Validation loss = 1.6242  \n",
      "\n",
      "Fold: 5  Epoch: 289  Training loss = 1.1902  Validation loss = 1.6219  \n",
      "\n",
      "Fold: 5  Epoch: 290  Training loss = 1.1900  Validation loss = 1.6201  \n",
      "\n",
      "Fold: 5  Epoch: 291  Training loss = 1.1897  Validation loss = 1.6160  \n",
      "\n",
      "Fold: 5  Epoch: 292  Training loss = 1.1897  Validation loss = 1.6135  \n",
      "\n",
      "Fold: 5  Epoch: 293  Training loss = 1.1894  Validation loss = 1.6153  \n",
      "\n",
      "Fold: 5  Epoch: 294  Training loss = 1.1894  Validation loss = 1.6111  \n",
      "\n",
      "Fold: 5  Epoch: 295  Training loss = 1.1889  Validation loss = 1.6137  \n",
      "\n",
      "Fold: 5  Epoch: 296  Training loss = 1.1887  Validation loss = 1.6093  \n",
      "\n",
      "Fold: 5  Epoch: 297  Training loss = 1.1884  Validation loss = 1.6090  \n",
      "\n",
      "Fold: 5  Epoch: 298  Training loss = 1.1882  Validation loss = 1.6070  \n",
      "\n",
      "Fold: 5  Epoch: 299  Training loss = 1.1880  Validation loss = 1.6080  \n",
      "\n",
      "Fold: 5  Epoch: 300  Training loss = 1.1878  Validation loss = 1.6042  \n",
      "\n",
      "Fold: 5  Epoch: 301  Training loss = 1.1881  Validation loss = 1.6100  \n",
      "\n",
      "Fold: 5  Epoch: 302  Training loss = 1.1875  Validation loss = 1.6022  \n",
      "\n",
      "Fold: 5  Epoch: 303  Training loss = 1.1872  Validation loss = 1.5985  \n",
      "\n",
      "Fold: 5  Epoch: 304  Training loss = 1.1869  Validation loss = 1.5945  \n",
      "\n",
      "Fold: 5  Epoch: 305  Training loss = 1.1869  Validation loss = 1.5934  \n",
      "\n",
      "Fold: 5  Epoch: 306  Training loss = 1.1867  Validation loss = 1.5914  \n",
      "\n",
      "Fold: 5  Epoch: 307  Training loss = 1.1864  Validation loss = 1.5907  \n",
      "\n",
      "Fold: 5  Epoch: 308  Training loss = 1.1861  Validation loss = 1.5942  \n",
      "\n",
      "Fold: 5  Epoch: 309  Training loss = 1.1860  Validation loss = 1.5927  \n",
      "\n",
      "Fold: 5  Epoch: 310  Training loss = 1.1861  Validation loss = 1.5996  \n",
      "\n",
      "Fold: 5  Epoch: 311  Training loss = 1.1860  Validation loss = 1.5997  \n",
      "\n",
      "Fold: 5  Epoch: 312  Training loss = 1.1855  Validation loss = 1.5909  \n",
      "\n",
      "Fold: 5  Epoch: 313  Training loss = 1.1854  Validation loss = 1.5871  \n",
      "\n",
      "Fold: 5  Epoch: 314  Training loss = 1.1852  Validation loss = 1.5810  \n",
      "\n",
      "Fold: 5  Epoch: 315  Training loss = 1.1844  Validation loss = 1.5809  \n",
      "\n",
      "Fold: 5  Epoch: 316  Training loss = 1.1840  Validation loss = 1.5793  \n",
      "\n",
      "Fold: 5  Epoch: 317  Training loss = 1.1838  Validation loss = 1.5778  \n",
      "\n",
      "Fold: 5  Epoch: 318  Training loss = 1.1836  Validation loss = 1.5756  \n",
      "\n",
      "Fold: 5  Epoch: 319  Training loss = 1.1842  Validation loss = 1.5806  \n",
      "\n",
      "Fold: 5  Epoch: 320  Training loss = 1.1841  Validation loss = 1.5757  \n",
      "\n",
      "Fold: 5  Epoch: 321  Training loss = 1.1834  Validation loss = 1.5731  \n",
      "\n",
      "Fold: 5  Epoch: 322  Training loss = 1.1831  Validation loss = 1.5676  \n",
      "\n",
      "Fold: 5  Epoch: 323  Training loss = 1.1826  Validation loss = 1.5631  \n",
      "\n",
      "Fold: 5  Epoch: 324  Training loss = 1.1826  Validation loss = 1.5640  \n",
      "\n",
      "Fold: 5  Epoch: 325  Training loss = 1.1829  Validation loss = 1.5689  \n",
      "\n",
      "Fold: 5  Epoch: 326  Training loss = 1.1823  Validation loss = 1.5602  \n",
      "\n",
      "Fold: 5  Epoch: 327  Training loss = 1.1820  Validation loss = 1.5542  \n",
      "\n",
      "Fold: 5  Epoch: 328  Training loss = 1.1820  Validation loss = 1.5554  \n",
      "\n",
      "Fold: 5  Epoch: 329  Training loss = 1.1817  Validation loss = 1.5541  \n",
      "\n",
      "Fold: 5  Epoch: 330  Training loss = 1.1818  Validation loss = 1.5541  \n",
      "\n",
      "Fold: 5  Epoch: 331  Training loss = 1.1811  Validation loss = 1.5522  \n",
      "\n",
      "Fold: 5  Epoch: 332  Training loss = 1.1810  Validation loss = 1.5457  \n",
      "\n",
      "Fold: 5  Epoch: 333  Training loss = 1.1812  Validation loss = 1.5501  \n",
      "\n",
      "Fold: 5  Epoch: 334  Training loss = 1.1810  Validation loss = 1.5493  \n",
      "\n",
      "Fold: 5  Epoch: 335  Training loss = 1.1805  Validation loss = 1.5407  \n",
      "\n",
      "Fold: 5  Epoch: 336  Training loss = 1.1803  Validation loss = 1.5336  \n",
      "\n",
      "Fold: 5  Epoch: 337  Training loss = 1.1799  Validation loss = 1.5369  \n",
      "\n",
      "Fold: 5  Epoch: 338  Training loss = 1.1796  Validation loss = 1.5377  \n",
      "\n",
      "Fold: 5  Epoch: 339  Training loss = 1.1794  Validation loss = 1.5413  \n",
      "\n",
      "Fold: 5  Epoch: 340  Training loss = 1.1792  Validation loss = 1.5400  \n",
      "\n",
      "Fold: 5  Epoch: 341  Training loss = 1.1790  Validation loss = 1.5346  \n",
      "\n",
      "Fold: 5  Epoch: 342  Training loss = 1.1788  Validation loss = 1.5349  \n",
      "\n",
      "Fold: 5  Epoch: 343  Training loss = 1.1783  Validation loss = 1.5300  \n",
      "\n",
      "Fold: 5  Epoch: 344  Training loss = 1.1784  Validation loss = 1.5278  \n",
      "\n",
      "Fold: 5  Epoch: 345  Training loss = 1.1783  Validation loss = 1.5273  \n",
      "\n",
      "Fold: 5  Epoch: 346  Training loss = 1.1781  Validation loss = 1.5258  \n",
      "\n",
      "Fold: 5  Epoch: 347  Training loss = 1.1782  Validation loss = 1.5215  \n",
      "\n",
      "Fold: 5  Epoch: 348  Training loss = 1.1777  Validation loss = 1.5211  \n",
      "\n",
      "Fold: 5  Epoch: 349  Training loss = 1.1771  Validation loss = 1.5228  \n",
      "\n",
      "Fold: 5  Epoch: 350  Training loss = 1.1771  Validation loss = 1.5176  \n",
      "\n",
      "Fold: 5  Epoch: 351  Training loss = 1.1770  Validation loss = 1.5129  \n",
      "\n",
      "Fold: 5  Epoch: 352  Training loss = 1.1767  Validation loss = 1.5108  \n",
      "\n",
      "Fold: 5  Epoch: 353  Training loss = 1.1766  Validation loss = 1.5088  \n",
      "\n",
      "Fold: 5  Epoch: 354  Training loss = 1.1764  Validation loss = 1.5117  \n",
      "\n",
      "Fold: 5  Epoch: 355  Training loss = 1.1761  Validation loss = 1.5124  \n",
      "\n",
      "Fold: 5  Epoch: 356  Training loss = 1.1759  Validation loss = 1.5096  \n",
      "\n",
      "Fold: 5  Epoch: 357  Training loss = 1.1762  Validation loss = 1.5126  \n",
      "\n",
      "Fold: 5  Epoch: 358  Training loss = 1.1757  Validation loss = 1.5084  \n",
      "\n",
      "Fold: 5  Epoch: 359  Training loss = 1.1757  Validation loss = 1.5091  \n",
      "\n",
      "Fold: 5  Epoch: 360  Training loss = 1.1756  Validation loss = 1.5080  \n",
      "\n",
      "Fold: 5  Epoch: 361  Training loss = 1.1752  Validation loss = 1.5020  \n",
      "\n",
      "Fold: 5  Epoch: 362  Training loss = 1.1747  Validation loss = 1.4987  \n",
      "\n",
      "Fold: 5  Epoch: 363  Training loss = 1.1743  Validation loss = 1.4961  \n",
      "\n",
      "Fold: 5  Epoch: 364  Training loss = 1.1740  Validation loss = 1.4983  \n",
      "\n",
      "Fold: 5  Epoch: 365  Training loss = 1.1739  Validation loss = 1.4927  \n",
      "\n",
      "Fold: 5  Epoch: 366  Training loss = 1.1740  Validation loss = 1.4897  \n",
      "\n",
      "Fold: 5  Epoch: 367  Training loss = 1.1735  Validation loss = 1.4887  \n",
      "\n",
      "Fold: 5  Epoch: 368  Training loss = 1.1732  Validation loss = 1.4907  \n",
      "\n",
      "Fold: 5  Epoch: 369  Training loss = 1.1730  Validation loss = 1.4901  \n",
      "\n",
      "Fold: 5  Epoch: 370  Training loss = 1.1727  Validation loss = 1.4884  \n",
      "\n",
      "Fold: 5  Epoch: 371  Training loss = 1.1727  Validation loss = 1.4880  \n",
      "\n",
      "Fold: 5  Epoch: 372  Training loss = 1.1724  Validation loss = 1.4901  \n",
      "\n",
      "Fold: 5  Epoch: 373  Training loss = 1.1722  Validation loss = 1.4900  \n",
      "\n",
      "Fold: 5  Epoch: 374  Training loss = 1.1726  Validation loss = 1.4932  \n",
      "\n",
      "Fold: 5  Epoch: 375  Training loss = 1.1720  Validation loss = 1.4842  \n",
      "\n",
      "Fold: 5  Epoch: 376  Training loss = 1.1719  Validation loss = 1.4837  \n",
      "\n",
      "Fold: 5  Epoch: 377  Training loss = 1.1718  Validation loss = 1.4784  \n",
      "\n",
      "Fold: 5  Epoch: 378  Training loss = 1.1717  Validation loss = 1.4771  \n",
      "\n",
      "Fold: 5  Epoch: 379  Training loss = 1.1713  Validation loss = 1.4732  \n",
      "\n",
      "Fold: 5  Epoch: 380  Training loss = 1.1712  Validation loss = 1.4735  \n",
      "\n",
      "Fold: 5  Epoch: 381  Training loss = 1.1712  Validation loss = 1.4679  \n",
      "\n",
      "Fold: 5  Epoch: 382  Training loss = 1.1703  Validation loss = 1.4738  \n",
      "\n",
      "Fold: 5  Epoch: 383  Training loss = 1.1703  Validation loss = 1.4754  \n",
      "\n",
      "Fold: 5  Epoch: 384  Training loss = 1.1701  Validation loss = 1.4692  \n",
      "\n",
      "Fold: 5  Epoch: 385  Training loss = 1.1706  Validation loss = 1.4620  \n",
      "\n",
      "Fold: 5  Epoch: 386  Training loss = 1.1710  Validation loss = 1.4598  \n",
      "\n",
      "Fold: 5  Epoch: 387  Training loss = 1.1706  Validation loss = 1.4611  \n",
      "\n",
      "Fold: 5  Epoch: 388  Training loss = 1.1706  Validation loss = 1.4591  \n",
      "\n",
      "Fold: 5  Epoch: 389  Training loss = 1.1699  Validation loss = 1.4577  \n",
      "\n",
      "Fold: 5  Epoch: 390  Training loss = 1.1695  Validation loss = 1.4565  \n",
      "\n",
      "Fold: 5  Epoch: 391  Training loss = 1.1690  Validation loss = 1.4554  \n",
      "\n",
      "Fold: 5  Epoch: 392  Training loss = 1.1686  Validation loss = 1.4601  \n",
      "\n",
      "Fold: 5  Epoch: 393  Training loss = 1.1683  Validation loss = 1.4613  \n",
      "\n",
      "Fold: 5  Epoch: 394  Training loss = 1.1683  Validation loss = 1.4605  \n",
      "\n",
      "Fold: 5  Epoch: 395  Training loss = 1.1681  Validation loss = 1.4588  \n",
      "\n",
      "Fold: 5  Epoch: 396  Training loss = 1.1686  Validation loss = 1.4582  \n",
      "\n",
      "Fold: 5  Epoch: 397  Training loss = 1.1696  Validation loss = 1.4616  \n",
      "\n",
      "Fold: 5  Epoch: 398  Training loss = 1.1681  Validation loss = 1.4553  \n",
      "\n",
      "Fold: 5  Epoch: 399  Training loss = 1.1677  Validation loss = 1.4488  \n",
      "\n",
      "Fold: 5  Epoch: 400  Training loss = 1.1675  Validation loss = 1.4464  \n",
      "\n",
      "Fold: 5  Epoch: 401  Training loss = 1.1673  Validation loss = 1.4452  \n",
      "\n",
      "Fold: 5  Epoch: 402  Training loss = 1.1681  Validation loss = 1.4509  \n",
      "\n",
      "Fold: 5  Epoch: 403  Training loss = 1.1687  Validation loss = 1.4545  \n",
      "\n",
      "Fold: 5  Epoch: 404  Training loss = 1.1704  Validation loss = 1.4572  \n",
      "\n",
      "Fold: 5  Epoch: 405  Training loss = 1.1698  Validation loss = 1.4542  \n",
      "\n",
      "Fold: 5  Epoch: 406  Training loss = 1.1694  Validation loss = 1.4526  \n",
      "\n",
      "Fold: 5  Epoch: 407  Training loss = 1.1683  Validation loss = 1.4474  \n",
      "\n",
      "Fold: 5  Epoch: 408  Training loss = 1.1680  Validation loss = 1.4441  \n",
      "\n",
      "Fold: 5  Epoch: 409  Training loss = 1.1660  Validation loss = 1.4367  \n",
      "\n",
      "Fold: 5  Epoch: 410  Training loss = 1.1658  Validation loss = 1.4366  \n",
      "\n",
      "Fold: 5  Epoch: 411  Training loss = 1.1655  Validation loss = 1.4351  \n",
      "\n",
      "Fold: 5  Epoch: 412  Training loss = 1.1661  Validation loss = 1.4353  \n",
      "\n",
      "Fold: 5  Epoch: 413  Training loss = 1.1666  Validation loss = 1.4376  \n",
      "\n",
      "Fold: 5  Epoch: 414  Training loss = 1.1659  Validation loss = 1.4308  \n",
      "\n",
      "Fold: 5  Epoch: 415  Training loss = 1.1660  Validation loss = 1.4300  \n",
      "\n",
      "Fold: 5  Epoch: 416  Training loss = 1.1662  Validation loss = 1.4323  \n",
      "\n",
      "Fold: 5  Epoch: 417  Training loss = 1.1657  Validation loss = 1.4304  \n",
      "\n",
      "Fold: 5  Epoch: 418  Training loss = 1.1657  Validation loss = 1.4313  \n",
      "\n",
      "Fold: 5  Epoch: 419  Training loss = 1.1648  Validation loss = 1.4252  \n",
      "\n",
      "Fold: 5  Epoch: 420  Training loss = 1.1651  Validation loss = 1.4285  \n",
      "\n",
      "Fold: 5  Epoch: 421  Training loss = 1.1643  Validation loss = 1.4250  \n",
      "\n",
      "Fold: 5  Epoch: 422  Training loss = 1.1643  Validation loss = 1.4223  \n",
      "\n",
      "Fold: 5  Epoch: 423  Training loss = 1.1639  Validation loss = 1.4213  \n",
      "\n",
      "Fold: 5  Epoch: 424  Training loss = 1.1638  Validation loss = 1.4175  \n",
      "\n",
      "Fold: 5  Epoch: 425  Training loss = 1.1638  Validation loss = 1.4201  \n",
      "\n",
      "Fold: 5  Epoch: 426  Training loss = 1.1637  Validation loss = 1.4177  \n",
      "\n",
      "Fold: 5  Epoch: 427  Training loss = 1.1633  Validation loss = 1.4188  \n",
      "\n",
      "Fold: 5  Epoch: 428  Training loss = 1.1632  Validation loss = 1.4194  \n",
      "\n",
      "Fold: 5  Epoch: 429  Training loss = 1.1634  Validation loss = 1.4203  \n",
      "\n",
      "Fold: 5  Epoch: 430  Training loss = 1.1647  Validation loss = 1.4208  \n",
      "\n",
      "Fold: 5  Epoch: 431  Training loss = 1.1653  Validation loss = 1.4221  \n",
      "\n",
      "Fold: 5  Epoch: 432  Training loss = 1.1661  Validation loss = 1.4212  \n",
      "\n",
      "Fold: 5  Epoch: 433  Training loss = 1.1661  Validation loss = 1.4211  \n",
      "\n",
      "Fold: 5  Epoch: 434  Training loss = 1.1639  Validation loss = 1.4156  \n",
      "\n",
      "Fold: 5  Epoch: 435  Training loss = 1.1623  Validation loss = 1.4141  \n",
      "\n",
      "Fold: 5  Epoch: 436  Training loss = 1.1623  Validation loss = 1.4156  \n",
      "\n",
      "Fold: 5  Epoch: 437  Training loss = 1.1613  Validation loss = 1.4133  \n",
      "\n",
      "Fold: 5  Epoch: 438  Training loss = 1.1607  Validation loss = 1.4083  \n",
      "\n",
      "Fold: 5  Epoch: 439  Training loss = 1.1641  Validation loss = 1.4182  \n",
      "\n",
      "Fold: 5  Epoch: 440  Training loss = 1.1605  Validation loss = 1.4087  \n",
      "\n",
      "Fold: 5  Epoch: 441  Training loss = 1.1603  Validation loss = 1.4082  \n",
      "\n",
      "Fold: 5  Epoch: 442  Training loss = 1.1607  Validation loss = 1.4108  \n",
      "\n",
      "Fold: 5  Epoch: 443  Training loss = 1.1605  Validation loss = 1.4117  \n",
      "\n",
      "Fold: 5  Epoch: 444  Training loss = 1.1601  Validation loss = 1.4086  \n",
      "\n",
      "Fold: 5  Epoch: 445  Training loss = 1.1609  Validation loss = 1.4123  \n",
      "\n",
      "Fold: 5  Epoch: 446  Training loss = 1.1611  Validation loss = 1.4120  \n",
      "\n",
      "Fold: 5  Epoch: 447  Training loss = 1.1611  Validation loss = 1.4108  \n",
      "\n",
      "Fold: 5  Epoch: 448  Training loss = 1.1605  Validation loss = 1.4097  \n",
      "\n",
      "Fold: 5  Epoch: 449  Training loss = 1.1593  Validation loss = 1.4059  \n",
      "\n",
      "Fold: 5  Epoch: 450  Training loss = 1.1590  Validation loss = 1.4035  \n",
      "\n",
      "Fold: 5  Epoch: 451  Training loss = 1.1590  Validation loss = 1.3951  \n",
      "\n",
      "Fold: 5  Epoch: 452  Training loss = 1.1587  Validation loss = 1.3943  \n",
      "\n",
      "Fold: 5  Epoch: 453  Training loss = 1.1585  Validation loss = 1.3931  \n",
      "\n",
      "Fold: 5  Epoch: 454  Training loss = 1.1584  Validation loss = 1.3970  \n",
      "\n",
      "Fold: 5  Epoch: 455  Training loss = 1.1584  Validation loss = 1.3967  \n",
      "\n",
      "Fold: 5  Epoch: 456  Training loss = 1.1579  Validation loss = 1.3915  \n",
      "\n",
      "Fold: 5  Epoch: 457  Training loss = 1.1582  Validation loss = 1.3972  \n",
      "\n",
      "Fold: 5  Epoch: 458  Training loss = 1.1578  Validation loss = 1.3953  \n",
      "\n",
      "Fold: 5  Epoch: 459  Training loss = 1.1578  Validation loss = 1.3966  \n",
      "\n",
      "Fold: 5  Epoch: 460  Training loss = 1.1576  Validation loss = 1.3942  \n",
      "\n",
      "Fold: 5  Epoch: 461  Training loss = 1.1574  Validation loss = 1.3902  \n",
      "\n",
      "Fold: 5  Epoch: 462  Training loss = 1.1575  Validation loss = 1.3937  \n",
      "\n",
      "Fold: 5  Epoch: 463  Training loss = 1.1587  Validation loss = 1.3843  \n",
      "\n",
      "Fold: 5  Epoch: 464  Training loss = 1.1575  Validation loss = 1.3876  \n",
      "\n",
      "Fold: 5  Epoch: 465  Training loss = 1.1588  Validation loss = 1.3826  \n",
      "\n",
      "Fold: 5  Epoch: 466  Training loss = 1.1599  Validation loss = 1.3779  \n",
      "\n",
      "Fold: 5  Epoch: 467  Training loss = 1.1617  Validation loss = 1.3743  \n",
      "\n",
      "Fold: 5  Epoch: 468  Training loss = 1.1614  Validation loss = 1.3763  \n",
      "\n",
      "Fold: 5  Epoch: 469  Training loss = 1.1642  Validation loss = 1.3711  \n",
      "\n",
      "Fold: 5  Epoch: 470  Training loss = 1.1614  Validation loss = 1.3740  \n",
      "\n",
      "Fold: 5  Epoch: 471  Training loss = 1.1589  Validation loss = 1.3805  \n",
      "\n",
      "Fold: 5  Epoch: 472  Training loss = 1.1572  Validation loss = 1.3837  \n",
      "\n",
      "Fold: 5  Epoch: 473  Training loss = 1.1571  Validation loss = 1.3838  \n",
      "\n",
      "Fold: 5  Epoch: 474  Training loss = 1.1570  Validation loss = 1.3822  \n",
      "\n",
      "Fold: 5  Epoch: 475  Training loss = 1.1576  Validation loss = 1.3797  \n",
      "\n",
      "Fold: 5  Epoch: 476  Training loss = 1.1570  Validation loss = 1.3807  \n",
      "\n",
      "Fold: 5  Epoch: 477  Training loss = 1.1571  Validation loss = 1.3790  \n",
      "\n",
      "Fold: 5  Epoch: 478  Training loss = 1.1585  Validation loss = 1.3719  \n",
      "\n",
      "Fold: 5  Epoch: 479  Training loss = 1.1581  Validation loss = 1.3733  \n",
      "\n",
      "Fold: 5  Epoch: 480  Training loss = 1.1584  Validation loss = 1.3749  \n",
      "\n",
      "Fold: 5  Epoch: 481  Training loss = 1.1576  Validation loss = 1.3752  \n",
      "\n",
      "Fold: 5  Epoch: 482  Training loss = 1.1569  Validation loss = 1.3772  \n",
      "\n",
      "Fold: 5  Epoch: 483  Training loss = 1.1563  Validation loss = 1.3735  \n",
      "\n",
      "Fold: 5  Epoch: 484  Training loss = 1.1565  Validation loss = 1.3645  \n",
      "\n",
      "Fold: 5  Epoch: 485  Training loss = 1.1569  Validation loss = 1.3607  \n",
      "\n",
      "Fold: 5  Epoch: 486  Training loss = 1.1549  Validation loss = 1.3645  \n",
      "\n",
      "Fold: 5  Epoch: 487  Training loss = 1.1549  Validation loss = 1.3708  \n",
      "\n",
      "Fold: 5  Epoch: 488  Training loss = 1.1547  Validation loss = 1.3686  \n",
      "\n",
      "Fold: 5  Epoch: 489  Training loss = 1.1543  Validation loss = 1.3694  \n",
      "\n",
      "Fold: 5  Epoch: 490  Training loss = 1.1544  Validation loss = 1.3712  \n",
      "\n",
      "Fold: 5  Epoch: 491  Training loss = 1.1537  Validation loss = 1.3677  \n",
      "\n",
      "Fold: 5  Epoch: 492  Training loss = 1.1535  Validation loss = 1.3645  \n",
      "\n",
      "Fold: 5  Epoch: 493  Training loss = 1.1536  Validation loss = 1.3663  \n",
      "\n",
      "Fold: 5  Epoch: 494  Training loss = 1.1535  Validation loss = 1.3642  \n",
      "\n",
      "Fold: 5  Epoch: 495  Training loss = 1.1532  Validation loss = 1.3640  \n",
      "\n",
      "Fold: 5  Epoch: 496  Training loss = 1.1531  Validation loss = 1.3606  \n",
      "\n",
      "Fold: 5  Epoch: 497  Training loss = 1.1537  Validation loss = 1.3610  \n",
      "\n",
      "Fold: 5  Epoch: 498  Training loss = 1.1529  Validation loss = 1.3593  \n",
      "\n",
      "Fold: 5  Epoch: 499  Training loss = 1.1520  Validation loss = 1.3554  \n",
      "\n",
      "Fold: 5  Epoch: 500  Training loss = 1.1513  Validation loss = 1.3497  \n",
      "\n",
      "Fold: 5  Epoch: 501  Training loss = 1.1513  Validation loss = 1.3475  \n",
      "\n",
      "Fold: 5  Epoch: 502  Training loss = 1.1510  Validation loss = 1.3464  \n",
      "\n",
      "Fold: 5  Epoch: 503  Training loss = 1.1522  Validation loss = 1.3550  \n",
      "\n",
      "Fold: 5  Epoch: 504  Training loss = 1.1526  Validation loss = 1.3539  \n",
      "\n",
      "Fold: 5  Epoch: 505  Training loss = 1.1540  Validation loss = 1.3566  \n",
      "\n",
      "Fold: 5  Epoch: 506  Training loss = 1.1546  Validation loss = 1.3583  \n",
      "\n",
      "Fold: 5  Epoch: 507  Training loss = 1.1507  Validation loss = 1.3484  \n",
      "\n",
      "Fold: 5  Epoch: 508  Training loss = 1.1506  Validation loss = 1.3480  \n",
      "\n",
      "Fold: 5  Epoch: 509  Training loss = 1.1509  Validation loss = 1.3498  \n",
      "\n",
      "Fold: 5  Epoch: 510  Training loss = 1.1505  Validation loss = 1.3478  \n",
      "\n",
      "Fold: 5  Epoch: 511  Training loss = 1.1500  Validation loss = 1.3419  \n",
      "\n",
      "Fold: 5  Epoch: 512  Training loss = 1.1501  Validation loss = 1.3435  \n",
      "\n",
      "Fold: 5  Epoch: 513  Training loss = 1.1497  Validation loss = 1.3410  \n",
      "\n",
      "Fold: 5  Epoch: 514  Training loss = 1.1496  Validation loss = 1.3371  \n",
      "\n",
      "Fold: 5  Epoch: 515  Training loss = 1.1493  Validation loss = 1.3331  \n",
      "\n",
      "Fold: 5  Epoch: 516  Training loss = 1.1489  Validation loss = 1.3348  \n",
      "\n",
      "Fold: 5  Epoch: 517  Training loss = 1.1490  Validation loss = 1.3317  \n",
      "\n",
      "Fold: 5  Epoch: 518  Training loss = 1.1487  Validation loss = 1.3375  \n",
      "\n",
      "Fold: 5  Epoch: 519  Training loss = 1.1488  Validation loss = 1.3387  \n",
      "\n",
      "Fold: 5  Epoch: 520  Training loss = 1.1485  Validation loss = 1.3365  \n",
      "\n",
      "Fold: 5  Epoch: 521  Training loss = 1.1485  Validation loss = 1.3330  \n",
      "\n",
      "Fold: 5  Epoch: 522  Training loss = 1.1482  Validation loss = 1.3354  \n",
      "\n",
      "Fold: 5  Epoch: 523  Training loss = 1.1484  Validation loss = 1.3366  \n",
      "\n",
      "Fold: 5  Epoch: 524  Training loss = 1.1484  Validation loss = 1.3360  \n",
      "\n",
      "Fold: 5  Epoch: 525  Training loss = 1.1479  Validation loss = 1.3312  \n",
      "\n",
      "Fold: 5  Epoch: 526  Training loss = 1.1483  Validation loss = 1.3311  \n",
      "\n",
      "Fold: 5  Epoch: 527  Training loss = 1.1482  Validation loss = 1.3287  \n",
      "\n",
      "Fold: 5  Epoch: 528  Training loss = 1.1480  Validation loss = 1.3310  \n",
      "\n",
      "Fold: 5  Epoch: 529  Training loss = 1.1474  Validation loss = 1.3268  \n",
      "\n",
      "Fold: 5  Epoch: 530  Training loss = 1.1479  Validation loss = 1.3326  \n",
      "\n",
      "Fold: 5  Epoch: 531  Training loss = 1.1513  Validation loss = 1.3376  \n",
      "\n",
      "Fold: 5  Epoch: 532  Training loss = 1.1479  Validation loss = 1.3317  \n",
      "\n",
      "Fold: 5  Epoch: 533  Training loss = 1.1464  Validation loss = 1.3241  \n",
      "\n",
      "Fold: 5  Epoch: 534  Training loss = 1.1466  Validation loss = 1.3248  \n",
      "\n",
      "Fold: 5  Epoch: 535  Training loss = 1.1474  Validation loss = 1.3259  \n",
      "\n",
      "Fold: 5  Epoch: 536  Training loss = 1.1464  Validation loss = 1.3226  \n",
      "\n",
      "Fold: 5  Epoch: 537  Training loss = 1.1459  Validation loss = 1.3180  \n",
      "\n",
      "Fold: 5  Epoch: 538  Training loss = 1.1458  Validation loss = 1.3200  \n",
      "\n",
      "Fold: 5  Epoch: 539  Training loss = 1.1457  Validation loss = 1.3178  \n",
      "\n",
      "Fold: 5  Epoch: 540  Training loss = 1.1457  Validation loss = 1.3128  \n",
      "\n",
      "Fold: 5  Epoch: 541  Training loss = 1.1458  Validation loss = 1.3108  \n",
      "\n",
      "Fold: 5  Epoch: 542  Training loss = 1.1463  Validation loss = 1.3085  \n",
      "\n",
      "Fold: 5  Epoch: 543  Training loss = 1.1453  Validation loss = 1.3120  \n",
      "\n",
      "Fold: 5  Epoch: 544  Training loss = 1.1452  Validation loss = 1.3132  \n",
      "\n",
      "Fold: 5  Epoch: 545  Training loss = 1.1453  Validation loss = 1.3132  \n",
      "\n",
      "Fold: 5  Epoch: 546  Training loss = 1.1458  Validation loss = 1.3150  \n",
      "\n",
      "Fold: 5  Epoch: 547  Training loss = 1.1450  Validation loss = 1.3108  \n",
      "\n",
      "Fold: 5  Epoch: 548  Training loss = 1.1449  Validation loss = 1.3082  \n",
      "\n",
      "Fold: 5  Epoch: 549  Training loss = 1.1445  Validation loss = 1.3017  \n",
      "\n",
      "Fold: 5  Epoch: 550  Training loss = 1.1443  Validation loss = 1.3044  \n",
      "\n",
      "Fold: 5  Epoch: 551  Training loss = 1.1438  Validation loss = 1.2987  \n",
      "\n",
      "Fold: 5  Epoch: 552  Training loss = 1.1455  Validation loss = 1.2914  \n",
      "\n",
      "Fold: 5  Epoch: 553  Training loss = 1.1441  Validation loss = 1.2959  \n",
      "\n",
      "Fold: 5  Epoch: 554  Training loss = 1.1456  Validation loss = 1.2910  \n",
      "\n",
      "Fold: 5  Epoch: 555  Training loss = 1.1464  Validation loss = 1.2889  \n",
      "\n",
      "Fold: 5  Epoch: 556  Training loss = 1.1442  Validation loss = 1.2928  \n",
      "\n",
      "Fold: 5  Epoch: 557  Training loss = 1.1439  Validation loss = 1.2950  \n",
      "\n",
      "Fold: 5  Epoch: 558  Training loss = 1.1437  Validation loss = 1.2929  \n",
      "\n",
      "Fold: 5  Epoch: 559  Training loss = 1.1463  Validation loss = 1.2851  \n",
      "\n",
      "Fold: 5  Epoch: 560  Training loss = 1.1442  Validation loss = 1.2891  \n",
      "\n",
      "Fold: 5  Epoch: 561  Training loss = 1.1451  Validation loss = 1.2873  \n",
      "\n",
      "Fold: 5  Epoch: 562  Training loss = 1.1430  Validation loss = 1.2887  \n",
      "\n",
      "Fold: 5  Epoch: 563  Training loss = 1.1420  Validation loss = 1.2929  \n",
      "\n",
      "Fold: 5  Epoch: 564  Training loss = 1.1419  Validation loss = 1.2921  \n",
      "\n",
      "Fold: 5  Epoch: 565  Training loss = 1.1419  Validation loss = 1.2911  \n",
      "\n",
      "Fold: 5  Epoch: 566  Training loss = 1.1425  Validation loss = 1.2946  \n",
      "\n",
      "Fold: 5  Epoch: 567  Training loss = 1.1413  Validation loss = 1.2897  \n",
      "\n",
      "Fold: 5  Epoch: 568  Training loss = 1.1406  Validation loss = 1.2843  \n",
      "\n",
      "Fold: 5  Epoch: 569  Training loss = 1.1406  Validation loss = 1.2865  \n",
      "\n",
      "Fold: 5  Epoch: 570  Training loss = 1.1404  Validation loss = 1.2851  \n",
      "\n",
      "Fold: 5  Epoch: 571  Training loss = 1.1406  Validation loss = 1.2789  \n",
      "\n",
      "Fold: 5  Epoch: 572  Training loss = 1.1401  Validation loss = 1.2802  \n",
      "\n",
      "Fold: 5  Epoch: 573  Training loss = 1.1402  Validation loss = 1.2778  \n",
      "\n",
      "Fold: 5  Epoch: 574  Training loss = 1.1402  Validation loss = 1.2772  \n",
      "\n",
      "Fold: 5  Epoch: 575  Training loss = 1.1397  Validation loss = 1.2776  \n",
      "\n",
      "Fold: 5  Epoch: 576  Training loss = 1.1393  Validation loss = 1.2744  \n",
      "\n",
      "Fold: 5  Epoch: 577  Training loss = 1.1391  Validation loss = 1.2734  \n",
      "\n",
      "Fold: 5  Epoch: 578  Training loss = 1.1389  Validation loss = 1.2691  \n",
      "\n",
      "Fold: 5  Epoch: 579  Training loss = 1.1398  Validation loss = 1.2633  \n",
      "\n",
      "Fold: 5  Epoch: 580  Training loss = 1.1396  Validation loss = 1.2627  \n",
      "\n",
      "Fold: 5  Epoch: 581  Training loss = 1.1384  Validation loss = 1.2652  \n",
      "\n",
      "Fold: 5  Epoch: 582  Training loss = 1.1381  Validation loss = 1.2665  \n",
      "\n",
      "Fold: 5  Epoch: 583  Training loss = 1.1381  Validation loss = 1.2664  \n",
      "\n",
      "Fold: 5  Epoch: 584  Training loss = 1.1380  Validation loss = 1.2621  \n",
      "\n",
      "Fold: 5  Epoch: 585  Training loss = 1.1380  Validation loss = 1.2649  \n",
      "\n",
      "Fold: 5  Epoch: 586  Training loss = 1.1377  Validation loss = 1.2575  \n",
      "\n",
      "Fold: 5  Epoch: 587  Training loss = 1.1375  Validation loss = 1.2560  \n",
      "\n",
      "Fold: 5  Epoch: 588  Training loss = 1.1372  Validation loss = 1.2585  \n",
      "\n",
      "Fold: 5  Epoch: 589  Training loss = 1.1372  Validation loss = 1.2556  \n",
      "\n",
      "Fold: 5  Epoch: 590  Training loss = 1.1369  Validation loss = 1.2568  \n",
      "\n",
      "Fold: 5  Epoch: 591  Training loss = 1.1381  Validation loss = 1.2516  \n",
      "\n",
      "Fold: 5  Epoch: 592  Training loss = 1.1367  Validation loss = 1.2542  \n",
      "\n",
      "Fold: 5  Epoch: 593  Training loss = 1.1366  Validation loss = 1.2544  \n",
      "\n",
      "Fold: 5  Epoch: 594  Training loss = 1.1363  Validation loss = 1.2526  \n",
      "\n",
      "Fold: 5  Epoch: 595  Training loss = 1.1362  Validation loss = 1.2501  \n",
      "\n",
      "Fold: 5  Epoch: 596  Training loss = 1.1363  Validation loss = 1.2542  \n",
      "\n",
      "Fold: 5  Epoch: 597  Training loss = 1.1358  Validation loss = 1.2511  \n",
      "\n",
      "Fold: 5  Epoch: 598  Training loss = 1.1356  Validation loss = 1.2476  \n",
      "\n",
      "Fold: 5  Epoch: 599  Training loss = 1.1355  Validation loss = 1.2460  \n",
      "\n",
      "Fold: 5  Epoch: 600  Training loss = 1.1357  Validation loss = 1.2418  \n",
      "\n",
      "Fold: 5  Epoch: 601  Training loss = 1.1352  Validation loss = 1.2422  \n",
      "\n",
      "Fold: 5  Epoch: 602  Training loss = 1.1354  Validation loss = 1.2446  \n",
      "\n",
      "Fold: 5  Epoch: 603  Training loss = 1.1358  Validation loss = 1.2457  \n",
      "\n",
      "Fold: 5  Epoch: 604  Training loss = 1.1350  Validation loss = 1.2428  \n",
      "\n",
      "Fold: 5  Epoch: 605  Training loss = 1.1352  Validation loss = 1.2426  \n",
      "\n",
      "Fold: 5  Epoch: 606  Training loss = 1.1345  Validation loss = 1.2381  \n",
      "\n",
      "Fold: 5  Epoch: 607  Training loss = 1.1341  Validation loss = 1.2397  \n",
      "\n",
      "Fold: 5  Epoch: 608  Training loss = 1.1340  Validation loss = 1.2379  \n",
      "\n",
      "Fold: 5  Epoch: 609  Training loss = 1.1345  Validation loss = 1.2356  \n",
      "\n",
      "Fold: 5  Epoch: 610  Training loss = 1.1345  Validation loss = 1.2349  \n",
      "\n",
      "Fold: 5  Epoch: 611  Training loss = 1.1348  Validation loss = 1.2330  \n",
      "\n",
      "Fold: 5  Epoch: 612  Training loss = 1.1346  Validation loss = 1.2329  \n",
      "\n",
      "Fold: 5  Epoch: 613  Training loss = 1.1337  Validation loss = 1.2337  \n",
      "\n",
      "Fold: 5  Epoch: 614  Training loss = 1.1333  Validation loss = 1.2337  \n",
      "\n",
      "Fold: 5  Epoch: 615  Training loss = 1.1330  Validation loss = 1.2365  \n",
      "\n",
      "Fold: 5  Epoch: 616  Training loss = 1.1335  Validation loss = 1.2378  \n",
      "\n",
      "Fold: 5  Epoch: 617  Training loss = 1.1338  Validation loss = 1.2378  \n",
      "\n",
      "Fold: 5  Epoch: 618  Training loss = 1.1358  Validation loss = 1.2412  \n",
      "\n",
      "Fold: 5  Epoch: 619  Training loss = 1.1339  Validation loss = 1.2386  \n",
      "\n",
      "Fold: 5  Epoch: 620  Training loss = 1.1325  Validation loss = 1.2337  \n",
      "\n",
      "Fold: 5  Epoch: 621  Training loss = 1.1319  Validation loss = 1.2272  \n",
      "\n",
      "Fold: 5  Epoch: 622  Training loss = 1.1315  Validation loss = 1.2257  \n",
      "\n",
      "Fold: 5  Epoch: 623  Training loss = 1.1315  Validation loss = 1.2280  \n",
      "\n",
      "Fold: 5  Epoch: 624  Training loss = 1.1316  Validation loss = 1.2269  \n",
      "\n",
      "Fold: 5  Epoch: 625  Training loss = 1.1315  Validation loss = 1.2270  \n",
      "\n",
      "Fold: 5  Epoch: 626  Training loss = 1.1313  Validation loss = 1.2198  \n",
      "\n",
      "Fold: 5  Epoch: 627  Training loss = 1.1320  Validation loss = 1.2156  \n",
      "\n",
      "Fold: 5  Epoch: 628  Training loss = 1.1311  Validation loss = 1.2161  \n",
      "\n",
      "Fold: 5  Epoch: 629  Training loss = 1.1317  Validation loss = 1.2135  \n",
      "\n",
      "Fold: 5  Epoch: 630  Training loss = 1.1341  Validation loss = 1.2076  \n",
      "\n",
      "Fold: 5  Epoch: 631  Training loss = 1.1304  Validation loss = 1.2150  \n",
      "\n",
      "Fold: 5  Epoch: 632  Training loss = 1.1298  Validation loss = 1.2164  \n",
      "\n",
      "Fold: 5  Epoch: 633  Training loss = 1.1297  Validation loss = 1.2159  \n",
      "\n",
      "Fold: 5  Epoch: 634  Training loss = 1.1300  Validation loss = 1.2108  \n",
      "\n",
      "Fold: 5  Epoch: 635  Training loss = 1.1293  Validation loss = 1.2130  \n",
      "\n",
      "Fold: 5  Epoch: 636  Training loss = 1.1291  Validation loss = 1.2103  \n",
      "\n",
      "Fold: 5  Epoch: 637  Training loss = 1.1290  Validation loss = 1.2118  \n",
      "\n",
      "Fold: 5  Epoch: 638  Training loss = 1.1287  Validation loss = 1.2081  \n",
      "\n",
      "Fold: 5  Epoch: 639  Training loss = 1.1285  Validation loss = 1.2084  \n",
      "\n",
      "Fold: 5  Epoch: 640  Training loss = 1.1283  Validation loss = 1.2067  \n",
      "\n",
      "Fold: 5  Epoch: 641  Training loss = 1.1281  Validation loss = 1.2055  \n",
      "\n",
      "Fold: 5  Epoch: 642  Training loss = 1.1280  Validation loss = 1.2063  \n",
      "\n",
      "Fold: 5  Epoch: 643  Training loss = 1.1282  Validation loss = 1.2043  \n",
      "\n",
      "Fold: 5  Epoch: 644  Training loss = 1.1280  Validation loss = 1.2037  \n",
      "\n",
      "Fold: 5  Epoch: 645  Training loss = 1.1274  Validation loss = 1.2048  \n",
      "\n",
      "Fold: 5  Epoch: 646  Training loss = 1.1272  Validation loss = 1.2031  \n",
      "\n",
      "Fold: 5  Epoch: 647  Training loss = 1.1279  Validation loss = 1.1983  \n",
      "\n",
      "Fold: 5  Epoch: 648  Training loss = 1.1281  Validation loss = 1.1972  \n",
      "\n",
      "Fold: 5  Epoch: 649  Training loss = 1.1280  Validation loss = 1.1972  \n",
      "\n",
      "Fold: 5  Epoch: 650  Training loss = 1.1271  Validation loss = 1.2017  \n",
      "\n",
      "Fold: 5  Epoch: 651  Training loss = 1.1269  Validation loss = 1.2047  \n",
      "\n",
      "Fold: 5  Epoch: 652  Training loss = 1.1269  Validation loss = 1.2076  \n",
      "\n",
      "Fold: 5  Epoch: 653  Training loss = 1.1276  Validation loss = 1.2098  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 649  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 1.1627  Validation loss = 0.8249  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 1.1624  Validation loss = 0.8257  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 1.1608  Validation loss = 0.8234  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 1.1607  Validation loss = 0.8260  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 1.1608  Validation loss = 0.8303  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 1.1592  Validation loss = 0.8225  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 1.1583  Validation loss = 0.8240  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 1.1584  Validation loss = 0.8253  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 1.1574  Validation loss = 0.8260  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 1.1573  Validation loss = 0.8264  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 1.1564  Validation loss = 0.8279  \n",
      "\n",
      "Fold: 6  Epoch: 12  Training loss = 1.1566  Validation loss = 0.8324  \n",
      "\n",
      "Fold: 6  Epoch: 13  Training loss = 1.1570  Validation loss = 0.8395  \n",
      "\n",
      "Fold: 6  Epoch: 14  Training loss = 1.1568  Validation loss = 0.8420  \n",
      "\n",
      "Fold: 6  Epoch: 15  Training loss = 1.1560  Validation loss = 0.8360  \n",
      "\n",
      "Fold: 6  Epoch: 16  Training loss = 1.1563  Validation loss = 0.8430  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 6  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 1.1340  Validation loss = 0.7967  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 1.1309  Validation loss = 0.8309  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 1.1291  Validation loss = 0.8539  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.1291  Validation loss = 0.8681  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 1.1263  Validation loss = 0.9146  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 1.1279  Validation loss = 0.8671  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 1.1256  Validation loss = 0.9042  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 1.1263  Validation loss = 0.8653  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 1.1258  Validation loss = 0.8541  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 1.1237  Validation loss = 0.8795  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 1.1231  Validation loss = 0.8956  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 1.1231  Validation loss = 0.9514  \n",
      "\n",
      "Fold: 7  Epoch: 13  Training loss = 1.1237  Validation loss = 1.0058  \n",
      "\n",
      "Fold: 7  Epoch: 14  Training loss = 1.1252  Validation loss = 1.0528  \n",
      "\n",
      "Fold: 7  Epoch: 15  Training loss = 1.1224  Validation loss = 1.0286  \n",
      "\n",
      "Fold: 7  Epoch: 16  Training loss = 1.1216  Validation loss = 1.0306  \n",
      "\n",
      "Fold: 7  Epoch: 17  Training loss = 1.1207  Validation loss = 1.0148  \n",
      "\n",
      "Fold: 7  Epoch: 18  Training loss = 1.1204  Validation loss = 1.0274  \n",
      "\n",
      "Fold: 7  Epoch: 19  Training loss = 1.1204  Validation loss = 1.0128  \n",
      "\n",
      "Fold: 7  Epoch: 20  Training loss = 1.1194  Validation loss = 0.9852  \n",
      "\n",
      "Fold: 7  Epoch: 21  Training loss = 1.1203  Validation loss = 1.0244  \n",
      "\n",
      "Fold: 7  Epoch: 22  Training loss = 1.1185  Validation loss = 0.9537  \n",
      "\n",
      "Fold: 7  Epoch: 23  Training loss = 1.1189  Validation loss = 0.9015  \n",
      "\n",
      "Fold: 7  Epoch: 24  Training loss = 1.1178  Validation loss = 1.0008  \n",
      "\n",
      "Fold: 7  Epoch: 25  Training loss = 1.1215  Validation loss = 1.1019  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 1  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 1.0598  Validation loss = 4.7082  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 1.0594  Validation loss = 4.7097  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 1.0524  Validation loss = 4.6554  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 1.0517  Validation loss = 4.6545  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 1.0507  Validation loss = 4.6421  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 1.0506  Validation loss = 4.6413  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 1.0534  Validation loss = 4.6740  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 1.0525  Validation loss = 4.6713  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 1.0515  Validation loss = 4.6675  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 1.0562  Validation loss = 4.7028  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 1.0542  Validation loss = 4.6912  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 1.0511  Validation loss = 4.6714  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 1.0579  Validation loss = 4.7195  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 1.0582  Validation loss = 4.7308  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 1.0529  Validation loss = 4.7027  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 1.0495  Validation loss = 4.6776  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 1.0489  Validation loss = 4.6796  \n",
      "\n",
      "Fold: 8  Epoch: 18  Training loss = 1.0542  Validation loss = 4.7219  \n",
      "\n",
      "Fold: 8  Epoch: 19  Training loss = 1.0469  Validation loss = 4.6580  \n",
      "\n",
      "Fold: 8  Epoch: 20  Training loss = 1.0464  Validation loss = 4.6274  \n",
      "\n",
      "Fold: 8  Epoch: 21  Training loss = 1.0473  Validation loss = 4.6050  \n",
      "\n",
      "Fold: 8  Epoch: 22  Training loss = 1.0461  Validation loss = 4.6301  \n",
      "\n",
      "Fold: 8  Epoch: 23  Training loss = 1.0461  Validation loss = 4.6440  \n",
      "\n",
      "Fold: 8  Epoch: 24  Training loss = 1.0481  Validation loss = 4.6906  \n",
      "\n",
      "Fold: 8  Epoch: 25  Training loss = 1.0461  Validation loss = 4.6640  \n",
      "\n",
      "Fold: 8  Epoch: 26  Training loss = 1.0466  Validation loss = 4.6375  \n",
      "\n",
      "Fold: 8  Epoch: 27  Training loss = 1.0468  Validation loss = 4.6329  \n",
      "\n",
      "Fold: 8  Epoch: 28  Training loss = 1.0460  Validation loss = 4.6389  \n",
      "\n",
      "Fold: 8  Epoch: 29  Training loss = 1.0461  Validation loss = 4.6284  \n",
      "\n",
      "Fold: 8  Epoch: 30  Training loss = 1.0477  Validation loss = 4.6064  \n",
      "\n",
      "Fold: 8  Epoch: 31  Training loss = 1.0443  Validation loss = 4.6433  \n",
      "\n",
      "Fold: 8  Epoch: 32  Training loss = 1.0446  Validation loss = 4.6369  \n",
      "\n",
      "Fold: 8  Epoch: 33  Training loss = 1.0440  Validation loss = 4.6317  \n",
      "\n",
      "Fold: 8  Epoch: 34  Training loss = 1.0429  Validation loss = 4.6477  \n",
      "\n",
      "Fold: 8  Epoch: 35  Training loss = 1.0427  Validation loss = 4.6714  \n",
      "\n",
      "Fold: 8  Epoch: 36  Training loss = 1.0423  Validation loss = 4.6687  \n",
      "\n",
      "Fold: 8  Epoch: 37  Training loss = 1.0420  Validation loss = 4.6645  \n",
      "\n",
      "Fold: 8  Epoch: 38  Training loss = 1.0413  Validation loss = 4.6570  \n",
      "\n",
      "Fold: 8  Epoch: 39  Training loss = 1.0410  Validation loss = 4.6489  \n",
      "\n",
      "Fold: 8  Epoch: 40  Training loss = 1.0406  Validation loss = 4.6581  \n",
      "\n",
      "Fold: 8  Epoch: 41  Training loss = 1.0403  Validation loss = 4.6331  \n",
      "\n",
      "Fold: 8  Epoch: 42  Training loss = 1.0411  Validation loss = 4.6115  \n",
      "\n",
      "Fold: 8  Epoch: 43  Training loss = 1.0429  Validation loss = 4.5872  \n",
      "\n",
      "Fold: 8  Epoch: 44  Training loss = 1.0407  Validation loss = 4.6056  \n",
      "\n",
      "Fold: 8  Epoch: 45  Training loss = 1.0392  Validation loss = 4.6217  \n",
      "\n",
      "Fold: 8  Epoch: 46  Training loss = 1.0394  Validation loss = 4.6071  \n",
      "\n",
      "Fold: 8  Epoch: 47  Training loss = 1.0391  Validation loss = 4.6469  \n",
      "\n",
      "Fold: 8  Epoch: 48  Training loss = 1.0389  Validation loss = 4.6477  \n",
      "\n",
      "Fold: 8  Epoch: 49  Training loss = 1.0396  Validation loss = 4.5812  \n",
      "\n",
      "Fold: 8  Epoch: 50  Training loss = 1.0387  Validation loss = 4.5820  \n",
      "\n",
      "Fold: 8  Epoch: 51  Training loss = 1.0386  Validation loss = 4.5778  \n",
      "\n",
      "Fold: 8  Epoch: 52  Training loss = 1.0375  Validation loss = 4.5925  \n",
      "\n",
      "Fold: 8  Epoch: 53  Training loss = 1.0409  Validation loss = 4.5489  \n",
      "\n",
      "Fold: 8  Epoch: 54  Training loss = 1.0415  Validation loss = 4.5420  \n",
      "\n",
      "Fold: 8  Epoch: 55  Training loss = 1.0387  Validation loss = 4.5610  \n",
      "\n",
      "Fold: 8  Epoch: 56  Training loss = 1.0398  Validation loss = 4.5481  \n",
      "\n",
      "Fold: 8  Epoch: 57  Training loss = 1.0372  Validation loss = 4.5744  \n",
      "\n",
      "Fold: 8  Epoch: 58  Training loss = 1.0359  Validation loss = 4.6141  \n",
      "\n",
      "Fold: 8  Epoch: 59  Training loss = 1.0358  Validation loss = 4.6110  \n",
      "\n",
      "Fold: 8  Epoch: 60  Training loss = 1.0371  Validation loss = 4.6403  \n",
      "\n",
      "Fold: 8  Epoch: 61  Training loss = 1.0360  Validation loss = 4.6350  \n",
      "\n",
      "Fold: 8  Epoch: 62  Training loss = 1.0357  Validation loss = 4.6341  \n",
      "\n",
      "Fold: 8  Epoch: 63  Training loss = 1.0349  Validation loss = 4.6034  \n",
      "\n",
      "Fold: 8  Epoch: 64  Training loss = 1.0345  Validation loss = 4.6012  \n",
      "\n",
      "Fold: 8  Epoch: 65  Training loss = 1.0342  Validation loss = 4.6138  \n",
      "\n",
      "Fold: 8  Epoch: 66  Training loss = 1.0347  Validation loss = 4.6279  \n",
      "\n",
      "Fold: 8  Epoch: 67  Training loss = 1.0347  Validation loss = 4.6245  \n",
      "\n",
      "Fold: 8  Epoch: 68  Training loss = 1.0342  Validation loss = 4.5928  \n",
      "\n",
      "Fold: 8  Epoch: 69  Training loss = 1.0345  Validation loss = 4.6215  \n",
      "\n",
      "Fold: 8  Epoch: 70  Training loss = 1.0351  Validation loss = 4.6299  \n",
      "\n",
      "Fold: 8  Epoch: 71  Training loss = 1.0331  Validation loss = 4.5932  \n",
      "\n",
      "Fold: 8  Epoch: 72  Training loss = 1.0325  Validation loss = 4.6138  \n",
      "\n",
      "Fold: 8  Epoch: 73  Training loss = 1.0330  Validation loss = 4.5891  \n",
      "\n",
      "Fold: 8  Epoch: 74  Training loss = 1.0333  Validation loss = 4.5871  \n",
      "\n",
      "Fold: 8  Epoch: 75  Training loss = 1.0334  Validation loss = 4.5954  \n",
      "\n",
      "Fold: 8  Epoch: 76  Training loss = 1.0324  Validation loss = 4.6045  \n",
      "\n",
      "Fold: 8  Epoch: 77  Training loss = 1.0315  Validation loss = 4.6250  \n",
      "\n",
      "Fold: 8  Epoch: 78  Training loss = 1.0313  Validation loss = 4.6066  \n",
      "\n",
      "Fold: 8  Epoch: 79  Training loss = 1.0313  Validation loss = 4.6128  \n",
      "\n",
      "Fold: 8  Epoch: 80  Training loss = 1.0308  Validation loss = 4.6219  \n",
      "\n",
      "Fold: 8  Epoch: 81  Training loss = 1.0326  Validation loss = 4.6654  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 54  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 1.5321  Validation loss = 8.1062  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 1.5247  Validation loss = 8.0830  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 1.5182  Validation loss = 8.0573  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 1.5122  Validation loss = 7.7577  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 1.5094  Validation loss = 7.7435  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 1.5077  Validation loss = 7.9046  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 1.5116  Validation loss = 8.0525  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 1.5073  Validation loss = 8.0101  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 1.5060  Validation loss = 8.0224  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 1.5038  Validation loss = 8.0216  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 1.5018  Validation loss = 7.7452  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 1.5013  Validation loss = 7.7442  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 1.5007  Validation loss = 8.0043  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 1.5002  Validation loss = 8.0162  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 1.5015  Validation loss = 8.0225  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 1.4990  Validation loss = 8.0190  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 1.4989  Validation loss = 8.0245  \n",
      "\n",
      "Fold: 9  Epoch: 18  Training loss = 1.4992  Validation loss = 8.0311  \n",
      "\n",
      "Fold: 9  Epoch: 19  Training loss = 1.4953  Validation loss = 8.0156  \n",
      "\n",
      "Fold: 9  Epoch: 20  Training loss = 1.4941  Validation loss = 8.0123  \n",
      "\n",
      "Fold: 9  Epoch: 21  Training loss = 1.4935  Validation loss = 8.0044  \n",
      "\n",
      "Fold: 9  Epoch: 22  Training loss = 1.4910  Validation loss = 8.0050  \n",
      "\n",
      "Fold: 9  Epoch: 23  Training loss = 1.4911  Validation loss = 8.0153  \n",
      "\n",
      "Fold: 9  Epoch: 24  Training loss = 1.4865  Validation loss = 7.9763  \n",
      "\n",
      "Fold: 9  Epoch: 25  Training loss = 1.4863  Validation loss = 7.9787  \n",
      "\n",
      "Fold: 9  Epoch: 26  Training loss = 1.4842  Validation loss = 7.9764  \n",
      "\n",
      "Fold: 9  Epoch: 27  Training loss = 1.4845  Validation loss = 7.9703  \n",
      "\n",
      "Fold: 9  Epoch: 28  Training loss = 1.4828  Validation loss = 7.9741  \n",
      "\n",
      "Fold: 9  Epoch: 29  Training loss = 1.4809  Validation loss = 7.9624  \n",
      "\n",
      "Fold: 9  Epoch: 30  Training loss = 1.4783  Validation loss = 7.9734  \n",
      "\n",
      "Fold: 9  Epoch: 31  Training loss = 1.4768  Validation loss = 7.9695  \n",
      "\n",
      "Fold: 9  Epoch: 32  Training loss = 1.4787  Validation loss = 8.0105  \n",
      "\n",
      "Fold: 9  Epoch: 33  Training loss = 1.4900  Validation loss = 8.0978  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 5  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 2.3500  Validation loss = 2.9746  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 2.3420  Validation loss = 2.9524  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 2.3182  Validation loss = 2.9191  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 2.2529  Validation loss = 2.9014  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 2.2522  Validation loss = 2.9021  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 2.2492  Validation loss = 2.8923  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 2.2453  Validation loss = 2.8769  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 2.2424  Validation loss = 2.8705  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 2.2395  Validation loss = 2.8621  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 2.2359  Validation loss = 2.8199  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 2.2341  Validation loss = 2.8416  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 2.2281  Validation loss = 2.7970  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 2.2266  Validation loss = 2.7922  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 2.2286  Validation loss = 2.7331  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 2.2257  Validation loss = 2.7206  \n",
      "\n",
      "Fold: 10  Epoch: 16  Training loss = 2.2224  Validation loss = 2.7102  \n",
      "\n",
      "Fold: 10  Epoch: 17  Training loss = 2.2182  Validation loss = 2.6913  \n",
      "\n",
      "Fold: 10  Epoch: 18  Training loss = 2.2151  Validation loss = 2.6807  \n",
      "\n",
      "Fold: 10  Epoch: 19  Training loss = 2.2129  Validation loss = 2.6714  \n",
      "\n",
      "Fold: 10  Epoch: 20  Training loss = 2.2077  Validation loss = 2.6563  \n",
      "\n",
      "Fold: 10  Epoch: 21  Training loss = 2.2036  Validation loss = 2.6431  \n",
      "\n",
      "Fold: 10  Epoch: 22  Training loss = 2.2029  Validation loss = 2.6425  \n",
      "\n",
      "Fold: 10  Epoch: 23  Training loss = 2.1991  Validation loss = 2.6256  \n",
      "\n",
      "Fold: 10  Epoch: 24  Training loss = 2.1972  Validation loss = 2.6196  \n",
      "\n",
      "Fold: 10  Epoch: 25  Training loss = 2.1953  Validation loss = 2.6070  \n",
      "\n",
      "Fold: 10  Epoch: 26  Training loss = 2.1935  Validation loss = 2.5979  \n",
      "\n",
      "Fold: 10  Epoch: 27  Training loss = 2.1924  Validation loss = 2.5933  \n",
      "\n",
      "Fold: 10  Epoch: 28  Training loss = 2.1915  Validation loss = 2.5879  \n",
      "\n",
      "Fold: 10  Epoch: 29  Training loss = 2.1904  Validation loss = 2.5828  \n",
      "\n",
      "Fold: 10  Epoch: 30  Training loss = 2.1872  Validation loss = 2.5796  \n",
      "\n",
      "Fold: 10  Epoch: 31  Training loss = 2.1839  Validation loss = 2.5746  \n",
      "\n",
      "Fold: 10  Epoch: 32  Training loss = 2.1828  Validation loss = 2.5757  \n",
      "\n",
      "Fold: 10  Epoch: 33  Training loss = 2.1816  Validation loss = 2.5588  \n",
      "\n",
      "Fold: 10  Epoch: 34  Training loss = 2.1811  Validation loss = 2.5435  \n",
      "\n",
      "Fold: 10  Epoch: 35  Training loss = 2.1772  Validation loss = 2.5277  \n",
      "\n",
      "Fold: 10  Epoch: 36  Training loss = 2.1745  Validation loss = 2.5221  \n",
      "\n",
      "Fold: 10  Epoch: 37  Training loss = 2.1687  Validation loss = 2.4952  \n",
      "\n",
      "Fold: 10  Epoch: 38  Training loss = 2.1681  Validation loss = 2.4894  \n",
      "\n",
      "Fold: 10  Epoch: 39  Training loss = 2.1666  Validation loss = 2.4794  \n",
      "\n",
      "Fold: 10  Epoch: 40  Training loss = 2.1625  Validation loss = 2.4577  \n",
      "\n",
      "Fold: 10  Epoch: 41  Training loss = 2.1593  Validation loss = 2.4632  \n",
      "\n",
      "Fold: 10  Epoch: 42  Training loss = 2.1579  Validation loss = 2.4569  \n",
      "\n",
      "Fold: 10  Epoch: 43  Training loss = 2.1524  Validation loss = 2.4498  \n",
      "\n",
      "Fold: 10  Epoch: 44  Training loss = 2.1500  Validation loss = 2.4219  \n",
      "\n",
      "Fold: 10  Epoch: 45  Training loss = 2.1477  Validation loss = 2.4131  \n",
      "\n",
      "Fold: 10  Epoch: 46  Training loss = 2.1455  Validation loss = 2.4184  \n",
      "\n",
      "Fold: 10  Epoch: 47  Training loss = 2.1436  Validation loss = 2.4039  \n",
      "\n",
      "Fold: 10  Epoch: 48  Training loss = 2.1409  Validation loss = 2.3923  \n",
      "\n",
      "Fold: 10  Epoch: 49  Training loss = 2.1357  Validation loss = 2.4014  \n",
      "\n",
      "Fold: 10  Epoch: 50  Training loss = 2.1387  Validation loss = 2.4310  \n",
      "\n",
      "Fold: 10  Epoch: 51  Training loss = 2.1495  Validation loss = 2.3896  \n",
      "\n",
      "Fold: 10  Epoch: 52  Training loss = 2.1507  Validation loss = 2.4047  \n",
      "\n",
      "Fold: 10  Epoch: 53  Training loss = 2.1454  Validation loss = 2.3669  \n",
      "\n",
      "Fold: 10  Epoch: 54  Training loss = 2.1438  Validation loss = 2.3663  \n",
      "\n",
      "Fold: 10  Epoch: 55  Training loss = 2.1436  Validation loss = 2.3497  \n",
      "\n",
      "Fold: 10  Epoch: 56  Training loss = 2.1467  Validation loss = 2.3334  \n",
      "\n",
      "Fold: 10  Epoch: 57  Training loss = 2.1419  Validation loss = 2.3394  \n",
      "\n",
      "Fold: 10  Epoch: 58  Training loss = 2.1384  Validation loss = 2.3463  \n",
      "\n",
      "Fold: 10  Epoch: 59  Training loss = 2.1359  Validation loss = 2.3212  \n",
      "\n",
      "Fold: 10  Epoch: 60  Training loss = 2.1317  Validation loss = 2.2627  \n",
      "\n",
      "Fold: 10  Epoch: 61  Training loss = 2.1310  Validation loss = 2.2723  \n",
      "\n",
      "Fold: 10  Epoch: 62  Training loss = 2.1292  Validation loss = 2.2579  \n",
      "\n",
      "Fold: 10  Epoch: 63  Training loss = 2.1284  Validation loss = 2.2643  \n",
      "\n",
      "Fold: 10  Epoch: 64  Training loss = 2.1271  Validation loss = 2.2561  \n",
      "\n",
      "Fold: 10  Epoch: 65  Training loss = 2.1258  Validation loss = 2.2565  \n",
      "\n",
      "Fold: 10  Epoch: 66  Training loss = 2.1229  Validation loss = 2.2293  \n",
      "\n",
      "Fold: 10  Epoch: 67  Training loss = 2.1223  Validation loss = 2.2336  \n",
      "\n",
      "Fold: 10  Epoch: 68  Training loss = 2.1203  Validation loss = 2.2256  \n",
      "\n",
      "Fold: 10  Epoch: 69  Training loss = 2.1195  Validation loss = 2.2425  \n",
      "\n",
      "Fold: 10  Epoch: 70  Training loss = 2.1177  Validation loss = 2.2303  \n",
      "\n",
      "Fold: 10  Epoch: 71  Training loss = 2.1159  Validation loss = 2.2156  \n",
      "\n",
      "Fold: 10  Epoch: 72  Training loss = 2.1136  Validation loss = 2.2161  \n",
      "\n",
      "Fold: 10  Epoch: 73  Training loss = 2.1123  Validation loss = 2.2046  \n",
      "\n",
      "Fold: 10  Epoch: 74  Training loss = 2.1105  Validation loss = 2.2136  \n",
      "\n",
      "Fold: 10  Epoch: 75  Training loss = 2.1084  Validation loss = 2.2031  \n",
      "\n",
      "Fold: 10  Epoch: 76  Training loss = 2.1067  Validation loss = 2.1945  \n",
      "\n",
      "Fold: 10  Epoch: 77  Training loss = 2.1049  Validation loss = 2.2053  \n",
      "\n",
      "Fold: 10  Epoch: 78  Training loss = 2.1049  Validation loss = 2.2290  \n",
      "\n",
      "Fold: 10  Epoch: 79  Training loss = 2.1027  Validation loss = 2.1985  \n",
      "\n",
      "Fold: 10  Epoch: 80  Training loss = 2.1029  Validation loss = 2.1761  \n",
      "\n",
      "Fold: 10  Epoch: 81  Training loss = 2.0984  Validation loss = 2.1719  \n",
      "\n",
      "Fold: 10  Epoch: 82  Training loss = 2.0970  Validation loss = 2.1536  \n",
      "\n",
      "Fold: 10  Epoch: 83  Training loss = 2.0949  Validation loss = 2.1518  \n",
      "\n",
      "Fold: 10  Epoch: 84  Training loss = 2.0927  Validation loss = 2.1442  \n",
      "\n",
      "Fold: 10  Epoch: 85  Training loss = 2.0917  Validation loss = 2.1311  \n",
      "\n",
      "Fold: 10  Epoch: 86  Training loss = 2.0884  Validation loss = 2.1142  \n",
      "\n",
      "Fold: 10  Epoch: 87  Training loss = 2.0867  Validation loss = 2.1224  \n",
      "\n",
      "Fold: 10  Epoch: 88  Training loss = 2.0851  Validation loss = 2.1389  \n",
      "\n",
      "Fold: 10  Epoch: 89  Training loss = 2.0842  Validation loss = 2.1554  \n",
      "\n",
      "Fold: 10  Epoch: 90  Training loss = 2.0832  Validation loss = 2.1624  \n",
      "\n",
      "Fold: 10  Epoch: 91  Training loss = 2.0831  Validation loss = 2.1885  \n",
      "\n",
      "Fold: 10  Epoch: 92  Training loss = 2.0820  Validation loss = 2.1925  \n",
      "\n",
      "Fold: 10  Epoch: 93  Training loss = 2.0814  Validation loss = 2.2236  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 86  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 2.1316  Validation loss = 1.5080  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 2.1281  Validation loss = 1.4919  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 2.1226  Validation loss = 1.4867  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 2.1194  Validation loss = 1.4866  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 2.1167  Validation loss = 1.4820  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 2.1167  Validation loss = 1.4854  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 2.1148  Validation loss = 1.4854  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 2.1120  Validation loss = 1.4811  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 2.1107  Validation loss = 1.4885  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 2.1093  Validation loss = 1.4832  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 2.1084  Validation loss = 1.4845  \n",
      "\n",
      "Fold: 11  Epoch: 12  Training loss = 2.1069  Validation loss = 1.4884  \n",
      "\n",
      "Fold: 11  Epoch: 13  Training loss = 2.1043  Validation loss = 1.4836  \n",
      "\n",
      "Fold: 11  Epoch: 14  Training loss = 2.1024  Validation loss = 1.4951  \n",
      "\n",
      "Fold: 11  Epoch: 15  Training loss = 2.1011  Validation loss = 1.4989  \n",
      "\n",
      "Fold: 11  Epoch: 16  Training loss = 2.0992  Validation loss = 1.4987  \n",
      "\n",
      "Fold: 11  Epoch: 17  Training loss = 2.0964  Validation loss = 1.4946  \n",
      "\n",
      "Fold: 11  Epoch: 18  Training loss = 2.0958  Validation loss = 1.4962  \n",
      "\n",
      "Fold: 11  Epoch: 19  Training loss = 2.0949  Validation loss = 1.4934  \n",
      "\n",
      "Fold: 11  Epoch: 20  Training loss = 2.0912  Validation loss = 1.4977  \n",
      "\n",
      "Fold: 11  Epoch: 21  Training loss = 2.0901  Validation loss = 1.5084  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 8  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 2.1010  Validation loss = 2.4020  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 2.1005  Validation loss = 2.4156  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 2.0973  Validation loss = 2.3928  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 2.0956  Validation loss = 2.3887  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 2.0962  Validation loss = 2.3878  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 2.0945  Validation loss = 2.3602  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 2.0924  Validation loss = 2.3408  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 2.0908  Validation loss = 2.3502  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 2.0880  Validation loss = 2.3412  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 2.0854  Validation loss = 2.3183  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 2.0846  Validation loss = 2.2697  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 2.0858  Validation loss = 2.1598  \n",
      "\n",
      "Fold: 12  Epoch: 13  Training loss = 2.0816  Validation loss = 2.2144  \n",
      "\n",
      "Fold: 12  Epoch: 14  Training loss = 2.0792  Validation loss = 2.2084  \n",
      "\n",
      "Fold: 12  Epoch: 15  Training loss = 2.0778  Validation loss = 2.2107  \n",
      "\n",
      "Fold: 12  Epoch: 16  Training loss = 2.0769  Validation loss = 2.1886  \n",
      "\n",
      "Fold: 12  Epoch: 17  Training loss = 2.0761  Validation loss = 2.1385  \n",
      "\n",
      "Fold: 12  Epoch: 18  Training loss = 2.0715  Validation loss = 2.1578  \n",
      "\n",
      "Fold: 12  Epoch: 19  Training loss = 2.0674  Validation loss = 2.1769  \n",
      "\n",
      "Fold: 12  Epoch: 20  Training loss = 2.0655  Validation loss = 2.2077  \n",
      "\n",
      "Fold: 12  Epoch: 21  Training loss = 2.0628  Validation loss = 2.0952  \n",
      "\n",
      "Fold: 12  Epoch: 22  Training loss = 2.0592  Validation loss = 2.3061  \n",
      "\n",
      "Fold: 12  Epoch: 23  Training loss = 2.0557  Validation loss = 2.1802  \n",
      "\n",
      "Fold: 12  Epoch: 24  Training loss = 2.0537  Validation loss = 2.0063  \n",
      "\n",
      "Fold: 12  Epoch: 25  Training loss = 2.0539  Validation loss = 1.9415  \n",
      "\n",
      "Fold: 12  Epoch: 26  Training loss = 2.0512  Validation loss = 1.9808  \n",
      "\n",
      "Fold: 12  Epoch: 27  Training loss = 2.0449  Validation loss = 2.0686  \n",
      "\n",
      "Fold: 12  Epoch: 28  Training loss = 2.0414  Validation loss = 2.0211  \n",
      "\n",
      "Fold: 12  Epoch: 29  Training loss = 2.0387  Validation loss = 1.9921  \n",
      "\n",
      "Fold: 12  Epoch: 30  Training loss = 2.0349  Validation loss = 2.1596  \n",
      "\n",
      "Fold: 12  Epoch: 31  Training loss = 2.0321  Validation loss = 2.1373  \n",
      "\n",
      "Fold: 12  Epoch: 32  Training loss = 2.0310  Validation loss = 1.8164  \n",
      "\n",
      "Fold: 12  Epoch: 33  Training loss = 2.0310  Validation loss = 1.7824  \n",
      "\n",
      "Fold: 12  Epoch: 34  Training loss = 2.0262  Validation loss = 1.9364  \n",
      "\n",
      "Fold: 12  Epoch: 35  Training loss = 2.0223  Validation loss = 2.1128  \n",
      "\n",
      "Fold: 12  Epoch: 36  Training loss = 2.0207  Validation loss = 1.9104  \n",
      "\n",
      "Fold: 12  Epoch: 37  Training loss = 2.0182  Validation loss = 1.9577  \n",
      "\n",
      "Fold: 12  Epoch: 38  Training loss = 2.0166  Validation loss = 1.9106  \n",
      "\n",
      "Fold: 12  Epoch: 39  Training loss = 2.0145  Validation loss = 2.0036  \n",
      "\n",
      "Fold: 12  Epoch: 40  Training loss = 2.0158  Validation loss = 1.8291  \n",
      "\n",
      "Fold: 12  Epoch: 41  Training loss = 2.0141  Validation loss = 1.8446  \n",
      "\n",
      "Fold: 12  Epoch: 42  Training loss = 2.0100  Validation loss = 1.9842  \n",
      "\n",
      "Fold: 12  Epoch: 43  Training loss = 2.0077  Validation loss = 2.0195  \n",
      "\n",
      "Fold: 12  Epoch: 44  Training loss = 2.0083  Validation loss = 1.9441  \n",
      "\n",
      "Fold: 12  Epoch: 45  Training loss = 2.0050  Validation loss = 2.1193  \n",
      "\n",
      "Fold: 12  Epoch: 46  Training loss = 2.0032  Validation loss = 2.1513  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 33  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 2.0155  Validation loss = 2.6183  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 2.0113  Validation loss = 2.6058  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 2.0080  Validation loss = 2.6589  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 2.0040  Validation loss = 2.6237  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 2.0012  Validation loss = 2.6067  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 1.9994  Validation loss = 2.6005  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 2.0005  Validation loss = 2.5832  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 1.9984  Validation loss = 2.5805  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 1.9940  Validation loss = 2.6161  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 2.0023  Validation loss = 2.5515  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 1.9953  Validation loss = 2.5630  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 1.9939  Validation loss = 2.5678  \n",
      "\n",
      "Fold: 13  Epoch: 13  Training loss = 1.9917  Validation loss = 2.6667  \n",
      "\n",
      "Fold: 13  Epoch: 14  Training loss = 1.9986  Validation loss = 2.7604  \n",
      "\n",
      "Fold: 13  Epoch: 15  Training loss = 1.9915  Validation loss = 2.7435  \n",
      "\n",
      "Fold: 13  Epoch: 16  Training loss = 1.9837  Validation loss = 2.6103  \n",
      "\n",
      "Fold: 13  Epoch: 17  Training loss = 1.9846  Validation loss = 2.5468  \n",
      "\n",
      "Fold: 13  Epoch: 18  Training loss = 1.9801  Validation loss = 2.5856  \n",
      "\n",
      "Fold: 13  Epoch: 19  Training loss = 1.9816  Validation loss = 2.6709  \n",
      "\n",
      "Fold: 13  Epoch: 20  Training loss = 1.9773  Validation loss = 2.5672  \n",
      "\n",
      "Fold: 13  Epoch: 21  Training loss = 1.9801  Validation loss = 2.5360  \n",
      "\n",
      "Fold: 13  Epoch: 22  Training loss = 1.9755  Validation loss = 2.5528  \n",
      "\n",
      "Fold: 13  Epoch: 23  Training loss = 1.9718  Validation loss = 2.6103  \n",
      "\n",
      "Fold: 13  Epoch: 24  Training loss = 1.9753  Validation loss = 2.5294  \n",
      "\n",
      "Fold: 13  Epoch: 25  Training loss = 1.9724  Validation loss = 2.5262  \n",
      "\n",
      "Fold: 13  Epoch: 26  Training loss = 1.9692  Validation loss = 2.6090  \n",
      "\n",
      "Fold: 13  Epoch: 27  Training loss = 1.9670  Validation loss = 2.6055  \n",
      "\n",
      "Fold: 13  Epoch: 28  Training loss = 1.9619  Validation loss = 2.6563  \n",
      "\n",
      "Fold: 13  Epoch: 29  Training loss = 1.9605  Validation loss = 2.6067  \n",
      "\n",
      "Fold: 13  Epoch: 30  Training loss = 1.9564  Validation loss = 2.6610  \n",
      "\n",
      "Fold: 13  Epoch: 31  Training loss = 1.9556  Validation loss = 2.6351  \n",
      "\n",
      "Fold: 13  Epoch: 32  Training loss = 1.9521  Validation loss = 2.6573  \n",
      "\n",
      "Fold: 13  Epoch: 33  Training loss = 1.9493  Validation loss = 2.7579  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 25  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 2.0523  Validation loss = 6.2769  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 2.0574  Validation loss = 6.2782  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 2.0556  Validation loss = 6.2689  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 2.0508  Validation loss = 6.2567  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 2.0504  Validation loss = 6.2533  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 2.0465  Validation loss = 6.2465  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 2.0449  Validation loss = 6.2434  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 2.0426  Validation loss = 6.2371  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 2.0391  Validation loss = 6.2281  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 2.0382  Validation loss = 6.2250  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 2.0467  Validation loss = 6.2096  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 2.0437  Validation loss = 6.2078  \n",
      "\n",
      "Fold: 14  Epoch: 13  Training loss = 2.0366  Validation loss = 6.1879  \n",
      "\n",
      "Fold: 14  Epoch: 14  Training loss = 2.0291  Validation loss = 6.1874  \n",
      "\n",
      "Fold: 14  Epoch: 15  Training loss = 2.0287  Validation loss = 6.1820  \n",
      "\n",
      "Fold: 14  Epoch: 16  Training loss = 2.0248  Validation loss = 6.1732  \n",
      "\n",
      "Fold: 14  Epoch: 17  Training loss = 2.0282  Validation loss = 6.1689  \n",
      "\n",
      "Fold: 14  Epoch: 18  Training loss = 2.0301  Validation loss = 6.1549  \n",
      "\n",
      "Fold: 14  Epoch: 19  Training loss = 2.0240  Validation loss = 6.1532  \n",
      "\n",
      "Fold: 14  Epoch: 20  Training loss = 2.0214  Validation loss = 6.1474  \n",
      "\n",
      "Fold: 14  Epoch: 21  Training loss = 2.0191  Validation loss = 6.1429  \n",
      "\n",
      "Fold: 14  Epoch: 22  Training loss = 2.0425  Validation loss = 6.1461  \n",
      "\n",
      "Fold: 14  Epoch: 23  Training loss = 2.0264  Validation loss = 6.1300  \n",
      "\n",
      "Fold: 14  Epoch: 24  Training loss = 2.0233  Validation loss = 6.1219  \n",
      "\n",
      "Fold: 14  Epoch: 25  Training loss = 2.0267  Validation loss = 6.1100  \n",
      "\n",
      "Fold: 14  Epoch: 26  Training loss = 2.0239  Validation loss = 6.1083  \n",
      "\n",
      "Fold: 14  Epoch: 27  Training loss = 2.0325  Validation loss = 6.0987  \n",
      "\n",
      "Fold: 14  Epoch: 28  Training loss = 2.0163  Validation loss = 6.1003  \n",
      "\n",
      "Fold: 14  Epoch: 29  Training loss = 2.0144  Validation loss = 6.0897  \n",
      "\n",
      "Fold: 14  Epoch: 30  Training loss = 2.0168  Validation loss = 6.0844  \n",
      "\n",
      "Fold: 14  Epoch: 31  Training loss = 2.0073  Validation loss = 6.0732  \n",
      "\n",
      "Fold: 14  Epoch: 32  Training loss = 2.0059  Validation loss = 6.0753  \n",
      "\n",
      "Fold: 14  Epoch: 33  Training loss = 2.0029  Validation loss = 6.0627  \n",
      "\n",
      "Fold: 14  Epoch: 34  Training loss = 2.0075  Validation loss = 6.0635  \n",
      "\n",
      "Fold: 14  Epoch: 35  Training loss = 2.0012  Validation loss = 6.0637  \n",
      "\n",
      "Fold: 14  Epoch: 36  Training loss = 1.9975  Validation loss = 6.0538  \n",
      "\n",
      "Fold: 14  Epoch: 37  Training loss = 1.9950  Validation loss = 6.0469  \n",
      "\n",
      "Fold: 14  Epoch: 38  Training loss = 1.9938  Validation loss = 6.0462  \n",
      "\n",
      "Fold: 14  Epoch: 39  Training loss = 2.0189  Validation loss = 6.0493  \n",
      "\n",
      "Fold: 14  Epoch: 40  Training loss = 2.0035  Validation loss = 6.0463  \n",
      "\n",
      "Fold: 14  Epoch: 41  Training loss = 2.0005  Validation loss = 6.0407  \n",
      "\n",
      "Fold: 14  Epoch: 42  Training loss = 1.9886  Validation loss = 6.0205  \n",
      "\n",
      "Fold: 14  Epoch: 43  Training loss = 1.9826  Validation loss = 6.0096  \n",
      "\n",
      "Fold: 14  Epoch: 44  Training loss = 1.9805  Validation loss = 5.9982  \n",
      "\n",
      "Fold: 14  Epoch: 45  Training loss = 1.9818  Validation loss = 6.0002  \n",
      "\n",
      "Fold: 14  Epoch: 46  Training loss = 1.9789  Validation loss = 5.9850  \n",
      "\n",
      "Fold: 14  Epoch: 47  Training loss = 1.9842  Validation loss = 5.9815  \n",
      "\n",
      "Fold: 14  Epoch: 48  Training loss = 1.9779  Validation loss = 5.9704  \n",
      "\n",
      "Fold: 14  Epoch: 49  Training loss = 1.9753  Validation loss = 5.9659  \n",
      "\n",
      "Fold: 14  Epoch: 50  Training loss = 1.9904  Validation loss = 5.9726  \n",
      "\n",
      "Fold: 14  Epoch: 51  Training loss = 1.9704  Validation loss = 5.9548  \n",
      "\n",
      "Fold: 14  Epoch: 52  Training loss = 1.9689  Validation loss = 5.9507  \n",
      "\n",
      "Fold: 14  Epoch: 53  Training loss = 1.9727  Validation loss = 5.9383  \n",
      "\n",
      "Fold: 14  Epoch: 54  Training loss = 1.9755  Validation loss = 5.9367  \n",
      "\n",
      "Fold: 14  Epoch: 55  Training loss = 1.9704  Validation loss = 5.9326  \n",
      "\n",
      "Fold: 14  Epoch: 56  Training loss = 1.9699  Validation loss = 5.9186  \n",
      "\n",
      "Fold: 14  Epoch: 57  Training loss = 1.9691  Validation loss = 5.9238  \n",
      "\n",
      "Fold: 14  Epoch: 58  Training loss = 1.9645  Validation loss = 5.9172  \n",
      "\n",
      "Fold: 14  Epoch: 59  Training loss = 1.9629  Validation loss = 5.9096  \n",
      "\n",
      "Fold: 14  Epoch: 60  Training loss = 1.9609  Validation loss = 5.9085  \n",
      "\n",
      "Fold: 14  Epoch: 61  Training loss = 1.9897  Validation loss = 5.9118  \n",
      "\n",
      "Fold: 14  Epoch: 62  Training loss = 1.9625  Validation loss = 5.8907  \n",
      "\n",
      "Fold: 14  Epoch: 63  Training loss = 1.9567  Validation loss = 5.8944  \n",
      "\n",
      "Fold: 14  Epoch: 64  Training loss = 1.9546  Validation loss = 5.8923  \n",
      "\n",
      "Fold: 14  Epoch: 65  Training loss = 1.9585  Validation loss = 5.8739  \n",
      "\n",
      "Fold: 14  Epoch: 66  Training loss = 1.9502  Validation loss = 5.8723  \n",
      "\n",
      "Fold: 14  Epoch: 67  Training loss = 1.9523  Validation loss = 5.8612  \n",
      "\n",
      "Fold: 14  Epoch: 68  Training loss = 1.9522  Validation loss = 5.8488  \n",
      "\n",
      "Fold: 14  Epoch: 69  Training loss = 1.9465  Validation loss = 5.8458  \n",
      "\n",
      "Fold: 14  Epoch: 70  Training loss = 1.9501  Validation loss = 5.8503  \n",
      "\n",
      "Fold: 14  Epoch: 71  Training loss = 1.9656  Validation loss = 5.8509  \n",
      "\n",
      "Fold: 14  Epoch: 72  Training loss = 1.9509  Validation loss = 5.8385  \n",
      "\n",
      "Fold: 14  Epoch: 73  Training loss = 1.9727  Validation loss = 5.8475  \n",
      "\n",
      "Fold: 14  Epoch: 74  Training loss = 1.9460  Validation loss = 5.8297  \n",
      "\n",
      "Fold: 14  Epoch: 75  Training loss = 1.9475  Validation loss = 5.8182  \n",
      "\n",
      "Fold: 14  Epoch: 76  Training loss = 1.9454  Validation loss = 5.8117  \n",
      "\n",
      "Fold: 14  Epoch: 77  Training loss = 1.9399  Validation loss = 5.8198  \n",
      "\n",
      "Fold: 14  Epoch: 78  Training loss = 1.9349  Validation loss = 5.8076  \n",
      "\n",
      "Fold: 14  Epoch: 79  Training loss = 1.9400  Validation loss = 5.8057  \n",
      "\n",
      "Fold: 14  Epoch: 80  Training loss = 1.9536  Validation loss = 5.7926  \n",
      "\n",
      "Fold: 14  Epoch: 81  Training loss = 1.9511  Validation loss = 5.7819  \n",
      "\n",
      "Fold: 14  Epoch: 82  Training loss = 1.9502  Validation loss = 5.7943  \n",
      "\n",
      "Fold: 14  Epoch: 83  Training loss = 1.9465  Validation loss = 5.7835  \n",
      "\n",
      "Fold: 14  Epoch: 84  Training loss = 1.9470  Validation loss = 5.7685  \n",
      "\n",
      "Fold: 14  Epoch: 85  Training loss = 1.9396  Validation loss = 5.7654  \n",
      "\n",
      "Fold: 14  Epoch: 86  Training loss = 1.9398  Validation loss = 5.7622  \n",
      "\n",
      "Fold: 14  Epoch: 87  Training loss = 1.9333  Validation loss = 5.7606  \n",
      "\n",
      "Fold: 14  Epoch: 88  Training loss = 1.9315  Validation loss = 5.7445  \n",
      "\n",
      "Fold: 14  Epoch: 89  Training loss = 1.9280  Validation loss = 5.7476  \n",
      "\n",
      "Fold: 14  Epoch: 90  Training loss = 1.9281  Validation loss = 5.7438  \n",
      "\n",
      "Fold: 14  Epoch: 91  Training loss = 1.9567  Validation loss = 5.7239  \n",
      "\n",
      "Fold: 14  Epoch: 92  Training loss = 1.9262  Validation loss = 5.7274  \n",
      "\n",
      "Fold: 14  Epoch: 93  Training loss = 1.9216  Validation loss = 5.7307  \n",
      "\n",
      "Fold: 14  Epoch: 94  Training loss = 1.9192  Validation loss = 5.7273  \n",
      "\n",
      "Fold: 14  Epoch: 95  Training loss = 1.9284  Validation loss = 5.7140  \n",
      "\n",
      "Fold: 14  Epoch: 96  Training loss = 1.9245  Validation loss = 5.7116  \n",
      "\n",
      "Fold: 14  Epoch: 97  Training loss = 1.9182  Validation loss = 5.7131  \n",
      "\n",
      "Fold: 14  Epoch: 98  Training loss = 1.9171  Validation loss = 5.7212  \n",
      "\n",
      "Fold: 14  Epoch: 99  Training loss = 1.9212  Validation loss = 5.7173  \n",
      "\n",
      "Fold: 14  Epoch: 100  Training loss = 1.9242  Validation loss = 5.7095  \n",
      "\n",
      "Fold: 14  Epoch: 101  Training loss = 1.9262  Validation loss = 5.6968  \n",
      "\n",
      "Fold: 14  Epoch: 102  Training loss = 1.9229  Validation loss = 5.6949  \n",
      "\n",
      "Fold: 14  Epoch: 103  Training loss = 1.9310  Validation loss = 5.6822  \n",
      "\n",
      "Fold: 14  Epoch: 104  Training loss = 1.9361  Validation loss = 5.6825  \n",
      "\n",
      "Fold: 14  Epoch: 105  Training loss = 1.9091  Validation loss = 5.6730  \n",
      "\n",
      "Fold: 14  Epoch: 106  Training loss = 1.9311  Validation loss = 5.6623  \n",
      "\n",
      "Fold: 14  Epoch: 107  Training loss = 1.9403  Validation loss = 5.6610  \n",
      "\n",
      "Fold: 14  Epoch: 108  Training loss = 1.9543  Validation loss = 5.6581  \n",
      "\n",
      "Fold: 14  Epoch: 109  Training loss = 1.9415  Validation loss = 5.6525  \n",
      "\n",
      "Fold: 14  Epoch: 110  Training loss = 1.9485  Validation loss = 5.6507  \n",
      "\n",
      "Fold: 14  Epoch: 111  Training loss = 1.9253  Validation loss = 5.6538  \n",
      "\n",
      "Fold: 14  Epoch: 112  Training loss = 1.9160  Validation loss = 5.6603  \n",
      "\n",
      "Fold: 14  Epoch: 113  Training loss = 1.9112  Validation loss = 5.6624  \n",
      "\n",
      "Fold: 14  Epoch: 114  Training loss = 1.9314  Validation loss = 5.6771  \n",
      "\n",
      "Fold: 14  Epoch: 115  Training loss = 1.9171  Validation loss = 5.6640  \n",
      "\n",
      "Fold: 14  Epoch: 116  Training loss = 1.9080  Validation loss = 5.6550  \n",
      "\n",
      "Fold: 14  Epoch: 117  Training loss = 1.9080  Validation loss = 5.6484  \n",
      "\n",
      "Fold: 14  Epoch: 118  Training loss = 1.9067  Validation loss = 5.6440  \n",
      "\n",
      "Fold: 14  Epoch: 119  Training loss = 1.9048  Validation loss = 5.6469  \n",
      "\n",
      "Fold: 14  Epoch: 120  Training loss = 1.9017  Validation loss = 5.6381  \n",
      "\n",
      "Fold: 14  Epoch: 121  Training loss = 1.9070  Validation loss = 5.6326  \n",
      "\n",
      "Fold: 14  Epoch: 122  Training loss = 1.9288  Validation loss = 5.6412  \n",
      "\n",
      "Fold: 14  Epoch: 123  Training loss = 1.8985  Validation loss = 5.6321  \n",
      "\n",
      "Fold: 14  Epoch: 124  Training loss = 1.8966  Validation loss = 5.6212  \n",
      "\n",
      "Fold: 14  Epoch: 125  Training loss = 1.8941  Validation loss = 5.6101  \n",
      "\n",
      "Fold: 14  Epoch: 126  Training loss = 1.9492  Validation loss = 5.6322  \n",
      "\n",
      "Fold: 14  Epoch: 127  Training loss = 1.8997  Validation loss = 5.6011  \n",
      "\n",
      "Fold: 14  Epoch: 128  Training loss = 1.8897  Validation loss = 5.6079  \n",
      "\n",
      "Fold: 14  Epoch: 129  Training loss = 1.8960  Validation loss = 5.6152  \n",
      "\n",
      "Fold: 14  Epoch: 130  Training loss = 1.9331  Validation loss = 5.6059  \n",
      "\n",
      "Fold: 14  Epoch: 131  Training loss = 1.9297  Validation loss = 5.5855  \n",
      "\n",
      "Fold: 14  Epoch: 132  Training loss = 1.9328  Validation loss = 5.5875  \n",
      "\n",
      "Fold: 14  Epoch: 133  Training loss = 1.9284  Validation loss = 5.5998  \n",
      "\n",
      "Fold: 14  Epoch: 134  Training loss = 1.9263  Validation loss = 5.5938  \n",
      "\n",
      "Fold: 14  Epoch: 135  Training loss = 1.9255  Validation loss = 5.5885  \n",
      "\n",
      "Fold: 14  Epoch: 136  Training loss = 1.9233  Validation loss = 5.5941  \n",
      "\n",
      "Fold: 14  Epoch: 137  Training loss = 1.9203  Validation loss = 5.5856  \n",
      "\n",
      "Fold: 14  Epoch: 138  Training loss = 1.9195  Validation loss = 5.5852  \n",
      "\n",
      "Fold: 14  Epoch: 139  Training loss = 1.9203  Validation loss = 5.5882  \n",
      "\n",
      "Fold: 14  Epoch: 140  Training loss = 1.9187  Validation loss = 5.5811  \n",
      "\n",
      "Fold: 14  Epoch: 141  Training loss = 1.9194  Validation loss = 5.5801  \n",
      "\n",
      "Fold: 14  Epoch: 142  Training loss = 1.9169  Validation loss = 5.5646  \n",
      "\n",
      "Fold: 14  Epoch: 143  Training loss = 1.9187  Validation loss = 5.5554  \n",
      "\n",
      "Fold: 14  Epoch: 144  Training loss = 1.9180  Validation loss = 5.5537  \n",
      "\n",
      "Fold: 14  Epoch: 145  Training loss = 1.9215  Validation loss = 5.5493  \n",
      "\n",
      "Fold: 14  Epoch: 146  Training loss = 1.9197  Validation loss = 5.5518  \n",
      "\n",
      "Fold: 14  Epoch: 147  Training loss = 1.9097  Validation loss = 5.5611  \n",
      "\n",
      "Fold: 14  Epoch: 148  Training loss = 1.9189  Validation loss = 5.5523  \n",
      "\n",
      "Fold: 14  Epoch: 149  Training loss = 1.9077  Validation loss = 5.5537  \n",
      "\n",
      "Fold: 14  Epoch: 150  Training loss = 1.9120  Validation loss = 5.5594  \n",
      "\n",
      "Fold: 14  Epoch: 151  Training loss = 1.9045  Validation loss = 5.5545  \n",
      "\n",
      "Fold: 14  Epoch: 152  Training loss = 1.9028  Validation loss = 5.5560  \n",
      "\n",
      "Fold: 14  Epoch: 153  Training loss = 1.9016  Validation loss = 5.5493  \n",
      "\n",
      "Fold: 14  Epoch: 154  Training loss = 1.8994  Validation loss = 5.5453  \n",
      "\n",
      "Fold: 14  Epoch: 155  Training loss = 1.9003  Validation loss = 5.5548  \n",
      "\n",
      "Fold: 14  Epoch: 156  Training loss = 1.9004  Validation loss = 5.5407  \n",
      "\n",
      "Fold: 14  Epoch: 157  Training loss = 1.9086  Validation loss = 5.5581  \n",
      "\n",
      "Fold: 14  Epoch: 158  Training loss = 1.8977  Validation loss = 5.5294  \n",
      "\n",
      "Fold: 14  Epoch: 159  Training loss = 1.8933  Validation loss = 5.5260  \n",
      "\n",
      "Fold: 14  Epoch: 160  Training loss = 1.8930  Validation loss = 5.5318  \n",
      "\n",
      "Fold: 14  Epoch: 161  Training loss = 1.8955  Validation loss = 5.5296  \n",
      "\n",
      "Fold: 14  Epoch: 162  Training loss = 1.9136  Validation loss = 5.5394  \n",
      "\n",
      "Fold: 14  Epoch: 163  Training loss = 1.8859  Validation loss = 5.5186  \n",
      "\n",
      "Fold: 14  Epoch: 164  Training loss = 1.8858  Validation loss = 5.5155  \n",
      "\n",
      "Fold: 14  Epoch: 165  Training loss = 1.8928  Validation loss = 5.5036  \n",
      "\n",
      "Fold: 14  Epoch: 166  Training loss = 1.8856  Validation loss = 5.5224  \n",
      "\n",
      "Fold: 14  Epoch: 167  Training loss = 1.8867  Validation loss = 5.5224  \n",
      "\n",
      "Fold: 14  Epoch: 168  Training loss = 1.8777  Validation loss = 5.5118  \n",
      "\n",
      "Fold: 14  Epoch: 169  Training loss = 1.8753  Validation loss = 5.5019  \n",
      "\n",
      "Fold: 14  Epoch: 170  Training loss = 1.8724  Validation loss = 5.5017  \n",
      "\n",
      "Fold: 14  Epoch: 171  Training loss = 1.8919  Validation loss = 5.4725  \n",
      "\n",
      "Fold: 14  Epoch: 172  Training loss = 1.8780  Validation loss = 5.4717  \n",
      "\n",
      "Fold: 14  Epoch: 173  Training loss = 1.8702  Validation loss = 5.4878  \n",
      "\n",
      "Fold: 14  Epoch: 174  Training loss = 1.8627  Validation loss = 5.4723  \n",
      "\n",
      "Fold: 14  Epoch: 175  Training loss = 1.8624  Validation loss = 5.4716  \n",
      "\n",
      "Fold: 14  Epoch: 176  Training loss = 1.8587  Validation loss = 5.4673  \n",
      "\n",
      "Fold: 14  Epoch: 177  Training loss = 1.8568  Validation loss = 5.4510  \n",
      "\n",
      "Fold: 14  Epoch: 178  Training loss = 1.8887  Validation loss = 5.4586  \n",
      "\n",
      "Fold: 14  Epoch: 179  Training loss = 1.9023  Validation loss = 5.4407  \n",
      "\n",
      "Fold: 14  Epoch: 180  Training loss = 1.8901  Validation loss = 5.4457  \n",
      "\n",
      "Fold: 14  Epoch: 181  Training loss = 1.8985  Validation loss = 5.4430  \n",
      "\n",
      "Fold: 14  Epoch: 182  Training loss = 1.8874  Validation loss = 5.4394  \n",
      "\n",
      "Fold: 14  Epoch: 183  Training loss = 1.9145  Validation loss = 5.4293  \n",
      "\n",
      "Fold: 14  Epoch: 184  Training loss = 1.8867  Validation loss = 5.4308  \n",
      "\n",
      "Fold: 14  Epoch: 185  Training loss = 1.8887  Validation loss = 5.4264  \n",
      "\n",
      "Fold: 14  Epoch: 186  Training loss = 1.8860  Validation loss = 5.4215  \n",
      "\n",
      "Fold: 14  Epoch: 187  Training loss = 1.8805  Validation loss = 5.4321  \n",
      "\n",
      "Fold: 14  Epoch: 188  Training loss = 1.8875  Validation loss = 5.4252  \n",
      "\n",
      "Fold: 14  Epoch: 189  Training loss = 1.8887  Validation loss = 5.4243  \n",
      "\n",
      "Fold: 14  Epoch: 190  Training loss = 1.8780  Validation loss = 5.4317  \n",
      "\n",
      "Fold: 14  Epoch: 191  Training loss = 1.8745  Validation loss = 5.4269  \n",
      "\n",
      "Fold: 14  Epoch: 192  Training loss = 1.8762  Validation loss = 5.4295  \n",
      "\n",
      "Fold: 14  Epoch: 193  Training loss = 1.8791  Validation loss = 5.4255  \n",
      "\n",
      "Fold: 14  Epoch: 194  Training loss = 1.8753  Validation loss = 5.4265  \n",
      "\n",
      "Fold: 14  Epoch: 195  Training loss = 1.8941  Validation loss = 5.4370  \n",
      "\n",
      "Fold: 14  Epoch: 196  Training loss = 1.8737  Validation loss = 5.4332  \n",
      "\n",
      "Fold: 14  Epoch: 197  Training loss = 1.8744  Validation loss = 5.4100  \n",
      "\n",
      "Fold: 14  Epoch: 198  Training loss = 1.8752  Validation loss = 5.4021  \n",
      "\n",
      "Fold: 14  Epoch: 199  Training loss = 1.8897  Validation loss = 5.3868  \n",
      "\n",
      "Fold: 14  Epoch: 200  Training loss = 1.8625  Validation loss = 5.3984  \n",
      "\n",
      "Fold: 14  Epoch: 201  Training loss = 1.8618  Validation loss = 5.3942  \n",
      "\n",
      "Fold: 14  Epoch: 202  Training loss = 1.8623  Validation loss = 5.3895  \n",
      "\n",
      "Fold: 14  Epoch: 203  Training loss = 1.8675  Validation loss = 5.3704  \n",
      "\n",
      "Fold: 14  Epoch: 204  Training loss = 1.8750  Validation loss = 5.3686  \n",
      "\n",
      "Fold: 14  Epoch: 205  Training loss = 1.8667  Validation loss = 5.3803  \n",
      "\n",
      "Fold: 14  Epoch: 206  Training loss = 1.8549  Validation loss = 5.3805  \n",
      "\n",
      "Fold: 14  Epoch: 207  Training loss = 1.8532  Validation loss = 5.3868  \n",
      "\n",
      "Fold: 14  Epoch: 208  Training loss = 1.8680  Validation loss = 5.4144  \n",
      "\n",
      "Fold: 14  Epoch: 209  Training loss = 1.8609  Validation loss = 5.4030  \n",
      "\n",
      "Fold: 14  Epoch: 210  Training loss = 1.8494  Validation loss = 5.3834  \n",
      "\n",
      "Fold: 14  Epoch: 211  Training loss = 1.8639  Validation loss = 5.3516  \n",
      "\n",
      "Fold: 14  Epoch: 212  Training loss = 1.8601  Validation loss = 5.3511  \n",
      "\n",
      "Fold: 14  Epoch: 213  Training loss = 1.8467  Validation loss = 5.3514  \n",
      "\n",
      "Fold: 14  Epoch: 214  Training loss = 1.8421  Validation loss = 5.3572  \n",
      "\n",
      "Fold: 14  Epoch: 215  Training loss = 1.8474  Validation loss = 5.3544  \n",
      "\n",
      "Fold: 14  Epoch: 216  Training loss = 1.8409  Validation loss = 5.3494  \n",
      "\n",
      "Fold: 14  Epoch: 217  Training loss = 1.8373  Validation loss = 5.3422  \n",
      "\n",
      "Fold: 14  Epoch: 218  Training loss = 1.8326  Validation loss = 5.3422  \n",
      "\n",
      "Fold: 14  Epoch: 219  Training loss = 1.8365  Validation loss = 5.3405  \n",
      "\n",
      "Fold: 14  Epoch: 220  Training loss = 1.8393  Validation loss = 5.3226  \n",
      "\n",
      "Fold: 14  Epoch: 221  Training loss = 1.8347  Validation loss = 5.3211  \n",
      "\n",
      "Fold: 14  Epoch: 222  Training loss = 1.8389  Validation loss = 5.3189  \n",
      "\n",
      "Fold: 14  Epoch: 223  Training loss = 1.8388  Validation loss = 5.3019  \n",
      "\n",
      "Fold: 14  Epoch: 224  Training loss = 1.8479  Validation loss = 5.3191  \n",
      "\n",
      "Fold: 14  Epoch: 225  Training loss = 1.8337  Validation loss = 5.2990  \n",
      "\n",
      "Fold: 14  Epoch: 226  Training loss = 1.8235  Validation loss = 5.2954  \n",
      "\n",
      "Fold: 14  Epoch: 227  Training loss = 1.8224  Validation loss = 5.3031  \n",
      "\n",
      "Fold: 14  Epoch: 228  Training loss = 1.8243  Validation loss = 5.2943  \n",
      "\n",
      "Fold: 14  Epoch: 229  Training loss = 1.8221  Validation loss = 5.3084  \n",
      "\n",
      "Fold: 14  Epoch: 230  Training loss = 1.8174  Validation loss = 5.2909  \n",
      "\n",
      "Fold: 14  Epoch: 231  Training loss = 1.8155  Validation loss = 5.2963  \n",
      "\n",
      "Fold: 14  Epoch: 232  Training loss = 1.8189  Validation loss = 5.2961  \n",
      "\n",
      "Fold: 14  Epoch: 233  Training loss = 1.8117  Validation loss = 5.2912  \n",
      "\n",
      "Fold: 14  Epoch: 234  Training loss = 1.8142  Validation loss = 5.2951  \n",
      "\n",
      "Fold: 14  Epoch: 235  Training loss = 1.8244  Validation loss = 5.2986  \n",
      "\n",
      "Fold: 14  Epoch: 236  Training loss = 1.8149  Validation loss = 5.2719  \n",
      "\n",
      "Fold: 14  Epoch: 237  Training loss = 1.8103  Validation loss = 5.2690  \n",
      "\n",
      "Fold: 14  Epoch: 238  Training loss = 1.8128  Validation loss = 5.2602  \n",
      "\n",
      "Fold: 14  Epoch: 239  Training loss = 1.8261  Validation loss = 5.2503  \n",
      "\n",
      "Fold: 14  Epoch: 240  Training loss = 1.8060  Validation loss = 5.2587  \n",
      "\n",
      "Fold: 14  Epoch: 241  Training loss = 1.8232  Validation loss = 5.2603  \n",
      "\n",
      "Fold: 14  Epoch: 242  Training loss = 1.8097  Validation loss = 5.2523  \n",
      "\n",
      "Fold: 14  Epoch: 243  Training loss = 1.8086  Validation loss = 5.2541  \n",
      "\n",
      "Fold: 14  Epoch: 244  Training loss = 1.8111  Validation loss = 5.2538  \n",
      "\n",
      "Fold: 14  Epoch: 245  Training loss = 1.8189  Validation loss = 5.2698  \n",
      "\n",
      "Fold: 14  Epoch: 246  Training loss = 1.8020  Validation loss = 5.2461  \n",
      "\n",
      "Fold: 14  Epoch: 247  Training loss = 1.8033  Validation loss = 5.2504  \n",
      "\n",
      "Fold: 14  Epoch: 248  Training loss = 1.8094  Validation loss = 5.2613  \n",
      "\n",
      "Fold: 14  Epoch: 249  Training loss = 1.8303  Validation loss = 5.2942  \n",
      "\n",
      "Fold: 14  Epoch: 250  Training loss = 1.8247  Validation loss = 5.2926  \n",
      "\n",
      "Fold: 14  Epoch: 251  Training loss = 1.8165  Validation loss = 5.2371  \n",
      "\n",
      "Fold: 14  Epoch: 252  Training loss = 1.8226  Validation loss = 5.2777  \n",
      "\n",
      "Fold: 14  Epoch: 253  Training loss = 1.8426  Validation loss = 5.2183  \n",
      "\n",
      "Fold: 14  Epoch: 254  Training loss = 1.8037  Validation loss = 5.2306  \n",
      "\n",
      "Fold: 14  Epoch: 255  Training loss = 1.8475  Validation loss = 5.2313  \n",
      "\n",
      "Fold: 14  Epoch: 256  Training loss = 1.8436  Validation loss = 5.2174  \n",
      "\n",
      "Fold: 14  Epoch: 257  Training loss = 1.8440  Validation loss = 5.2127  \n",
      "\n",
      "Fold: 14  Epoch: 258  Training loss = 1.8411  Validation loss = 5.2129  \n",
      "\n",
      "Fold: 14  Epoch: 259  Training loss = 1.8469  Validation loss = 5.1960  \n",
      "\n",
      "Fold: 14  Epoch: 260  Training loss = 1.8272  Validation loss = 5.2177  \n",
      "\n",
      "Fold: 14  Epoch: 261  Training loss = 1.8285  Validation loss = 5.2104  \n",
      "\n",
      "Fold: 14  Epoch: 262  Training loss = 1.8206  Validation loss = 5.2026  \n",
      "\n",
      "Fold: 14  Epoch: 263  Training loss = 1.8438  Validation loss = 5.1771  \n",
      "\n",
      "Fold: 14  Epoch: 264  Training loss = 1.8358  Validation loss = 5.1850  \n",
      "\n",
      "Fold: 14  Epoch: 265  Training loss = 1.8152  Validation loss = 5.1836  \n",
      "\n",
      "Fold: 14  Epoch: 266  Training loss = 1.8224  Validation loss = 5.2098  \n",
      "\n",
      "Fold: 14  Epoch: 267  Training loss = 1.8368  Validation loss = 5.1739  \n",
      "\n",
      "Fold: 14  Epoch: 268  Training loss = 1.8092  Validation loss = 5.1937  \n",
      "\n",
      "Fold: 14  Epoch: 269  Training loss = 1.8186  Validation loss = 5.1832  \n",
      "\n",
      "Fold: 14  Epoch: 270  Training loss = 1.8172  Validation loss = 5.1816  \n",
      "\n",
      "Fold: 14  Epoch: 271  Training loss = 1.8145  Validation loss = 5.1761  \n",
      "\n",
      "Fold: 14  Epoch: 272  Training loss = 1.8344  Validation loss = 5.1621  \n",
      "\n",
      "Fold: 14  Epoch: 273  Training loss = 1.8072  Validation loss = 5.1700  \n",
      "\n",
      "Fold: 14  Epoch: 274  Training loss = 1.8040  Validation loss = 5.1528  \n",
      "\n",
      "Fold: 14  Epoch: 275  Training loss = 1.8116  Validation loss = 5.1333  \n",
      "\n",
      "Fold: 14  Epoch: 276  Training loss = 1.8245  Validation loss = 5.1260  \n",
      "\n",
      "Fold: 14  Epoch: 277  Training loss = 1.8046  Validation loss = 5.1214  \n",
      "\n",
      "Fold: 14  Epoch: 278  Training loss = 1.8107  Validation loss = 5.1210  \n",
      "\n",
      "Fold: 14  Epoch: 279  Training loss = 1.8142  Validation loss = 5.1361  \n",
      "\n",
      "Fold: 14  Epoch: 280  Training loss = 1.8330  Validation loss = 5.1531  \n",
      "\n",
      "Fold: 14  Epoch: 281  Training loss = 1.7902  Validation loss = 5.1276  \n",
      "\n",
      "Fold: 14  Epoch: 282  Training loss = 1.7819  Validation loss = 5.1370  \n",
      "\n",
      "Fold: 14  Epoch: 283  Training loss = 1.7788  Validation loss = 5.1280  \n",
      "\n",
      "Fold: 14  Epoch: 284  Training loss = 1.7768  Validation loss = 5.1208  \n",
      "\n",
      "Fold: 14  Epoch: 285  Training loss = 1.7772  Validation loss = 5.1251  \n",
      "\n",
      "Fold: 14  Epoch: 286  Training loss = 1.7785  Validation loss = 5.1167  \n",
      "\n",
      "Fold: 14  Epoch: 287  Training loss = 1.7964  Validation loss = 5.1309  \n",
      "\n",
      "Fold: 14  Epoch: 288  Training loss = 1.7738  Validation loss = 5.1232  \n",
      "\n",
      "Fold: 14  Epoch: 289  Training loss = 1.7918  Validation loss = 5.1108  \n",
      "\n",
      "Fold: 14  Epoch: 290  Training loss = 1.8226  Validation loss = 5.1008  \n",
      "\n",
      "Fold: 14  Epoch: 291  Training loss = 1.7726  Validation loss = 5.1102  \n",
      "\n",
      "Fold: 14  Epoch: 292  Training loss = 1.8226  Validation loss = 5.1279  \n",
      "\n",
      "Fold: 14  Epoch: 293  Training loss = 1.7861  Validation loss = 5.1078  \n",
      "\n",
      "Fold: 14  Epoch: 294  Training loss = 1.8531  Validation loss = 5.1119  \n",
      "\n",
      "Fold: 14  Epoch: 295  Training loss = 1.8303  Validation loss = 5.1220  \n",
      "\n",
      "Fold: 14  Epoch: 296  Training loss = 1.8256  Validation loss = 5.1155  \n",
      "\n",
      "Fold: 14  Epoch: 297  Training loss = 1.8280  Validation loss = 5.1122  \n",
      "\n",
      "Fold: 14  Epoch: 298  Training loss = 1.8433  Validation loss = 5.1006  \n",
      "\n",
      "Fold: 14  Epoch: 299  Training loss = 1.8213  Validation loss = 5.1167  \n",
      "\n",
      "Fold: 14  Epoch: 300  Training loss = 1.8332  Validation loss = 5.1549  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 298  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 2.2113  Validation loss = 6.1164  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 2.2000  Validation loss = 6.1074  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 2.2204  Validation loss = 6.1003  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 2.2055  Validation loss = 6.1049  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 2.2048  Validation loss = 6.0948  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 2.1972  Validation loss = 6.0856  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 2.1916  Validation loss = 6.0815  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 2.2013  Validation loss = 6.0827  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 2.1856  Validation loss = 6.0777  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 2.1944  Validation loss = 6.0682  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 2.1900  Validation loss = 6.0663  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 2.1837  Validation loss = 6.0610  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 2.1858  Validation loss = 6.0510  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 2.1816  Validation loss = 6.0461  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 2.1770  Validation loss = 6.0520  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 2.1864  Validation loss = 6.0450  \n",
      "\n",
      "Fold: 15  Epoch: 17  Training loss = 2.1706  Validation loss = 6.0385  \n",
      "\n",
      "Fold: 15  Epoch: 18  Training loss = 2.1719  Validation loss = 6.0273  \n",
      "\n",
      "Fold: 15  Epoch: 19  Training loss = 2.1705  Validation loss = 6.0140  \n",
      "\n",
      "Fold: 15  Epoch: 20  Training loss = 2.1613  Validation loss = 6.0236  \n",
      "\n",
      "Fold: 15  Epoch: 21  Training loss = 2.1782  Validation loss = 6.0210  \n",
      "\n",
      "Fold: 15  Epoch: 22  Training loss = 2.1592  Validation loss = 6.0136  \n",
      "\n",
      "Fold: 15  Epoch: 23  Training loss = 2.1649  Validation loss = 6.0118  \n",
      "\n",
      "Fold: 15  Epoch: 24  Training loss = 2.1652  Validation loss = 6.0164  \n",
      "\n",
      "Fold: 15  Epoch: 25  Training loss = 2.1602  Validation loss = 6.0015  \n",
      "\n",
      "Fold: 15  Epoch: 26  Training loss = 2.1480  Validation loss = 5.9991  \n",
      "\n",
      "Fold: 15  Epoch: 27  Training loss = 2.1466  Validation loss = 5.9941  \n",
      "\n",
      "Fold: 15  Epoch: 28  Training loss = 2.1663  Validation loss = 5.9874  \n",
      "\n",
      "Fold: 15  Epoch: 29  Training loss = 2.1651  Validation loss = 6.0010  \n",
      "\n",
      "Fold: 15  Epoch: 30  Training loss = 2.1532  Validation loss = 5.9932  \n",
      "\n",
      "Fold: 15  Epoch: 31  Training loss = 2.1520  Validation loss = 6.0039  \n",
      "\n",
      "Fold: 15  Epoch: 32  Training loss = 2.1511  Validation loss = 5.9978  \n",
      "\n",
      "Fold: 15  Epoch: 33  Training loss = 2.1373  Validation loss = 5.9824  \n",
      "\n",
      "Fold: 15  Epoch: 34  Training loss = 2.1494  Validation loss = 5.9782  \n",
      "\n",
      "Fold: 15  Epoch: 35  Training loss = 2.1400  Validation loss = 5.9697  \n",
      "\n",
      "Fold: 15  Epoch: 36  Training loss = 2.1393  Validation loss = 5.9458  \n",
      "\n",
      "Fold: 15  Epoch: 37  Training loss = 2.1508  Validation loss = 5.9478  \n",
      "\n",
      "Fold: 15  Epoch: 38  Training loss = 2.1406  Validation loss = 5.9400  \n",
      "\n",
      "Fold: 15  Epoch: 39  Training loss = 2.1594  Validation loss = 5.9436  \n",
      "\n",
      "Fold: 15  Epoch: 40  Training loss = 2.1282  Validation loss = 5.9630  \n",
      "\n",
      "Fold: 15  Epoch: 41  Training loss = 2.1306  Validation loss = 5.9600  \n",
      "\n",
      "Fold: 15  Epoch: 42  Training loss = 2.1330  Validation loss = 5.9543  \n",
      "\n",
      "Fold: 15  Epoch: 43  Training loss = 2.1393  Validation loss = 5.9282  \n",
      "\n",
      "Fold: 15  Epoch: 44  Training loss = 2.1273  Validation loss = 5.9137  \n",
      "\n",
      "Fold: 15  Epoch: 45  Training loss = 2.1205  Validation loss = 5.8953  \n",
      "\n",
      "Fold: 15  Epoch: 46  Training loss = 2.1305  Validation loss = 5.8974  \n",
      "\n",
      "Fold: 15  Epoch: 47  Training loss = 2.1190  Validation loss = 5.8908  \n",
      "\n",
      "Fold: 15  Epoch: 48  Training loss = 2.1309  Validation loss = 5.9175  \n",
      "\n",
      "Fold: 15  Epoch: 49  Training loss = 2.1263  Validation loss = 5.8824  \n",
      "\n",
      "Fold: 15  Epoch: 50  Training loss = 2.1274  Validation loss = 5.8979  \n",
      "\n",
      "Fold: 15  Epoch: 51  Training loss = 2.1237  Validation loss = 5.9033  \n",
      "\n",
      "Fold: 15  Epoch: 52  Training loss = 2.1116  Validation loss = 5.8997  \n",
      "\n",
      "Fold: 15  Epoch: 53  Training loss = 2.1212  Validation loss = 5.9760  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 49  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 2.5729  Validation loss = 4.2467  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 2.5582  Validation loss = 4.3471  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 2.5439  Validation loss = 4.4351  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 2.5535  Validation loss = 4.5073  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 2.5358  Validation loss = 4.4806  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 2.5366  Validation loss = 4.5036  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 2.5318  Validation loss = 4.3760  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 2.5411  Validation loss = 4.4646  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 2.5241  Validation loss = 4.2895  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 2.5200  Validation loss = 4.3848  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 2.5304  Validation loss = 4.4178  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 2.5119  Validation loss = 4.2765  \n",
      "\n",
      "Fold: 16  Epoch: 13  Training loss = 2.5114  Validation loss = 4.3198  \n",
      "\n",
      "Fold: 16  Epoch: 14  Training loss = 2.5102  Validation loss = 4.2675  \n",
      "\n",
      "Fold: 16  Epoch: 15  Training loss = 2.5138  Validation loss = 4.3369  \n",
      "\n",
      "Fold: 16  Epoch: 16  Training loss = 2.5214  Validation loss = 4.3904  \n",
      "\n",
      "Fold: 16  Epoch: 17  Training loss = 2.5323  Validation loss = 4.4606  \n",
      "\n",
      "Fold: 16  Epoch: 18  Training loss = 2.4903  Validation loss = 4.2381  \n",
      "\n",
      "Fold: 16  Epoch: 19  Training loss = 2.5082  Validation loss = 4.4084  \n",
      "\n",
      "Fold: 16  Epoch: 20  Training loss = 2.4840  Validation loss = 4.3293  \n",
      "\n",
      "Fold: 16  Epoch: 21  Training loss = 2.4893  Validation loss = 4.2895  \n",
      "\n",
      "Fold: 16  Epoch: 22  Training loss = 2.5063  Validation loss = 4.3641  \n",
      "\n",
      "Fold: 16  Epoch: 23  Training loss = 2.4831  Validation loss = 4.2209  \n",
      "\n",
      "Fold: 16  Epoch: 24  Training loss = 2.5040  Validation loss = 3.9977  \n",
      "\n",
      "Fold: 16  Epoch: 25  Training loss = 2.4712  Validation loss = 4.1018  \n",
      "\n",
      "Fold: 16  Epoch: 26  Training loss = 2.4711  Validation loss = 4.1415  \n",
      "\n",
      "Fold: 16  Epoch: 27  Training loss = 2.4660  Validation loss = 4.1072  \n",
      "\n",
      "Fold: 16  Epoch: 28  Training loss = 2.4614  Validation loss = 4.0384  \n",
      "\n",
      "Fold: 16  Epoch: 29  Training loss = 2.4605  Validation loss = 4.0397  \n",
      "\n",
      "Fold: 16  Epoch: 30  Training loss = 2.4575  Validation loss = 3.9921  \n",
      "\n",
      "Fold: 16  Epoch: 31  Training loss = 2.4553  Validation loss = 3.9709  \n",
      "\n",
      "Fold: 16  Epoch: 32  Training loss = 2.4504  Validation loss = 3.9344  \n",
      "\n",
      "Fold: 16  Epoch: 33  Training loss = 2.4477  Validation loss = 3.9190  \n",
      "\n",
      "Fold: 16  Epoch: 34  Training loss = 2.4488  Validation loss = 3.9232  \n",
      "\n",
      "Fold: 16  Epoch: 35  Training loss = 2.4411  Validation loss = 4.0860  \n",
      "\n",
      "Fold: 16  Epoch: 36  Training loss = 2.4422  Validation loss = 4.1059  \n",
      "\n",
      "Fold: 16  Epoch: 37  Training loss = 2.4311  Validation loss = 3.9084  \n",
      "\n",
      "Fold: 16  Epoch: 38  Training loss = 2.4274  Validation loss = 3.9342  \n",
      "\n",
      "Fold: 16  Epoch: 39  Training loss = 2.4240  Validation loss = 3.9183  \n",
      "\n",
      "Fold: 16  Epoch: 40  Training loss = 2.4210  Validation loss = 3.8802  \n",
      "\n",
      "Fold: 16  Epoch: 41  Training loss = 2.4207  Validation loss = 3.8125  \n",
      "\n",
      "Fold: 16  Epoch: 42  Training loss = 2.4197  Validation loss = 3.9369  \n",
      "\n",
      "Fold: 16  Epoch: 43  Training loss = 2.4143  Validation loss = 4.0223  \n",
      "\n",
      "Fold: 16  Epoch: 44  Training loss = 2.4136  Validation loss = 4.0439  \n",
      "\n",
      "Fold: 16  Epoch: 45  Training loss = 2.4239  Validation loss = 4.0210  \n",
      "\n",
      "Fold: 16  Epoch: 46  Training loss = 2.4036  Validation loss = 3.9028  \n",
      "\n",
      "Fold: 16  Epoch: 47  Training loss = 2.4757  Validation loss = 3.9151  \n",
      "\n",
      "Fold: 16  Epoch: 48  Training loss = 2.4044  Validation loss = 3.9915  \n",
      "\n",
      "Fold: 16  Epoch: 49  Training loss = 2.3918  Validation loss = 3.9485  \n",
      "\n",
      "Fold: 16  Epoch: 50  Training loss = 2.3885  Validation loss = 3.9723  \n",
      "\n",
      "Fold: 16  Epoch: 51  Training loss = 2.3843  Validation loss = 3.9485  \n",
      "\n",
      "Fold: 16  Epoch: 52  Training loss = 2.3810  Validation loss = 4.0115  \n",
      "\n",
      "Fold: 16  Epoch: 53  Training loss = 2.3783  Validation loss = 3.9767  \n",
      "\n",
      "Fold: 16  Epoch: 54  Training loss = 2.3766  Validation loss = 3.9560  \n",
      "\n",
      "Fold: 16  Epoch: 55  Training loss = 2.3747  Validation loss = 3.8467  \n",
      "\n",
      "Fold: 16  Epoch: 56  Training loss = 2.3872  Validation loss = 3.7490  \n",
      "\n",
      "Fold: 16  Epoch: 57  Training loss = 2.3835  Validation loss = 3.8073  \n",
      "\n",
      "Fold: 16  Epoch: 58  Training loss = 2.3678  Validation loss = 3.9061  \n",
      "\n",
      "Fold: 16  Epoch: 59  Training loss = 2.4060  Validation loss = 3.6330  \n",
      "\n",
      "Fold: 16  Epoch: 60  Training loss = 2.3578  Validation loss = 3.8001  \n",
      "\n",
      "Fold: 16  Epoch: 61  Training loss = 2.3557  Validation loss = 4.0121  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 59  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 2.5137  Validation loss = 3.1991  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 2.5453  Validation loss = 2.9617  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 2.5248  Validation loss = 3.0004  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 2.5860  Validation loss = 2.9831  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 2.5563  Validation loss = 2.8355  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 2.6395  Validation loss = 2.8033  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 2.5036  Validation loss = 3.2975  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 2.4888  Validation loss = 3.1552  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 2.4985  Validation loss = 3.0040  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 2.4991  Validation loss = 2.9854  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 2.6568  Validation loss = 2.7989  \n",
      "\n",
      "Fold: 17  Epoch: 12  Training loss = 2.7109  Validation loss = 2.8678  \n",
      "\n",
      "Fold: 17  Epoch: 13  Training loss = 2.5954  Validation loss = 2.8290  \n",
      "\n",
      "Fold: 17  Epoch: 14  Training loss = 2.4828  Validation loss = 3.3657  \n",
      "\n",
      "Fold: 17  Epoch: 15  Training loss = 2.4685  Validation loss = 3.4033  \n",
      "\n",
      "Fold: 17  Epoch: 16  Training loss = 2.4899  Validation loss = 2.9838  \n",
      "\n",
      "Fold: 17  Epoch: 17  Training loss = 2.4988  Validation loss = 3.4246  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 11  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 2.6168  Validation loss = 1.1643  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 2.6705  Validation loss = 1.0335  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 2.5845  Validation loss = 1.2216  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 2.5754  Validation loss = 1.2264  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 2.5726  Validation loss = 1.2640  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 2.5655  Validation loss = 1.2412  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 2.5659  Validation loss = 1.2303  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 2.5652  Validation loss = 1.1629  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 2.5410  Validation loss = 1.1736  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 2.5691  Validation loss = 1.1187  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 2.5589  Validation loss = 1.1885  \n",
      "\n",
      "Fold: 18  Epoch: 12  Training loss = 2.5718  Validation loss = 1.0701  \n",
      "\n",
      "Fold: 18  Epoch: 13  Training loss = 2.5454  Validation loss = 1.1020  \n",
      "\n",
      "Fold: 18  Epoch: 14  Training loss = 2.5475  Validation loss = 1.0992  \n",
      "\n",
      "Fold: 18  Epoch: 15  Training loss = 2.5608  Validation loss = 1.0402  \n",
      "\n",
      "Fold: 18  Epoch: 16  Training loss = 2.5339  Validation loss = 1.0795  \n",
      "\n",
      "Fold: 18  Epoch: 17  Training loss = 2.5307  Validation loss = 1.0680  \n",
      "\n",
      "Fold: 18  Epoch: 18  Training loss = 2.5865  Validation loss = 1.1702  \n",
      "\n",
      "Fold: 18  Epoch: 19  Training loss = 2.5531  Validation loss = 1.0966  \n",
      "\n",
      "Fold: 18  Epoch: 20  Training loss = 2.5474  Validation loss = 1.1849  \n",
      "\n",
      "Fold: 18  Epoch: 21  Training loss = 2.5994  Validation loss = 1.0964  \n",
      "\n",
      "Fold: 18  Epoch: 22  Training loss = 2.5760  Validation loss = 1.0340  \n",
      "\n",
      "Fold: 18  Epoch: 23  Training loss = 2.5718  Validation loss = 1.0228  \n",
      "\n",
      "Fold: 18  Epoch: 24  Training loss = 2.5318  Validation loss = 1.0814  \n",
      "\n",
      "Fold: 18  Epoch: 25  Training loss = 2.6130  Validation loss = 0.9672  \n",
      "\n",
      "Fold: 18  Epoch: 26  Training loss = 2.6402  Validation loss = 1.0253  \n",
      "\n",
      "Fold: 18  Epoch: 27  Training loss = 2.5179  Validation loss = 1.1400  \n",
      "\n",
      "Fold: 18  Epoch: 28  Training loss = 2.4946  Validation loss = 1.1718  \n",
      "\n",
      "Fold: 18  Epoch: 29  Training loss = 2.4912  Validation loss = 1.1349  \n",
      "\n",
      "Fold: 18  Epoch: 30  Training loss = 2.6407  Validation loss = 0.9738  \n",
      "\n",
      "Fold: 18  Epoch: 31  Training loss = 2.5309  Validation loss = 1.0422  \n",
      "\n",
      "Fold: 18  Epoch: 32  Training loss = 2.5529  Validation loss = 1.0967  \n",
      "\n",
      "Fold: 18  Epoch: 33  Training loss = 2.4717  Validation loss = 1.0581  \n",
      "\n",
      "Fold: 18  Epoch: 34  Training loss = 2.5036  Validation loss = 1.0005  \n",
      "\n",
      "Fold: 18  Epoch: 35  Training loss = 2.4739  Validation loss = 1.0180  \n",
      "\n",
      "Fold: 18  Epoch: 36  Training loss = 2.4562  Validation loss = 1.0303  \n",
      "\n",
      "Fold: 18  Epoch: 37  Training loss = 2.4928  Validation loss = 0.9939  \n",
      "\n",
      "Fold: 18  Epoch: 38  Training loss = 2.5096  Validation loss = 1.0069  \n",
      "\n",
      "Fold: 18  Epoch: 39  Training loss = 2.4637  Validation loss = 1.0244  \n",
      "\n",
      "Fold: 18  Epoch: 40  Training loss = 2.4504  Validation loss = 1.0529  \n",
      "\n",
      "Fold: 18  Epoch: 41  Training loss = 2.5383  Validation loss = 1.0101  \n",
      "\n",
      "Fold: 18  Epoch: 42  Training loss = 2.4500  Validation loss = 1.0686  \n",
      "\n",
      "Fold: 18  Epoch: 43  Training loss = 2.4581  Validation loss = 1.0881  \n",
      "\n",
      "Fold: 18  Epoch: 44  Training loss = 2.4691  Validation loss = 1.0660  \n",
      "\n",
      "Fold: 18  Epoch: 45  Training loss = 2.4698  Validation loss = 1.0203  \n",
      "\n",
      "Fold: 18  Epoch: 46  Training loss = 2.4647  Validation loss = 1.0092  \n",
      "\n",
      "Fold: 18  Epoch: 47  Training loss = 2.4623  Validation loss = 1.0216  \n",
      "\n",
      "Fold: 18  Epoch: 48  Training loss = 2.4352  Validation loss = 1.0378  \n",
      "\n",
      "Fold: 18  Epoch: 49  Training loss = 2.4903  Validation loss = 1.0781  \n",
      "\n",
      "Fold: 18  Epoch: 50  Training loss = 2.4443  Validation loss = 0.9955  \n",
      "\n",
      "Fold: 18  Epoch: 51  Training loss = 2.4301  Validation loss = 1.0340  \n",
      "\n",
      "Fold: 18  Epoch: 52  Training loss = 2.4990  Validation loss = 0.9505  \n",
      "\n",
      "Fold: 18  Epoch: 53  Training loss = 2.5406  Validation loss = 0.8852  \n",
      "\n",
      "Fold: 18  Epoch: 54  Training loss = 2.4676  Validation loss = 0.9248  \n",
      "\n",
      "Fold: 18  Epoch: 55  Training loss = 2.4330  Validation loss = 1.0333  \n",
      "\n",
      "Fold: 18  Epoch: 56  Training loss = 2.4450  Validation loss = 1.0314  \n",
      "\n",
      "Fold: 18  Epoch: 57  Training loss = 2.4264  Validation loss = 1.0264  \n",
      "\n",
      "Fold: 18  Epoch: 58  Training loss = 2.4030  Validation loss = 1.0278  \n",
      "\n",
      "Fold: 18  Epoch: 59  Training loss = 2.4036  Validation loss = 1.0653  \n",
      "\n",
      "Fold: 18  Epoch: 60  Training loss = 2.4144  Validation loss = 1.0344  \n",
      "\n",
      "Fold: 18  Epoch: 61  Training loss = 2.4072  Validation loss = 1.0327  \n",
      "\n",
      "Fold: 18  Epoch: 62  Training loss = 2.3822  Validation loss = 1.0591  \n",
      "\n",
      "Fold: 18  Epoch: 63  Training loss = 2.4086  Validation loss = 1.1005  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 53  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 2.3706  Validation loss = 2.2409  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 2.3925  Validation loss = 2.1860  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 2.4535  Validation loss = 2.0921  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 2.3583  Validation loss = 2.2794  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 2.3571  Validation loss = 2.2759  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 2.3628  Validation loss = 2.1213  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 2.3793  Validation loss = 2.1966  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 2.4216  Validation loss = 2.1460  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 2.3791  Validation loss = 2.0586  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 2.3436  Validation loss = 2.3139  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 2.3795  Validation loss = 2.2690  \n",
      "\n",
      "Fold: 19  Epoch: 12  Training loss = 2.4005  Validation loss = 2.1944  \n",
      "\n",
      "Fold: 19  Epoch: 13  Training loss = 2.3507  Validation loss = 2.2984  \n",
      "\n",
      "Fold: 19  Epoch: 14  Training loss = 2.3503  Validation loss = 2.3715  \n",
      "\n",
      "Fold: 19  Epoch: 15  Training loss = 2.3771  Validation loss = 2.4043  \n",
      "\n",
      "Fold: 19  Epoch: 16  Training loss = 2.3498  Validation loss = 2.4012  \n",
      "\n",
      "Fold: 19  Epoch: 17  Training loss = 2.3307  Validation loss = 2.3690  \n",
      "\n",
      "Fold: 19  Epoch: 18  Training loss = 2.3674  Validation loss = 2.2991  \n",
      "\n",
      "Fold: 19  Epoch: 19  Training loss = 2.3422  Validation loss = 2.3255  \n",
      "\n",
      "Fold: 19  Epoch: 20  Training loss = 2.3282  Validation loss = 2.2285  \n",
      "\n",
      "Fold: 19  Epoch: 21  Training loss = 2.3196  Validation loss = 2.1513  \n",
      "\n",
      "Fold: 19  Epoch: 22  Training loss = 2.3536  Validation loss = 2.1930  \n",
      "\n",
      "Fold: 19  Epoch: 23  Training loss = 2.3110  Validation loss = 2.2877  \n",
      "\n",
      "Fold: 19  Epoch: 24  Training loss = 2.3269  Validation loss = 2.3084  \n",
      "\n",
      "Fold: 19  Epoch: 25  Training loss = 2.3255  Validation loss = 2.4381  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 9  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 2.3781  Validation loss = 1.3559  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 2.3899  Validation loss = 1.3881  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 2.3981  Validation loss = 1.2192  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 2.3523  Validation loss = 1.3175  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 2.3520  Validation loss = 1.2739  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 2.5274  Validation loss = 1.4055  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 2.3461  Validation loss = 1.2912  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 2.3737  Validation loss = 1.3449  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 2.3849  Validation loss = 1.2670  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 2.3507  Validation loss = 1.2708  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 2.3940  Validation loss = 1.2386  \n",
      "\n",
      "Fold: 20  Epoch: 12  Training loss = 2.3726  Validation loss = 1.2941  \n",
      "\n",
      "Fold: 20  Epoch: 13  Training loss = 2.3535  Validation loss = 1.1880  \n",
      "\n",
      "Fold: 20  Epoch: 14  Training loss = 2.4353  Validation loss = 1.1067  \n",
      "\n",
      "Fold: 20  Epoch: 15  Training loss = 2.3567  Validation loss = 1.1955  \n",
      "\n",
      "Fold: 20  Epoch: 16  Training loss = 2.3777  Validation loss = 1.3025  \n",
      "\n",
      "Fold: 20  Epoch: 17  Training loss = 2.3859  Validation loss = 1.1819  \n",
      "\n",
      "Fold: 20  Epoch: 18  Training loss = 2.3782  Validation loss = 1.2224  \n",
      "\n",
      "Fold: 20  Epoch: 19  Training loss = 2.3373  Validation loss = 1.3025  \n",
      "\n",
      "Fold: 20  Epoch: 20  Training loss = 2.3294  Validation loss = 1.2679  \n",
      "\n",
      "Fold: 20  Epoch: 21  Training loss = 2.3207  Validation loss = 1.2701  \n",
      "\n",
      "Fold: 20  Epoch: 22  Training loss = 2.3261  Validation loss = 1.2495  \n",
      "\n",
      "Fold: 20  Epoch: 23  Training loss = 2.3513  Validation loss = 1.3113  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 14  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 2.3547  Validation loss = 4.4803  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 2.2953  Validation loss = 4.9002  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 2.3359  Validation loss = 5.4000  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 2.3084  Validation loss = 4.6671  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 2.3434  Validation loss = 5.4575  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 2.3445  Validation loss = 4.9591  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 2.3207  Validation loss = 5.4648  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 2.3027  Validation loss = 4.9520  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 2.3214  Validation loss = 4.9288  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 2.3262  Validation loss = 4.9400  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 2.2872  Validation loss = 5.2207  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 2.3088  Validation loss = 5.1681  \n",
      "\n",
      "Fold: 21  Epoch: 13  Training loss = 2.3047  Validation loss = 4.8700  \n",
      "\n",
      "Fold: 21  Epoch: 14  Training loss = 2.3029  Validation loss = 5.2524  \n",
      "\n",
      "Fold: 21  Epoch: 15  Training loss = 2.3317  Validation loss = 5.1504  \n",
      "\n",
      "Fold: 21  Epoch: 16  Training loss = 2.3060  Validation loss = 4.4869  \n",
      "\n",
      "Fold: 21  Epoch: 17  Training loss = 2.3405  Validation loss = 5.0953  \n",
      "\n",
      "Fold: 21  Epoch: 18  Training loss = 2.2690  Validation loss = 5.0639  \n",
      "\n",
      "Fold: 21  Epoch: 19  Training loss = 2.2859  Validation loss = 5.1621  \n",
      "\n",
      "Fold: 21  Epoch: 20  Training loss = 2.2750  Validation loss = 4.7117  \n",
      "\n",
      "Fold: 21  Epoch: 21  Training loss = 2.2707  Validation loss = 5.1445  \n",
      "\n",
      "Fold: 21  Epoch: 22  Training loss = 2.2544  Validation loss = 5.0111  \n",
      "\n",
      "Fold: 21  Epoch: 23  Training loss = 2.3101  Validation loss = 5.3882  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 1  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 2.5058  Validation loss = 3.4153  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 2.5907  Validation loss = 2.1386  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 2.5005  Validation loss = 2.3048  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 2.5186  Validation loss = 2.8563  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 2.4358  Validation loss = 3.2520  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 2.5255  Validation loss = 2.1358  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 2.3752  Validation loss = 2.7986  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 2.3922  Validation loss = 1.5829  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 2.3441  Validation loss = 2.2506  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 2.3989  Validation loss = 2.0347  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 2.3896  Validation loss = 3.4588  \n",
      "\n",
      "Fold: 22  Epoch: 12  Training loss = 2.3330  Validation loss = 3.2513  \n",
      "\n",
      "Fold: 22  Epoch: 13  Training loss = 2.3834  Validation loss = 2.5656  \n",
      "\n",
      "Fold: 22  Epoch: 14  Training loss = 2.3328  Validation loss = 3.7245  \n",
      "\n",
      "Fold: 22  Epoch: 15  Training loss = 2.3249  Validation loss = 2.2717  \n",
      "\n",
      "Fold: 22  Epoch: 16  Training loss = 2.4413  Validation loss = 2.5972  \n",
      "\n",
      "Fold: 22  Epoch: 17  Training loss = 2.3569  Validation loss = 3.5063  \n",
      "\n",
      "Fold: 22  Epoch: 18  Training loss = 2.3561  Validation loss = 3.4121  \n",
      "\n",
      "Fold: 22  Epoch: 19  Training loss = 2.3301  Validation loss = 3.5407  \n",
      "\n",
      "Fold: 22  Epoch: 20  Training loss = 2.3398  Validation loss = 1.9290  \n",
      "\n",
      "Fold: 22  Epoch: 21  Training loss = 2.3038  Validation loss = 2.3402  \n",
      "\n",
      "Fold: 22  Epoch: 22  Training loss = 2.2780  Validation loss = 2.9470  \n",
      "\n",
      "Fold: 22  Epoch: 23  Training loss = 2.3134  Validation loss = 1.7490  \n",
      "\n",
      "Fold: 22  Epoch: 24  Training loss = 2.3410  Validation loss = 3.5060  \n",
      "\n",
      "Fold: 22  Epoch: 25  Training loss = 2.2624  Validation loss = 2.0669  \n",
      "\n",
      "Fold: 22  Epoch: 26  Training loss = 2.3978  Validation loss = 2.9683  \n",
      "\n",
      "Fold: 22  Epoch: 27  Training loss = 2.2860  Validation loss = 3.3890  \n",
      "\n",
      "Fold: 22  Epoch: 28  Training loss = 2.3035  Validation loss = 3.5350  \n",
      "\n",
      "Fold: 22  Epoch: 29  Training loss = 2.2565  Validation loss = 2.8964  \n",
      "\n",
      "Fold: 22  Epoch: 30  Training loss = 2.2426  Validation loss = 2.3656  \n",
      "\n",
      "Fold: 22  Epoch: 31  Training loss = 2.2417  Validation loss = 2.6621  \n",
      "\n",
      "Fold: 22  Epoch: 32  Training loss = 2.3015  Validation loss = 3.2818  \n",
      "\n",
      "Fold: 22  Epoch: 33  Training loss = 2.2893  Validation loss = 3.8688  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 8  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 2.3600  Validation loss = 1.9154  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 2.6710  Validation loss = 1.3884  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 2.2348  Validation loss = 1.6009  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 2.2994  Validation loss = 1.7970  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 2.4231  Validation loss = 1.2411  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 2.3122  Validation loss = 2.0638  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 2.4690  Validation loss = 2.7343  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 2.4932  Validation loss = 2.6630  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 2.3477  Validation loss = 1.5826  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 2.3063  Validation loss = 0.9524  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 2.3045  Validation loss = 1.7277  \n",
      "\n",
      "Fold: 23  Epoch: 12  Training loss = 2.3382  Validation loss = 0.8756  \n",
      "\n",
      "Fold: 23  Epoch: 13  Training loss = 2.4713  Validation loss = 2.7181  \n",
      "\n",
      "Fold: 23  Epoch: 14  Training loss = 2.3874  Validation loss = 2.5342  \n",
      "\n",
      "Fold: 23  Epoch: 15  Training loss = 2.2998  Validation loss = 0.9190  \n",
      "\n",
      "Fold: 23  Epoch: 16  Training loss = 2.2959  Validation loss = 1.7768  \n",
      "\n",
      "Fold: 23  Epoch: 17  Training loss = 2.3051  Validation loss = 2.1276  \n",
      "\n",
      "Fold: 23  Epoch: 18  Training loss = 2.2754  Validation loss = 1.2848  \n",
      "\n",
      "Fold: 23  Epoch: 19  Training loss = 2.2794  Validation loss = 0.7877  \n",
      "\n",
      "Fold: 23  Epoch: 20  Training loss = 2.3994  Validation loss = 2.6269  \n",
      "\n",
      "Fold: 23  Epoch: 21  Training loss = 2.2565  Validation loss = 0.9045  \n",
      "\n",
      "Fold: 23  Epoch: 22  Training loss = 2.2172  Validation loss = 2.1415  \n",
      "\n",
      "Fold: 23  Epoch: 23  Training loss = 2.2430  Validation loss = 0.9592  \n",
      "\n",
      "Fold: 23  Epoch: 24  Training loss = 2.2703  Validation loss = 0.8407  \n",
      "\n",
      "Fold: 23  Epoch: 25  Training loss = 2.2348  Validation loss = 1.2756  \n",
      "\n",
      "Fold: 23  Epoch: 26  Training loss = 2.2139  Validation loss = 1.0390  \n",
      "\n",
      "Fold: 23  Epoch: 27  Training loss = 2.3424  Validation loss = 2.7133  \n",
      "\n",
      "Fold: 23  Epoch: 28  Training loss = 2.2286  Validation loss = 1.4517  \n",
      "\n",
      "Fold: 23  Epoch: 29  Training loss = 2.2483  Validation loss = 2.4243  \n",
      "\n",
      "Fold: 23  Epoch: 30  Training loss = 2.2143  Validation loss = 0.9764  \n",
      "\n",
      "Fold: 23  Epoch: 31  Training loss = 2.2668  Validation loss = 0.8930  \n",
      "\n",
      "Fold: 23  Epoch: 32  Training loss = 2.6662  Validation loss = 2.0155  \n",
      "\n",
      "Fold: 23  Epoch: 33  Training loss = 2.5089  Validation loss = 2.2591  \n",
      "\n",
      "Fold: 23  Epoch: 34  Training loss = 2.2006  Validation loss = 1.2983  \n",
      "\n",
      "Fold: 23  Epoch: 35  Training loss = 2.1734  Validation loss = 1.1951  \n",
      "\n",
      "Fold: 23  Epoch: 36  Training loss = 2.2526  Validation loss = 2.1363  \n",
      "\n",
      "Fold: 23  Epoch: 37  Training loss = 2.2185  Validation loss = 2.2684  \n",
      "\n",
      "Fold: 23  Epoch: 38  Training loss = 2.2024  Validation loss = 1.7287  \n",
      "\n",
      "Fold: 23  Epoch: 39  Training loss = 2.1734  Validation loss = 1.1355  \n",
      "\n",
      "Fold: 23  Epoch: 40  Training loss = 2.1759  Validation loss = 1.7804  \n",
      "\n",
      "Fold: 23  Epoch: 41  Training loss = 2.1559  Validation loss = 1.2185  \n",
      "\n",
      "Fold: 23  Epoch: 42  Training loss = 2.1632  Validation loss = 1.6092  \n",
      "\n",
      "Fold: 23  Epoch: 43  Training loss = 2.1596  Validation loss = 1.2570  \n",
      "\n",
      "Fold: 23  Epoch: 44  Training loss = 2.1705  Validation loss = 1.8203  \n",
      "\n",
      "Fold: 23  Epoch: 45  Training loss = 2.1777  Validation loss = 1.8381  \n",
      "\n",
      "Fold: 23  Epoch: 46  Training loss = 2.1708  Validation loss = 1.0992  \n",
      "\n",
      "Fold: 23  Epoch: 47  Training loss = 2.1647  Validation loss = 1.7848  \n",
      "\n",
      "Fold: 23  Epoch: 48  Training loss = 2.2726  Validation loss = 1.0414  \n",
      "\n",
      "Fold: 23  Epoch: 49  Training loss = 2.1663  Validation loss = 1.2888  \n",
      "\n",
      "Fold: 23  Epoch: 50  Training loss = 2.2062  Validation loss = 1.1358  \n",
      "\n",
      "Fold: 23  Epoch: 51  Training loss = 2.2081  Validation loss = 1.3177  \n",
      "\n",
      "Fold: 23  Epoch: 52  Training loss = 2.2291  Validation loss = 0.8965  \n",
      "\n",
      "Fold: 23  Epoch: 53  Training loss = 2.1320  Validation loss = 1.1322  \n",
      "\n",
      "Fold: 23  Epoch: 54  Training loss = 2.1644  Validation loss = 0.9387  \n",
      "\n",
      "Fold: 23  Epoch: 55  Training loss = 2.1554  Validation loss = 1.8718  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 19  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 2.1299  Validation loss = 0.7358  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 2.1528  Validation loss = 0.8720  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 2.3056  Validation loss = 0.9052  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 2.3266  Validation loss = 1.0193  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 2.2271  Validation loss = 1.0915  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 2.2252  Validation loss = 0.9578  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 2.2374  Validation loss = 0.9592  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 2.1596  Validation loss = 0.9626  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 2.1317  Validation loss = 0.9795  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 2.1663  Validation loss = 0.8478  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 2.1176  Validation loss = 0.8464  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 2.1226  Validation loss = 0.8710  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 2.1542  Validation loss = 0.8758  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 2.1474  Validation loss = 0.8722  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 2.2285  Validation loss = 0.8830  \n",
      "\n",
      "Fold: 24  Epoch: 16  Training loss = 2.1667  Validation loss = 0.7106  \n",
      "\n",
      "Fold: 24  Epoch: 17  Training loss = 2.1397  Validation loss = 0.7607  \n",
      "\n",
      "Fold: 24  Epoch: 18  Training loss = 2.2598  Validation loss = 0.7954  \n",
      "\n",
      "Fold: 24  Epoch: 19  Training loss = 2.1142  Validation loss = 0.9358  \n",
      "\n",
      "Fold: 24  Epoch: 20  Training loss = 2.1530  Validation loss = 0.7417  \n",
      "\n",
      "Fold: 24  Epoch: 21  Training loss = 2.1274  Validation loss = 0.7158  \n",
      "\n",
      "Fold: 24  Epoch: 22  Training loss = 2.2002  Validation loss = 0.7131  \n",
      "\n",
      "Fold: 24  Epoch: 23  Training loss = 2.1172  Validation loss = 0.7400  \n",
      "\n",
      "Fold: 24  Epoch: 24  Training loss = 2.1584  Validation loss = 0.7464  \n",
      "\n",
      "Fold: 24  Epoch: 25  Training loss = 2.1040  Validation loss = 0.6891  \n",
      "\n",
      "Fold: 24  Epoch: 26  Training loss = 2.0964  Validation loss = 0.7006  \n",
      "\n",
      "Fold: 24  Epoch: 27  Training loss = 2.1825  Validation loss = 0.7305  \n",
      "\n",
      "Fold: 24  Epoch: 28  Training loss = 2.1383  Validation loss = 0.6417  \n",
      "\n",
      "Fold: 24  Epoch: 29  Training loss = 2.1406  Validation loss = 0.6745  \n",
      "\n",
      "Fold: 24  Epoch: 30  Training loss = 2.1351  Validation loss = 0.7650  \n",
      "\n",
      "Fold: 24  Epoch: 31  Training loss = 2.1344  Validation loss = 0.6785  \n",
      "\n",
      "Fold: 24  Epoch: 32  Training loss = 2.1537  Validation loss = 0.6021  \n",
      "\n",
      "Fold: 24  Epoch: 33  Training loss = 2.1813  Validation loss = 0.6149  \n",
      "\n",
      "Fold: 24  Epoch: 34  Training loss = 2.1175  Validation loss = 0.7464  \n",
      "\n",
      "Fold: 24  Epoch: 35  Training loss = 2.2147  Validation loss = 0.7249  \n",
      "\n",
      "Fold: 24  Epoch: 36  Training loss = 2.1439  Validation loss = 0.7096  \n",
      "\n",
      "Fold: 24  Epoch: 37  Training loss = 2.1008  Validation loss = 0.6810  \n",
      "\n",
      "Fold: 24  Epoch: 38  Training loss = 2.1298  Validation loss = 0.6423  \n",
      "\n",
      "Fold: 24  Epoch: 39  Training loss = 2.1118  Validation loss = 0.7731  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 32  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 2.0530  Validation loss = 2.7776  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 2.0455  Validation loss = 2.6985  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 2.0692  Validation loss = 2.5631  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 2.0752  Validation loss = 2.6010  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 2.5232  Validation loss = 3.0935  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 2.0640  Validation loss = 2.6212  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 2.0589  Validation loss = 2.7144  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 2.1268  Validation loss = 2.6343  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 2.0985  Validation loss = 2.5897  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 2.0513  Validation loss = 2.7076  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 2.1028  Validation loss = 2.6892  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 2.0440  Validation loss = 2.6827  \n",
      "\n",
      "Fold: 25  Epoch: 13  Training loss = 2.0334  Validation loss = 2.6990  \n",
      "\n",
      "Fold: 25  Epoch: 14  Training loss = 2.0338  Validation loss = 2.6043  \n",
      "\n",
      "Fold: 25  Epoch: 15  Training loss = 2.0212  Validation loss = 2.6594  \n",
      "\n",
      "Fold: 25  Epoch: 16  Training loss = 2.0108  Validation loss = 2.6505  \n",
      "\n",
      "Fold: 25  Epoch: 17  Training loss = 2.0056  Validation loss = 2.7280  \n",
      "\n",
      "Fold: 25  Epoch: 18  Training loss = 2.0582  Validation loss = 2.7002  \n",
      "\n",
      "Fold: 25  Epoch: 19  Training loss = 2.4896  Validation loss = 2.9762  \n",
      "\n",
      "Fold: 25  Epoch: 20  Training loss = 2.0383  Validation loss = 2.5878  \n",
      "\n",
      "Fold: 25  Epoch: 21  Training loss = 2.0153  Validation loss = 2.7075  \n",
      "\n",
      "Fold: 25  Epoch: 22  Training loss = 2.0076  Validation loss = 2.7033  \n",
      "\n",
      "Fold: 25  Epoch: 23  Training loss = 2.0014  Validation loss = 2.7623  \n",
      "\n",
      "Fold: 25  Epoch: 24  Training loss = 2.0347  Validation loss = 2.8613  \n",
      "\n",
      "Fold: 25  Epoch: 25  Training loss = 2.0585  Validation loss = 2.8583  \n",
      "\n",
      "Fold: 25  Epoch: 26  Training loss = 2.0216  Validation loss = 2.6898  \n",
      "\n",
      "Fold: 25  Epoch: 27  Training loss = 2.0047  Validation loss = 2.6692  \n",
      "\n",
      "Fold: 25  Epoch: 28  Training loss = 2.0351  Validation loss = 2.6384  \n",
      "\n",
      "Fold: 25  Epoch: 29  Training loss = 2.0462  Validation loss = 2.8434  \n",
      "\n",
      "Fold: 25  Epoch: 30  Training loss = 1.9934  Validation loss = 2.7877  \n",
      "\n",
      "Fold: 25  Epoch: 31  Training loss = 2.0041  Validation loss = 2.7560  \n",
      "\n",
      "Fold: 25  Epoch: 32  Training loss = 2.0041  Validation loss = 2.8475  \n",
      "\n",
      "Fold: 25  Epoch: 33  Training loss = 1.9999  Validation loss = 2.6759  \n",
      "\n",
      "Fold: 25  Epoch: 34  Training loss = 2.0164  Validation loss = 2.8061  \n",
      "\n",
      "Fold: 25  Epoch: 35  Training loss = 1.9953  Validation loss = 2.7775  \n",
      "\n",
      "Fold: 25  Epoch: 36  Training loss = 2.0131  Validation loss = 2.5986  \n",
      "\n",
      "Fold: 25  Epoch: 37  Training loss = 1.9967  Validation loss = 2.6479  \n",
      "\n",
      "Fold: 25  Epoch: 38  Training loss = 1.9889  Validation loss = 2.6950  \n",
      "\n",
      "Fold: 25  Epoch: 39  Training loss = 2.0178  Validation loss = 2.7802  \n",
      "\n",
      "Fold: 25  Epoch: 40  Training loss = 2.0182  Validation loss = 2.7009  \n",
      "\n",
      "Fold: 25  Epoch: 41  Training loss = 2.0832  Validation loss = 2.2671  \n",
      "\n",
      "Fold: 25  Epoch: 42  Training loss = 1.9862  Validation loss = 2.6564  \n",
      "\n",
      "Fold: 25  Epoch: 43  Training loss = 1.9805  Validation loss = 2.7304  \n",
      "\n",
      "Fold: 25  Epoch: 44  Training loss = 1.9865  Validation loss = 2.6785  \n",
      "\n",
      "Fold: 25  Epoch: 45  Training loss = 1.9974  Validation loss = 2.6331  \n",
      "\n",
      "Fold: 25  Epoch: 46  Training loss = 1.9802  Validation loss = 2.6906  \n",
      "\n",
      "Fold: 25  Epoch: 47  Training loss = 2.0003  Validation loss = 2.6901  \n",
      "\n",
      "Fold: 25  Epoch: 48  Training loss = 1.9774  Validation loss = 2.7992  \n",
      "\n",
      "Fold: 25  Epoch: 49  Training loss = 1.9867  Validation loss = 2.6506  \n",
      "\n",
      "Fold: 25  Epoch: 50  Training loss = 1.9880  Validation loss = 2.6332  \n",
      "\n",
      "Fold: 25  Epoch: 51  Training loss = 1.9833  Validation loss = 2.5976  \n",
      "\n",
      "Fold: 25  Epoch: 52  Training loss = 1.9738  Validation loss = 2.7221  \n",
      "\n",
      "Fold: 25  Epoch: 53  Training loss = 1.9813  Validation loss = 2.7053  \n",
      "\n",
      "Fold: 25  Epoch: 54  Training loss = 1.9778  Validation loss = 2.7536  \n",
      "\n",
      "Fold: 25  Epoch: 55  Training loss = 1.9966  Validation loss = 2.8086  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 41  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 2.1067  Validation loss = 2.8444  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 2.0879  Validation loss = 2.8051  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 2.0830  Validation loss = 2.6774  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 2.0969  Validation loss = 2.7728  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 2.1127  Validation loss = 2.8858  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 2.1109  Validation loss = 2.8691  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 2.0865  Validation loss = 2.9642  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 2.1978  Validation loss = 2.8165  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 2.1483  Validation loss = 2.9140  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 2.0858  Validation loss = 2.8586  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 2.0578  Validation loss = 2.7103  \n",
      "\n",
      "Fold: 26  Epoch: 12  Training loss = 2.0744  Validation loss = 2.7276  \n",
      "\n",
      "Fold: 26  Epoch: 13  Training loss = 2.1018  Validation loss = 2.7503  \n",
      "\n",
      "Fold: 26  Epoch: 14  Training loss = 2.1038  Validation loss = 2.7698  \n",
      "\n",
      "Fold: 26  Epoch: 15  Training loss = 2.0505  Validation loss = 2.7527  \n",
      "\n",
      "Fold: 26  Epoch: 16  Training loss = 2.0731  Validation loss = 2.8027  \n",
      "\n",
      "Fold: 26  Epoch: 17  Training loss = 2.0688  Validation loss = 2.7559  \n",
      "\n",
      "Fold: 26  Epoch: 18  Training loss = 2.0507  Validation loss = 2.6786  \n",
      "\n",
      "Fold: 26  Epoch: 19  Training loss = 2.0668  Validation loss = 2.6367  \n",
      "\n",
      "Fold: 26  Epoch: 20  Training loss = 2.1036  Validation loss = 2.6839  \n",
      "\n",
      "Fold: 26  Epoch: 21  Training loss = 2.0917  Validation loss = 2.7462  \n",
      "\n",
      "Fold: 26  Epoch: 22  Training loss = 2.1266  Validation loss = 2.8599  \n",
      "\n",
      "Fold: 26  Epoch: 23  Training loss = 2.0707  Validation loss = 2.6424  \n",
      "\n",
      "Fold: 26  Epoch: 24  Training loss = 2.0702  Validation loss = 2.5999  \n",
      "\n",
      "Fold: 26  Epoch: 25  Training loss = 2.0791  Validation loss = 2.6106  \n",
      "\n",
      "Fold: 26  Epoch: 26  Training loss = 2.0446  Validation loss = 2.6533  \n",
      "\n",
      "Fold: 26  Epoch: 27  Training loss = 2.0510  Validation loss = 2.6601  \n",
      "\n",
      "Fold: 26  Epoch: 28  Training loss = 2.0496  Validation loss = 2.6831  \n",
      "\n",
      "Fold: 26  Epoch: 29  Training loss = 2.0347  Validation loss = 2.6492  \n",
      "\n",
      "Fold: 26  Epoch: 30  Training loss = 2.0429  Validation loss = 2.6185  \n",
      "\n",
      "Fold: 26  Epoch: 31  Training loss = 2.0737  Validation loss = 2.5038  \n",
      "\n",
      "Fold: 26  Epoch: 32  Training loss = 2.0491  Validation loss = 2.5627  \n",
      "\n",
      "Fold: 26  Epoch: 33  Training loss = 2.0472  Validation loss = 2.5985  \n",
      "\n",
      "Fold: 26  Epoch: 34  Training loss = 2.0498  Validation loss = 2.6288  \n",
      "\n",
      "Fold: 26  Epoch: 35  Training loss = 2.0363  Validation loss = 2.5700  \n",
      "\n",
      "Fold: 26  Epoch: 36  Training loss = 2.0680  Validation loss = 2.6715  \n",
      "\n",
      "Fold: 26  Epoch: 37  Training loss = 2.0798  Validation loss = 2.5072  \n",
      "\n",
      "Fold: 26  Epoch: 38  Training loss = 2.1710  Validation loss = 2.3534  \n",
      "\n",
      "Fold: 26  Epoch: 39  Training loss = 2.0478  Validation loss = 2.6005  \n",
      "\n",
      "Fold: 26  Epoch: 40  Training loss = 2.0437  Validation loss = 2.6121  \n",
      "\n",
      "Fold: 26  Epoch: 41  Training loss = 2.0361  Validation loss = 2.6365  \n",
      "\n",
      "Fold: 26  Epoch: 42  Training loss = 2.0722  Validation loss = 2.6148  \n",
      "\n",
      "Fold: 26  Epoch: 43  Training loss = 2.1360  Validation loss = 2.6634  \n",
      "\n",
      "Fold: 26  Epoch: 44  Training loss = 2.0531  Validation loss = 2.6053  \n",
      "\n",
      "Fold: 26  Epoch: 45  Training loss = 2.0650  Validation loss = 2.6490  \n",
      "\n",
      "Fold: 26  Epoch: 46  Training loss = 2.0394  Validation loss = 2.7402  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 38  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 2.0047  Validation loss = 0.7080  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 2.0986  Validation loss = 0.9107  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 2.0209  Validation loss = 1.2733  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 1.9869  Validation loss = 0.8449  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 2.0063  Validation loss = 1.0645  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 2.0607  Validation loss = 1.2748  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 1.9769  Validation loss = 0.9852  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 1.9760  Validation loss = 1.1740  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 1.9639  Validation loss = 0.9732  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 1.9697  Validation loss = 0.7093  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 1.9494  Validation loss = 0.8143  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 1.9589  Validation loss = 1.0410  \n",
      "\n",
      "Fold: 27  Epoch: 13  Training loss = 1.9757  Validation loss = 1.0789  \n",
      "\n",
      "Fold: 27  Epoch: 14  Training loss = 1.9920  Validation loss = 1.2341  \n",
      "\n",
      "Fold: 27  Epoch: 15  Training loss = 2.0113  Validation loss = 0.9150  \n",
      "\n",
      "Fold: 27  Epoch: 16  Training loss = 2.0887  Validation loss = 0.9134  \n",
      "\n",
      "Fold: 27  Epoch: 17  Training loss = 1.9807  Validation loss = 1.0212  \n",
      "\n",
      "Fold: 27  Epoch: 18  Training loss = 1.9730  Validation loss = 1.2515  \n",
      "\n",
      "Fold: 27  Epoch: 19  Training loss = 1.9667  Validation loss = 1.0183  \n",
      "\n",
      "Fold: 27  Epoch: 20  Training loss = 1.9551  Validation loss = 1.0079  \n",
      "\n",
      "Fold: 27  Epoch: 21  Training loss = 1.9693  Validation loss = 1.1701  \n",
      "\n",
      "Fold: 27  Epoch: 22  Training loss = 2.0615  Validation loss = 0.6631  \n",
      "\n",
      "Fold: 27  Epoch: 23  Training loss = 1.9928  Validation loss = 0.8463  \n",
      "\n",
      "Fold: 27  Epoch: 24  Training loss = 1.9931  Validation loss = 0.9648  \n",
      "\n",
      "Fold: 27  Epoch: 25  Training loss = 1.9395  Validation loss = 0.7285  \n",
      "\n",
      "Fold: 27  Epoch: 26  Training loss = 1.9257  Validation loss = 0.5981  \n",
      "\n",
      "Fold: 27  Epoch: 27  Training loss = 1.9234  Validation loss = 0.7402  \n",
      "\n",
      "Fold: 27  Epoch: 28  Training loss = 2.0369  Validation loss = 1.0208  \n",
      "\n",
      "Fold: 27  Epoch: 29  Training loss = 1.9524  Validation loss = 0.6240  \n",
      "\n",
      "Fold: 27  Epoch: 30  Training loss = 1.9626  Validation loss = 0.9988  \n",
      "\n",
      "Fold: 27  Epoch: 31  Training loss = 2.0273  Validation loss = 0.6145  \n",
      "\n",
      "Fold: 27  Epoch: 32  Training loss = 1.9308  Validation loss = 0.9015  \n",
      "\n",
      "Fold: 27  Epoch: 33  Training loss = 1.9504  Validation loss = 1.0838  \n",
      "\n",
      "Fold: 27  Epoch: 34  Training loss = 2.0257  Validation loss = 1.3856  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 26  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 1.9487  Validation loss = 0.6498  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 1.9181  Validation loss = 0.6234  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 1.9016  Validation loss = 0.8138  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 1.9025  Validation loss = 0.5959  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 1.8929  Validation loss = 0.8126  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 1.9126  Validation loss = 0.6282  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 1.8830  Validation loss = 0.6800  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 1.8968  Validation loss = 0.6578  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 1.9409  Validation loss = 0.6618  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 1.8816  Validation loss = 0.7308  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 1.8844  Validation loss = 0.6906  \n",
      "\n",
      "Fold: 28  Epoch: 12  Training loss = 1.8734  Validation loss = 0.8529  \n",
      "\n",
      "Fold: 28  Epoch: 13  Training loss = 1.9090  Validation loss = 0.6516  \n",
      "\n",
      "Fold: 28  Epoch: 14  Training loss = 1.9021  Validation loss = 0.6614  \n",
      "\n",
      "Fold: 28  Epoch: 15  Training loss = 1.9103  Validation loss = 0.6739  \n",
      "\n",
      "Fold: 28  Epoch: 16  Training loss = 1.8922  Validation loss = 0.8056  \n",
      "\n",
      "Fold: 28  Epoch: 17  Training loss = 1.8769  Validation loss = 0.8203  \n",
      "\n",
      "Fold: 28  Epoch: 18  Training loss = 1.9168  Validation loss = 0.6489  \n",
      "\n",
      "Fold: 28  Epoch: 19  Training loss = 1.8755  Validation loss = 0.6726  \n",
      "\n",
      "Fold: 28  Epoch: 20  Training loss = 1.9055  Validation loss = 0.6583  \n",
      "\n",
      "Fold: 28  Epoch: 21  Training loss = 1.9514  Validation loss = 0.6467  \n",
      "\n",
      "Fold: 28  Epoch: 22  Training loss = 1.8866  Validation loss = 0.6505  \n",
      "\n",
      "Fold: 28  Epoch: 23  Training loss = 1.9544  Validation loss = 0.7163  \n",
      "\n",
      "Fold: 28  Epoch: 24  Training loss = 1.8786  Validation loss = 0.6676  \n",
      "\n",
      "Fold: 28  Epoch: 25  Training loss = 1.8895  Validation loss = 0.8195  \n",
      "\n",
      "Fold: 28  Epoch: 26  Training loss = 1.8995  Validation loss = 0.8183  \n",
      "\n",
      "Fold: 28  Epoch: 27  Training loss = 1.8616  Validation loss = 0.6217  \n",
      "\n",
      "Fold: 28  Epoch: 28  Training loss = 1.9067  Validation loss = 0.7556  \n",
      "\n",
      "Fold: 28  Epoch: 29  Training loss = 1.9445  Validation loss = 0.6579  \n",
      "\n",
      "Fold: 28  Epoch: 30  Training loss = 1.8762  Validation loss = 0.9284  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 4  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 1.8462  Validation loss = 0.9502  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 1.8453  Validation loss = 0.9336  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 1.9427  Validation loss = 0.9301  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 1.8964  Validation loss = 0.9893  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 1.8731  Validation loss = 1.0092  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 1.9010  Validation loss = 1.0489  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 1.8386  Validation loss = 0.9833  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 1.9545  Validation loss = 1.0126  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 1.8525  Validation loss = 0.9740  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 1.8330  Validation loss = 0.9870  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 1.8458  Validation loss = 0.9974  \n",
      "\n",
      "Fold: 29  Epoch: 12  Training loss = 1.8427  Validation loss = 0.9959  \n",
      "\n",
      "Fold: 29  Epoch: 13  Training loss = 1.8216  Validation loss = 0.9755  \n",
      "\n",
      "Fold: 29  Epoch: 14  Training loss = 1.8233  Validation loss = 0.9857  \n",
      "\n",
      "Fold: 29  Epoch: 15  Training loss = 1.8344  Validation loss = 0.9507  \n",
      "\n",
      "Fold: 29  Epoch: 16  Training loss = 1.8259  Validation loss = 0.9696  \n",
      "\n",
      "Fold: 29  Epoch: 17  Training loss = 1.8516  Validation loss = 0.9574  \n",
      "\n",
      "Fold: 29  Epoch: 18  Training loss = 1.8205  Validation loss = 0.9676  \n",
      "\n",
      "Fold: 29  Epoch: 19  Training loss = 1.8265  Validation loss = 0.9593  \n",
      "\n",
      "Fold: 29  Epoch: 20  Training loss = 1.8179  Validation loss = 0.9539  \n",
      "\n",
      "Fold: 29  Epoch: 21  Training loss = 1.8247  Validation loss = 0.9684  \n",
      "\n",
      "Fold: 29  Epoch: 22  Training loss = 1.8421  Validation loss = 0.9554  \n",
      "\n",
      "Fold: 29  Epoch: 23  Training loss = 1.8448  Validation loss = 0.9681  \n",
      "\n",
      "Fold: 29  Epoch: 24  Training loss = 1.9188  Validation loss = 0.9639  \n",
      "\n",
      "Fold: 29  Epoch: 25  Training loss = 1.8208  Validation loss = 0.9847  \n",
      "\n",
      "Fold: 29  Epoch: 26  Training loss = 1.8417  Validation loss = 0.9918  \n",
      "\n",
      "Fold: 29  Epoch: 27  Training loss = 1.8347  Validation loss = 1.0197  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 3  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 1.8279  Validation loss = 2.2316  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 1.8360  Validation loss = 1.4228  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 1.9574  Validation loss = 1.3271  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 1.8251  Validation loss = 2.0738  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 1.8488  Validation loss = 1.7101  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 1.8110  Validation loss = 1.7223  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 1.8016  Validation loss = 2.1253  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 1.8024  Validation loss = 1.8399  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 1.8148  Validation loss = 1.1579  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 1.7882  Validation loss = 1.7058  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 1.7965  Validation loss = 1.6926  \n",
      "\n",
      "Fold: 30  Epoch: 12  Training loss = 1.8360  Validation loss = 1.9837  \n",
      "\n",
      "Fold: 30  Epoch: 13  Training loss = 1.8178  Validation loss = 2.0794  \n",
      "\n",
      "Fold: 30  Epoch: 14  Training loss = 1.7845  Validation loss = 1.6670  \n",
      "\n",
      "Fold: 30  Epoch: 15  Training loss = 1.8590  Validation loss = 1.0021  \n",
      "\n",
      "Fold: 30  Epoch: 16  Training loss = 1.7947  Validation loss = 1.8214  \n",
      "\n",
      "Fold: 30  Epoch: 17  Training loss = 1.7776  Validation loss = 2.0039  \n",
      "\n",
      "Fold: 30  Epoch: 18  Training loss = 1.8191  Validation loss = 1.6218  \n",
      "\n",
      "Fold: 30  Epoch: 19  Training loss = 1.7797  Validation loss = 1.8456  \n",
      "\n",
      "Fold: 30  Epoch: 20  Training loss = 1.7977  Validation loss = 2.2988  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 15  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 1.8196  Validation loss = 1.8881  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 1.8044  Validation loss = 1.9532  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 1.8103  Validation loss = 1.9864  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 1.8691  Validation loss = 2.0299  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 1.8525  Validation loss = 2.0383  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 1.8024  Validation loss = 2.0153  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 1.8023  Validation loss = 2.0197  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 1.8329  Validation loss = 2.0310  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 1.8442  Validation loss = 2.0323  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 1.7840  Validation loss = 2.1328  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 1.8013  Validation loss = 2.0849  \n",
      "\n",
      "Fold: 31  Epoch: 12  Training loss = 1.7726  Validation loss = 1.9653  \n",
      "\n",
      "Fold: 31  Epoch: 13  Training loss = 1.7683  Validation loss = 1.9848  \n",
      "\n",
      "Fold: 31  Epoch: 14  Training loss = 1.7794  Validation loss = 2.0570  \n",
      "\n",
      "Fold: 31  Epoch: 15  Training loss = 1.7920  Validation loss = 1.9146  \n",
      "\n",
      "Fold: 31  Epoch: 16  Training loss = 1.8083  Validation loss = 1.7971  \n",
      "\n",
      "Fold: 31  Epoch: 17  Training loss = 1.7711  Validation loss = 1.9107  \n",
      "\n",
      "Fold: 31  Epoch: 18  Training loss = 1.7679  Validation loss = 1.9268  \n",
      "\n",
      "Fold: 31  Epoch: 19  Training loss = 1.8105  Validation loss = 2.0053  \n",
      "\n",
      "Fold: 31  Epoch: 20  Training loss = 1.8364  Validation loss = 2.3609  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 16  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 1.5887  Validation loss = 1.5580  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 1.5253  Validation loss = 1.5314  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 1.5112  Validation loss = 1.6105  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 1.5056  Validation loss = 1.2766  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 1.4930  Validation loss = 1.4743  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 1.5228  Validation loss = 1.7610  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 1.4935  Validation loss = 1.8393  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 1.5084  Validation loss = 1.6693  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 1.4758  Validation loss = 1.6622  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 1.5262  Validation loss = 1.7085  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 1.4687  Validation loss = 1.5243  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 1.5045  Validation loss = 1.6517  \n",
      "\n",
      "Fold: 32  Epoch: 13  Training loss = 1.4770  Validation loss = 1.7640  \n",
      "\n",
      "Fold: 32  Epoch: 14  Training loss = 1.4609  Validation loss = 1.7575  \n",
      "\n",
      "Fold: 32  Epoch: 15  Training loss = 1.4693  Validation loss = 1.8738  \n",
      "\n",
      "Fold: 32  Epoch: 16  Training loss = 1.4936  Validation loss = 1.7419  \n",
      "\n",
      "Fold: 32  Epoch: 17  Training loss = 1.4656  Validation loss = 1.8511  \n",
      "\n",
      "Fold: 32  Epoch: 18  Training loss = 1.4664  Validation loss = 1.7503  \n",
      "\n",
      "Fold: 32  Epoch: 19  Training loss = 1.4519  Validation loss = 1.7334  \n",
      "\n",
      "Fold: 32  Epoch: 20  Training loss = 1.4549  Validation loss = 1.9593  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 4  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 84\n",
      "Average validation error: 2.64551\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 1.4395  Test loss = 2.6741  \n",
      "\n",
      "Epoch: 2  Training loss = 1.4320  Test loss = 2.6638  \n",
      "\n",
      "Epoch: 3  Training loss = 1.4259  Test loss = 2.6551  \n",
      "\n",
      "Epoch: 4  Training loss = 1.4209  Test loss = 2.6477  \n",
      "\n",
      "Epoch: 5  Training loss = 1.4168  Test loss = 2.6415  \n",
      "\n",
      "Epoch: 6  Training loss = 1.4135  Test loss = 2.6361  \n",
      "\n",
      "Epoch: 7  Training loss = 1.4107  Test loss = 2.6315  \n",
      "\n",
      "Epoch: 8  Training loss = 1.4085  Test loss = 2.6275  \n",
      "\n",
      "Epoch: 9  Training loss = 1.4066  Test loss = 2.6240  \n",
      "\n",
      "Epoch: 10  Training loss = 1.4050  Test loss = 2.6210  \n",
      "\n",
      "Epoch: 11  Training loss = 1.4036  Test loss = 2.6183  \n",
      "\n",
      "Epoch: 12  Training loss = 1.4024  Test loss = 2.6159  \n",
      "\n",
      "Epoch: 13  Training loss = 1.4014  Test loss = 2.6137  \n",
      "\n",
      "Epoch: 14  Training loss = 1.4005  Test loss = 2.6118  \n",
      "\n",
      "Epoch: 15  Training loss = 1.3997  Test loss = 2.6100  \n",
      "\n",
      "Epoch: 16  Training loss = 1.3989  Test loss = 2.6084  \n",
      "\n",
      "Epoch: 17  Training loss = 1.3982  Test loss = 2.6070  \n",
      "\n",
      "Epoch: 18  Training loss = 1.3975  Test loss = 2.6056  \n",
      "\n",
      "Epoch: 19  Training loss = 1.3969  Test loss = 2.6044  \n",
      "\n",
      "Epoch: 20  Training loss = 1.3964  Test loss = 2.6032  \n",
      "\n",
      "Epoch: 21  Training loss = 1.3958  Test loss = 2.6021  \n",
      "\n",
      "Epoch: 22  Training loss = 1.3953  Test loss = 2.6011  \n",
      "\n",
      "Epoch: 23  Training loss = 1.3948  Test loss = 2.6002  \n",
      "\n",
      "Epoch: 24  Training loss = 1.3943  Test loss = 2.5993  \n",
      "\n",
      "Epoch: 25  Training loss = 1.3938  Test loss = 2.5984  \n",
      "\n",
      "Epoch: 26  Training loss = 1.3933  Test loss = 2.5976  \n",
      "\n",
      "Epoch: 27  Training loss = 1.3929  Test loss = 2.5968  \n",
      "\n",
      "Epoch: 28  Training loss = 1.3924  Test loss = 2.5961  \n",
      "\n",
      "Epoch: 29  Training loss = 1.3920  Test loss = 2.5953  \n",
      "\n",
      "Epoch: 30  Training loss = 1.3915  Test loss = 2.5947  \n",
      "\n",
      "Epoch: 31  Training loss = 1.3911  Test loss = 2.5940  \n",
      "\n",
      "Epoch: 32  Training loss = 1.3907  Test loss = 2.5933  \n",
      "\n",
      "Epoch: 33  Training loss = 1.3903  Test loss = 2.5927  \n",
      "\n",
      "Epoch: 34  Training loss = 1.3899  Test loss = 2.5921  \n",
      "\n",
      "Epoch: 35  Training loss = 1.3895  Test loss = 2.5915  \n",
      "\n",
      "Epoch: 36  Training loss = 1.3891  Test loss = 2.5909  \n",
      "\n",
      "Epoch: 37  Training loss = 1.3887  Test loss = 2.5903  \n",
      "\n",
      "Epoch: 38  Training loss = 1.3883  Test loss = 2.5897  \n",
      "\n",
      "Epoch: 39  Training loss = 1.3879  Test loss = 2.5892  \n",
      "\n",
      "Epoch: 40  Training loss = 1.3875  Test loss = 2.5886  \n",
      "\n",
      "Epoch: 41  Training loss = 1.3871  Test loss = 2.5880  \n",
      "\n",
      "Epoch: 42  Training loss = 1.3867  Test loss = 2.5875  \n",
      "\n",
      "Epoch: 43  Training loss = 1.3864  Test loss = 2.5870  \n",
      "\n",
      "Epoch: 44  Training loss = 1.3860  Test loss = 2.5864  \n",
      "\n",
      "Epoch: 45  Training loss = 1.3856  Test loss = 2.5859  \n",
      "\n",
      "Epoch: 46  Training loss = 1.3852  Test loss = 2.5854  \n",
      "\n",
      "Epoch: 47  Training loss = 1.3849  Test loss = 2.5848  \n",
      "\n",
      "Epoch: 48  Training loss = 1.3845  Test loss = 2.5843  \n",
      "\n",
      "Epoch: 49  Training loss = 1.3841  Test loss = 2.5838  \n",
      "\n",
      "Epoch: 50  Training loss = 1.3838  Test loss = 2.5833  \n",
      "\n",
      "Epoch: 51  Training loss = 1.3834  Test loss = 2.5827  \n",
      "\n",
      "Epoch: 52  Training loss = 1.3830  Test loss = 2.5822  \n",
      "\n",
      "Epoch: 53  Training loss = 1.3827  Test loss = 2.5817  \n",
      "\n",
      "Epoch: 54  Training loss = 1.3823  Test loss = 2.5812  \n",
      "\n",
      "Epoch: 55  Training loss = 1.3819  Test loss = 2.5807  \n",
      "\n",
      "Epoch: 56  Training loss = 1.3816  Test loss = 2.5802  \n",
      "\n",
      "Epoch: 57  Training loss = 1.3812  Test loss = 2.5797  \n",
      "\n",
      "Epoch: 58  Training loss = 1.3808  Test loss = 2.5791  \n",
      "\n",
      "Epoch: 59  Training loss = 1.3805  Test loss = 2.5786  \n",
      "\n",
      "Epoch: 60  Training loss = 1.3801  Test loss = 2.5781  \n",
      "\n",
      "Epoch: 61  Training loss = 1.3797  Test loss = 2.5776  \n",
      "\n",
      "Epoch: 62  Training loss = 1.3793  Test loss = 2.5771  \n",
      "\n",
      "Epoch: 63  Training loss = 1.3790  Test loss = 2.5766  \n",
      "\n",
      "Epoch: 64  Training loss = 1.3786  Test loss = 2.5761  \n",
      "\n",
      "Epoch: 65  Training loss = 1.3782  Test loss = 2.5755  \n",
      "\n",
      "Epoch: 66  Training loss = 1.3778  Test loss = 2.5750  \n",
      "\n",
      "Epoch: 67  Training loss = 1.3775  Test loss = 2.5745  \n",
      "\n",
      "Epoch: 68  Training loss = 1.3771  Test loss = 2.5740  \n",
      "\n",
      "Epoch: 69  Training loss = 1.3767  Test loss = 2.5735  \n",
      "\n",
      "Epoch: 70  Training loss = 1.3763  Test loss = 2.5730  \n",
      "\n",
      "Epoch: 71  Training loss = 1.3759  Test loss = 2.5724  \n",
      "\n",
      "Epoch: 72  Training loss = 1.3755  Test loss = 2.5719  \n",
      "\n",
      "Epoch: 73  Training loss = 1.3751  Test loss = 2.5714  \n",
      "\n",
      "Epoch: 74  Training loss = 1.3747  Test loss = 2.5709  \n",
      "\n",
      "Epoch: 75  Training loss = 1.3743  Test loss = 2.5703  \n",
      "\n",
      "Epoch: 76  Training loss = 1.3739  Test loss = 2.5698  \n",
      "\n",
      "Epoch: 77  Training loss = 1.3735  Test loss = 2.5693  \n",
      "\n",
      "Epoch: 78  Training loss = 1.3730  Test loss = 2.5688  \n",
      "\n",
      "Epoch: 79  Training loss = 1.3726  Test loss = 2.5682  \n",
      "\n",
      "Epoch: 80  Training loss = 1.3722  Test loss = 2.5677  \n",
      "\n",
      "Epoch: 81  Training loss = 1.3717  Test loss = 2.5672  \n",
      "\n",
      "Epoch: 82  Training loss = 1.3713  Test loss = 2.5667  \n",
      "\n",
      "Epoch: 83  Training loss = 1.3709  Test loss = 2.5661  \n",
      "\n",
      "Epoch: 84  Training loss = 1.3704  Test loss = 2.5656  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXl4VNX5xz8n+0oSSAhLgIRAWAybEJRFUbGKoLSIWlzq\nrtX6cwO1ta3aaqvVYituddeiKG1BUQEVFapWo7ILJIRFsgCBJJCN7Jmc3x9n7mT2mSSThMycz/P4\nRO7c5Uwy873f+573fY+QUqLRaDQa/yGouweg0Wg0Gt+ihV2j0Wj8DC3sGo1G42doYddoNBo/Qwu7\nRqPR+Bla2DUajcbP0MKu0Wg0foYWdo1Go/EztLBrNBqNnxHSHRdNTEyUqamp3XFpjUaj6bFs3ry5\nTEqZ5Gm/bhH21NRUNm3a1B2X1mg0mh6LEKLAm/10KEaj0Wj8DC3sGo1G42doYddoNBo/Qwu7RqPR\n+Bla2DUajcbP0MKu0Wg0foYWdo1Go/Ezepawr14Nf/lLd49Co9FoTmp6lrB/+ik89lh3j0Kj0WhO\nanwi7EKIu4UQu4QQO4UQ7wghInxxXgf69oWqKqiv75TTazQajT/QYWEXQgwE7gAmSSkzgWBgQUfP\n65S+fdXP0tJOOb1Go9H4A74KxYQAkUKIECAKOOyj89piCHtJSaecXqPRaPyBDgu7lPIQsBgoBIqB\nSinluo6e1yla2DUajcYjvgjFJAA/BdKAAUC0EOIqJ/vdLITYJITYVNreUIoWdo1Go/GIL0Ix5wIH\npJSlUsom4F1gqv1OUsqXpJSTpJSTkpI8thN2jhZ2jUaj8YgvhL0QOF0IESWEEMBMINcH53UkJgYi\nIrSwazQajRt8EWP/DlgBbAF2mM/5UkfP6xQhlGvXwq7RaDQu8ckKSlLKh4CHfHEuj2hh12g0Grf0\nrMpT0MKu0Wg0HtDCrtFoNH5GzxV2Kbt7JBqNRnNS0jOFvbFR9YzRaDQajQM9U9hBh2M0Go3GBVrY\nNRqNxs/Qwq7RaDR+Rs8Vdt26V6PRaJzS84Td6DOjHbtGo9E4pecJe1gYxMdrYddoNBoX9DxhB12k\npNFoNG4ICGHfsmULgwYNokTfDDSdRGFhIU1NTd09DI0GCBBhX7t2LQcPHuTAgQOdOCiNP7Jq1SoW\nL17sdp/i4mLS09MZOXIkb7zxBs3NzV00Oo3GOQEh7Js3bwagpqams0ak8VNef/11/v73v7vdp7Cw\nkObmZmpra7nuuusYNWoUS5cu1QKv6TZ6rrCXlYHJ5NXuhrDX1tZ25qg0fkhpaSmlpaVIN72JysrK\nAOXu33//fWJiYrjmmmvIysqivr6+q4aq0VjoucIuJRw75nHXkpISioqKAO3YNW2npKSEpqYmqtz0\nJjLW8E1KSmLu3Lls2bKFhx9+mG3btunwn6Zb6LnCDl6FYwy3DlrYNW3HEG3DlTvDeM1Yy1cIQVZW\nFgAVFRWdPEKNxpGAEnYditG0hYaGBotTL3VT6VxWVkZYWBgxMTGWbfHx8QBUVlZ27iA1GicEhLCn\npKQA2rFr2oa1mLtz7KWlpSQmJqLWclcYwq4du6Y7CAhhnz59OqCFXdM2rIXdk2M3wjAGWtg13UmP\nEvZFixYxbNgwSEiA4GCPwl5aWkpRURGTJk0iKipKh2ICmOrqahobG9t0TFsduzVa2DXdSY8S9pCQ\nEIqKipBCQGKiR2E34usTJ04kOjpaO/YAJisri0ceeaRNx1hXKrfVsUdERBAWFqaFXdMt9ChhT0pK\norGxkerqaq+KlAxhnzBhAtHR0dqxByi1tbXk5eWxb9++Nh1niHlMTIzHrBh7xw7KtWth13QHPUrY\njS9PWVmZV8K+adMmhg8fTlxcHFFRUdqxByiFhYUAHD9+vE3HlZaWEhoaSnp6ukvH3tTURHl5uRZ2\nzUlFjxJ243G3tLTUa8c+ceJEAB2KCWAKCgqA9gl7YmIiffv2dSnsxjntQzGghV3TffQoYW+LYzcm\nTq2FXYdiApP8/HwAjnlRqWxNSUkJffv2JTEx0WUoxtiuHbvmZKJHCbuDY6+uhro6p/taT5wCOhQT\nwHTEsSclJZGUlOTSsRvbXQm7TYHSihWwZEmbxqDRtIceJewOjh1crn1qCPupp54K6FBMIGMIe2Vl\nZZs6LhrCnpiYSFVVldN0Sft2AtbYOPbXX4fLLoNHH23HO9Bo2kaPEvbY2FjCwsJaHTu4DMds3rzZ\nMnEK6Dz2AMYQdmhbXrkRijFE21k4xpNjr6iogFdfhRtuUMs6Hj+uGthpNJ1IjxJ2IURrvNMLYTfC\nMKAdeyCTn59PREQE4H04xugTYzh2cC7s7mLscXFxXFVfDzfeCOefDw88AM3N4KZTpEbjC3wi7EKI\neCHECiHEbiFErhBiii/O6wxLvNONsJeVlVFYWOgg7NqxBx6NjY0cPnyYsWPHAt4Lu3WIxWZux8l+\ncXFxhIaGOrw2dedOXgYazjkH3nsPBg5UL7RxElejaSu+cuxLgI+llCOBcUCuj87rgDeO3X7iFFQo\npq6ujpaWls4amuYk5ODBg0gpmTBhAuC9sBtVp54cu7N2AgCsX89Z77zDGqDgqacgIkJVS6sTtf2N\naDRtoMPCLoSIA84EXgWQUjZKKTstx8vi2KOjITLSqbBv2rQJaJ04BeXYQbfuDTSM+LrxWfBW2A13\nbh1jd+XYnU2csno1prAwLgHKjc9cnz7qp3bsmk7GF449DSgFXhdCbBVCvCKEiPbBeZ1icexCuMxl\n37x5M8OGDbNMnIIW9kDFyGFvq2O3XhWpd+/eCCHa5tizs6kZOZJ6rCZstbBrughfCHsIcCrwDynl\nBKAG+I39TkKIm4UQm4QQm9w1VPJEYmIiFRUVNDU1uRX2SZMm2WyLiooCdOveQKOgoAAhBJmZmQgh\n2hWKCQkJISEhwaVjdxD2hgbYsoVGcyhQC7umq/GFsB8EDkopvzP/ewVK6G2QUr4kpZwkpZzk9NHV\nS4xjjx075lTYy8vLKSwstDg0A8Oxa2EPLAoKCujfvz+RkZHEx8d7XX1aWlpKSEiIpf2usyIlKaXz\nUMyWLdDYSNC0aYDVKkrx8epJUwu7ppPpsLBLKY8ARUKIEeZNM4Gcjp7XFZ7aChgLV6elpdlsNxy7\nDsUEFvn5+aSmpgLQu3fvNoVikpKSLKsiOWsrUFNTQ319vaNjz84GIOLsswErxx4crNYS0MKu6WR8\nlRVzO7BMCPEDMB7otPI6p43ArAo+Dh06BMBAI7XMTFc79uuuu4677767S66lcU1BQQFDhgwB2ibs\nJSUlNk7cmWN3WXWanQ2pqUSmpRESEmJbFNWnjxZ2TafjE2GXUm4zh1nGSil/JqUs98V5neHg2Jua\nwKofx8ki7J988gmff/55l1xL4xyTyURRUVG7hL20tJS+Rkotzh27y+Kk7GyYMgUhhGMjMC3smi6g\nR1WeghPHDjbhGEPY+/fvb3NcV4ZiampqKC4uJj8/H6nLx7uN4uJimpubLaGYPn36tDkUY5CUlERZ\nWZnN39NpO4GiIjh0CKaoGj0t7JruoMcJex9zZoGrIqWDBw+SnJxMWFiYzXFd6dj3798PqHU2ddvW\n7sNIdfRFKCYxMZHm5mabbo1OQzHm+Dqnnw5oYdd0Dz1O2ENDQ4mPj3fr2O3DMNC1eezWS7AZ4qLp\neoziJGthLy8v91h9bPSJsQ7FOCtScurYs7NVlem4cYAWdk330OOEHdy3FXAl7F2Zx66F/eTAEPbB\ngwcDStillLY90p3gzIk7aytQVlZGSEiITSEc2dkwaZLq5IgLYa+pgfr69r8xjcYDPVLYLRkKhlOy\nclGuhD0yMhLoOmE3rqeFvfsoKCggKSnJ8rTWu3dvwHP1qXVxkoEzx24UJxkpkdTXqxz2Ka098ByE\n3fjMateu6UR6pLBbHHtYmMoLNn8R6+vrOXbsGCkpKbYH7NtH0OzZJEdEdFkoZvz48cTExGhh70by\n8/MtYRjwXtit+8QYuArF2IRhtmxRWVp2wm7zhKCrTzVdQI8Udpuc4uRklYmA61RH3nwTPvmE08LC\nusyxDx8+nNTUVJtFHjRdi3UOO7QKu6fqU+s+MQauQjFOJ06thD0uLo7a2trW1Ze0sGu6gB4p7IZj\nl1LC1KnwxRfQ3Oxa2NetA2BoaGinC3tdXR1FRUUMGzaM1NRU7di7CSklBQUFllRH6FgoJjo6msjI\nSKehGAvmwiT69bNsMloSWFy7FnZNF9AjhT0pKYnGxkaqq6th1iyoqIDvvnMu7OXl8P33AAwOCur0\nUMyBAwcAtLB3MyUlJdTX17c7FGPdJ8bAvkjJJhQjpaUwyRrjHLoRmKYr6ZHCbvNY/JOfqB4cH33k\nXNg3bABzelsKnT95unfvXkAJ+5AhQ6isrNS57N2Afaoj771HQnEx4J2wW/eJMbAOAZpMJo4fP97q\n6ouK4PBhLeyak4IeLeylpaWqY96UKfDxxxw6dIiYmBh69erVuvO6dRAbC6NG0b+lpdMdu5HqaDh2\n0Jkx3YEh7KmpqWAywZVXErJ4MXFxcV6FYpx1ILV27MePH0dK2erYncTXwYmwR0RAVJQWdk2n0iOF\n3WHV+FmzYPNmqvftY+DAgbZOa906OPtsSEsjuamp0x37vn376N27NwkJCVrYuxEbx15QAHV1sH+/\nV9Wn9u0EDKwdu0Oue3a2WtHLXJhk4CDsoIuUNJ1OjxR2G8cOcMEFAAzevds2DLN/Pxw4AOedBykp\nJDU0dImwDxs2DMAi7DozpuvJz88nLi5OFQ/lmpfgbYOwW6c6Glg7docGYN9+qwqT7Ba11sKu6Q56\npLA7OPbx4yE5mXGHD9sKuzkbxhD2+IYGmrtQ2Pv06UN0dLR27N2ATaqjIexHjzKgA6GYpKQkqqur\nady8mYj//IdbgdEffAC/+51DYZKBFvYexqFDagWsHk6PFPbY2FhCQ0NbhT0oCHneeUyvrSXFuqvj\nunUwZAgMGwbmoqVe1dWdNq6GhgYKCwstwi6E0JkxPmTLli2MGTOGb7/91uO+NqmOhrADI0JD3Qq7\n0SfGVYw9Dgg56yyynnmG54EBzzwDjz+u5np++lOHY6KjowkODnYsUtLCfvLR0ACjR8Mzz3T3SDpM\njxR2IYTDwgeV06bRBzjVZFIbmpth/Xrl1oWAQYMASOhEx56fn09LS4tF2EHFeP1V2O+77z7mzJnT\nWnzTiRw5coSf/vSn7Ny5kwcffNDtvkYOu41jN/dzSfew7qlhFpyFYpKSkrgVCDpxgrevu45koP74\ncVVtWlKiairsEEIQFxfn2FZAC/vJx549UFWlQrg9nB4p7OCYU1wwfDgmINNchcr336s/0nnnqX+b\nHXtSY6PH7n7txTojxsCfHftnn33G2rVrWbRoUadep6GhgYsvvpjjx49zzTXX8Omnn7JlyxaX+1dU\nVFBVVaWEXUrYvRvOPx+Awc3NlowWZzgrTjJI7tWLO4GySZPYGBdHbUwMEQkJyji4wWkjsOPHVbaO\n5uRh50718+jR7h2HD+ixwm7v2AtOnOB7IGXXLrXh00/VF+6cc9S/zbH3FFR1aGfgStgrKio8dhTs\niRQWFhIbG8uzzz7LG2+80SnXkFJyyy23kJ2dzdKlS1myZAm9evXi8ccfd3mMTapjSYkqUps6FeLj\nGVBbi8lkoqqqyumxztoJGAz96iv6AdvPP9/5ItYucCrsUqrCOs3JgyHsduso90R6rLDbO/ZDhw7x\nERCdk6O6Pa5bB1lZYK42JDaWhshIUui8nuz79u2jV69eNmXm/poZU1NTw7Fjx7j33nuZOXMmt9xy\nCxs3bvT5dZYsWcIbb7zBgw8+yPz584mLi+PWW29lxYoVNu2RrbFJdTTi66NGwdChJJrnWFyFY5w1\nAAPAZKLv0qV8D+wymwqHJfFc4FTYQYdjTjYMU6gde/dh79gPHTrEp0FBCCnhP/+B775TValW1Pbu\nzSC8rz5tbm5m5syZrDOyazxgZMRY59H7ay57kTnkNXToUJYvX06/fv24+OKLLaEMX7Bu3ToWLVrE\nvHnzeOihhyzb77zzTkJCQli8eLHT42xWTrIW9vR04s1i6krYXYZiVq4kOD+fx4GyY8c67thBC/vJ\nhnbs3U9iYiIVFRU0NTUBStgP9e+vJqYefljFL434upn6xMQ2tRU4fvw469ev5+mnn/Zqf+tURwN/\nFfbCwkJALWKRmJjIe++9R1lZGZdddpnlb9IRpJRcd911jB49mqVLlxIU1PpR7d+/P9deey1vvPEG\nR44ccTi2oKCAyMhI5ahzcyEmRs2xDB1KVEkJQbh37A59YqSEv/wFMjL4X58+lJaWOjYAc4MW9h5A\nbS38+CNER6u5uR6+EEqPFXbDLRktWA8dOsSAlBQ1SXb0qPoym9edNGhKTm5TKMaIw65bt85jjLyp\nqYn8/HyGDx9usz0xMZHIyEi/FnaACRMm8PLLL/PFF1+wcOHCDp//6NGjHD58mJtuuomYmBiH1++5\n5x4aGxsdbrofffQRb775JqNHj1ZPTrm5MHKkmm9JTyeouZkU3Au7Q5+Yzz6DrVvhvvvobV7UWodi\n/IzcXHUDP+MM9e8e7tp7rLDb98c+ePCgKk6aNUvtcNZZluXJDEz9+5MM1Ho5aWWIeVNTE6tXr3a7\nb2FhIc3NzQ6O3chl97cYe2FhIUFBQQwYMMCy7aqrrmLhwoU8++yzvPTSSx06f05ODgCjR492+vrw\n4cOZP38+zz//PFVVVTQ2NnLvvfcye/Zs+vfvz1tvvaV2zM1VYRiA9HQAhuJZ2G14/HEYMACuuoqk\npCQKCwupra31OhQTFxdHTU1N65OMFvaTDyMMM3Om+qmFvXuwX9HGsiTerFnKrc+b53CMTEkhCDAd\nPOjVNawzJ1asWOF2X2cZMQb+mPJYWFjIgAEDCLUroX/iiSeYNWsWt912G1988UW7z59rjo2PMkTZ\nCb/+9a+prKzkgQce4IwzzmDx4sXceuutfPfdd4wcOVI9Uh861CrsQ4cCkI77GLuNYG/aBJ9/Dnfd\nBeHhJCYmWsbWFscOVp+nuDjVkVQL+8nDrl3KCBq1CD18ArXHCru1Yz9x4gRVVVVqSbzERNU+9brr\nHI4JMhcp0UZhnzZtGh9//DEnTpxwua91u157/FHYi4qKLGEYa4KDg1m+fDnDhg1j/vz5/Pjjj+06\nf05ODr169bJ5IrBn0qRJzJw5k6effpq8vDxWrFjB888/b1lvlt271U9D2AcNgpAQRrmpPnXoE7N4\nsRLiX/4SaG0rAG0Xdks4RgiVraWF/eRh5071OTE+b9qxdw/WjcAc+rDHxjotGgkxT2QGHz7s1TUM\nYb/++uupr69n7dq1Lvfdt28f0dHRJCcnO7yWmprK8ePHXeZO90QKCwudCjuo0MMHH3xAS0sLc+fO\nbdf7zs3NZdSoUQ490e1ZvHgxV199NVu3bmX+/Pn2J1E/R45UP4ODITWVDA/CbnHsBw/CihVw441g\nbgVtLeZtyYoB3S/mpGbXLjjlFDD+plrYuwdrx+5ySTw7ws0x1lAvH7MMQZo9ezbJycluwzHOUh0N\n/C2XvaWlxaVjNxg+fDj//ve/2b17N1dddRWmNlZZ5ubmuoyvWzN+/Hj++c9/kpaW5vji7t0QEmKJ\nrQOQnk66lE6FvaGhgcrKylbBfuEFtUjLbbdZ9nG2DqontLB3Mc3NsHy5ZYEdj1RVQWEhZGaqrJjo\naB2K6S5CQ0OJi4tz7thdENmvH1VAmFX+uzsMYU9ISGDevHmsWbOG2upqVdVqV5LuLNXRwOhZ4i/C\nXlpaSkNDg1thBzj33HN56qmn+PDDD1m6dKnX5y8vL+fIkSNu4+tekZsLw4fbttIdOpSUpianC1rb\n9Fivr4eXXoKLLgKrm4a1sHfIset+MZ3H22/D5ZertZC9wTxRT2am+pmcrB17d5JkTj07aI6ZexL2\nqKgoDgJRXn6hqqqqCAsLIzw8nEsuuYTa2lq2PfGEyo//6CPLfiaTiR9//NGlsPtbLruR6jjImLNw\nw2233ca4ceN44oknvO7RY0xOeuPYPZyoNb5ukJ5Or+Zmmp3c3G2qTv/9b1XBfPvtNvsYLj0oKMhh\nTVRXuHTsVpXTGh9iPFl7+30zMmJOOUX97NtXO/buJDEx0eLYExISiIqKcrt/UFAQh4OCiPYy3bGq\nqsqyzN6MGTPo06cPOUYV6n/+Y9mvqKiIpqYml8Let29fIiIi/E7YnTr2qir1KGxGCMF9993H7t27\n+fDDD706v5Hq2CHH3tiouvQ5EXaAXk5E1VJ1mpgITz+tjjXS38wYLr1Pnz42RVPucBuKcdGMTNNO\nqqtb12Ewf049snOnWq7QaPOsHXsrQohgIcRWIYT7hG8fYjh2S6qjFxwNDSXOy8m8yspKi7CHhITw\ns5/9jOJt29SLq1Yp8cB9qiN0X1/2NWvWMGfOHJ93s3Qp7PX1kJEBv/2tzebLLruM1NRUHn/8cZdd\nFa3Jzc0lMjKyte1ue9i7V1Uf2wu7OeUxsbLSYSyGYx906BBs3qzcut2cieHYvQ3DAMTExCCEcBT2\nhgZV8ajxHWvWqN+rEN4L+65dqg+7caPu21cLuxV3Arke9/Ih1o7dW2EvCw+nV02N6qHtAWvHDnDJ\nJZcQb/Qer6hQ/d5xn+po0B3C/v7777N27Vqbnjq+oLCwkOjoaBISEmxf+Phj9Qj7wgvKuZsJCQlh\n0aJFZGdn87///c/j+XNychgxYgTBwcHtH6R1jxhrzMI+xGRyaC2xY8cOQkNDGfjuuyrF8Re/cDit\nIezeTpyCelJ06Mmui5Q6h5UrleOeOLFtjt2Ir4MS9tJS7ydfT0J8IuxCiBRgDvCKL87nLdYxdm+F\n/VhUlHrTTnqM2GMv7Oeccw4poaGURUcjY2M5+txzXH311SxcuJCkpCS3OdfdIex79uwBfD9pa6Q6\nOmQALV+uFnSurga7Nr7XX389iYmJbtvtGnibEePhJOrniBG222NiqOvVy2n16YYNG7hg3DhC3nsP\nrr9eFbrZERUVRVRUVJscO6hwjMMqSqCF3ZfU1cHatao4MS3NO2E/dkxpgRFfB3VjMJlUz/weiq8c\n+1PAfUCX3uISExNpbGykuLjYa2GvML6sXhQp2Qt7WFgYI3v3Jq++njVBQYSsXs3q997juuuu48sv\nv3Qbcx0yZAjHjh1zW+Tka/Ly8oDOE3Ybamrgww/hmmtU9d4zz9g4nqioKG6//XbWrFnDTmOyygkn\nTpygoKDANxkxQ4ao1DU7avv3d6g+raqqYvPmzdwVGam+1FYpjvbMmDGD0+36EHlC94vpAj75RIW2\n5s+HwYOVsHsK/Rmteu0dO/ToCdQOC7sQ4kKgREq52cN+NwshNgkhNvkqNGDtmlLMKyR5otq8RFp7\nhB1gcFgYxSYTXyQl0Qc4tGwZzz//vCphd0NX57JXVVVZOh8WevtI6iVOhf3DD9WXasECuPNO2LdP\nuScrbrvtNqKionjiiSdcntu4GXnt2D/6CMaOtVnXFGht/uWE5sGDHRz7V199RZDJxNSdO2H2bNvc\ndzvWrl3LPffc4934zGhh7wJWrlQVvTNmKGGvr/eceWSYDGfC3oPj7L5w7NOAuUKIfGA5cI4Q4i37\nnaSUL0kpJ0kpJ7X1MdYV1nFObx37CSNFzUthjzNuBGaia2q48Prr+esPP0BMDJFr1nh13a5OeTTC\nMODbm0ldXR0lJSWOwr58uSrHnj5dPQoPHAhLltjs0qdPH2666Sbeeecdlzcbb3rE2PDYY7Bjh0pB\nNZZFbGmBvDzH+LoZkZ7OIKDcypFt2LCBq4KDCS8vd0hx9AVa2DuZxkZlLubOVXULxufTk6nZtUtV\nFVvrh1E9HsjCLqW8X0qZIqVMBRYA66WUV3V4ZF5gfYPwVthlXBy1QrSKgBscHHtTExw/TsTgwSqW\nfOGF8N57Nul9rjCEPdfeWXYShrBHRET4VNiNmgEbYa+oUM75sstU2X5oqAplfPZZ66OumYULFyKl\n5O9//7vT8+fk5BASEuJ2ItrC7t3w1Vdw7bWt69uWlakvc12dS2EPHTmSYKDJagWmL9av5w9hYcr9\n2y3Q4gschN1Y2UsLu2/4/HOorFRhGGgVdk/fc2Pi1Hq+SIdiupf2OPao6GiOBAd7dOwNDQ00NDTY\nCrtxBzfu6JdcombPv/rK43WTk5OZPHkyDz/8cJeI+549exBCMG3aNJ+GYpymOr7/vnJMCxa0brvp\nJoiIULF2KwYPHszll1/Oyy+/bCt0ZnJzcxk+fLhD10invPqqahnwl78ot5afD3PmgLFEnwthjzI/\ndgtzg7Ly8nKGbd3K4Lo6eOCB1rQ3H+Ig7GFhqqdRgAr7xx9/THl5ue9OuHKl+n0aN2VvHLuUrT1i\nrOndWxmUQHbs1kgp/yulvNCX53SH4djDwsK8Tj+Ljo7moBAehd1oJ+BW2C+4QBU2WBUruUIIwcqV\nK4mKimLu3Llef6hNJhMPPPAAs2bN4v/+7/946qmnWL16NXl5eW5zwvPy8hgyZAgZGRk+dexOhX35\nclXcMXly67bERLjySli61CG74I477qCmpoZ33nnH4fw5OTnehWEaG+Gf/1SP3snJcOaZ8K9/qfzz\nG25Q+7g4T7g5fh9mdnNf/ve//B6oSUuDiy/2fO12EBcXR3V1Nc3WT3fO+sXU1qrf28qVnTKOk4GS\nkhIuuOACnrG76beb5mZVV3LhhRAerrb16aOeqt0J+9Gj6vdvHV8HdWNPStKOvbuIjY1VeccDB3rs\nAmgQHR1NYUtL+4Td+EMbj2pRUcohvvuuyqTwQEpKCitXrqSgoIAFCxbYfsmd0NTUxC9+8Qv+9Kc/\nUVRUxJtvvsndd9/NRRddxMiRI1liF8O2Zs+ePYwYMYIhQ4ZQXl5uaTXbUQoLCxFCtD4hlZWp3jkL\nFjh21LzzThUSecU2C3bSpEmMHTuWV1991WZ7Q0MD+/fv927i9IMP1NPSjTe2bps7V12rulrdWFzd\n7Pv1o04IosyTy8dfeYVTgLCHH+4Utw5OerKD834x776rep1ccgksXOhVvUVPY5c5PLd9+3bfnPDL\nL9Xv0brk0v7YAAAgAElEQVS7pxCtmTGucDZxatDDi5R6tLALIUhMTPQ6DAMq7S7fZEIePuxWjL1y\n7KC+gEePwtdfe3X9adOm8dxzz7Fu3Truv/9+l/vV1dUxb9483nnnHR5//HF27dpFRUUFpaWlZGdn\nk5aWxoYNG5weK6Vkz549ZGRkWKo3fRWOKSwspF+/foQbzmjlSvV7tA7DGIwZA2efDc8+69Bm4MYb\nb2Tz5s1sMyp5pWRvXh4mk8k7x/7KK6q/ut26tlx7rXrNze8WITgUFqYWtm5pYdr69RRERRF6+eWe\nr9tOvO4Xs3Spevq5/Xb4+9/V78/c5M5fMITdXdprm1i5UrlzY/U0AythLy4uZtmyZbbLYhrzP/ah\nGOjxbQV6tLADZGVltSmnODo6moOAMJncFil55dhBpcZFRHgVjjG46aabuO2221i8eHHrEm7W166s\n5MazzqL3mjXsnDGD+/71L7jrLsuN7PTTT2fatGlsNGLJoJ5AFiyA7GyKi4s5ceIEI0aMsIRMfBWO\ncUh1XL5cpRWOHev8gDvvVBNYdgVLV155JeHh4cq1SwmXXsqoiRPJAea8+CLcfTc895zzv1FBgeoH\ncv31KhZqzw03KLfrhqMxMSRWV1P15ptk1NezdfZs5+fyEYawOxQpWTv2Q4fUhPPVV6teNe+8A9u2\nwamngoubeFupr69n1apVXrV26CwMYd+3bx91dXVtP0FLi2oZ8a9/wX33qd/TBRc41iwMGmQR9r/+\n9a9cddVVpKWlsXjxYlV1vHOnemqy/j4b9PRGYFLKLv9v4sSJsrt45pln5BwlJVJ++63L/T744AMJ\nyI0bN7ZuvOceKSMipGxpsd354ouljIuT8plnpKyt9WocjY2NcsaMGTI0NFRmZWXJSy+9VN57771y\n9S23yGMhIWp8IGVMjJQjR6r//9//LMc//fTTEpAHDx5UGy65RO0TEiL3/N//SUCuW7dOHjx4UALy\nH//4h9e/I3dkZGTISy+9VP3j0CEphZDyD39wfYDJJOWMGer3c+iQzUuXX365jI+Plw2vvCIlyF2j\nRsn/gDSNGSNldLR6P4MHS7lnj+05H3xQXTc/v93v44Nhw2SNELI8NVXmgfz6iy/afS5v2LBhgwTk\n+vXrWzfefrv6vRg8/rh6z3v3tm7btUv9/UNCpCwo6PA47r//fgnI1atXd/hc7eXMM8+UQggJyM2b\nN3t30KFDUv7jH1LOmiVlr16t34+wMCknTZLS2Xn+8Ae1T329nDlzpkxPT5fnnnuuBGRiYqI8OGiQ\nbJo+3fn1Fi6UMjpaHj58WJ599tmyqKio9bXNm6X8+c+lvPFGKe+9V8pHH1Vjy8lp+y+jjQCbpBca\nG3DC/uqrr8pxxodixQqX+7311lsSkHusReUXv5ByyBDHnXNzpZw2TZ0zOVnKJ56QsqrK41hKS0vl\nHXfcIc877zyZkZEhw8LC5Csgy0Fu/9WvpNy+XcrmZilPnJBy4EApJ05UQimlzM7OloB87733pFy/\nXl170SIpL7pISpDLQRbs3ClNJpMMDQ2Vv/nNb9r6q3KgpaVFRkZGykWLFqkNTz2lrpub6/7AvXul\njIxUY7O6KX7++eeyP8j6qCgpp0+XCy67TA4dOtS4mJTffSdlYqKU/fpJuXOn2t7cLGVKipTnn9+h\n97JsyhSLONwUFiYbGxs7dD5PbN26VQLy3Xffbd1oCE9Tk3q/o0dLOXWq48Hbt6v9li7t0BgOHz4s\nIyMjJSAvuuiiDp3LQnGxo9FxQ0tLi+zTp48888wzJSDfeOMNVzuqm9pjj0l52mmtQp6eLuWtt0r5\nyitSbtkiZUOD64u99po6Zv9+2bdvX3n99ddLKaX8+uuv5ezzz5cnQH6Ynu782L/8RUqQ/3z+eQnI\n5557rvW1X/1KyuBgKfv3V0bPGNvgwepv2YloYXfB8uXLZR/jD/HUUy73e978Bz1y5EjrxvPOkzIr\ny/kBLS1S/ve/Uv7kJ+rcCQlubxzOMJlMsj4rSzY6+3K/9ZY672uvSSmlrKurkyEhIfL3v/mNlJmZ\nUqamqqcFk0l+OH26bAbZMmKElDt3yrS0NHn55Ze3aSzOKC0tlYBcsmSJ2nDeeVKecop3Bz/5pBr/\n229bNpmam+VnkZGyPihIyj175NixY+WcOXNsj9u1S32B+vSRctMmKdeu9XhT9oZX58+XEmRBaKi8\n4Cc/6dC5vOHAgQMSkK+Z/35SSvWEB1IePareG0j5wguOBzc3SxkbqwStA9x6660yJCRELliwQAYF\nBcnCwsIOnU9+/rka89ChUt53n5QbN3oU+SNHjkhAPvnkkzI8PFzec889rS/W10v5ySfqSSYtrVUw\ns7Kk/POf1c29DTcR+dlnUoI8/u67EpB/+9vfWl/Ly5MS5K0REdJkNks2mG8Kv7/ySgnIK664ovW1\nadOktHb6tbXqcw1S/vvf3o+vHXgr7D0+xt5WoqKiOAa0hIe7zYxxOXnqZE1TQM3Cz5ihYr/ffafi\np3/9a5vGFhQURPiBA4Q6m6W/4gqYMkVNClZVERERwZgxY0h+7z0VK/zb39QEUlAQL8TFcVNaGqKi\nAmbMYNigQT6ZPHVIddy9GyZM8O7gO++E006DO+5Q2SxA0NtvM7Oujt+0tJDX0kJeXp5jRszo0apO\nICYGzjlH5ZknJamVjTpA/fDhNAG/a2pihl3P9c7A5eQpqDj70qUqVe+yyxyO3bZjB7lxcZi86Izp\nwMqVUF7O/v37efnll7npppt49NFHkVI6ZCW1lRpz3N+UlqY+f1lZqhXDY4+57NFixNfHZ2Rw2ZAh\npKxdq4rZpk1T+ePnn68mvzMz4cUX1Xf0++9VK+hTTnG6lrFLzJ/T4u++A2DMmDGtr5kn7bPr622q\ntC2Yv+cHt2wB4GsjOaKlBX74AcaNa903MlL93dLT1YT3SUDACXu0eYKlPjHRo7CHhIQQERHRuvHo\nUecTLfZMnqwWaLCqbPSKY8dUloR9R0JQH+glS9QYHn0UgLPGjOGqPXuQM2fCz35m2XXPnj1UT5yo\nioOOHWNKTIxPJk9thL2uTk1MZWR4d3BwsCooqqpSGR+HD8Mdd9CQlcWzQvDAAw/Q0NDgPCMmPV2J\ne3KyylO/5hpV4NMBQocOpS/wFnD22Wd36Fze0KtXL+c92UFNEL/9tkrXtGuFfODAAWbNmsW/Dx5E\n7NypUjm9ZetWlbV1++08+OCDhIaG8sADD5CWlsZ5553HK6+84jHl1h2HP/uMg8Cn99yjPpevvqra\nIv/2t/Dww06PycnJIQs46xe/YOmePdyZkwNvvaXSTG+4AVavVt+DDz6Am2+2LfUHHnvsMV5++WXv\nBmjuH1Vhzr7JtDZM27YhQ0LIAb7//nvHY83f86q9e4mPj6egoEAtwXnggPobWAs7qM/3nXdCdjZ8\n+6134+tEAlbY63r3dltubCyyYcmPb2lRTtOVY7cnPV19QL1crQlQ/U3AubCDckRXX61cwf793FRQ\nQIyUFC5caHEyTU1N/Pjjj2RkZFgKhiZJyeHDh2nqYE60zZJ4xk1r+HDvT3DKKcpx/+tf6sZXX0/4\nW29x/uzZ/MecVeQyh33QIJWvvHAhLFrUkbcBQO/evalA1UKceuqpHT6fJ4KCgoiNjXUu7MuWqRv6\n1VfbHFNWVsasWbNobGzkQHIyQVIq9+ot775rOf+Bt9/mrrvuon///gD88pe/5NChQ6y1a9TWFsIP\nHCAXcz56794qS+nTT1XK6R/+AK+/7nBM1X//yzohEL17s+rqq0kDyg8cUDfup59WdSGRkS7ezrv8\n9re/ddmOwoHISOjbl6b9+0lMTCTZ+ru7fTuMGkV4bKxzYTfvm9DczE033QSYXbuRe28v7ADXXaf6\n+J8Erj3ghN1YPu9EfLxHx24ThikvV7nY3jh2AKPXyf793g/Ok7CDeswNDYUrrmDkl1/yLPC1lVj8\n+OOPmEwmJeyDB0PfvoyorKSlpcWy6Hd7KSwsJCIiQlX5mhcX8dqxG/z61+pLsXu3evLIyOBGqyIj\nt10y+/WDJ59UPztIb3OvljPPPJOQkJAOn88bXDYCe+stFV46/3zLS3V1dcydO5eCggI++OADTrvz\nTgCOvvee9xd891047TSOhYfzbHAw91rdEC+88EL69+/Piy++2L43IyVJZWWtwm4ghFoE/Cc/UY7b\nWKYO4IcfuO3996kPC0OsX0/4ggXkAzvt+gk54+DBg9x4440IIcjLy7PNR3fH4MGEFReTmZlpW8S4\nbRti/HgmTZrkXNjNVe3JwI033khUVFSrsAcFOS9qiolRrTRWrvR+kY9OIuCE3XDslYawGysi2eEg\n7EZOa1scO7Rd2ENDW9dedMaAAepR9/vvoU8fHo+IsMlnN+KFI0aMUF+yrCwGmAW9o+EYmwU2jLhk\nWxw7qPe3YgUsXqzi7cCcOXNITk5m4MCBDt00O4s+ZlHtijCMgUthb2hQcyjm/jgmk4krrriCb7/9\nlmXLljF9+nQuvekmdglB2WovV57cvRtycth3+unc2dDAqSYTCVbHhoaGcsMNN/DRRx+173NRVESk\nyeQo7Ork6m88erQKBW3fDjk5yHPPpcZk4pl582DIEEtoZMeOHW4vZTKZuOqqq2hsbLQsiv7DDz94\nNUw5aBDx1dW28fXSUhUKHD+eyZMns23bNhoaGmwPjIigNjSUweHhDB8+nNNOO61V2DMyVNW5M4zO\noL5ql9BOAk7YDcde2ru3qpg8cMDpfg7C7qzq1B2GsLclzp6Xp5y+Jwe5cCH89KeIl14ifeJEG2E3\n+plnGE568mSiCwuJoePVpzbFSXv2QP/+qvFSWxk2TIVTzAVBoaGhPPPMMzz00EMdGl9byMzM5K9/\n/SvXX399l13TYRWlmJjWuYJrrrFsvuuuu1i1ahVPPfUU881l8omJiRxNS2NgYSF1dkv6gRK/efPm\nkZ6ezvTp01l26aUA3PrJJ2zo1w9TVhb85jc2MXrjScnrmLUVNZs2AXAgIoK8vDzq6+ttd+jVS60/\n2quXKuKbOZMWIThLSvqaCwpTUlKIi4vzWIH6+OOP88UXX/Dcc89xqfl9bd261atxVsXHk9LSQqZ1\ndalVOGXy5Mk0NTU5bW9QGhRERkKCpZnetm3baNm61XkYxmDwYHUze/ll6MJFdewJOGE3HHuxIdpG\n+MMOh17szqpO3V9IhQza6tjdhWEMIiJU06N588jKymLLli2WSbA9e/aQmJhoCTWQlYWQkon4zrGb\nL9T2MIwbLr30UksssysICgrinnvucVy3tRNxcOxCKNeemQnjxwOwZcsWnn32We666y7uMD/RGPSf\nP594Kfnsuecczv3cc8+xatUqRowYQVhYGGP37WNjcDCf7d7Nnx59lOBnnlGTtI89ZjlmyJAhXHDB\nBbz66qttnn8pM3c0HTpnDiaTyZLtYkNKimrnfOIEmExsfOwx9gKnmEVWCEFmZqZbx/7tt9/y4IMP\nsmDBAq6++moGDx5MQkKC18J+MCiIWGC89VOw0cbCLOzgOIFaV1fHwYYGBptbZ0ybNo0Yk4mgwkL3\nwg6qarqy0ukcQ1cRcMJuOPZDxqOUs1QnfODYQTlTbx17c7Pa1xthtyIrK4u6ujrLFysvL6/Vrasd\nADgnOrpDwt7Q0EBxcXGnCXsgkJiYSFFRkW0myl/+oh7bzfHft956y5K9Ys+Ia68FYPdrr9lsP3Dg\nAPfffz8XXHABa9asYf0bbzCmvp6sRx+lsbGR6667TqWaXn21mqMwtysGNYl65MgRPvzwwza9l/qt\nWzkGzDGPyWVDrzFjYMsW2LqVjeYnjVOs3POYMWPYuXOnKqqxo6qqiiuuuIKUlBT+8Y9/IIRACMGE\nCRO8FvY8cyx+tPX6tdu2qWwbc5+pfv36OQj79u3bOQIkmZd3nDJlChY59yTsp52mUpOXLPGqOWBn\nEHDCHhwcTHh4OMelVBMkbhy7Q4w9KKh1gQRvSE/33rHn56tOfu0QdsASjjG6OlpITIS0NKaFhXUo\nFGNMvA4ePFhNJJeWamFvIxdddBHHjx/no48+at149dVw1lmACqcsX76c2bNntz5xWRE0ciR1kZH0\nzstjr3nyWkrJzTffTHBwMC+++KKa/zAmWOfNs+1rb0y8L1pkyTOfPXs2AwcO5KGHHuKRRx7hpZde\nYtWqVXzzzTduJyjD9u1jtxD85LzziI6Odt+pMT0dBg5k165dJCQk0M9q8jszM5OKigqnE/uPPPII\nBQUFLFu2zFIHADBhwgR27Njh1VPGVnMvnhjr1tHbt1uekIQQTJ482UHYN23aRAlqxTRQbZdnmTOK\njGPdcvfd6rt/222qvfQ33yhz6CK/39cEnLCDCsfU1NQoYWqLY09KalujqPR01djJm0ZH3mTEOGHY\nsGHEx8ezceNGyzqnGfaCO3kyYxsaOuTYbXLY25sRE+AYk8SuCoM2bNhAcXExV155pfMTBAUhpk1j\nCvCKuRXya6+9xmeffcYTTzyh0lBBZcOMGeM4sW1MvK9apZ7k3n6bECn5/e9/z8GDB3nwwQf55S9/\nybx585g2bZrb+YfeJSUUx8cTFhbGmDFjvGrBm5OTw+jRo22yU4xJTfs4e0NDA6+//rplLNZMmDCB\nhoYGdu/e7fGaXxmfecPU1Ner9XCtxHny5Mnk5eXZhMk2bdpEbXQ0QUY2HHBmXBxlQmDyJhw7b15r\nsdW116oCrORkiI+3zRTqJAJb2EeMcCrsTU1N1NXVOTp2b+PrBkbKo9Wjr0vaKexCCCZNmsTGjRst\nGTHOhD2ptpb6ggKnj7xOsXNrNsJu/M60sLeJ0NBQrr32WlavXk1xcbHD68uWLSM2NpYLL3S9Vk3E\nWWcxGlj12mvk5+ezcOFCZsyYwc0336x2OHpU5YS7WjDk17+GF15Qce8rr4S0NG6pqKB8/34aGhoo\nKipi8+bNzJkzh/Xr1zv/vJSWEtfURK25JfS4cePYvn2728+WlJJdu3bZhGEAl5kxq1at4tixY07n\nXSaYq509hWMaGxv5Zt8+moODW4V91y4VHrEKpxhx9k3mCWHj/yNTUxFSWjpwjmxoYJuU7MrJcXtd\nQCVAfPyx+h7t2aMmkpcsUZPkaWmej+8gASnsUVFR6jEzI0NNKFkvfoCblr1tia9D2zJj8vLURJqR\nAtcGJk+ezI4dOywpYCPsbw7mcE1mfT1lnlZtB8jJUYUW5go6KSXvvvsuERERyhXu2aPCUkOHtnms\ngc7111+PyWTin//8p832uro6Vq5cyfz584l0UaADwNSpAKSXlTFjxgyampp45ZVXCDIWCHn/ffW4\n70rYg4Phl79Uf+PVq1XL5fvvh9NPJywkhJSUFE499VTmzp1LaWkp+5x8duvNghpsFuWxY8dSUVFB\nkZuCv6NHj3L8+HEHYe/duzcDBgxwcOyvvPIKQ4YM4SdO1p8dMWIEkZGRHoV9z549NJlM1PXp0yrs\nxpOFlWOfNGkS0BrOPHHiBLm5uSQaxXJHj0JzMwmHDrEdq/YC3hAWpp6cZs9W6b1PP932FOF2EJDC\nbhOKAQfX3uY+Ma5oS5FSXp76krWDrKwsmpub+fe//40QgnTjhmJw6qnIoCCy8DIz5uuv1eOnORb8\n5ptv8v777/PII4+oFgt79ijX0cGy/kAkIyODM888k1dffdXG4a5evZrq6mrXYRiDrCxkcDDnx8ZS\nWFjIn/70J9uFv999V33urPO2nREUpKo8P/tMhQv27rUphTfCH/9z0p+m5IsvAEgw32TGmd2vu3BM\njtnl2gs74JAZ8+OPP/LZZ59xww03tN6wrAgODmbs2LEehd04pxgypLXKfNs2lbFm9R1JSEggIyPD\nEmffunUrLS0tpEycaH7DJbBnD0GNjeTHxbVN2LuJgBR2i2M3nK3dBKpLx97WUEzv3iqm5o1j3727\nzWEYA2MC9dNPPyU1NdW2vw1AdDT1Q4cyGS9z2Y3ij6++oqioiNtvv50zzjiDu+++W23XGTEd4oYb\nbmDfvn18+eWXlm3Lli2jf//+ngumYmIQY8dyycCB/PznP+dOc0UqoNpXfP65cuttaZZ16aXqJm20\nIABGjRpFQkKCUxGr3bSJGmDI9OmAcuzgXtiNrC1nLSPGjBlDTk4OJnMGyauvvkpQUJDK5nHBhAkT\n2LZtm9vwz86dOwkJCSFy5MhWx75tm1oUxu6GYT2BaoRkhhux/aNHLU4/LCtLC/vJisWxp6erP7AL\nx27JY6+pUbGytjp2UO7Jk2OvrFQfnnYK+8CBA+nfvz8tLS2O8XUz4rTTmAwU5Od7PqFZ2OW333LT\nNddgMpl44403CA4OVo/5e/Z0yeOkv3LJJZfQq1cvywTo8ePHWbt2LQsWLFC/Y09MncrAgwdZ/tZb\ntvuvXq2etNq6IHevXnDuuUrYzUIZFBTE1KlTnYpY8N695ALDzZ/X2NhYhg4d6lHY7TNiDDIzM2lo\naGDfvn00Nzfz+uuvc8EFF5BibuLljAkTJlBZWckBFwWGoBx7RkYGwampKomhqckmI8aayZMnc/jw\nYQ4dOsSmTZtISUkhyXi6KClRN4TQUIacfz75+fkcPnzY5XVPBgJb2MPDVfm+p1BMW4uTrElP9+zY\n2zlxao3h2l0Je/j06fQBTngo30ZKJez9+yPq6qjYsIG///3vDDXi6cXF6kanHXu7iYqK4sorr2TF\nihVUVFSwYsUKmpqaPIdhDKZMUZOf1nHpEyfgtddUfrb5s9AmLr5YVWFbifO0adPYvXu3w7xMwpEj\nHO7VizCrUJwxgeoKY+LU2aLz1pkxa9eupbi42GOxmjcTqDt37lTnHjxYNfH75hs1n+ZC2EEVKm3c\nuFHF3ePjVXpoSYn6vYwezZQZM4A2xtm7gYAUdksoBpRAeQrFtKc4yWDYMLVGp7ucWx8Ku8PEqRlx\n2mkARHlquHToEFRUUGZ2fTePGGHTpEunOvqGG264gfr6et5++22WLVvGiBEjvO8yaY5t8803yqG/\n+KL6nG3YAHfd5RBm8Iq5c9VxVuEYI87+zTfftO5XXU1iXR3Vdm563Lhx7Nu3TxkmO1xlxBiMGjUK\nIQQ7d+7k5Zdfpn///syZM8ftcDMzMwkODnYp7NXV1Rw4cEBl3RhFdR98YAzWYf9x48YREhLCunXr\n2Lt3r/o+CdG69qnZ6Y8fP761IdhJTEAKu8WxQ2suu1WszqVjb4+wp6er9Cp3se28PJWt0IEsk+l2\n8U4HMjNpCAoi2dPkqTkMc//69ewLCuKKQYNsXZZOdfQJp556KuPHj2fx4sV8+eWXXHnllU7drFNS\nU1W7in/+U8WLb7lFhca+/Rbuuad9A0pKgjPOaC1uQpmF0NBQGxFrNH8+hF2sfNy4cUgpnbYHOHr0\nKOXl5S5bMkdFRTFs2DA++eQT1q5dy7XXXuux42ZkZCQjR450KezGZK3FsQN8+KG6eTmZWI6IiGDc\nuHGWxeWNTBmSk9WT0ZEjMG4coaGhTJ48WQv7yYiNsI8YoUILVjEzn4ZijIwFd+GYvDwl6h3IMjnr\nrLPYtm2bReAdCA3lYFISw6wr8Jxh/uL+OzcX09SpRGzerB5jDfbsUSEsoxhG0y6EENx4442WGPEV\nV1zRloOVa//+e2Ua3ntP9ao3P5W1m4svViJmvnlHRkYyceJEGxE7al41qZe5kZeBu8wYY+LUlWMH\n5cCzs7NpaWnhhhtu8Gq47loLGDeYzMzM1s/q3r1uOzNOnjyZE+bGXRONjJi+fcHIbze/x6lTp7J1\n61bHxmcnEQEp7EYoRkrpNOWxsrKSoKAgS18ZSyimvTF2cD+B6m3zLw+MGzfOres7PnQoY5qbqbHu\nMGjPDz9Q27cvVUDoOeeo9gHW4Rtj4rQ9j/saG6644grCw8M5/fTTHVNUPfHYY2qBjp071epZbcmC\nccW8eeqnlWufNm0aGzdutIjYiY0baQQGmdsgGKSmptKrV692C7sRZ585c6bXv4sJEyZQXFzMUcN4\nWbFjxw6ioqJIS0tT6Y1GiwY3fV6MOHtaWpqlrTN9+7Y+zZuPHTNmDCaTydLW4WQkIL+d0dHRSCnV\nh9UQVCthN9oJWETy6FFVsGPu9NYm+vdXK7m4cuwtLcpJ+EDYPdE4fjzRtLoup/zwA8fN8VNpuH9z\nJz9Apzr6kISEBFauXMkLL7zQ9oMzMmx6uPuEQYPUxKtdnL2xsZHNmzcDIHbvZi+QYRdWEUIwduxY\np8Kek5ND7969bVcwssMIIbalw6e7CdSdO3dyyimntObBG+EYN31eDGG3hGGgNfyakmIpHjQWg8nN\nzfV6rF1NQAq74cRrampUFkFkpM0EqtM+Me2Jr4NyUu6agRUWqv4VXSDs4WecAUDtf//rfIeGBti9\nmxLze40cPVr1FzGEvblZvQ8t7D5jzpw5ljDGScHFF6sQj3l1MWMC1QjH9Dp8mKLoaKfVsePGjeOH\nH36gxSp0V1tbyxdffOHQI8aeuXPnsmLFCku/dW8YbxZpZ8K+Y8cO28U1vBD2ESNGMHXqVEsPfKD1\nKd3qbzRixAiEEFrYTzaMnuy1tbUqpDB8uFPHbqE9xUnWuEt59EFGjLf0nTqVciDYvPK6A7m5YDJx\n0PzYGhcfrybUvvpKPY4a2T1a2P0XIxyzahUAffv2Zfjw4UrYGxpIPnGCSrsFpg3GjRvHiRMnyDfX\nSjQ1NXHZZZeRl5fHIg/r1IaGhjJ//nynlaauSEhIIDU11UHYS0pKKC0ttV282hB2NzfR4OBgvv76\na37+85+3bjQMndVxkZGRpKametWErLsIaGG3mUC1c+wOi2y017GDmkD98UfbSUiDLhT2AQMH8j2Q\nuGuX8/ah5onTA7GxrXMMZ5yhUiDz83VGTCAwYoRa0s4uHPPNN9/QtGsXwYB00frCegLVmARds2YN\nzz//PD/72c86Zbj2E6gVFRWWalwjVAOosNWiRW1fL9eJsINK0WyrY8/JyWHWrFkdXnvYGzos7EKI\nQRpdwd4AABIhSURBVEKIDUKIHCHELiHEnZ6P6l6MUIxNLvuBA5b1T30aigHl2OvrbTJvLOTlqfh9\nR54IvCQkJISvEhJIOn5cuXN7duyAiAgOBAe3zjGYwzd89ZUW9kDh4ovhiy/AXJg0bdo0ysrK2PbO\nOwDEuCiAyszMJCgoiO3bt3Pvvffy5ptv8vDDD3PLLbd02lAnTJjAvn37qKqqYt26dWRmZvKf//yH\nBx98kBnmYiJAFXUtXtz2SeYZM+BPf4KLLrLZPHLkSPLy8ixtENzR2NjIww8/zIQJE2y6sHYmvnDs\nzcAiKeVo4HTgNiGE84TVkwSnjt1q/VMbYW9qguPHOya87pqBGT1ifJHV4AU7jbFYOTILP/wAo0dT\nceJE6/vPzFQVeF9+qYQ9Pl4t3qHxXy6+WD1dmgt6jBTavPffpwUY4KKfTVRUFMOHD+fpp5/mb3/7\nG7fffju///3vO3Wohiu/9NJLOf/88+nVqxfffvstf/zjH72vC3BHeDj87ndqHs6KUaNGUV9f77Gp\n3saNG5k4cSIPPfQQF198Mbm5uV2ygHqHhV1KWSyl3GL+/2ogF3AehDtJcBB2w4GawyI2wl5aqn52\n1LGD8zi7j1IdvWbAALZFRbkW9rFjbUNRQUEwfXqrY8/I6LKbkKabGD9efWYXLoTHH2fE4MH06dOH\n0L17OQCMcDMBOW7cOMrLy7n88st56qmnfCOubjCE/dNPP2XhwoVs3rzZNqulkxg1ahSA2zj7H//4\nR04//XTKy8v54IMPeOedd+jbBU/m4OMYuxAiFZgAfOfktZuFEJuEEJtKDbHsJpyGYsASaqisrPRN\ncZLB4MGq8b69Yz9xQsWvu1DY4+PjWR0WBlu32i4AUlKiquvMwm4TijrjDPW72bRJN/8KBIRQC0Oc\ncQb85jeIjAweTElhNJAfGWkxRs64+eabueOOO3jjjTfaNBHaXgYMGMDzzz/Pl19+yZNPPum+l70P\n8ZTyeOzYMf7whz/w05/+lF27dnGRXSins/HZb14IEQOsBO6SUlbZvy6lfElKOUlKOSkpKclXl20X\nDo49IcGy/mlzczO1tbW+6RNjEBKiysDtHbvRxKkLhT0uLo7/GIspWxWiYJSCuxJ2UG1hdXw9MBgx\nQpXgf/EFpKRwx/btjAEqPEw+zpw5kyVLltg0COtMhBDceuutriuuO4k+ffqQlJTkUti/Nfe2v/PO\nO20TMboInwi7ECIUJerLpJROnvFPLmzy2A3MPWOqq6sBH7UTsMa+fe+xY2otxPh4tR5iFxEfH8+O\nmhrk+PG24RijB/vYsbZPLAATJ7bGGLWwBxZnngnZ2eT++c9sAIrtWgkEMu4yY7755huCg4O7JCzk\nDF9kxQjgVSBXSvm3jg+p87HJYzcwpzz6tAGYNUYuu5Rqceu5c1UK4QcfqOrULiIuLk5V3c6erboD\nGmtv/vCDSgVLSnJ07GFhrX1ItLAHHkIwdNEiFs+ezfhf/aq7R3PSYAi7s8U+srOzGTdunNuwVWfi\nC8c+DfgFcI4QYpv5v9k+OG+n4dKxHz3KCXOOqeXxqaQEIiIgNrZjF01PV72gS0pUTm12Nrz1VmuY\no4uIj48HoNyYmTcXorBjh6XrnUMeP8DMma3rN2oCjvDwcNasWdPlIY+TmVGjRlFeXo79nGFzczPf\nf/89U432yt2AL7Ji/ielFFLKsVLK8eb/1vpicJ1FSEgIYWFhjo4daDY/Wtk49r59O54JYqQZXnaZ\nEtMlS+CSSzp2znZgCHtpYqJ6zytXqlYBu3bB2LGOcwwG99yjJlw7eoPTaPwEVxOoO3fupKamhilT\npnTHsIAArTwFu9a94JDyaDN52tEwDLSmPH75Jdx3H9x+e8fP2Q4MJ15RWanylf/7X/juO1VAZZ44\nBRyFPSJCVSRqNBqgNeXRXtiNhUm0sHcDDsJuXv802JwC6ODYO8rQoWptyauuUi1XuwnDsVcawm4y\nwZ//rF50J+wajcaGQYMGER0d7ZDLnp2dTb9+/UhNTe2egRHAwm6zPB5Y1j9NXb+e3wG9Dx9WE50d\n7RNjEBGhmmgtXdqtvcwtjr2iQmW7DB4MH32kVnAaNcpxIW+NRuMUIQQjR450cOzZ2dlMmTKl04uz\n3BGwwu7g2AEef5zKXr34E5B87rlqotBXwg4qtbGbqzZtHLsQrSvajxwJ4eHasWs0bcA+5bGkpIT9\n+/d3axgGAljYo6KiHIX9kkt48eqrGQC0/OMfKu4eFgbWXeJ6ODaOHVqF3bzQgRZ2jcZ7Ro4cSVFR\nkWVJvezsbKB74+sQwMIeHR1tG4oxU1VVRU2vXgTdcgusXQu1tdCG5v8nO6GhoURFRSnHDmrtzHPP\ntfThNrZrYddoPGNMoOaZky6ys7MJDQ1tXTO1mwhoYXdw7DjJ4fbDhldxcXGtjj04GD791HLz0o5d\no/Ee+8yY7OxsJkyY0GU9a1wRsMLuMHlqxqHq0g+Jj49vdex26MlTjcZ7hg0bRkhICLm5uTQ1NbFx\n48ZuD8NAAAu7O8ceCMJucex2VFVVta6epNFo3BIaGkp6ejq7d+9m+/bt1NXVaWHvTvr06cPx48ep\nq6uz2R4Iwm4TirHDaADWnalaGk1PwsiMMSZOu7OVgEHACvv06dNpamqyVIkZBIKwewrF+Pv712h8\nyahRo9i7dy9ffvklAwcOZNCgQd09pMAV9jPOOIPg4GA2bNhgs92hZa0f4s6xO20AptFoXDJq1Cia\nm5tZvXr1SRGGgQAW9tjYWCZNmuQg7IHgWA3H7qzdaCC8f43GlxjNwOrr60+KMAwEsLADnH322Xz/\n/feW4gKTycQJ64Wc/ZS4uDgaGxupr693eC0Qnlg0Gl9iCDt0f2GSQcALe3NzM19//TWAReD9PRRh\n01bADu3YNZq2ERsbS0pKCmFhYZbFtbubgBb2adOmERoaagnHBEpxjkNbASt0jF2jaTunn346Z511\nFuHh4d09FABCunsA3Ul0dDSTJ08OOGHXjl2j8S1vvvkmLS0t3T0MCwHt2EGFYzZt2kRlZWXACbu9\nY3e5epJGo3FLRETESVXUp4X97LNpaWnhq6++ChhhN0It9o49UN6/RuPvBLywT5kyhbCwMDZs2BAw\nnQ1dOXYt7BqNfxDwwh4ZGcmUKVPYsGFDwAibq8lT3QBMo/EPAl7YQYVjtm3bRn5+PuD/wh4dHU1w\ncLAOxWg0fooWdpSwSylZvXo1oPJS/RkhhNO2AoESitJo/B0t7MBpp51GREQE27dvJzY2lqBuXGy6\nq3DWCEw7do3GP/B/BfOC8PBwpk+fDgSOqDlz7DrGrtH4B1rYzZx99tlA4Ai7duwajf+ihd1MIAq7\nM8euV0/SaHo+WtjNTJo0iejo6IAR9ri4OAfHrldP0mj8g4DuFWNNaGgoDzzwAMnJyd09lC7BlWMP\nlBubRuPPaGG34te//nV3D6HLiIuLo7q6GpPJRHBwMKA7O2o0/oJPQjFCiFlCiDwhxD4hxG98cU5N\n52K0FTAmTI3/145do+n5dFjYhRDBwHPABcBo4HIhxOiOnlfTuThrK6BXT9Jo/ANfOPbJwD4p5Y9S\nykZgOfBTH5xX04k468muHbtG4x/4QtgHAkVW/z5o3maDEOJmIcQmIcSm0tJSH1xW0xGcdXjUMXaN\nxj/osnRHKeVLUspJUspJSUlJXXVZjQuc9WTXjl2j8Q98IeyHgEFW/04xb9OcxNg79qamJr16kkbj\nJ/hC2DcCw4UQaUKIMGAB8IEPzqvpROwde3V1NRA4lbcajT/T4Tx2KWWzEOL/gE+AYOA1KeWuDo9M\n06nYZ8XoPjEajf/gkwIlKeVaYK0vzqXpGkJCQoiOjrY4dt3ZUaPxH3SvmADGuq2Aduwajf+ghT2A\nse7JrldP0mj8By3sAYx1T3bt2DUa/0ELewDjLBSjY+waTc9HC3sAY92TXTt2jcZ/0MIewFg79srK\nSr16kkbjJ2hhD2AMxy6ltLQT0KsnaTQ9Hy3sAUx8fDxNTU3U1dXpPjEajR+hhT2AsW4roDs7ajT+\ngxb2AMa6EZh27BqN/6CFPYCx7hejV0/SaPwHLewBjPUqStqxazT+gxb2AMY+FKNj7BqNf6CFPYCx\nnzzVjl2j8Q+0sAcwhmMvKyvTqydpNH6EFvYAJjIykpCQEIqK1FrkWtg1Gv9AC3sAI4QgPj6ewsJC\nQDcA02j8BS3sAU5cXJx27BqNn6GFPcCxduxa2DUa/0ALe4ATFxenW/ZqNH6GFvYAx8iMAS3sGo2/\noIU9wLEWdj15qtH4B1rYAxxrMdeOXaPxD7SwBziGY9erJ2k0/oMW9gDHcOx69SSNxn/Qwh7gGI5d\nx9c1Gv9BC3uAY+3YNRqNf6CFPcAxHLsWdo3Gf9DCHuBox67R+B9a2AMc7dg1Gv+jQ8IuhPirEGK3\nEOIHIcR7Qoh4z0dpTib05KlG43901LF/CmRKKccCe4D7Oz4kTVdiOPXY2NhuHolGo/EVIR05WEq5\nzuqf3wKXdGw4mq4mODiYJ598knPPPbe7h6LRaHyEkFL65kRCfAj8S0r5lovXbwZuBhg8ePDEgoIC\nn1xXo9FoAgUhxGYp5SRP+3l07EKIz4B+Tl76nZTyffM+vwOagWWuziOlfAl4CWDSpEm+uZtoNBqN\nxgGPwi6ldPuMLoS4FrgQmCl9Zf81Go1G0246FGMXQswC7gNmSClrfTMkjUaj0XSEjmbFPAvEAp8K\nIbYJIV7wwZg0Go1G0wE6mhUzzFcD0Wg0Go1v0JWnGo1G42doYddoNBo/Qwu7RqPR+Bk+K1Bq00WF\nKAXaW6GUCJT5cDhdTU8ef08eO/Ts8ffksYMev68YIqVM8rRTtwh7RxBCbPKm8upkpSePvyePHXr2\n+Hvy2EGPv6vRoRiNRqPxM7SwazQazf+3dzYhVpVhHP/9sezDotEKGRphDCSZRY4uSkmijGKSaNWi\naOHCpQuFNg5B0LJN5SKC6GsTFdmXzKKvyfWYptboMGk04Ig2LZKgRWT9W7zvjcuQ482gc57L84PD\ned/n3MXvzH3uc88859xz+oyIhf2VpgX+I5H9I7tDbP/I7pD+/yvheuxJkiTJ0kQ8Yk+SJEmWIFRh\nlzQmaVbSaUl7m/a5HJJel7QgabortkrS55JO1fXKJh0vhaQ1kg5KOinphKTdNd56f0nXSjok6Xh1\nf7bG10qaqvnzrqTlTbsuhaRlko5KmqjzEP6S5iR9W+8fdbjGWp83HSQNSNpfH/s5I2lLJH8IVNgl\nLQNeAh4GRoAnJI00a3VZ3gTGFsX2ApO21wGTdd5GLgJP2R4BNgO76t87gv9vwDbbG4BRYEzSZuA5\n4IV6j6OfgZ0NOvbCbmCmax7J/37bo12XCEbImw77gE9srwc2UN6DSP5gO8QCbAE+7ZqPA+NNe/Xg\nPQxMd81ngcE6HgRmm3bscT8+Bh6M5g9cD3wN3E35gclV/5RPbVuAIUoB2QZMAIriD8wBtyyKhcgb\n4CbgB+r5x2j+nSXMETtwG3Cmaz5fY9FYbftcHZ8HVjcp0wuShoGNwBRB/Gsb4xiwQHno+vfABdsX\n60vanj8vUp518Ged30wcfwOfSTpSH4kJQfIGWAv8BLxR22CvSlpBHH8gUCumH3H5+m/1ZUmSbgDe\nB/bY/qV7W5v9bf9he5Ry5HsXsL5hpZ6R9AiwYPtI0y5XyFbbmyht012S7u3e2Oa8odzKfBPwsu2N\nwK8saru03B+IVdjPAmu65kM1Fo0fJQ0C1PVCwz6XRNLVlKL+lu0PajiMP4DtC8BBSutiQFLnGQRt\nzp97gEclzQHvUNox+wjib/tsXS8AH1K+WKPkzTwwb3uqzvdTCn0UfyBWYf8KWFevDFgOPA4caNjp\nSjgA7KjjHZTedeuQJOA1YMb2812bWu8v6VZJA3V8HeXcwAylwD9WX9ZKdwDb47aHbA9T8vxL208S\nwF/SCkk3dsbAQ8A0AfIGwPZ54IykO2roAeAkQfz/pukm/788sbEd+I7SL326aZ8efN8GzgG/U44E\ndlJ6pZPAKeALYFXTnpdw30r5d/Mb4FhdtkfwB+4Ejlb3aeCZGr8dOAScBt4DrmnatYd9uQ+YiOJf\nHY/X5UTncxohb7r2YRQ4XPPnI2BlJH/b+cvTJEmSfiNSKyZJkiTpgSzsSZIkfUYW9iRJkj4jC3uS\nJEmfkYU9SZKkz8jCniRJ0mdkYU+SJOkzsrAnSZL0GX8BMrtRacfevbAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc1da160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDeNiNzNrGBe7mVnD/AtE8rIfVGRwngAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xca02fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 2.5656189607 \n",
      "Fixed scheme MAE:  1.72428095619\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 1.3704  Test loss = 2.5820  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 1.3938  Test loss = 1.4619  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 1.3880  Test loss = 0.6241  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 1.3796  Test loss = 0.7088  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 1.2485  Test loss = 1.5222  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 1.2221  Test loss = 0.0546  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 1.2176  Test loss = 0.2030  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 1.1717  Test loss = 0.0461  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 1.0588  Test loss = 2.1434  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 1.0892  Test loss = 0.0629  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 1.0492  Test loss = 1.0540  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 1.0362  Test loss = 1.9928  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 0.9986  Test loss = 1.0008  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 1.0057  Test loss = 3.6894  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 1.1041  Test loss = 2.8937  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 1.1579  Test loss = 3.0797  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 1.0678  Test loss = 0.8528  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 1.0603  Test loss = 0.7655  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 1.0641  Test loss = 1.4941  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 1.0087  Test loss = 0.9915  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 0.9207  Test loss = 2.3114  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 0.9640  Test loss = 3.4077  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 1.0445  Test loss = 0.7313  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 1.0440  Test loss = 0.2265  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 0.9776  Test loss = 0.3837  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 0.9755  Test loss = 0.8161  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 0.9788  Test loss = 0.0605  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 0.9677  Test loss = 2.0907  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 0.9596  Test loss = 0.7320  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 0.9639  Test loss = 0.2653  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 0.9557  Test loss = 4.4073  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 1.0943  Test loss = 0.0414  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 1.0015  Test loss = 1.9853  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 1.0313  Test loss = 0.7346  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 1.0087  Test loss = 0.2080  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 0.9925  Test loss = 5.6486  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 1.1661  Test loss = 1.0618  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 1.1578  Test loss = 2.0439  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 1.1785  Test loss = 0.9608  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 1.1819  Test loss = 3.3054  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 1.1165  Test loss = 0.0958  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 1.1165  Test loss = 1.2145  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 1.1263  Test loss = 2.6219  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 1.1674  Test loss = 13.3669  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 2.0043  Test loss = 6.2335  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.1482  Test loss = 0.8598  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.1509  Test loss = 1.5335  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.1581  Test loss = 0.2824  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 1.7411  Test loss = 2.9567  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 1.7678  Test loss = 2.5071  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 1.7941  Test loss = 2.1197  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 1.8119  Test loss = 0.6593  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 1.7650  Test loss = 0.5297  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 1.7645  Test loss = 2.2346  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 1.7860  Test loss = 0.5300  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 1.7845  Test loss = 1.4876  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 1.7572  Test loss = 1.2956  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 1.7609  Test loss = 2.2041  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 1.7803  Test loss = 1.2325  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 1.7831  Test loss = 1.6022  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.7469  Test loss = 1.4826  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.7557  Test loss = 2.0809  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 1.7723  Test loss = 0.4534  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 1.7732  Test loss = 0.6087  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 1.7374  Test loss = 0.2044  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 1.7367  Test loss = 1.2338  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.7434  Test loss = 1.0318  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 1.7466  Test loss = 2.4188  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 1.7240  Test loss = 5.5512  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 1.8564  Test loss = 0.7704  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 1.8588  Test loss = 1.5886  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 1.8650  Test loss = 2.8160  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 1.7832  Test loss = 2.9310  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 1.8172  Test loss = 0.3571  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 1.8162  Test loss = 1.1197  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 1.8157  Test loss = 0.0947  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 1.7634  Test loss = 0.8593  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGNwz6ZSh8QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd2a2dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.52431303955 \n",
      "Updating scheme MAE:  1.68591344452\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
