{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"/models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"1Q/1_cell/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 750\n",
    "learning_rate = 1e-1\n",
    "batch_size = 5\n",
    "early_stop_iters = 15\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 12 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    stacked_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell]*3, state_is_tuple = True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('/Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag1',\n",
    "                                       'inflation.lag2',\n",
    "                                       'inflation.lag3',\n",
    "                                       'inflation.lag4']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag1',\n",
    "                                   'unemp.lag2',\n",
    "                                   'unemp.lag3',\n",
    "                                   'unemp.lag4']])\n",
    "train_4lag_oil = np.array(train[['oil.lag1',\n",
    "                                 'oil.lag2',\n",
    "                                 'oil.lag3',\n",
    "                                 'oil.lag4']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('/Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag1',\n",
    "                                     'inflation.lag2',\n",
    "                                     'inflation.lag3',\n",
    "                                     'inflation.lag4']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag1',\n",
    "                                 'unemp.lag2',\n",
    "                                 'unemp.lag3',\n",
    "                                 'unemp.lag4']])\n",
    "test_4lag_oil = np.array(test[['oil.lag1',\n",
    "                               'oil.lag2',\n",
    "                               'oil.lag3',\n",
    "                               'oil.lag4']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 12 \n",
      "Learning rate = 0.1 \n",
      "Epochs = 750 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 15 \n",
      "Learning rate = 0.1\n",
      "Fold: 1  Epoch: 1  Training loss = 2.5523  Validation loss = 2.0375  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 2.5099  Validation loss = 1.9388  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 3.3916  Validation loss = 1.2575  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 2.4988  Validation loss = 0.9003  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 2.3264  Validation loss = 2.0101  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 2.3417  Validation loss = 2.0981  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 2.2649  Validation loss = 1.8302  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 2.2000  Validation loss = 1.8619  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 2.1723  Validation loss = 1.7727  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 2.2136  Validation loss = 1.7662  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 2.3308  Validation loss = 2.1434  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 2.1460  Validation loss = 1.9230  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 2.2358  Validation loss = 2.1843  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 2.1966  Validation loss = 2.3574  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 2.1920  Validation loss = 2.1677  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 2.2130  Validation loss = 2.2434  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 2.1562  Validation loss = 1.7309  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 2.2834  Validation loss = 2.3766  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 4  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.3897  Validation loss = 2.0643  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.3426  Validation loss = 2.0848  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.2739  Validation loss = 2.1024  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.1830  Validation loss = 1.9585  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.0909  Validation loss = 2.2122  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.1715  Validation loss = 2.2309  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.2337  Validation loss = 2.2690  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.1753  Validation loss = 2.0318  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.2047  Validation loss = 1.6985  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.0641  Validation loss = 2.0432  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.2539  Validation loss = 1.7475  \n",
      "\n",
      "Fold: 2  Epoch: 12  Training loss = 2.1421  Validation loss = 2.1156  \n",
      "\n",
      "Fold: 2  Epoch: 13  Training loss = 2.1453  Validation loss = 2.1005  \n",
      "\n",
      "Fold: 2  Epoch: 14  Training loss = 2.1173  Validation loss = 2.0184  \n",
      "\n",
      "Fold: 2  Epoch: 15  Training loss = 2.1105  Validation loss = 1.7081  \n",
      "\n",
      "Fold: 2  Epoch: 16  Training loss = 2.2002  Validation loss = 1.6790  \n",
      "\n",
      "Fold: 2  Epoch: 17  Training loss = 2.0952  Validation loss = 2.0648  \n",
      "\n",
      "Fold: 2  Epoch: 18  Training loss = 2.0591  Validation loss = 2.1622  \n",
      "\n",
      "Fold: 2  Epoch: 19  Training loss = 2.1203  Validation loss = 2.0349  \n",
      "\n",
      "Fold: 2  Epoch: 20  Training loss = 2.0855  Validation loss = 2.0992  \n",
      "\n",
      "Fold: 2  Epoch: 21  Training loss = 2.3626  Validation loss = 1.5893  \n",
      "\n",
      "Fold: 2  Epoch: 22  Training loss = 2.0868  Validation loss = 1.8602  \n",
      "\n",
      "Fold: 2  Epoch: 23  Training loss = 2.0296  Validation loss = 2.0349  \n",
      "\n",
      "Fold: 2  Epoch: 24  Training loss = 2.0286  Validation loss = 2.0796  \n",
      "\n",
      "Fold: 2  Epoch: 25  Training loss = 2.2285  Validation loss = 1.7182  \n",
      "\n",
      "Fold: 2  Epoch: 26  Training loss = 1.9828  Validation loss = 1.7942  \n",
      "\n",
      "Fold: 2  Epoch: 27  Training loss = 2.1976  Validation loss = 2.1729  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 21  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.4442  Validation loss = 3.1633  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.3102  Validation loss = 3.3983  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.3584  Validation loss = 3.8333  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.2467  Validation loss = 3.1736  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.2734  Validation loss = 2.7647  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.2125  Validation loss = 3.2770  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.2775  Validation loss = 3.6140  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.1950  Validation loss = 3.0928  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.2032  Validation loss = 2.9754  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.1814  Validation loss = 3.1475  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.1762  Validation loss = 3.0682  \n",
      "\n",
      "Fold: 3  Epoch: 12  Training loss = 1.2150  Validation loss = 3.4010  \n",
      "\n",
      "Fold: 3  Epoch: 13  Training loss = 1.1701  Validation loss = 2.7595  \n",
      "\n",
      "Fold: 3  Epoch: 14  Training loss = 1.1503  Validation loss = 2.2626  \n",
      "\n",
      "Fold: 3  Epoch: 15  Training loss = 1.1215  Validation loss = 1.9874  \n",
      "\n",
      "Fold: 3  Epoch: 16  Training loss = 1.1600  Validation loss = 3.0355  \n",
      "\n",
      "Fold: 3  Epoch: 17  Training loss = 1.1829  Validation loss = 2.5757  \n",
      "\n",
      "Fold: 3  Epoch: 18  Training loss = 1.1615  Validation loss = 2.7237  \n",
      "\n",
      "Fold: 3  Epoch: 19  Training loss = 1.1233  Validation loss = 2.5954  \n",
      "\n",
      "Fold: 3  Epoch: 20  Training loss = 1.1475  Validation loss = 2.2027  \n",
      "\n",
      "Fold: 3  Epoch: 21  Training loss = 1.1141  Validation loss = 2.6582  \n",
      "\n",
      "Fold: 3  Epoch: 22  Training loss = 1.0789  Validation loss = 2.0113  \n",
      "\n",
      "Fold: 3  Epoch: 23  Training loss = 1.0772  Validation loss = 1.8414  \n",
      "\n",
      "Fold: 3  Epoch: 24  Training loss = 1.0888  Validation loss = 1.9111  \n",
      "\n",
      "Fold: 3  Epoch: 25  Training loss = 1.1517  Validation loss = 2.2205  \n",
      "\n",
      "Fold: 3  Epoch: 26  Training loss = 1.1688  Validation loss = 2.2505  \n",
      "\n",
      "Fold: 3  Epoch: 27  Training loss = 1.0715  Validation loss = 2.2553  \n",
      "\n",
      "Fold: 3  Epoch: 28  Training loss = 1.0709  Validation loss = 1.6936  \n",
      "\n",
      "Fold: 3  Epoch: 29  Training loss = 1.0805  Validation loss = 2.0137  \n",
      "\n",
      "Fold: 3  Epoch: 30  Training loss = 1.0852  Validation loss = 1.9927  \n",
      "\n",
      "Fold: 3  Epoch: 31  Training loss = 1.0594  Validation loss = 1.8296  \n",
      "\n",
      "Fold: 3  Epoch: 32  Training loss = 1.0588  Validation loss = 1.9384  \n",
      "\n",
      "Fold: 3  Epoch: 33  Training loss = 1.0956  Validation loss = 1.8731  \n",
      "\n",
      "Fold: 3  Epoch: 34  Training loss = 1.0539  Validation loss = 1.7614  \n",
      "\n",
      "Fold: 3  Epoch: 35  Training loss = 1.0567  Validation loss = 1.9752  \n",
      "\n",
      "Fold: 3  Epoch: 36  Training loss = 1.0665  Validation loss = 1.8812  \n",
      "\n",
      "Fold: 3  Epoch: 37  Training loss = 1.0704  Validation loss = 2.0066  \n",
      "\n",
      "Fold: 3  Epoch: 38  Training loss = 1.0654  Validation loss = 1.9067  \n",
      "\n",
      "Fold: 3  Epoch: 39  Training loss = 1.0696  Validation loss = 1.6202  \n",
      "\n",
      "Fold: 3  Epoch: 40  Training loss = 1.0551  Validation loss = 1.9369  \n",
      "\n",
      "Fold: 3  Epoch: 41  Training loss = 1.0394  Validation loss = 1.7337  \n",
      "\n",
      "Fold: 3  Epoch: 42  Training loss = 1.0937  Validation loss = 1.4449  \n",
      "\n",
      "Fold: 3  Epoch: 43  Training loss = 1.0274  Validation loss = 1.4854  \n",
      "\n",
      "Fold: 3  Epoch: 44  Training loss = 1.1090  Validation loss = 2.0958  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 42  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.0653  Validation loss = 3.4401  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.0958  Validation loss = 3.4671  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.0550  Validation loss = 3.2430  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.1122  Validation loss = 2.6995  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.0303  Validation loss = 2.9502  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.0832  Validation loss = 2.8461  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.0235  Validation loss = 3.1651  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.0013  Validation loss = 2.6358  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.0835  Validation loss = 2.8485  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 0.9960  Validation loss = 3.1007  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.0456  Validation loss = 2.6841  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 0.9952  Validation loss = 3.3306  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.0136  Validation loss = 2.8469  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 0.9728  Validation loss = 3.2279  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.0937  Validation loss = 2.7029  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 0.9899  Validation loss = 2.9812  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 0.9401  Validation loss = 2.9829  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 0.9570  Validation loss = 2.5566  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 0.9477  Validation loss = 3.0327  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 0.9410  Validation loss = 2.9139  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 0.9221  Validation loss = 2.8867  \n",
      "\n",
      "Fold: 4  Epoch: 22  Training loss = 0.9248  Validation loss = 2.8048  \n",
      "\n",
      "Fold: 4  Epoch: 23  Training loss = 0.9866  Validation loss = 2.8311  \n",
      "\n",
      "Fold: 4  Epoch: 24  Training loss = 0.9348  Validation loss = 2.7143  \n",
      "\n",
      "Fold: 4  Epoch: 25  Training loss = 0.9027  Validation loss = 2.8852  \n",
      "\n",
      "Fold: 4  Epoch: 26  Training loss = 0.9054  Validation loss = 2.8408  \n",
      "\n",
      "Fold: 4  Epoch: 27  Training loss = 0.9263  Validation loss = 2.7048  \n",
      "\n",
      "Fold: 4  Epoch: 28  Training loss = 1.0374  Validation loss = 3.2187  \n",
      "\n",
      "Fold: 4  Epoch: 29  Training loss = 0.9589  Validation loss = 2.9151  \n",
      "\n",
      "Fold: 4  Epoch: 30  Training loss = 0.9409  Validation loss = 3.0330  \n",
      "\n",
      "Fold: 4  Epoch: 31  Training loss = 0.9233  Validation loss = 2.9081  \n",
      "\n",
      "Fold: 4  Epoch: 32  Training loss = 1.0724  Validation loss = 2.6000  \n",
      "\n",
      "Fold: 4  Epoch: 33  Training loss = 0.9032  Validation loss = 2.8080  \n",
      "\n",
      "Fold: 4  Epoch: 34  Training loss = 1.0052  Validation loss = 2.9828  \n",
      "\n",
      "Fold: 4  Epoch: 35  Training loss = 0.9340  Validation loss = 2.9788  \n",
      "\n",
      "Fold: 4  Epoch: 36  Training loss = 0.9156  Validation loss = 3.1303  \n",
      "\n",
      "Fold: 4  Epoch: 37  Training loss = 0.9079  Validation loss = 2.8213  \n",
      "\n",
      "Fold: 4  Epoch: 38  Training loss = 0.9699  Validation loss = 3.0985  \n",
      "\n",
      "Fold: 4  Epoch: 39  Training loss = 0.9456  Validation loss = 2.6824  \n",
      "\n",
      "Fold: 4  Epoch: 40  Training loss = 0.9382  Validation loss = 2.7085  \n",
      "\n",
      "Fold: 4  Epoch: 41  Training loss = 0.8600  Validation loss = 2.8126  \n",
      "\n",
      "Fold: 4  Epoch: 42  Training loss = 0.9798  Validation loss = 3.5228  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 18  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.0688  Validation loss = 2.6627  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.1472  Validation loss = 1.5998  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.1964  Validation loss = 1.5085  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.0708  Validation loss = 1.3425  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.0139  Validation loss = 1.2264  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 1.0709  Validation loss = 1.5700  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 1.0602  Validation loss = 1.6105  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.0873  Validation loss = 1.2227  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 0.9367  Validation loss = 1.2742  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.0770  Validation loss = 0.9317  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 0.9675  Validation loss = 1.4048  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 0.9508  Validation loss = 1.2741  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 0.9380  Validation loss = 1.5477  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.1828  Validation loss = 1.7171  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 0.9869  Validation loss = 1.2667  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 0.9763  Validation loss = 1.4511  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 0.9529  Validation loss = 1.3014  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 0.9446  Validation loss = 1.1746  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 0.9467  Validation loss = 1.1963  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 1.0701  Validation loss = 1.1139  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 0.9100  Validation loss = 0.8684  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 1.0568  Validation loss = 1.4082  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 0.9044  Validation loss = 1.0531  \n",
      "\n",
      "Fold: 5  Epoch: 24  Training loss = 1.2372  Validation loss = 1.8127  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 21  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 1.2184  Validation loss = 1.1662  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 1.0733  Validation loss = 1.0634  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 1.1623  Validation loss = 1.4503  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 0.9571  Validation loss = 1.2238  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 0.9613  Validation loss = 1.3389  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 1.0754  Validation loss = 1.6560  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 0.9429  Validation loss = 1.3894  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 1.1943  Validation loss = 1.6981  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 1.0616  Validation loss = 1.9023  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 0.9530  Validation loss = 1.3241  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 1.0314  Validation loss = 1.4564  \n",
      "\n",
      "Fold: 6  Epoch: 12  Training loss = 1.0049  Validation loss = 1.3163  \n",
      "\n",
      "Fold: 6  Epoch: 13  Training loss = 0.9028  Validation loss = 1.3422  \n",
      "\n",
      "Fold: 6  Epoch: 14  Training loss = 0.8934  Validation loss = 1.3951  \n",
      "\n",
      "Fold: 6  Epoch: 15  Training loss = 0.9019  Validation loss = 1.5047  \n",
      "\n",
      "Fold: 6  Epoch: 16  Training loss = 0.8795  Validation loss = 1.5296  \n",
      "\n",
      "Fold: 6  Epoch: 17  Training loss = 0.8796  Validation loss = 1.5319  \n",
      "\n",
      "Fold: 6  Epoch: 18  Training loss = 0.8805  Validation loss = 1.4719  \n",
      "\n",
      "Fold: 6  Epoch: 19  Training loss = 0.8488  Validation loss = 1.4443  \n",
      "\n",
      "Fold: 6  Epoch: 20  Training loss = 0.9418  Validation loss = 1.2347  \n",
      "\n",
      "Fold: 6  Epoch: 21  Training loss = 0.8534  Validation loss = 1.2678  \n",
      "\n",
      "Fold: 6  Epoch: 22  Training loss = 0.9272  Validation loss = 1.2321  \n",
      "\n",
      "Fold: 6  Epoch: 23  Training loss = 1.1319  Validation loss = 1.1197  \n",
      "\n",
      "Fold: 6  Epoch: 24  Training loss = 0.8619  Validation loss = 1.2399  \n",
      "\n",
      "Fold: 6  Epoch: 25  Training loss = 0.8506  Validation loss = 1.4648  \n",
      "\n",
      "Fold: 6  Epoch: 26  Training loss = 1.0366  Validation loss = 1.6301  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 2  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 1.0046  Validation loss = 2.3540  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 0.8937  Validation loss = 1.8998  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 0.8593  Validation loss = 2.3314  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.0124  Validation loss = 2.4807  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 0.9145  Validation loss = 2.5462  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 0.8724  Validation loss = 2.4589  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 0.8641  Validation loss = 2.5371  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 0.9088  Validation loss = 2.3692  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 0.9080  Validation loss = 2.3558  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 0.8552  Validation loss = 2.4131  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 0.9528  Validation loss = 2.3036  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 0.9507  Validation loss = 2.7718  \n",
      "\n",
      "Fold: 7  Epoch: 13  Training loss = 0.8960  Validation loss = 2.5957  \n",
      "\n",
      "Fold: 7  Epoch: 14  Training loss = 0.8747  Validation loss = 2.6071  \n",
      "\n",
      "Fold: 7  Epoch: 15  Training loss = 0.8582  Validation loss = 2.8199  \n",
      "\n",
      "Fold: 7  Epoch: 16  Training loss = 0.8390  Validation loss = 2.5264  \n",
      "\n",
      "Fold: 7  Epoch: 17  Training loss = 0.8030  Validation loss = 2.3882  \n",
      "\n",
      "Fold: 7  Epoch: 18  Training loss = 0.9915  Validation loss = 1.9438  \n",
      "\n",
      "Fold: 7  Epoch: 19  Training loss = 0.9610  Validation loss = 1.9793  \n",
      "\n",
      "Fold: 7  Epoch: 20  Training loss = 0.8432  Validation loss = 2.4498  \n",
      "\n",
      "Fold: 7  Epoch: 21  Training loss = 0.8474  Validation loss = 2.5292  \n",
      "\n",
      "Fold: 7  Epoch: 22  Training loss = 0.8051  Validation loss = 2.5261  \n",
      "\n",
      "Fold: 7  Epoch: 23  Training loss = 1.0008  Validation loss = 2.7212  \n",
      "\n",
      "Fold: 7  Epoch: 24  Training loss = 0.8977  Validation loss = 2.3395  \n",
      "\n",
      "Fold: 7  Epoch: 25  Training loss = 0.9058  Validation loss = 2.2641  \n",
      "\n",
      "Fold: 7  Epoch: 26  Training loss = 0.9659  Validation loss = 2.1930  \n",
      "\n",
      "Fold: 7  Epoch: 27  Training loss = 0.8458  Validation loss = 2.2113  \n",
      "\n",
      "Fold: 7  Epoch: 28  Training loss = 0.9040  Validation loss = 1.9632  \n",
      "\n",
      "Fold: 7  Epoch: 29  Training loss = 0.8579  Validation loss = 2.4288  \n",
      "\n",
      "Fold: 7  Epoch: 30  Training loss = 0.8514  Validation loss = 2.4688  \n",
      "\n",
      "Fold: 7  Epoch: 31  Training loss = 1.0589  Validation loss = 1.5059  \n",
      "\n",
      "Fold: 7  Epoch: 32  Training loss = 0.8269  Validation loss = 2.2108  \n",
      "\n",
      "Fold: 7  Epoch: 33  Training loss = 0.8212  Validation loss = 2.5073  \n",
      "\n",
      "Fold: 7  Epoch: 34  Training loss = 1.0092  Validation loss = 2.0423  \n",
      "\n",
      "Fold: 7  Epoch: 35  Training loss = 0.8536  Validation loss = 2.4433  \n",
      "\n",
      "Fold: 7  Epoch: 36  Training loss = 0.8902  Validation loss = 2.1977  \n",
      "\n",
      "Fold: 7  Epoch: 37  Training loss = 0.8521  Validation loss = 1.9938  \n",
      "\n",
      "Fold: 7  Epoch: 38  Training loss = 0.9368  Validation loss = 2.3264  \n",
      "\n",
      "Fold: 7  Epoch: 39  Training loss = 1.1544  Validation loss = 1.8635  \n",
      "\n",
      "Fold: 7  Epoch: 40  Training loss = 0.8691  Validation loss = 2.4734  \n",
      "\n",
      "Fold: 7  Epoch: 41  Training loss = 0.7895  Validation loss = 2.2824  \n",
      "\n",
      "Fold: 7  Epoch: 42  Training loss = 0.9169  Validation loss = 1.9067  \n",
      "\n",
      "Fold: 7  Epoch: 43  Training loss = 0.9256  Validation loss = 2.4447  \n",
      "\n",
      "Fold: 7  Epoch: 44  Training loss = 0.8991  Validation loss = 1.6344  \n",
      "\n",
      "Fold: 7  Epoch: 45  Training loss = 0.9217  Validation loss = 2.2761  \n",
      "\n",
      "Fold: 7  Epoch: 46  Training loss = 0.8518  Validation loss = 2.0101  \n",
      "\n",
      "Fold: 7  Epoch: 47  Training loss = 0.8910  Validation loss = 2.3694  \n",
      "\n",
      "Fold: 7  Epoch: 48  Training loss = 0.8482  Validation loss = 1.9469  \n",
      "\n",
      "Fold: 7  Epoch: 49  Training loss = 0.8904  Validation loss = 1.9940  \n",
      "\n",
      "Fold: 7  Epoch: 50  Training loss = 0.8209  Validation loss = 2.0849  \n",
      "\n",
      "Fold: 7  Epoch: 51  Training loss = 0.8111  Validation loss = 2.0320  \n",
      "\n",
      "Fold: 7  Epoch: 52  Training loss = 0.7979  Validation loss = 1.8685  \n",
      "\n",
      "Fold: 7  Epoch: 53  Training loss = 0.7994  Validation loss = 1.8706  \n",
      "\n",
      "Fold: 7  Epoch: 54  Training loss = 0.7967  Validation loss = 2.0486  \n",
      "\n",
      "Fold: 7  Epoch: 55  Training loss = 0.7785  Validation loss = 2.1623  \n",
      "\n",
      "Fold: 7  Epoch: 56  Training loss = 0.7844  Validation loss = 2.0915  \n",
      "\n",
      "Fold: 7  Epoch: 57  Training loss = 0.7927  Validation loss = 2.3131  \n",
      "\n",
      "Fold: 7  Epoch: 58  Training loss = 0.8434  Validation loss = 2.3942  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 31  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 0.9564  Validation loss = 7.8718  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 0.8769  Validation loss = 7.4326  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 0.9143  Validation loss = 7.5679  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 0.8619  Validation loss = 7.2649  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 0.9229  Validation loss = 7.0708  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 0.8589  Validation loss = 7.4189  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 0.9032  Validation loss = 7.1948  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 1.0062  Validation loss = 6.6685  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 1.1470  Validation loss = 6.5530  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 0.8742  Validation loss = 7.2262  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 0.8632  Validation loss = 7.4631  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 1.1250  Validation loss = 6.6337  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 0.9014  Validation loss = 6.9648  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 0.8808  Validation loss = 7.1002  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 0.9737  Validation loss = 6.7039  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 0.8649  Validation loss = 7.1666  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 0.8926  Validation loss = 7.3818  \n",
      "\n",
      "Fold: 8  Epoch: 18  Training loss = 0.8533  Validation loss = 7.4030  \n",
      "\n",
      "Fold: 8  Epoch: 19  Training loss = 0.9458  Validation loss = 6.9274  \n",
      "\n",
      "Fold: 8  Epoch: 20  Training loss = 0.9264  Validation loss = 7.1222  \n",
      "\n",
      "Fold: 8  Epoch: 21  Training loss = 0.9046  Validation loss = 6.9405  \n",
      "\n",
      "Fold: 8  Epoch: 22  Training loss = 0.9154  Validation loss = 7.4577  \n",
      "\n",
      "Fold: 8  Epoch: 23  Training loss = 0.8390  Validation loss = 7.0329  \n",
      "\n",
      "Fold: 8  Epoch: 24  Training loss = 1.0664  Validation loss = 6.5291  \n",
      "\n",
      "Fold: 8  Epoch: 25  Training loss = 1.0042  Validation loss = 6.3924  \n",
      "\n",
      "Fold: 8  Epoch: 26  Training loss = 0.8400  Validation loss = 6.9058  \n",
      "\n",
      "Fold: 8  Epoch: 27  Training loss = 0.8204  Validation loss = 6.8902  \n",
      "\n",
      "Fold: 8  Epoch: 28  Training loss = 0.8261  Validation loss = 7.1013  \n",
      "\n",
      "Fold: 8  Epoch: 29  Training loss = 0.8750  Validation loss = 7.4135  \n",
      "\n",
      "Fold: 8  Epoch: 30  Training loss = 0.9155  Validation loss = 7.5335  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 25  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 1.8092  Validation loss = 6.7920  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 1.9603  Validation loss = 7.0389  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 1.7652  Validation loss = 7.5501  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 1.8414  Validation loss = 7.9876  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 1.6080  Validation loss = 7.1911  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 1.6180  Validation loss = 7.7060  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 1.4933  Validation loss = 6.9909  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 1.4818  Validation loss = 6.5688  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 2.0909  Validation loss = 6.2889  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 1.6339  Validation loss = 6.4753  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 1.3315  Validation loss = 6.5449  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 1.3389  Validation loss = 6.2182  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 1.3262  Validation loss = 6.3309  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 1.3904  Validation loss = 6.6435  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 1.4164  Validation loss = 6.1540  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 1.3278  Validation loss = 6.6811  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 1.3994  Validation loss = 6.5671  \n",
      "\n",
      "Fold: 9  Epoch: 18  Training loss = 1.3077  Validation loss = 6.6571  \n",
      "\n",
      "Fold: 9  Epoch: 19  Training loss = 1.2830  Validation loss = 6.7781  \n",
      "\n",
      "Fold: 9  Epoch: 20  Training loss = 1.5153  Validation loss = 6.9975  \n",
      "\n",
      "Fold: 9  Epoch: 21  Training loss = 1.3131  Validation loss = 6.4653  \n",
      "\n",
      "Fold: 9  Epoch: 22  Training loss = 1.2398  Validation loss = 5.8502  \n",
      "\n",
      "Fold: 9  Epoch: 23  Training loss = 1.3846  Validation loss = 6.7780  \n",
      "\n",
      "Fold: 9  Epoch: 24  Training loss = 1.2518  Validation loss = 6.4788  \n",
      "\n",
      "Fold: 9  Epoch: 25  Training loss = 1.2290  Validation loss = 6.2539  \n",
      "\n",
      "Fold: 9  Epoch: 26  Training loss = 1.2869  Validation loss = 5.8691  \n",
      "\n",
      "Fold: 9  Epoch: 27  Training loss = 1.2710  Validation loss = 6.4508  \n",
      "\n",
      "Fold: 9  Epoch: 28  Training loss = 1.2268  Validation loss = 6.0536  \n",
      "\n",
      "Fold: 9  Epoch: 29  Training loss = 1.3045  Validation loss = 5.8112  \n",
      "\n",
      "Fold: 9  Epoch: 30  Training loss = 1.2203  Validation loss = 5.9182  \n",
      "\n",
      "Fold: 9  Epoch: 31  Training loss = 1.1811  Validation loss = 6.0296  \n",
      "\n",
      "Fold: 9  Epoch: 32  Training loss = 1.2699  Validation loss = 5.8401  \n",
      "\n",
      "Fold: 9  Epoch: 33  Training loss = 1.4342  Validation loss = 6.7848  \n",
      "\n",
      "Fold: 9  Epoch: 34  Training loss = 1.2161  Validation loss = 6.1732  \n",
      "\n",
      "Fold: 9  Epoch: 35  Training loss = 1.2581  Validation loss = 5.7665  \n",
      "\n",
      "Fold: 9  Epoch: 36  Training loss = 1.2507  Validation loss = 6.0375  \n",
      "\n",
      "Fold: 9  Epoch: 37  Training loss = 1.1595  Validation loss = 5.8930  \n",
      "\n",
      "Fold: 9  Epoch: 38  Training loss = 1.1856  Validation loss = 5.8368  \n",
      "\n",
      "Fold: 9  Epoch: 39  Training loss = 1.1661  Validation loss = 5.6039  \n",
      "\n",
      "Fold: 9  Epoch: 40  Training loss = 1.2101  Validation loss = 6.0097  \n",
      "\n",
      "Fold: 9  Epoch: 41  Training loss = 1.1537  Validation loss = 5.8895  \n",
      "\n",
      "Fold: 9  Epoch: 42  Training loss = 1.1943  Validation loss = 6.0978  \n",
      "\n",
      "Fold: 9  Epoch: 43  Training loss = 1.2179  Validation loss = 6.0336  \n",
      "\n",
      "Fold: 9  Epoch: 44  Training loss = 1.1979  Validation loss = 6.0038  \n",
      "\n",
      "Fold: 9  Epoch: 45  Training loss = 1.2114  Validation loss = 5.5135  \n",
      "\n",
      "Fold: 9  Epoch: 46  Training loss = 1.1503  Validation loss = 5.9082  \n",
      "\n",
      "Fold: 9  Epoch: 47  Training loss = 1.1407  Validation loss = 5.8314  \n",
      "\n",
      "Fold: 9  Epoch: 48  Training loss = 1.1606  Validation loss = 5.8131  \n",
      "\n",
      "Fold: 9  Epoch: 49  Training loss = 1.1817  Validation loss = 6.2765  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 45  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 1.7180  Validation loss = 3.6187  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 1.7817  Validation loss = 2.2620  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 1.6945  Validation loss = 2.8487  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 1.7624  Validation loss = 1.5185  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 1.6204  Validation loss = 2.1226  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 1.6521  Validation loss = 2.6031  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 1.5724  Validation loss = 2.3114  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 1.5899  Validation loss = 1.9368  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 2.3519  Validation loss = 2.6150  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 1.6177  Validation loss = 1.8631  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 1.6513  Validation loss = 2.0713  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 1.6784  Validation loss = 2.2303  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 1.5663  Validation loss = 2.1795  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 1.6057  Validation loss = 2.6920  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 1.5730  Validation loss = 1.9143  \n",
      "\n",
      "Fold: 10  Epoch: 16  Training loss = 1.5202  Validation loss = 2.1755  \n",
      "\n",
      "Fold: 10  Epoch: 17  Training loss = 1.5115  Validation loss = 1.5644  \n",
      "\n",
      "Fold: 10  Epoch: 18  Training loss = 1.6639  Validation loss = 2.1445  \n",
      "\n",
      "Fold: 10  Epoch: 19  Training loss = 1.6665  Validation loss = 2.3884  \n",
      "\n",
      "Fold: 10  Epoch: 20  Training loss = 1.4357  Validation loss = 1.8297  \n",
      "\n",
      "Fold: 10  Epoch: 21  Training loss = 1.5776  Validation loss = 2.0466  \n",
      "\n",
      "Fold: 10  Epoch: 22  Training loss = 1.6520  Validation loss = 1.8451  \n",
      "\n",
      "Fold: 10  Epoch: 23  Training loss = 1.6089  Validation loss = 1.6294  \n",
      "\n",
      "Fold: 10  Epoch: 24  Training loss = 1.4763  Validation loss = 1.8829  \n",
      "\n",
      "Fold: 10  Epoch: 25  Training loss = 1.6384  Validation loss = 2.1109  \n",
      "\n",
      "Fold: 10  Epoch: 26  Training loss = 1.6409  Validation loss = 1.9817  \n",
      "\n",
      "Fold: 10  Epoch: 27  Training loss = 1.5129  Validation loss = 2.1146  \n",
      "\n",
      "Fold: 10  Epoch: 28  Training loss = 1.5108  Validation loss = 1.7430  \n",
      "\n",
      "Fold: 10  Epoch: 29  Training loss = 1.5953  Validation loss = 2.0484  \n",
      "\n",
      "Fold: 10  Epoch: 30  Training loss = 1.5560  Validation loss = 2.0835  \n",
      "\n",
      "Fold: 10  Epoch: 31  Training loss = 1.4222  Validation loss = 1.7209  \n",
      "\n",
      "Fold: 10  Epoch: 32  Training loss = 1.4995  Validation loss = 2.2788  \n",
      "\n",
      "Fold: 10  Epoch: 33  Training loss = 1.4661  Validation loss = 0.6972  \n",
      "\n",
      "Fold: 10  Epoch: 34  Training loss = 1.5122  Validation loss = 0.5743  \n",
      "\n",
      "Fold: 10  Epoch: 35  Training loss = 1.5024  Validation loss = 2.3803  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 34  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 1.5009  Validation loss = 2.8740  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 1.4824  Validation loss = 3.0015  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 1.4468  Validation loss = 2.7840  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 1.4530  Validation loss = 2.9276  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 1.6775  Validation loss = 2.8201  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 1.5581  Validation loss = 3.0364  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 1.4432  Validation loss = 2.5703  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 1.4876  Validation loss = 2.9678  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 1.5028  Validation loss = 2.8517  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 1.5237  Validation loss = 3.3093  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 1.4507  Validation loss = 3.1096  \n",
      "\n",
      "Fold: 11  Epoch: 12  Training loss = 1.4367  Validation loss = 3.2085  \n",
      "\n",
      "Fold: 11  Epoch: 13  Training loss = 2.0783  Validation loss = 3.2224  \n",
      "\n",
      "Fold: 11  Epoch: 14  Training loss = 1.6053  Validation loss = 2.8278  \n",
      "\n",
      "Fold: 11  Epoch: 15  Training loss = 1.5025  Validation loss = 3.0080  \n",
      "\n",
      "Fold: 11  Epoch: 16  Training loss = 1.5233  Validation loss = 3.0220  \n",
      "\n",
      "Fold: 11  Epoch: 17  Training loss = 1.5583  Validation loss = 3.2556  \n",
      "\n",
      "Fold: 11  Epoch: 18  Training loss = 1.5656  Validation loss = 3.3148  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 7  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 1.6636  Validation loss = 1.3472  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 1.5745  Validation loss = 1.7948  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 1.5881  Validation loss = 1.4226  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 1.5801  Validation loss = 2.3448  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 1.5672  Validation loss = 1.3313  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 1.5679  Validation loss = 2.5982  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 1.5475  Validation loss = 1.2266  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 1.5187  Validation loss = 1.5331  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 1.5453  Validation loss = 1.9849  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 1.5663  Validation loss = 0.9939  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 1.5274  Validation loss = 0.9639  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 1.5380  Validation loss = 0.7741  \n",
      "\n",
      "Fold: 12  Epoch: 13  Training loss = 1.6903  Validation loss = 0.8310  \n",
      "\n",
      "Fold: 12  Epoch: 14  Training loss = 1.4753  Validation loss = 1.1425  \n",
      "\n",
      "Fold: 12  Epoch: 15  Training loss = 1.6570  Validation loss = 1.4970  \n",
      "\n",
      "Fold: 12  Epoch: 16  Training loss = 1.5386  Validation loss = 1.5109  \n",
      "\n",
      "Fold: 12  Epoch: 17  Training loss = 1.4842  Validation loss = 1.5022  \n",
      "\n",
      "Fold: 12  Epoch: 18  Training loss = 1.5015  Validation loss = 1.7425  \n",
      "\n",
      "Fold: 12  Epoch: 19  Training loss = 1.5105  Validation loss = 0.8401  \n",
      "\n",
      "Fold: 12  Epoch: 20  Training loss = 1.5290  Validation loss = 2.7637  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 12  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 1.4430  Validation loss = 3.3123  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 1.9837  Validation loss = 3.1622  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 1.6171  Validation loss = 3.6932  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 1.6225  Validation loss = 3.3858  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 1.5952  Validation loss = 4.0943  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 1.7334  Validation loss = 3.8220  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 1.6425  Validation loss = 4.0030  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 1.8797  Validation loss = 4.3058  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 1.7321  Validation loss = 3.5131  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 1.8813  Validation loss = 4.4126  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 1.8020  Validation loss = 3.5388  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 1.8128  Validation loss = 3.3323  \n",
      "\n",
      "Fold: 13  Epoch: 13  Training loss = 1.7846  Validation loss = 3.6208  \n",
      "\n",
      "Fold: 13  Epoch: 14  Training loss = 1.8103  Validation loss = 3.5282  \n",
      "\n",
      "Fold: 13  Epoch: 15  Training loss = 1.8485  Validation loss = 4.1683  \n",
      "\n",
      "Fold: 13  Epoch: 16  Training loss = 1.7017  Validation loss = 4.3560  \n",
      "\n",
      "Fold: 13  Epoch: 17  Training loss = 1.6895  Validation loss = 4.2445  \n",
      "\n",
      "Fold: 13  Epoch: 18  Training loss = 1.6876  Validation loss = 3.6824  \n",
      "\n",
      "Fold: 13  Epoch: 19  Training loss = 1.7285  Validation loss = 3.0572  \n",
      "\n",
      "Fold: 13  Epoch: 20  Training loss = 1.6609  Validation loss = 4.0037  \n",
      "\n",
      "Fold: 13  Epoch: 21  Training loss = 1.7046  Validation loss = 2.9966  \n",
      "\n",
      "Fold: 13  Epoch: 22  Training loss = 1.8034  Validation loss = 2.9443  \n",
      "\n",
      "Fold: 13  Epoch: 23  Training loss = 1.6557  Validation loss = 3.2060  \n",
      "\n",
      "Fold: 13  Epoch: 24  Training loss = 1.6116  Validation loss = 3.7122  \n",
      "\n",
      "Fold: 13  Epoch: 25  Training loss = 1.6753  Validation loss = 3.6256  \n",
      "\n",
      "Fold: 13  Epoch: 26  Training loss = 1.6564  Validation loss = 3.6120  \n",
      "\n",
      "Fold: 13  Epoch: 27  Training loss = 1.6266  Validation loss = 3.1461  \n",
      "\n",
      "Fold: 13  Epoch: 28  Training loss = 1.6952  Validation loss = 3.4864  \n",
      "\n",
      "Fold: 13  Epoch: 29  Training loss = 1.7150  Validation loss = 3.6923  \n",
      "\n",
      "Fold: 13  Epoch: 30  Training loss = 1.6275  Validation loss = 3.7207  \n",
      "\n",
      "Fold: 13  Epoch: 31  Training loss = 1.5980  Validation loss = 3.4126  \n",
      "\n",
      "Fold: 13  Epoch: 32  Training loss = 1.7122  Validation loss = 4.3148  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 22  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 2.1512  Validation loss = 7.2578  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 1.9402  Validation loss = 5.7779  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 1.8469  Validation loss = 5.4994  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 1.9887  Validation loss = 5.1486  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 1.9470  Validation loss = 5.3559  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 1.7953  Validation loss = 5.7695  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 2.0115  Validation loss = 4.9453  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 1.8772  Validation loss = 5.6558  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 1.9040  Validation loss = 5.5327  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 1.8816  Validation loss = 5.6114  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 2.0320  Validation loss = 4.7422  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 1.9794  Validation loss = 5.6220  \n",
      "\n",
      "Fold: 14  Epoch: 13  Training loss = 1.8371  Validation loss = 5.4698  \n",
      "\n",
      "Fold: 14  Epoch: 14  Training loss = 2.1577  Validation loss = 4.5294  \n",
      "\n",
      "Fold: 14  Epoch: 15  Training loss = 1.9091  Validation loss = 5.8932  \n",
      "\n",
      "Fold: 14  Epoch: 16  Training loss = 1.9060  Validation loss = 6.4435  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 14  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 2.4957  Validation loss = 5.3963  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 2.2625  Validation loss = 6.5923  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 2.2494  Validation loss = 6.0923  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 2.2856  Validation loss = 6.9904  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 2.2541  Validation loss = 6.4762  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 2.2100  Validation loss = 6.0844  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 2.2398  Validation loss = 6.7865  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 2.3388  Validation loss = 6.8191  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 2.3225  Validation loss = 6.4547  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 2.2122  Validation loss = 5.9802  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 2.1627  Validation loss = 6.0363  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 2.2283  Validation loss = 5.1411  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 2.3934  Validation loss = 5.1307  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 2.3890  Validation loss = 5.8035  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 2.3224  Validation loss = 5.9997  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 2.3567  Validation loss = 5.0544  \n",
      "\n",
      "Fold: 15  Epoch: 17  Training loss = 2.4579  Validation loss = 5.3575  \n",
      "\n",
      "Fold: 15  Epoch: 18  Training loss = 2.6544  Validation loss = 7.3038  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 16  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 2.7281  Validation loss = 5.6969  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 2.8417  Validation loss = 5.9209  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 2.6300  Validation loss = 5.4965  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 2.6765  Validation loss = 5.8719  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 2.9113  Validation loss = 3.7018  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 2.7888  Validation loss = 5.8496  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 2.7273  Validation loss = 5.2854  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 2.7221  Validation loss = 6.0429  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 2.8086  Validation loss = 5.9514  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 2.6738  Validation loss = 5.2041  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 3.4284  Validation loss = 6.5418  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 2.9698  Validation loss = 4.1506  \n",
      "\n",
      "Fold: 16  Epoch: 13  Training loss = 3.2672  Validation loss = 5.8906  \n",
      "\n",
      "Fold: 16  Epoch: 14  Training loss = 2.9542  Validation loss = 5.1007  \n",
      "\n",
      "Fold: 16  Epoch: 15  Training loss = 2.8485  Validation loss = 5.4590  \n",
      "\n",
      "Fold: 16  Epoch: 16  Training loss = 2.8866  Validation loss = 6.3815  \n",
      "\n",
      "Fold: 16  Epoch: 17  Training loss = 2.8481  Validation loss = 5.6779  \n",
      "\n",
      "Fold: 16  Epoch: 18  Training loss = 2.8893  Validation loss = 5.5294  \n",
      "\n",
      "Fold: 16  Epoch: 19  Training loss = 2.8553  Validation loss = 5.5331  \n",
      "\n",
      "Fold: 16  Epoch: 20  Training loss = 3.0646  Validation loss = 6.4097  \n",
      "\n",
      "Fold: 16  Epoch: 21  Training loss = 3.0611  Validation loss = 4.4906  \n",
      "\n",
      "Fold: 16  Epoch: 22  Training loss = 2.9486  Validation loss = 6.1676  \n",
      "\n",
      "Fold: 16  Epoch: 23  Training loss = 2.9407  Validation loss = 6.3979  \n",
      "\n",
      "Fold: 16  Epoch: 24  Training loss = 2.9138  Validation loss = 5.0652  \n",
      "\n",
      "Fold: 16  Epoch: 25  Training loss = 3.2658  Validation loss = 3.5471  \n",
      "\n",
      "Fold: 16  Epoch: 26  Training loss = 3.1057  Validation loss = 3.4113  \n",
      "\n",
      "Fold: 16  Epoch: 27  Training loss = 3.7280  Validation loss = 4.2052  \n",
      "\n",
      "Fold: 16  Epoch: 28  Training loss = 3.0105  Validation loss = 4.5882  \n",
      "\n",
      "Fold: 16  Epoch: 29  Training loss = 2.8927  Validation loss = 4.2748  \n",
      "\n",
      "Fold: 16  Epoch: 30  Training loss = 2.8963  Validation loss = 4.9943  \n",
      "\n",
      "Fold: 16  Epoch: 31  Training loss = 2.9869  Validation loss = 5.6187  \n",
      "\n",
      "Fold: 16  Epoch: 32  Training loss = 2.8441  Validation loss = 4.8189  \n",
      "\n",
      "Fold: 16  Epoch: 33  Training loss = 2.9918  Validation loss = 5.6422  \n",
      "\n",
      "Fold: 16  Epoch: 34  Training loss = 2.8297  Validation loss = 4.6366  \n",
      "\n",
      "Fold: 16  Epoch: 35  Training loss = 2.8405  Validation loss = 4.5106  \n",
      "\n",
      "Fold: 16  Epoch: 36  Training loss = 2.8593  Validation loss = 4.6322  \n",
      "\n",
      "Fold: 16  Epoch: 37  Training loss = 3.9467  Validation loss = 2.4904  \n",
      "\n",
      "Fold: 16  Epoch: 38  Training loss = 2.9110  Validation loss = 5.6104  \n",
      "\n",
      "Fold: 16  Epoch: 39  Training loss = 2.9367  Validation loss = 4.0742  \n",
      "\n",
      "Fold: 16  Epoch: 40  Training loss = 2.8754  Validation loss = 4.1041  \n",
      "\n",
      "Fold: 16  Epoch: 41  Training loss = 3.0947  Validation loss = 6.0156  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 37  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 3.0443  Validation loss = 3.0051  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 3.0264  Validation loss = 2.8633  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 3.0239  Validation loss = 2.8149  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 3.1654  Validation loss = 3.1771  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 3.0274  Validation loss = 2.4763  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 3.0708  Validation loss = 3.0927  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 2.9797  Validation loss = 3.5086  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 3.2037  Validation loss = 3.3225  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 3.0049  Validation loss = 3.7308  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 2.9890  Validation loss = 3.3849  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 3.0331  Validation loss = 3.1308  \n",
      "\n",
      "Fold: 17  Epoch: 12  Training loss = 3.2988  Validation loss = 2.7552  \n",
      "\n",
      "Fold: 17  Epoch: 13  Training loss = 3.0673  Validation loss = 2.4630  \n",
      "\n",
      "Fold: 17  Epoch: 14  Training loss = 3.0850  Validation loss = 2.1089  \n",
      "\n",
      "Fold: 17  Epoch: 15  Training loss = 2.9856  Validation loss = 2.6689  \n",
      "\n",
      "Fold: 17  Epoch: 16  Training loss = 3.4125  Validation loss = 3.2170  \n",
      "\n",
      "Fold: 17  Epoch: 17  Training loss = 2.9607  Validation loss = 3.2401  \n",
      "\n",
      "Fold: 17  Epoch: 18  Training loss = 2.9444  Validation loss = 2.4366  \n",
      "\n",
      "Fold: 17  Epoch: 19  Training loss = 2.9826  Validation loss = 3.0494  \n",
      "\n",
      "Fold: 17  Epoch: 20  Training loss = 2.9754  Validation loss = 2.7314  \n",
      "\n",
      "Fold: 17  Epoch: 21  Training loss = 3.0489  Validation loss = 2.9691  \n",
      "\n",
      "Fold: 17  Epoch: 22  Training loss = 2.9224  Validation loss = 2.3927  \n",
      "\n",
      "Fold: 17  Epoch: 23  Training loss = 2.9069  Validation loss = 3.0067  \n",
      "\n",
      "Fold: 17  Epoch: 24  Training loss = 2.9170  Validation loss = 2.6443  \n",
      "\n",
      "Fold: 17  Epoch: 25  Training loss = 3.0759  Validation loss = 2.5348  \n",
      "\n",
      "Fold: 17  Epoch: 26  Training loss = 3.0106  Validation loss = 2.7239  \n",
      "\n",
      "Fold: 17  Epoch: 27  Training loss = 2.9664  Validation loss = 2.3963  \n",
      "\n",
      "Fold: 17  Epoch: 28  Training loss = 3.0918  Validation loss = 2.2075  \n",
      "\n",
      "Fold: 17  Epoch: 29  Training loss = 3.0018  Validation loss = 2.3813  \n",
      "\n",
      "Fold: 17  Epoch: 30  Training loss = 2.9089  Validation loss = 2.1927  \n",
      "\n",
      "Fold: 17  Epoch: 31  Training loss = 2.8779  Validation loss = 1.9046  \n",
      "\n",
      "Fold: 17  Epoch: 32  Training loss = 2.8782  Validation loss = 2.4484  \n",
      "\n",
      "Fold: 17  Epoch: 33  Training loss = 2.8609  Validation loss = 2.9041  \n",
      "\n",
      "Fold: 17  Epoch: 34  Training loss = 2.9743  Validation loss = 2.6885  \n",
      "\n",
      "Fold: 17  Epoch: 35  Training loss = 2.9498  Validation loss = 1.9159  \n",
      "\n",
      "Fold: 17  Epoch: 36  Training loss = 2.9880  Validation loss = 2.1649  \n",
      "\n",
      "Fold: 17  Epoch: 37  Training loss = 3.1170  Validation loss = 2.4138  \n",
      "\n",
      "Fold: 17  Epoch: 38  Training loss = 2.8652  Validation loss = 1.7961  \n",
      "\n",
      "Fold: 17  Epoch: 39  Training loss = 2.9962  Validation loss = 2.0651  \n",
      "\n",
      "Fold: 17  Epoch: 40  Training loss = 2.8775  Validation loss = 2.4351  \n",
      "\n",
      "Fold: 17  Epoch: 41  Training loss = 2.9238  Validation loss = 2.3492  \n",
      "\n",
      "Fold: 17  Epoch: 42  Training loss = 2.9373  Validation loss = 1.9272  \n",
      "\n",
      "Fold: 17  Epoch: 43  Training loss = 2.8669  Validation loss = 1.9602  \n",
      "\n",
      "Fold: 17  Epoch: 44  Training loss = 2.7949  Validation loss = 2.9218  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 38  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 2.9178  Validation loss = 4.5180  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 3.0434  Validation loss = 3.0992  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 2.9957  Validation loss = 2.8628  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 3.0707  Validation loss = 3.8125  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 2.8405  Validation loss = 2.9207  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 2.8245  Validation loss = 2.7989  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 2.7334  Validation loss = 2.8028  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 2.9179  Validation loss = 2.5641  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 2.8210  Validation loss = 2.5294  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 2.8597  Validation loss = 2.5936  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 2.8188  Validation loss = 3.0232  \n",
      "\n",
      "Fold: 18  Epoch: 12  Training loss = 2.9954  Validation loss = 3.8035  \n",
      "\n",
      "Fold: 18  Epoch: 13  Training loss = 2.8449  Validation loss = 3.3256  \n",
      "\n",
      "Fold: 18  Epoch: 14  Training loss = 3.1320  Validation loss = 2.8711  \n",
      "\n",
      "Fold: 18  Epoch: 15  Training loss = 3.0229  Validation loss = 2.6306  \n",
      "\n",
      "Fold: 18  Epoch: 16  Training loss = 2.9978  Validation loss = 2.6725  \n",
      "\n",
      "Fold: 18  Epoch: 17  Training loss = 3.1988  Validation loss = 1.9065  \n",
      "\n",
      "Fold: 18  Epoch: 18  Training loss = 3.0684  Validation loss = 2.0534  \n",
      "\n",
      "Fold: 18  Epoch: 19  Training loss = 3.0441  Validation loss = 2.3950  \n",
      "\n",
      "Fold: 18  Epoch: 20  Training loss = 3.0949  Validation loss = 1.9058  \n",
      "\n",
      "Fold: 18  Epoch: 21  Training loss = 3.0708  Validation loss = 2.9803  \n",
      "\n",
      "Fold: 18  Epoch: 22  Training loss = 3.3987  Validation loss = 1.8317  \n",
      "\n",
      "Fold: 18  Epoch: 23  Training loss = 3.0254  Validation loss = 2.2090  \n",
      "\n",
      "Fold: 18  Epoch: 24  Training loss = 3.0034  Validation loss = 2.3181  \n",
      "\n",
      "Fold: 18  Epoch: 25  Training loss = 3.1130  Validation loss = 2.7609  \n",
      "\n",
      "Fold: 18  Epoch: 26  Training loss = 3.1552  Validation loss = 1.9470  \n",
      "\n",
      "Fold: 18  Epoch: 27  Training loss = 3.0449  Validation loss = 2.1114  \n",
      "\n",
      "Fold: 18  Epoch: 28  Training loss = 3.2157  Validation loss = 2.7149  \n",
      "\n",
      "Fold: 18  Epoch: 29  Training loss = 2.9954  Validation loss = 2.4159  \n",
      "\n",
      "Fold: 18  Epoch: 30  Training loss = 3.0190  Validation loss = 2.1344  \n",
      "\n",
      "Fold: 18  Epoch: 31  Training loss = 3.0118  Validation loss = 2.1249  \n",
      "\n",
      "Fold: 18  Epoch: 32  Training loss = 2.9896  Validation loss = 2.3530  \n",
      "\n",
      "Fold: 18  Epoch: 33  Training loss = 2.9813  Validation loss = 2.2129  \n",
      "\n",
      "Fold: 18  Epoch: 34  Training loss = 2.9818  Validation loss = 2.2613  \n",
      "\n",
      "Fold: 18  Epoch: 35  Training loss = 2.9970  Validation loss = 2.4200  \n",
      "\n",
      "Fold: 18  Epoch: 36  Training loss = 3.1101  Validation loss = 2.7487  \n",
      "\n",
      "Fold: 18  Epoch: 37  Training loss = 3.1817  Validation loss = 1.7634  \n",
      "\n",
      "Fold: 18  Epoch: 38  Training loss = 3.0498  Validation loss = 2.4059  \n",
      "\n",
      "Fold: 18  Epoch: 39  Training loss = 2.9945  Validation loss = 2.1030  \n",
      "\n",
      "Fold: 18  Epoch: 40  Training loss = 3.0432  Validation loss = 2.4939  \n",
      "\n",
      "Fold: 18  Epoch: 41  Training loss = 3.3152  Validation loss = 1.9508  \n",
      "\n",
      "Fold: 18  Epoch: 42  Training loss = 3.0875  Validation loss = 1.9458  \n",
      "\n",
      "Fold: 18  Epoch: 43  Training loss = 3.0689  Validation loss = 1.9155  \n",
      "\n",
      "Fold: 18  Epoch: 44  Training loss = 2.9956  Validation loss = 2.0329  \n",
      "\n",
      "Fold: 18  Epoch: 45  Training loss = 3.2150  Validation loss = 3.0920  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 37  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 3.0918  Validation loss = 2.4277  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 3.3182  Validation loss = 1.8872  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 3.0243  Validation loss = 2.9957  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 3.0436  Validation loss = 2.0491  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 3.0335  Validation loss = 2.2144  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 3.0338  Validation loss = 2.5044  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 3.3811  Validation loss = 3.5560  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 3.0502  Validation loss = 2.1012  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 2.9945  Validation loss = 2.7841  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 3.2054  Validation loss = 3.4166  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 2.9682  Validation loss = 2.3911  \n",
      "\n",
      "Fold: 19  Epoch: 12  Training loss = 2.9614  Validation loss = 2.8293  \n",
      "\n",
      "Fold: 19  Epoch: 13  Training loss = 3.2172  Validation loss = 3.4183  \n",
      "\n",
      "Fold: 19  Epoch: 14  Training loss = 3.0580  Validation loss = 2.8779  \n",
      "\n",
      "Fold: 19  Epoch: 15  Training loss = 3.0650  Validation loss = 2.6470  \n",
      "\n",
      "Fold: 19  Epoch: 16  Training loss = 3.1457  Validation loss = 3.5719  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 2  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 3.0125  Validation loss = 3.2756  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 3.0344  Validation loss = 2.9236  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 3.0109  Validation loss = 2.6074  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 3.1306  Validation loss = 3.0482  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 3.0351  Validation loss = 2.6742  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 3.6659  Validation loss = 3.2169  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 3.0211  Validation loss = 2.0464  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 3.1285  Validation loss = 2.1311  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 3.1584  Validation loss = 2.6065  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 3.1858  Validation loss = 2.3775  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 3.0780  Validation loss = 1.9305  \n",
      "\n",
      "Fold: 20  Epoch: 12  Training loss = 3.0841  Validation loss = 2.2323  \n",
      "\n",
      "Fold: 20  Epoch: 13  Training loss = 2.9880  Validation loss = 2.1884  \n",
      "\n",
      "Fold: 20  Epoch: 14  Training loss = 3.0486  Validation loss = 2.6396  \n",
      "\n",
      "Fold: 20  Epoch: 15  Training loss = 3.1344  Validation loss = 2.1168  \n",
      "\n",
      "Fold: 20  Epoch: 16  Training loss = 3.0269  Validation loss = 2.5528  \n",
      "\n",
      "Fold: 20  Epoch: 17  Training loss = 2.9653  Validation loss = 2.5052  \n",
      "\n",
      "Fold: 20  Epoch: 18  Training loss = 2.9756  Validation loss = 2.4535  \n",
      "\n",
      "Fold: 20  Epoch: 19  Training loss = 2.9707  Validation loss = 2.0606  \n",
      "\n",
      "Fold: 20  Epoch: 20  Training loss = 2.9869  Validation loss = 2.2781  \n",
      "\n",
      "Fold: 20  Epoch: 21  Training loss = 3.0737  Validation loss = 2.3485  \n",
      "\n",
      "Fold: 20  Epoch: 22  Training loss = 3.0713  Validation loss = 1.5165  \n",
      "\n",
      "Fold: 20  Epoch: 23  Training loss = 2.9914  Validation loss = 1.7190  \n",
      "\n",
      "Fold: 20  Epoch: 24  Training loss = 3.2505  Validation loss = 1.8126  \n",
      "\n",
      "Fold: 20  Epoch: 25  Training loss = 3.0816  Validation loss = 1.5379  \n",
      "\n",
      "Fold: 20  Epoch: 26  Training loss = 2.9865  Validation loss = 1.6777  \n",
      "\n",
      "Fold: 20  Epoch: 27  Training loss = 3.0109  Validation loss = 1.5574  \n",
      "\n",
      "Fold: 20  Epoch: 28  Training loss = 3.0933  Validation loss = 2.3946  \n",
      "\n",
      "Fold: 20  Epoch: 29  Training loss = 2.9581  Validation loss = 1.6960  \n",
      "\n",
      "Fold: 20  Epoch: 30  Training loss = 2.9709  Validation loss = 1.8122  \n",
      "\n",
      "Fold: 20  Epoch: 31  Training loss = 2.9811  Validation loss = 2.0663  \n",
      "\n",
      "Fold: 20  Epoch: 32  Training loss = 2.9544  Validation loss = 1.5503  \n",
      "\n",
      "Fold: 20  Epoch: 33  Training loss = 3.1220  Validation loss = 2.0131  \n",
      "\n",
      "Fold: 20  Epoch: 34  Training loss = 2.9517  Validation loss = 1.7808  \n",
      "\n",
      "Fold: 20  Epoch: 35  Training loss = 3.8290  Validation loss = 3.1782  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 22  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 2.9600  Validation loss = 4.5738  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 2.9814  Validation loss = 4.0339  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 3.0694  Validation loss = 3.6303  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 2.9720  Validation loss = 3.9788  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 3.0739  Validation loss = 4.4787  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 2.9581  Validation loss = 3.7829  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 2.9570  Validation loss = 3.6172  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 2.9685  Validation loss = 3.4409  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 2.9783  Validation loss = 3.5782  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 3.0628  Validation loss = 3.4154  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 3.1085  Validation loss = 2.9739  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 3.0133  Validation loss = 3.5627  \n",
      "\n",
      "Fold: 21  Epoch: 13  Training loss = 3.0005  Validation loss = 3.6034  \n",
      "\n",
      "Fold: 21  Epoch: 14  Training loss = 3.1973  Validation loss = 3.3068  \n",
      "\n",
      "Fold: 21  Epoch: 15  Training loss = 2.9780  Validation loss = 4.0646  \n",
      "\n",
      "Fold: 21  Epoch: 16  Training loss = 3.0503  Validation loss = 3.4815  \n",
      "\n",
      "Fold: 21  Epoch: 17  Training loss = 2.9586  Validation loss = 3.7067  \n",
      "\n",
      "Fold: 21  Epoch: 18  Training loss = 3.0372  Validation loss = 4.3505  \n",
      "\n",
      "Fold: 21  Epoch: 19  Training loss = 3.2786  Validation loss = 4.3682  \n",
      "\n",
      "Fold: 21  Epoch: 20  Training loss = 3.0373  Validation loss = 3.2442  \n",
      "\n",
      "Fold: 21  Epoch: 21  Training loss = 2.9455  Validation loss = 3.4058  \n",
      "\n",
      "Fold: 21  Epoch: 22  Training loss = 3.0592  Validation loss = 3.1722  \n",
      "\n",
      "Fold: 21  Epoch: 23  Training loss = 2.9468  Validation loss = 3.5775  \n",
      "\n",
      "Fold: 21  Epoch: 24  Training loss = 2.9721  Validation loss = 3.1250  \n",
      "\n",
      "Fold: 21  Epoch: 25  Training loss = 3.1268  Validation loss = 2.9183  \n",
      "\n",
      "Fold: 21  Epoch: 26  Training loss = 3.0122  Validation loss = 3.4331  \n",
      "\n",
      "Fold: 21  Epoch: 27  Training loss = 3.1097  Validation loss = 4.1529  \n",
      "\n",
      "Fold: 21  Epoch: 28  Training loss = 3.0099  Validation loss = 3.5068  \n",
      "\n",
      "Fold: 21  Epoch: 29  Training loss = 3.0949  Validation loss = 2.8850  \n",
      "\n",
      "Fold: 21  Epoch: 30  Training loss = 3.0313  Validation loss = 3.4180  \n",
      "\n",
      "Fold: 21  Epoch: 31  Training loss = 3.1080  Validation loss = 3.7139  \n",
      "\n",
      "Fold: 21  Epoch: 32  Training loss = 2.9592  Validation loss = 3.5858  \n",
      "\n",
      "Fold: 21  Epoch: 33  Training loss = 2.9658  Validation loss = 3.3357  \n",
      "\n",
      "Fold: 21  Epoch: 34  Training loss = 2.9739  Validation loss = 3.5385  \n",
      "\n",
      "Fold: 21  Epoch: 35  Training loss = 3.0396  Validation loss = 3.9529  \n",
      "\n",
      "Fold: 21  Epoch: 36  Training loss = 3.0849  Validation loss = 3.4982  \n",
      "\n",
      "Fold: 21  Epoch: 37  Training loss = 2.9533  Validation loss = 3.2622  \n",
      "\n",
      "Fold: 21  Epoch: 38  Training loss = 2.9524  Validation loss = 3.7052  \n",
      "\n",
      "Fold: 21  Epoch: 39  Training loss = 2.9652  Validation loss = 3.2367  \n",
      "\n",
      "Fold: 21  Epoch: 40  Training loss = 3.3353  Validation loss = 2.6547  \n",
      "\n",
      "Fold: 21  Epoch: 41  Training loss = 2.9618  Validation loss = 3.7436  \n",
      "\n",
      "Fold: 21  Epoch: 42  Training loss = 2.9981  Validation loss = 3.3572  \n",
      "\n",
      "Fold: 21  Epoch: 43  Training loss = 2.9547  Validation loss = 3.6427  \n",
      "\n",
      "Fold: 21  Epoch: 44  Training loss = 2.9724  Validation loss = 3.4075  \n",
      "\n",
      "Fold: 21  Epoch: 45  Training loss = 2.9650  Validation loss = 3.5042  \n",
      "\n",
      "Fold: 21  Epoch: 46  Training loss = 3.1744  Validation loss = 2.8956  \n",
      "\n",
      "Fold: 21  Epoch: 47  Training loss = 2.9539  Validation loss = 3.0915  \n",
      "\n",
      "Fold: 21  Epoch: 48  Training loss = 3.5831  Validation loss = 4.1021  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 40  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 3.1327  Validation loss = 2.2346  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 3.0844  Validation loss = 1.6887  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 3.0129  Validation loss = 1.8281  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 3.4326  Validation loss = 1.9552  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 2.8953  Validation loss = 1.9532  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 3.0424  Validation loss = 1.3988  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 2.9443  Validation loss = 1.7557  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 3.0450  Validation loss = 2.3535  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 2.9449  Validation loss = 1.5683  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 2.9358  Validation loss = 1.9542  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 2.9760  Validation loss = 2.1589  \n",
      "\n",
      "Fold: 22  Epoch: 12  Training loss = 2.9515  Validation loss = 2.0748  \n",
      "\n",
      "Fold: 22  Epoch: 13  Training loss = 2.9826  Validation loss = 1.9174  \n",
      "\n",
      "Fold: 22  Epoch: 14  Training loss = 3.0110  Validation loss = 1.6472  \n",
      "\n",
      "Fold: 22  Epoch: 15  Training loss = 2.9320  Validation loss = 1.5395  \n",
      "\n",
      "Fold: 22  Epoch: 16  Training loss = 2.9168  Validation loss = 1.7933  \n",
      "\n",
      "Fold: 22  Epoch: 17  Training loss = 2.9175  Validation loss = 1.9784  \n",
      "\n",
      "Fold: 22  Epoch: 18  Training loss = 2.9270  Validation loss = 1.9653  \n",
      "\n",
      "Fold: 22  Epoch: 19  Training loss = 2.9789  Validation loss = 1.6097  \n",
      "\n",
      "Fold: 22  Epoch: 20  Training loss = 2.9678  Validation loss = 1.9712  \n",
      "\n",
      "Fold: 22  Epoch: 21  Training loss = 2.9943  Validation loss = 1.8160  \n",
      "\n",
      "Fold: 22  Epoch: 22  Training loss = 2.9662  Validation loss = 1.6877  \n",
      "\n",
      "Fold: 22  Epoch: 23  Training loss = 3.1224  Validation loss = 2.0164  \n",
      "\n",
      "Fold: 22  Epoch: 24  Training loss = 2.9416  Validation loss = 1.8166  \n",
      "\n",
      "Fold: 22  Epoch: 25  Training loss = 3.0322  Validation loss = 1.6946  \n",
      "\n",
      "Fold: 22  Epoch: 26  Training loss = 2.9220  Validation loss = 1.3413  \n",
      "\n",
      "Fold: 22  Epoch: 27  Training loss = 2.9219  Validation loss = 1.2989  \n",
      "\n",
      "Fold: 22  Epoch: 28  Training loss = 2.8426  Validation loss = 1.4235  \n",
      "\n",
      "Fold: 22  Epoch: 29  Training loss = 2.8456  Validation loss = 1.4919  \n",
      "\n",
      "Fold: 22  Epoch: 30  Training loss = 2.8571  Validation loss = 1.4306  \n",
      "\n",
      "Fold: 22  Epoch: 31  Training loss = 2.8536  Validation loss = 1.5288  \n",
      "\n",
      "Fold: 22  Epoch: 32  Training loss = 3.3915  Validation loss = 2.3604  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 27  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 2.8280  Validation loss = 3.1887  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 2.7162  Validation loss = 2.1201  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 2.8414  Validation loss = 1.8697  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 2.7885  Validation loss = 3.0386  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 2.6886  Validation loss = 2.5194  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 2.7223  Validation loss = 2.0923  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 2.8205  Validation loss = 2.4306  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 2.7166  Validation loss = 2.3657  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 2.7546  Validation loss = 2.0233  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 2.8936  Validation loss = 1.9478  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 2.7211  Validation loss = 2.7657  \n",
      "\n",
      "Fold: 23  Epoch: 12  Training loss = 2.7493  Validation loss = 2.8983  \n",
      "\n",
      "Fold: 23  Epoch: 13  Training loss = 2.6704  Validation loss = 2.2284  \n",
      "\n",
      "Fold: 23  Epoch: 14  Training loss = 2.6771  Validation loss = 2.1743  \n",
      "\n",
      "Fold: 23  Epoch: 15  Training loss = 2.7000  Validation loss = 2.4417  \n",
      "\n",
      "Fold: 23  Epoch: 16  Training loss = 2.6789  Validation loss = 2.7839  \n",
      "\n",
      "Fold: 23  Epoch: 17  Training loss = 2.6688  Validation loss = 2.5232  \n",
      "\n",
      "Fold: 23  Epoch: 18  Training loss = 2.8048  Validation loss = 2.2509  \n",
      "\n",
      "Fold: 23  Epoch: 19  Training loss = 2.7933  Validation loss = 2.5298  \n",
      "\n",
      "Fold: 23  Epoch: 20  Training loss = 2.7602  Validation loss = 3.1632  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 3  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 2.6728  Validation loss = 2.2367  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 2.7169  Validation loss = 2.4116  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 2.5862  Validation loss = 1.8738  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 2.6005  Validation loss = 1.5293  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 2.9198  Validation loss = 2.0896  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 2.6648  Validation loss = 1.5177  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 2.6423  Validation loss = 1.1436  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 2.6596  Validation loss = 1.2100  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 2.6350  Validation loss = 1.6449  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 2.6386  Validation loss = 1.3535  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 2.6887  Validation loss = 1.3213  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 2.6279  Validation loss = 1.2084  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 2.6967  Validation loss = 1.0884  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 2.6904  Validation loss = 1.0781  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 2.6601  Validation loss = 1.2709  \n",
      "\n",
      "Fold: 24  Epoch: 16  Training loss = 2.7925  Validation loss = 1.1396  \n",
      "\n",
      "Fold: 24  Epoch: 17  Training loss = 2.5591  Validation loss = 1.0807  \n",
      "\n",
      "Fold: 24  Epoch: 18  Training loss = 2.5588  Validation loss = 2.0233  \n",
      "\n",
      "Fold: 24  Epoch: 19  Training loss = 2.7379  Validation loss = 1.4460  \n",
      "\n",
      "Fold: 24  Epoch: 20  Training loss = 2.6651  Validation loss = 1.4036  \n",
      "\n",
      "Fold: 24  Epoch: 21  Training loss = 2.5745  Validation loss = 1.0391  \n",
      "\n",
      "Fold: 24  Epoch: 22  Training loss = 2.6914  Validation loss = 1.3184  \n",
      "\n",
      "Fold: 24  Epoch: 23  Training loss = 2.7732  Validation loss = 1.5063  \n",
      "\n",
      "Fold: 24  Epoch: 24  Training loss = 2.6410  Validation loss = 1.3680  \n",
      "\n",
      "Fold: 24  Epoch: 25  Training loss = 2.6033  Validation loss = 1.3674  \n",
      "\n",
      "Fold: 24  Epoch: 26  Training loss = 2.6739  Validation loss = 1.1621  \n",
      "\n",
      "Fold: 24  Epoch: 27  Training loss = 2.5367  Validation loss = 1.2090  \n",
      "\n",
      "Fold: 24  Epoch: 28  Training loss = 2.5715  Validation loss = 1.4722  \n",
      "\n",
      "Fold: 24  Epoch: 29  Training loss = 2.5907  Validation loss = 1.0250  \n",
      "\n",
      "Fold: 24  Epoch: 30  Training loss = 2.6043  Validation loss = 1.4693  \n",
      "\n",
      "Fold: 24  Epoch: 31  Training loss = 3.3888  Validation loss = 1.0475  \n",
      "\n",
      "Fold: 24  Epoch: 32  Training loss = 2.6696  Validation loss = 1.1684  \n",
      "\n",
      "Fold: 24  Epoch: 33  Training loss = 2.5737  Validation loss = 1.0391  \n",
      "\n",
      "Fold: 24  Epoch: 34  Training loss = 2.6060  Validation loss = 1.1738  \n",
      "\n",
      "Fold: 24  Epoch: 35  Training loss = 2.6265  Validation loss = 1.0622  \n",
      "\n",
      "Fold: 24  Epoch: 36  Training loss = 2.7541  Validation loss = 1.7656  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 29  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 2.5615  Validation loss = 2.2117  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 2.5013  Validation loss = 1.9067  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 2.5614  Validation loss = 1.4866  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 2.5112  Validation loss = 2.0951  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 2.5024  Validation loss = 1.6754  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 2.5534  Validation loss = 1.5486  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 2.4864  Validation loss = 1.9693  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 2.6783  Validation loss = 1.5512  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 2.5121  Validation loss = 1.7601  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 2.5087  Validation loss = 1.6748  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 2.5352  Validation loss = 1.5912  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 2.5200  Validation loss = 1.8739  \n",
      "\n",
      "Fold: 25  Epoch: 13  Training loss = 2.7959  Validation loss = 3.1332  \n",
      "\n",
      "Fold: 25  Epoch: 14  Training loss = 2.5168  Validation loss = 1.6299  \n",
      "\n",
      "Fold: 25  Epoch: 15  Training loss = 2.5812  Validation loss = 2.1099  \n",
      "\n",
      "Fold: 25  Epoch: 16  Training loss = 2.4955  Validation loss = 1.9004  \n",
      "\n",
      "Fold: 25  Epoch: 17  Training loss = 2.5464  Validation loss = 2.3371  \n",
      "\n",
      "Fold: 25  Epoch: 18  Training loss = 2.5505  Validation loss = 1.6687  \n",
      "\n",
      "Fold: 25  Epoch: 19  Training loss = 2.5719  Validation loss = 1.3572  \n",
      "\n",
      "Fold: 25  Epoch: 20  Training loss = 2.5580  Validation loss = 2.5697  \n",
      "\n",
      "Fold: 25  Epoch: 21  Training loss = 2.5254  Validation loss = 2.2698  \n",
      "\n",
      "Fold: 25  Epoch: 22  Training loss = 2.5432  Validation loss = 1.5114  \n",
      "\n",
      "Fold: 25  Epoch: 23  Training loss = 2.5328  Validation loss = 2.0645  \n",
      "\n",
      "Fold: 25  Epoch: 24  Training loss = 2.5238  Validation loss = 2.3619  \n",
      "\n",
      "Fold: 25  Epoch: 25  Training loss = 2.5186  Validation loss = 1.7177  \n",
      "\n",
      "Fold: 25  Epoch: 26  Training loss = 2.5334  Validation loss = 1.6436  \n",
      "\n",
      "Fold: 25  Epoch: 27  Training loss = 2.5344  Validation loss = 1.7907  \n",
      "\n",
      "Fold: 25  Epoch: 28  Training loss = 2.5343  Validation loss = 2.2577  \n",
      "\n",
      "Fold: 25  Epoch: 29  Training loss = 2.6459  Validation loss = 2.4688  \n",
      "\n",
      "Fold: 25  Epoch: 30  Training loss = 2.4861  Validation loss = 1.9553  \n",
      "\n",
      "Fold: 25  Epoch: 31  Training loss = 2.4865  Validation loss = 1.8037  \n",
      "\n",
      "Fold: 25  Epoch: 32  Training loss = 2.5864  Validation loss = 1.5148  \n",
      "\n",
      "Fold: 25  Epoch: 33  Training loss = 2.5874  Validation loss = 2.5742  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 19  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 2.6182  Validation loss = 2.7659  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 2.5844  Validation loss = 2.8186  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 2.4927  Validation loss = 2.0220  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 2.4975  Validation loss = 2.3499  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 2.6808  Validation loss = 3.5051  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 2.4918  Validation loss = 1.8718  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 2.5400  Validation loss = 1.9473  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 2.7258  Validation loss = 3.6214  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 2.5671  Validation loss = 3.1168  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 2.4922  Validation loss = 1.9825  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 2.6043  Validation loss = 1.3063  \n",
      "\n",
      "Fold: 26  Epoch: 12  Training loss = 2.4925  Validation loss = 2.1575  \n",
      "\n",
      "Fold: 26  Epoch: 13  Training loss = 2.5303  Validation loss = 2.2137  \n",
      "\n",
      "Fold: 26  Epoch: 14  Training loss = 2.5355  Validation loss = 1.4977  \n",
      "\n",
      "Fold: 26  Epoch: 15  Training loss = 2.5122  Validation loss = 2.4943  \n",
      "\n",
      "Fold: 26  Epoch: 16  Training loss = 2.5605  Validation loss = 2.9098  \n",
      "\n",
      "Fold: 26  Epoch: 17  Training loss = 2.5645  Validation loss = 1.5794  \n",
      "\n",
      "Fold: 26  Epoch: 18  Training loss = 2.5347  Validation loss = 2.6292  \n",
      "\n",
      "Fold: 26  Epoch: 19  Training loss = 2.5728  Validation loss = 2.2074  \n",
      "\n",
      "Fold: 26  Epoch: 20  Training loss = 2.6051  Validation loss = 1.7513  \n",
      "\n",
      "Fold: 26  Epoch: 21  Training loss = 2.5574  Validation loss = 2.0401  \n",
      "\n",
      "Fold: 26  Epoch: 22  Training loss = 2.5427  Validation loss = 2.8659  \n",
      "\n",
      "Fold: 26  Epoch: 23  Training loss = 2.5030  Validation loss = 1.8415  \n",
      "\n",
      "Fold: 26  Epoch: 24  Training loss = 2.5416  Validation loss = 1.7635  \n",
      "\n",
      "Fold: 26  Epoch: 25  Training loss = 2.8426  Validation loss = 0.9042  \n",
      "\n",
      "Fold: 26  Epoch: 26  Training loss = 2.4988  Validation loss = 2.1113  \n",
      "\n",
      "Fold: 26  Epoch: 27  Training loss = 2.5464  Validation loss = 3.0088  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 25  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 2.4725  Validation loss = 1.7591  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 2.4981  Validation loss = 1.3872  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 2.4855  Validation loss = 2.1569  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 2.5749  Validation loss = 0.9665  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 2.4404  Validation loss = 1.8825  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 2.4282  Validation loss = 1.7104  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 2.5036  Validation loss = 2.1403  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 2.5295  Validation loss = 2.0590  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 2.4897  Validation loss = 1.7909  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 2.5032  Validation loss = 1.9791  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 2.4834  Validation loss = 1.9654  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 2.5031  Validation loss = 1.1422  \n",
      "\n",
      "Fold: 27  Epoch: 13  Training loss = 2.4554  Validation loss = 1.7931  \n",
      "\n",
      "Fold: 27  Epoch: 14  Training loss = 2.4382  Validation loss = 1.4439  \n",
      "\n",
      "Fold: 27  Epoch: 15  Training loss = 2.4573  Validation loss = 1.8475  \n",
      "\n",
      "Fold: 27  Epoch: 16  Training loss = 2.4423  Validation loss = 1.7810  \n",
      "\n",
      "Fold: 27  Epoch: 17  Training loss = 2.4571  Validation loss = 1.8359  \n",
      "\n",
      "Fold: 27  Epoch: 18  Training loss = 2.5331  Validation loss = 0.9711  \n",
      "\n",
      "Fold: 27  Epoch: 19  Training loss = 2.5051  Validation loss = 1.1818  \n",
      "\n",
      "Fold: 27  Epoch: 20  Training loss = 2.4803  Validation loss = 1.4027  \n",
      "\n",
      "Fold: 27  Epoch: 21  Training loss = 2.5384  Validation loss = 1.4325  \n",
      "\n",
      "Fold: 27  Epoch: 22  Training loss = 2.4625  Validation loss = 1.5533  \n",
      "\n",
      "Fold: 27  Epoch: 23  Training loss = 2.4512  Validation loss = 1.4980  \n",
      "\n",
      "Fold: 27  Epoch: 24  Training loss = 2.5858  Validation loss = 2.5475  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 4  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 2.4651  Validation loss = 2.0721  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 2.3600  Validation loss = 1.4193  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 2.3535  Validation loss = 1.6391  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 2.3402  Validation loss = 1.6204  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 2.5098  Validation loss = 1.1214  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 2.3841  Validation loss = 1.4116  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 2.3762  Validation loss = 1.5824  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 2.3257  Validation loss = 1.3827  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 2.3977  Validation loss = 1.2365  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 2.4634  Validation loss = 1.8893  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 2.3459  Validation loss = 1.4048  \n",
      "\n",
      "Fold: 28  Epoch: 12  Training loss = 2.3876  Validation loss = 1.6077  \n",
      "\n",
      "Fold: 28  Epoch: 13  Training loss = 2.4305  Validation loss = 1.0216  \n",
      "\n",
      "Fold: 28  Epoch: 14  Training loss = 2.4335  Validation loss = 1.1705  \n",
      "\n",
      "Fold: 28  Epoch: 15  Training loss = 2.3691  Validation loss = 1.1368  \n",
      "\n",
      "Fold: 28  Epoch: 16  Training loss = 2.3340  Validation loss = 1.2415  \n",
      "\n",
      "Fold: 28  Epoch: 17  Training loss = 2.4159  Validation loss = 1.0349  \n",
      "\n",
      "Fold: 28  Epoch: 18  Training loss = 2.3447  Validation loss = 1.3716  \n",
      "\n",
      "Fold: 28  Epoch: 19  Training loss = 2.3293  Validation loss = 1.1739  \n",
      "\n",
      "Fold: 28  Epoch: 20  Training loss = 2.4448  Validation loss = 0.7177  \n",
      "\n",
      "Fold: 28  Epoch: 21  Training loss = 2.3542  Validation loss = 1.4644  \n",
      "\n",
      "Fold: 28  Epoch: 22  Training loss = 2.3446  Validation loss = 1.4040  \n",
      "\n",
      "Fold: 28  Epoch: 23  Training loss = 2.3443  Validation loss = 1.4047  \n",
      "\n",
      "Fold: 28  Epoch: 24  Training loss = 2.3328  Validation loss = 1.0867  \n",
      "\n",
      "Fold: 28  Epoch: 25  Training loss = 2.3191  Validation loss = 0.9683  \n",
      "\n",
      "Fold: 28  Epoch: 26  Training loss = 2.3579  Validation loss = 0.9553  \n",
      "\n",
      "Fold: 28  Epoch: 27  Training loss = 2.3470  Validation loss = 0.9721  \n",
      "\n",
      "Fold: 28  Epoch: 28  Training loss = 2.4173  Validation loss = 1.3994  \n",
      "\n",
      "Fold: 28  Epoch: 29  Training loss = 2.3298  Validation loss = 1.2024  \n",
      "\n",
      "Fold: 28  Epoch: 30  Training loss = 2.4573  Validation loss = 0.6764  \n",
      "\n",
      "Fold: 28  Epoch: 31  Training loss = 2.3335  Validation loss = 0.8229  \n",
      "\n",
      "Fold: 28  Epoch: 32  Training loss = 2.3256  Validation loss = 0.7724  \n",
      "\n",
      "Fold: 28  Epoch: 33  Training loss = 2.3338  Validation loss = 1.2763  \n",
      "\n",
      "Fold: 28  Epoch: 34  Training loss = 2.5371  Validation loss = 0.6638  \n",
      "\n",
      "Fold: 28  Epoch: 35  Training loss = 2.3535  Validation loss = 0.9848  \n",
      "\n",
      "Fold: 28  Epoch: 36  Training loss = 2.5650  Validation loss = 0.6241  \n",
      "\n",
      "Fold: 28  Epoch: 37  Training loss = 2.4393  Validation loss = 1.3867  \n",
      "\n",
      "Fold: 28  Epoch: 38  Training loss = 2.6205  Validation loss = 0.7146  \n",
      "\n",
      "Fold: 28  Epoch: 39  Training loss = 2.3934  Validation loss = 0.8225  \n",
      "\n",
      "Fold: 28  Epoch: 40  Training loss = 2.3646  Validation loss = 0.9848  \n",
      "\n",
      "Fold: 28  Epoch: 41  Training loss = 2.3673  Validation loss = 0.8655  \n",
      "\n",
      "Fold: 28  Epoch: 42  Training loss = 2.3665  Validation loss = 0.7903  \n",
      "\n",
      "Fold: 28  Epoch: 43  Training loss = 2.3714  Validation loss = 1.0439  \n",
      "\n",
      "Fold: 28  Epoch: 44  Training loss = 2.3689  Validation loss = 0.9471  \n",
      "\n",
      "Fold: 28  Epoch: 45  Training loss = 2.3504  Validation loss = 0.8865  \n",
      "\n",
      "Fold: 28  Epoch: 46  Training loss = 2.5665  Validation loss = 1.4664  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 36  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 2.2697  Validation loss = 4.2155  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 2.2231  Validation loss = 4.1064  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 2.1579  Validation loss = 4.0392  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 2.1731  Validation loss = 3.9811  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 2.1862  Validation loss = 3.9378  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 2.1995  Validation loss = 4.4562  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 2.2998  Validation loss = 3.2663  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 2.1419  Validation loss = 4.0598  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 2.1765  Validation loss = 4.3347  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 2.2218  Validation loss = 4.4236  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 2.1751  Validation loss = 4.0774  \n",
      "\n",
      "Fold: 29  Epoch: 12  Training loss = 2.1880  Validation loss = 3.7173  \n",
      "\n",
      "Fold: 29  Epoch: 13  Training loss = 2.1905  Validation loss = 3.8783  \n",
      "\n",
      "Fold: 29  Epoch: 14  Training loss = 2.1315  Validation loss = 3.9824  \n",
      "\n",
      "Fold: 29  Epoch: 15  Training loss = 2.1845  Validation loss = 4.4297  \n",
      "\n",
      "Fold: 29  Epoch: 16  Training loss = 2.1449  Validation loss = 3.9434  \n",
      "\n",
      "Fold: 29  Epoch: 17  Training loss = 2.1478  Validation loss = 3.9804  \n",
      "\n",
      "Fold: 29  Epoch: 18  Training loss = 2.0775  Validation loss = 4.2046  \n",
      "\n",
      "Fold: 29  Epoch: 19  Training loss = 2.1378  Validation loss = 4.1826  \n",
      "\n",
      "Fold: 29  Epoch: 20  Training loss = 2.1489  Validation loss = 4.2219  \n",
      "\n",
      "Fold: 29  Epoch: 21  Training loss = 2.2516  Validation loss = 4.9671  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 7  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 2.3431  Validation loss = 1.6975  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 2.3014  Validation loss = 1.8546  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 2.3166  Validation loss = 1.7212  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 2.3386  Validation loss = 1.7062  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 3.3431  Validation loss = 3.0942  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 2.8830  Validation loss = 0.8065  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 2.8222  Validation loss = 0.9347  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 2.7503  Validation loss = 0.8247  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 2.7341  Validation loss = 2.0037  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 2.7262  Validation loss = 0.9604  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 2.5814  Validation loss = 1.6185  \n",
      "\n",
      "Fold: 30  Epoch: 12  Training loss = 2.7702  Validation loss = 2.4351  \n",
      "\n",
      "Fold: 30  Epoch: 13  Training loss = 2.5138  Validation loss = 1.0044  \n",
      "\n",
      "Fold: 30  Epoch: 14  Training loss = 2.4929  Validation loss = 1.0880  \n",
      "\n",
      "Fold: 30  Epoch: 15  Training loss = 2.5021  Validation loss = 0.9822  \n",
      "\n",
      "Fold: 30  Epoch: 16  Training loss = 2.6615  Validation loss = 0.9997  \n",
      "\n",
      "Fold: 30  Epoch: 17  Training loss = 2.5269  Validation loss = 1.4757  \n",
      "\n",
      "Fold: 30  Epoch: 18  Training loss = 2.5053  Validation loss = 0.9236  \n",
      "\n",
      "Fold: 30  Epoch: 19  Training loss = 2.6186  Validation loss = 0.5091  \n",
      "\n",
      "Fold: 30  Epoch: 20  Training loss = 2.4072  Validation loss = 0.6738  \n",
      "\n",
      "Fold: 30  Epoch: 21  Training loss = 2.5077  Validation loss = 1.6064  \n",
      "\n",
      "Fold: 30  Epoch: 22  Training loss = 2.2736  Validation loss = 1.1957  \n",
      "\n",
      "Fold: 30  Epoch: 23  Training loss = 2.2460  Validation loss = 1.4011  \n",
      "\n",
      "Fold: 30  Epoch: 24  Training loss = 2.2155  Validation loss = 1.4726  \n",
      "\n",
      "Fold: 30  Epoch: 25  Training loss = 2.2620  Validation loss = 1.2091  \n",
      "\n",
      "Fold: 30  Epoch: 26  Training loss = 2.5691  Validation loss = 1.3656  \n",
      "\n",
      "Fold: 30  Epoch: 27  Training loss = 2.4998  Validation loss = 1.1612  \n",
      "\n",
      "Fold: 30  Epoch: 28  Training loss = 2.2985  Validation loss = 0.9250  \n",
      "\n",
      "Fold: 30  Epoch: 29  Training loss = 2.2430  Validation loss = 0.9374  \n",
      "\n",
      "Fold: 30  Epoch: 30  Training loss = 2.1761  Validation loss = 0.8762  \n",
      "\n",
      "Fold: 30  Epoch: 31  Training loss = 2.2885  Validation loss = 1.0568  \n",
      "\n",
      "Fold: 30  Epoch: 32  Training loss = 2.1886  Validation loss = 0.8779  \n",
      "\n",
      "Fold: 30  Epoch: 33  Training loss = 2.2098  Validation loss = 1.1996  \n",
      "\n",
      "Fold: 30  Epoch: 34  Training loss = 2.2384  Validation loss = 0.8017  \n",
      "\n",
      "Fold: 30  Epoch: 35  Training loss = 2.2481  Validation loss = 1.0972  \n",
      "\n",
      "Fold: 30  Epoch: 36  Training loss = 2.3934  Validation loss = 0.8979  \n",
      "\n",
      "Fold: 30  Epoch: 37  Training loss = 2.2852  Validation loss = 0.9672  \n",
      "\n",
      "Fold: 30  Epoch: 38  Training loss = 2.1826  Validation loss = 1.0572  \n",
      "\n",
      "Fold: 30  Epoch: 39  Training loss = 2.1564  Validation loss = 0.7815  \n",
      "\n",
      "Fold: 30  Epoch: 40  Training loss = 2.2315  Validation loss = 0.5776  \n",
      "\n",
      "Fold: 30  Epoch: 41  Training loss = 2.2771  Validation loss = 1.0869  \n",
      "\n",
      "Fold: 30  Epoch: 42  Training loss = 2.2910  Validation loss = 0.8090  \n",
      "\n",
      "Fold: 30  Epoch: 43  Training loss = 2.1188  Validation loss = 1.1706  \n",
      "\n",
      "Fold: 30  Epoch: 44  Training loss = 2.2476  Validation loss = 1.0493  \n",
      "\n",
      "Fold: 30  Epoch: 45  Training loss = 2.4532  Validation loss = 1.2417  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 19  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 2.1184  Validation loss = 0.3149  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 2.1252  Validation loss = 0.5173  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 2.0706  Validation loss = 0.9258  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 2.0775  Validation loss = 1.3469  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 2.0433  Validation loss = 0.5747  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 1.9915  Validation loss = 1.3082  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 2.0065  Validation loss = 0.5616  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 2.0040  Validation loss = 1.1844  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 2.0076  Validation loss = 1.5187  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 2.0634  Validation loss = 1.7693  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 2.0935  Validation loss = 1.8650  \n",
      "\n",
      "Fold: 31  Epoch: 12  Training loss = 2.0079  Validation loss = 1.1882  \n",
      "\n",
      "Fold: 31  Epoch: 13  Training loss = 2.0020  Validation loss = 1.1180  \n",
      "\n",
      "Fold: 31  Epoch: 14  Training loss = 2.0028  Validation loss = 0.8360  \n",
      "\n",
      "Fold: 31  Epoch: 15  Training loss = 2.0092  Validation loss = 0.8403  \n",
      "\n",
      "Fold: 31  Epoch: 16  Training loss = 2.1639  Validation loss = 2.0496  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 1  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 2.2205  Validation loss = 6.4494  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 1.8551  Validation loss = 5.7490  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 2.0562  Validation loss = 6.3094  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 1.9268  Validation loss = 5.9496  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 1.8470  Validation loss = 5.6990  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 1.8185  Validation loss = 5.4191  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 1.8711  Validation loss = 5.7992  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 1.8626  Validation loss = 5.8066  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 1.8130  Validation loss = 5.0916  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 1.8240  Validation loss = 5.6065  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 1.8066  Validation loss = 4.9429  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 1.8527  Validation loss = 4.7638  \n",
      "\n",
      "Fold: 32  Epoch: 13  Training loss = 1.7623  Validation loss = 5.2094  \n",
      "\n",
      "Fold: 32  Epoch: 14  Training loss = 1.8559  Validation loss = 4.7242  \n",
      "\n",
      "Fold: 32  Epoch: 15  Training loss = 1.7875  Validation loss = 5.1877  \n",
      "\n",
      "Fold: 32  Epoch: 16  Training loss = 1.9612  Validation loss = 4.6037  \n",
      "\n",
      "Fold: 32  Epoch: 17  Training loss = 2.0116  Validation loss = 5.3916  \n",
      "\n",
      "Fold: 32  Epoch: 18  Training loss = 1.9378  Validation loss = 5.3257  \n",
      "\n",
      "Fold: 32  Epoch: 19  Training loss = 1.9019  Validation loss = 5.3799  \n",
      "\n",
      "Fold: 32  Epoch: 20  Training loss = 1.9521  Validation loss = 4.9112  \n",
      "\n",
      "Fold: 32  Epoch: 21  Training loss = 1.9235  Validation loss = 5.6230  \n",
      "\n",
      "Fold: 32  Epoch: 22  Training loss = 1.8389  Validation loss = 5.5535  \n",
      "\n",
      "Fold: 32  Epoch: 23  Training loss = 1.8459  Validation loss = 5.4990  \n",
      "\n",
      "Fold: 32  Epoch: 24  Training loss = 1.9405  Validation loss = 4.6866  \n",
      "\n",
      "Fold: 32  Epoch: 25  Training loss = 1.8110  Validation loss = 4.8900  \n",
      "\n",
      "Fold: 32  Epoch: 26  Training loss = 1.8287  Validation loss = 4.6154  \n",
      "\n",
      "Fold: 32  Epoch: 27  Training loss = 2.1089  Validation loss = 5.6123  \n",
      "\n",
      "Fold: 32  Epoch: 28  Training loss = 1.9280  Validation loss = 5.8140  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 16  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 21\n",
      "Average validation error: 3.443\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 2.0071  Test loss = 3.9716  \n",
      "\n",
      "Epoch: 2  Training loss = 1.8888  Test loss = 3.7323  \n",
      "\n",
      "Epoch: 3  Training loss = 1.8375  Test loss = 3.5919  \n",
      "\n",
      "Epoch: 4  Training loss = 1.8112  Test loss = 3.5045  \n",
      "\n",
      "Epoch: 5  Training loss = 1.7944  Test loss = 3.4465  \n",
      "\n",
      "Epoch: 6  Training loss = 1.7818  Test loss = 3.4054  \n",
      "\n",
      "Epoch: 7  Training loss = 1.7712  Test loss = 3.3744  \n",
      "\n",
      "Epoch: 8  Training loss = 1.7618  Test loss = 3.3497  \n",
      "\n",
      "Epoch: 9  Training loss = 1.7533  Test loss = 3.3290  \n",
      "\n",
      "Epoch: 10  Training loss = 1.7454  Test loss = 3.3110  \n",
      "\n",
      "Epoch: 11  Training loss = 1.7379  Test loss = 3.2948  \n",
      "\n",
      "Epoch: 12  Training loss = 1.7310  Test loss = 3.2798  \n",
      "\n",
      "Epoch: 13  Training loss = 1.7243  Test loss = 3.2658  \n",
      "\n",
      "Epoch: 14  Training loss = 1.7181  Test loss = 3.2526  \n",
      "\n",
      "Epoch: 15  Training loss = 1.7120  Test loss = 3.2401  \n",
      "\n",
      "Epoch: 16  Training loss = 1.7063  Test loss = 3.2280  \n",
      "\n",
      "Epoch: 17  Training loss = 1.7008  Test loss = 3.2165  \n",
      "\n",
      "Epoch: 18  Training loss = 1.6955  Test loss = 3.2053  \n",
      "\n",
      "Epoch: 19  Training loss = 1.6903  Test loss = 3.1946  \n",
      "\n",
      "Epoch: 20  Training loss = 1.6854  Test loss = 3.1841  \n",
      "\n",
      "Epoch: 21  Training loss = 1.6806  Test loss = 3.1741  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1427245ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f14241b70d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 3.1740562097 \n",
      "Fixed scheme MAE:  2.30966133447\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 1.6806  Test loss = 3.3096  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 1.6988  Test loss = 2.7054  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 1.7313  Test loss = 1.2588  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 1.7302  Test loss = 0.8600  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 1.4581  Test loss = 1.4460  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 1.4631  Test loss = 0.9862  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 1.4654  Test loss = 0.0935  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 1.4576  Test loss = 1.0262  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 1.2340  Test loss = 1.4441  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 1.2333  Test loss = 1.6659  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 1.2044  Test loss = 2.1997  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 1.2328  Test loss = 1.1866  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 1.1807  Test loss = 0.3215  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 1.1767  Test loss = 0.7225  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 1.1781  Test loss = 2.3989  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 1.2140  Test loss = 3.6764  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 1.2454  Test loss = 4.0894  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 1.3441  Test loss = 1.1725  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 1.3516  Test loss = 2.1762  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 1.2581  Test loss = 0.3566  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 1.1300  Test loss = 1.3683  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 1.1190  Test loss = 2.5058  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 1.1599  Test loss = 1.1737  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 1.1686  Test loss = 2.6605  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 1.1942  Test loss = 0.8360  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 1.1909  Test loss = 0.3712  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 1.1905  Test loss = 1.7360  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 1.2096  Test loss = 2.2396  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 1.1027  Test loss = 0.2379  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 1.1025  Test loss = 0.0846  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 1.1008  Test loss = 3.3833  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 1.1317  Test loss = 0.9669  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 1.0803  Test loss = 0.8048  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 1.0794  Test loss = 0.7633  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 1.0091  Test loss = 0.9341  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 1.0155  Test loss = 5.6597  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 1.1932  Test loss = 0.0335  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 1.1299  Test loss = 0.8824  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 1.1327  Test loss = 0.3822  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 1.1303  Test loss = 2.7750  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 1.1374  Test loss = 2.5824  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 1.1815  Test loss = 3.5235  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 1.2570  Test loss = 0.9071  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 1.2584  Test loss = 16.0518  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 2.1760  Test loss = 5.9021  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.2958  Test loss = 0.9741  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.2989  Test loss = 0.1429  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.2989  Test loss = 0.1392  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 1.9807  Test loss = 2.3116  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 1.9982  Test loss = 6.9539  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 2.1753  Test loss = 1.2919  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 2.1808  Test loss = 0.8194  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 1.9743  Test loss = 1.7721  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 1.9860  Test loss = 1.3721  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 1.9920  Test loss = 0.0135  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 1.9860  Test loss = 3.0118  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 2.0027  Test loss = 2.8257  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 2.0325  Test loss = 0.4314  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 2.0332  Test loss = 1.3376  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 2.0400  Test loss = 0.4231  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.9928  Test loss = 0.5869  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.9877  Test loss = 0.6776  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 1.9892  Test loss = 1.0020  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 1.9931  Test loss = 0.6070  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 1.9873  Test loss = 1.9117  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 2.0009  Test loss = 0.3642  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.9998  Test loss = 1.0973  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 2.0042  Test loss = 3.6732  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 2.0500  Test loss = 4.1348  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 2.1131  Test loss = 0.0419  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 2.1108  Test loss = 0.6283  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 2.1122  Test loss = 1.8249  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 2.0236  Test loss = 2.6088  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 2.0492  Test loss = 1.0765  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 2.0445  Test loss = 0.6728  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 2.0419  Test loss = 0.5612  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 1.9940  Test loss = 1.0418  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5l+nwFHHOZPGYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f14240b8050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.80150827834 \n",
      "Updating scheme MAE:  1.79471540423\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
