{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"1Q/1_cell/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 750\n",
    "learning_rate = 1e-2\n",
    "batch_size = 5\n",
    "early_stop_iters = 15\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 12 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    stacked_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell]*3, state_is_tuple = True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag1',\n",
    "                                       'inflation.lag2',\n",
    "                                       'inflation.lag3',\n",
    "                                       'inflation.lag4']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag1',\n",
    "                                   'unemp.lag2',\n",
    "                                   'unemp.lag3',\n",
    "                                   'unemp.lag4']])\n",
    "train_4lag_oil = np.array(train[['oil.lag1',\n",
    "                                 'oil.lag2',\n",
    "                                 'oil.lag3',\n",
    "                                 'oil.lag4']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag1',\n",
    "                                     'inflation.lag2',\n",
    "                                     'inflation.lag3',\n",
    "                                     'inflation.lag4']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag1',\n",
    "                                 'unemp.lag2',\n",
    "                                 'unemp.lag3',\n",
    "                                 'unemp.lag4']])\n",
    "test_4lag_oil = np.array(test[['oil.lag1',\n",
    "                               'oil.lag2',\n",
    "                               'oil.lag3',\n",
    "                               'oil.lag4']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 12 \n",
      "Learning rate = 0.01 \n",
      "Epochs = 750 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 15 \n",
      "Learning rate = 0.01\n",
      "Fold: 1  Epoch: 1  Training loss = 2.8937  Validation loss = 2.6100  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 2.7298  Validation loss = 1.9805  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 2.7104  Validation loss = 1.9130  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 2.6925  Validation loss = 1.8184  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 2.6680  Validation loss = 1.5493  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 2.6861  Validation loss = 1.1806  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 2.6688  Validation loss = 1.1498  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 2.6266  Validation loss = 1.7681  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 2.6125  Validation loss = 1.5153  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 2.6080  Validation loss = 1.8213  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 2.5996  Validation loss = 1.7257  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 2.5625  Validation loss = 1.5466  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 2.5576  Validation loss = 1.4894  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 2.5675  Validation loss = 1.5203  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 2.5666  Validation loss = 1.9585  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 2.5540  Validation loss = 1.9210  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 2.5400  Validation loss = 1.9647  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 7  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.3560  Validation loss = 1.9772  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.2986  Validation loss = 1.8928  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.2751  Validation loss = 1.8170  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.2837  Validation loss = 1.9492  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.1935  Validation loss = 1.7801  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.1956  Validation loss = 1.8087  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.1192  Validation loss = 2.2648  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.1177  Validation loss = 1.9144  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 1.9913  Validation loss = 1.8782  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 1.9568  Validation loss = 2.4141  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.0731  Validation loss = 2.0540  \n",
      "\n",
      "Fold: 2  Epoch: 12  Training loss = 2.0164  Validation loss = 2.8258  \n",
      "\n",
      "Fold: 2  Epoch: 13  Training loss = 1.9598  Validation loss = 2.5757  \n",
      "\n",
      "Fold: 2  Epoch: 14  Training loss = 1.9676  Validation loss = 2.7278  \n",
      "\n",
      "Fold: 2  Epoch: 15  Training loss = 1.8563  Validation loss = 2.2252  \n",
      "\n",
      "Fold: 2  Epoch: 16  Training loss = 1.8230  Validation loss = 2.4258  \n",
      "\n",
      "Fold: 2  Epoch: 17  Training loss = 1.8072  Validation loss = 2.3408  \n",
      "\n",
      "Fold: 2  Epoch: 18  Training loss = 1.7448  Validation loss = 2.6217  \n",
      "\n",
      "Fold: 2  Epoch: 19  Training loss = 1.8569  Validation loss = 2.5432  \n",
      "\n",
      "Fold: 2  Epoch: 20  Training loss = 1.7165  Validation loss = 2.7202  \n",
      "\n",
      "Fold: 2  Epoch: 21  Training loss = 1.7384  Validation loss = 2.5486  \n",
      "\n",
      "Fold: 2  Epoch: 22  Training loss = 1.6707  Validation loss = 2.9190  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 5  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.2741  Validation loss = 1.5610  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.2833  Validation loss = 1.2268  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.4193  Validation loss = 1.2017  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.2712  Validation loss = 1.2494  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.2424  Validation loss = 0.4813  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.2114  Validation loss = 0.6211  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.1877  Validation loss = 0.4563  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.1940  Validation loss = 0.5381  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.2168  Validation loss = 0.4501  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.1343  Validation loss = 0.5973  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.1491  Validation loss = 0.6244  \n",
      "\n",
      "Fold: 3  Epoch: 12  Training loss = 1.1086  Validation loss = 0.7828  \n",
      "\n",
      "Fold: 3  Epoch: 13  Training loss = 1.1161  Validation loss = 0.4346  \n",
      "\n",
      "Fold: 3  Epoch: 14  Training loss = 1.0638  Validation loss = 0.6552  \n",
      "\n",
      "Fold: 3  Epoch: 15  Training loss = 1.0625  Validation loss = 0.9471  \n",
      "\n",
      "Fold: 3  Epoch: 16  Training loss = 1.1505  Validation loss = 0.9515  \n",
      "\n",
      "Fold: 3  Epoch: 17  Training loss = 1.0650  Validation loss = 1.2437  \n",
      "\n",
      "Fold: 3  Epoch: 18  Training loss = 1.0541  Validation loss = 0.8553  \n",
      "\n",
      "Fold: 3  Epoch: 19  Training loss = 1.0654  Validation loss = 0.4005  \n",
      "\n",
      "Fold: 3  Epoch: 20  Training loss = 1.0603  Validation loss = 0.5746  \n",
      "\n",
      "Fold: 3  Epoch: 21  Training loss = 1.0628  Validation loss = 0.9161  \n",
      "\n",
      "Fold: 3  Epoch: 22  Training loss = 1.0707  Validation loss = 0.7057  \n",
      "\n",
      "Fold: 3  Epoch: 23  Training loss = 1.0697  Validation loss = 0.8945  \n",
      "\n",
      "Fold: 3  Epoch: 24  Training loss = 1.0356  Validation loss = 0.7473  \n",
      "\n",
      "Fold: 3  Epoch: 25  Training loss = 1.1387  Validation loss = 0.5897  \n",
      "\n",
      "Fold: 3  Epoch: 26  Training loss = 1.0233  Validation loss = 0.7966  \n",
      "\n",
      "Fold: 3  Epoch: 27  Training loss = 1.0476  Validation loss = 0.7083  \n",
      "\n",
      "Fold: 3  Epoch: 28  Training loss = 1.0725  Validation loss = 1.3089  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 19  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 0.9567  Validation loss = 1.5515  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 0.9638  Validation loss = 1.5327  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 0.9419  Validation loss = 1.4032  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 0.9445  Validation loss = 1.4579  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 0.9626  Validation loss = 1.3971  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.0864  Validation loss = 1.5449  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.0155  Validation loss = 1.4538  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 0.9453  Validation loss = 1.6766  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 0.9572  Validation loss = 1.6419  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 0.9866  Validation loss = 1.8412  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 0.9379  Validation loss = 1.6706  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 0.9168  Validation loss = 1.5351  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 0.9305  Validation loss = 1.6011  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 0.9276  Validation loss = 1.5793  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 0.9407  Validation loss = 1.7474  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 0.9174  Validation loss = 1.7786  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 0.9369  Validation loss = 1.6923  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 0.9733  Validation loss = 1.5514  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 0.9255  Validation loss = 1.5867  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 0.9276  Validation loss = 1.5591  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 0.9288  Validation loss = 1.8693  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 5  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 0.9487  Validation loss = 1.1998  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 0.9292  Validation loss = 1.1547  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 0.9162  Validation loss = 1.1581  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 0.8968  Validation loss = 1.1812  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 0.9533  Validation loss = 1.1747  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 0.9091  Validation loss = 1.1951  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 0.8873  Validation loss = 1.2123  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 0.8625  Validation loss = 1.1886  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 0.8735  Validation loss = 1.2250  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 0.8656  Validation loss = 1.2412  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 0.8477  Validation loss = 1.2492  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 0.8467  Validation loss = 1.2605  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 0.8805  Validation loss = 1.2830  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 0.8934  Validation loss = 1.3946  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 0.8341  Validation loss = 1.3987  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 0.8412  Validation loss = 1.4005  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 2  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 0.9026  Validation loss = 1.4690  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 0.8757  Validation loss = 1.4470  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 0.8774  Validation loss = 1.4829  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 0.8841  Validation loss = 1.4158  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 0.9264  Validation loss = 1.5772  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 0.8897  Validation loss = 1.6279  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 0.8819  Validation loss = 1.6262  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 0.8643  Validation loss = 1.4874  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 0.8417  Validation loss = 1.4292  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 0.8596  Validation loss = 1.3060  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 0.9031  Validation loss = 1.4302  \n",
      "\n",
      "Fold: 6  Epoch: 12  Training loss = 0.8411  Validation loss = 1.4805  \n",
      "\n",
      "Fold: 6  Epoch: 13  Training loss = 0.8776  Validation loss = 1.6283  \n",
      "\n",
      "Fold: 6  Epoch: 14  Training loss = 0.8328  Validation loss = 1.6370  \n",
      "\n",
      "Fold: 6  Epoch: 15  Training loss = 0.8333  Validation loss = 1.5450  \n",
      "\n",
      "Fold: 6  Epoch: 16  Training loss = 0.8864  Validation loss = 1.5671  \n",
      "\n",
      "Fold: 6  Epoch: 17  Training loss = 0.8646  Validation loss = 1.6741  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 10  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 0.9007  Validation loss = 1.1679  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 0.8703  Validation loss = 1.4479  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 0.8581  Validation loss = 1.5040  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 0.8829  Validation loss = 1.2510  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 0.8553  Validation loss = 1.3525  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 0.8652  Validation loss = 1.8029  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 0.8556  Validation loss = 1.6970  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 0.8677  Validation loss = 2.4352  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 0.8489  Validation loss = 1.2901  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 0.8391  Validation loss = 1.3535  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 0.9088  Validation loss = 1.9431  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 0.8390  Validation loss = 1.3738  \n",
      "\n",
      "Fold: 7  Epoch: 13  Training loss = 0.8581  Validation loss = 1.0843  \n",
      "\n",
      "Fold: 7  Epoch: 14  Training loss = 0.8490  Validation loss = 1.6036  \n",
      "\n",
      "Fold: 7  Epoch: 15  Training loss = 0.8452  Validation loss = 1.6398  \n",
      "\n",
      "Fold: 7  Epoch: 16  Training loss = 0.8470  Validation loss = 1.2868  \n",
      "\n",
      "Fold: 7  Epoch: 17  Training loss = 0.8483  Validation loss = 1.6852  \n",
      "\n",
      "Fold: 7  Epoch: 18  Training loss = 0.8755  Validation loss = 1.2773  \n",
      "\n",
      "Fold: 7  Epoch: 19  Training loss = 0.9329  Validation loss = 1.1781  \n",
      "\n",
      "Fold: 7  Epoch: 20  Training loss = 0.8533  Validation loss = 1.4030  \n",
      "\n",
      "Fold: 7  Epoch: 21  Training loss = 0.8562  Validation loss = 1.9801  \n",
      "\n",
      "Fold: 7  Epoch: 22  Training loss = 0.8279  Validation loss = 1.5608  \n",
      "\n",
      "Fold: 7  Epoch: 23  Training loss = 0.8254  Validation loss = 1.4689  \n",
      "\n",
      "Fold: 7  Epoch: 24  Training loss = 0.8292  Validation loss = 1.3330  \n",
      "\n",
      "Fold: 7  Epoch: 25  Training loss = 0.8357  Validation loss = 1.6756  \n",
      "\n",
      "Fold: 7  Epoch: 26  Training loss = 0.8819  Validation loss = 1.2615  \n",
      "\n",
      "Fold: 7  Epoch: 27  Training loss = 0.8257  Validation loss = 1.4942  \n",
      "\n",
      "Fold: 7  Epoch: 28  Training loss = 0.8454  Validation loss = 1.2876  \n",
      "\n",
      "Fold: 7  Epoch: 29  Training loss = 0.8410  Validation loss = 1.3644  \n",
      "\n",
      "Fold: 7  Epoch: 30  Training loss = 0.8256  Validation loss = 1.5151  \n",
      "\n",
      "Fold: 7  Epoch: 31  Training loss = 0.8747  Validation loss = 1.9789  \n",
      "\n",
      "Fold: 7  Epoch: 32  Training loss = 0.8350  Validation loss = 1.8389  \n",
      "\n",
      "Fold: 7  Epoch: 33  Training loss = 0.8697  Validation loss = 1.2697  \n",
      "\n",
      "Fold: 7  Epoch: 34  Training loss = 0.8333  Validation loss = 1.3321  \n",
      "\n",
      "Fold: 7  Epoch: 35  Training loss = 0.8274  Validation loss = 1.2685  \n",
      "\n",
      "Fold: 7  Epoch: 36  Training loss = 0.8313  Validation loss = 1.2369  \n",
      "\n",
      "Fold: 7  Epoch: 37  Training loss = 0.8436  Validation loss = 1.6466  \n",
      "\n",
      "Fold: 7  Epoch: 38  Training loss = 0.8263  Validation loss = 1.4014  \n",
      "\n",
      "Fold: 7  Epoch: 39  Training loss = 0.8194  Validation loss = 1.4467  \n",
      "\n",
      "Fold: 7  Epoch: 40  Training loss = 0.8241  Validation loss = 1.4150  \n",
      "\n",
      "Fold: 7  Epoch: 41  Training loss = 0.8500  Validation loss = 1.4984  \n",
      "\n",
      "Fold: 7  Epoch: 42  Training loss = 0.8437  Validation loss = 1.7927  \n",
      "\n",
      "Fold: 7  Epoch: 43  Training loss = 0.8042  Validation loss = 1.4072  \n",
      "\n",
      "Fold: 7  Epoch: 44  Training loss = 0.8446  Validation loss = 1.5280  \n",
      "\n",
      "Fold: 7  Epoch: 45  Training loss = 0.8374  Validation loss = 1.2382  \n",
      "\n",
      "Fold: 7  Epoch: 46  Training loss = 0.8296  Validation loss = 1.8681  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 13  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 0.7936  Validation loss = 3.9845  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 0.7743  Validation loss = 4.0609  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 0.7751  Validation loss = 4.2579  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 0.7869  Validation loss = 4.2851  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 0.7941  Validation loss = 4.4201  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 0.7991  Validation loss = 4.2893  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 0.7631  Validation loss = 4.1330  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 0.7393  Validation loss = 4.0138  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 0.7442  Validation loss = 3.9528  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 0.7578  Validation loss = 3.9776  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 0.7624  Validation loss = 4.1509  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 0.7514  Validation loss = 4.0760  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 0.7469  Validation loss = 4.0522  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 0.7567  Validation loss = 3.9071  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 0.7545  Validation loss = 3.9508  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 0.7738  Validation loss = 3.9509  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 0.7500  Validation loss = 4.0436  \n",
      "\n",
      "Fold: 8  Epoch: 18  Training loss = 0.7550  Validation loss = 3.9978  \n",
      "\n",
      "Fold: 8  Epoch: 19  Training loss = 0.7537  Validation loss = 3.9133  \n",
      "\n",
      "Fold: 8  Epoch: 20  Training loss = 0.7330  Validation loss = 4.1554  \n",
      "\n",
      "Fold: 8  Epoch: 21  Training loss = 0.8015  Validation loss = 3.8076  \n",
      "\n",
      "Fold: 8  Epoch: 22  Training loss = 0.7602  Validation loss = 4.1265  \n",
      "\n",
      "Fold: 8  Epoch: 23  Training loss = 0.7249  Validation loss = 3.9462  \n",
      "\n",
      "Fold: 8  Epoch: 24  Training loss = 0.7473  Validation loss = 4.0871  \n",
      "\n",
      "Fold: 8  Epoch: 25  Training loss = 0.7532  Validation loss = 4.0800  \n",
      "\n",
      "Fold: 8  Epoch: 26  Training loss = 0.7559  Validation loss = 4.1613  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 21  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 1.1969  Validation loss = 6.8229  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 1.1361  Validation loss = 6.8697  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 1.0940  Validation loss = 6.8668  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 1.1457  Validation loss = 7.1485  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 1.0838  Validation loss = 7.0372  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 1.1343  Validation loss = 7.0780  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 1.1210  Validation loss = 7.1405  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 1.0739  Validation loss = 6.9735  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 1.0505  Validation loss = 6.8532  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 1.0628  Validation loss = 6.8589  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 1.0881  Validation loss = 6.6569  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 1.0603  Validation loss = 6.7312  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 1.0349  Validation loss = 6.6659  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 1.0337  Validation loss = 6.7635  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 1.0051  Validation loss = 6.8224  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 0.9879  Validation loss = 6.7747  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 0.9765  Validation loss = 6.5960  \n",
      "\n",
      "Fold: 9  Epoch: 18  Training loss = 0.9872  Validation loss = 6.5791  \n",
      "\n",
      "Fold: 9  Epoch: 19  Training loss = 1.4339  Validation loss = 6.4789  \n",
      "\n",
      "Fold: 9  Epoch: 20  Training loss = 1.0482  Validation loss = 6.7302  \n",
      "\n",
      "Fold: 9  Epoch: 21  Training loss = 0.9927  Validation loss = 6.6751  \n",
      "\n",
      "Fold: 9  Epoch: 22  Training loss = 0.9509  Validation loss = 6.6326  \n",
      "\n",
      "Fold: 9  Epoch: 23  Training loss = 0.9347  Validation loss = 6.4183  \n",
      "\n",
      "Fold: 9  Epoch: 24  Training loss = 0.9093  Validation loss = 6.2796  \n",
      "\n",
      "Fold: 9  Epoch: 25  Training loss = 0.9425  Validation loss = 6.2535  \n",
      "\n",
      "Fold: 9  Epoch: 26  Training loss = 0.9678  Validation loss = 6.3750  \n",
      "\n",
      "Fold: 9  Epoch: 27  Training loss = 0.9080  Validation loss = 6.3521  \n",
      "\n",
      "Fold: 9  Epoch: 28  Training loss = 0.9170  Validation loss = 6.1921  \n",
      "\n",
      "Fold: 9  Epoch: 29  Training loss = 0.9047  Validation loss = 6.0523  \n",
      "\n",
      "Fold: 9  Epoch: 30  Training loss = 0.9814  Validation loss = 6.0285  \n",
      "\n",
      "Fold: 9  Epoch: 31  Training loss = 0.9926  Validation loss = 6.0727  \n",
      "\n",
      "Fold: 9  Epoch: 32  Training loss = 0.8821  Validation loss = 6.1633  \n",
      "\n",
      "Fold: 9  Epoch: 33  Training loss = 0.8919  Validation loss = 6.1016  \n",
      "\n",
      "Fold: 9  Epoch: 34  Training loss = 0.9270  Validation loss = 6.0425  \n",
      "\n",
      "Fold: 9  Epoch: 35  Training loss = 0.8411  Validation loss = 6.2403  \n",
      "\n",
      "Fold: 9  Epoch: 36  Training loss = 1.2894  Validation loss = 6.3105  \n",
      "\n",
      "Fold: 9  Epoch: 37  Training loss = 1.1172  Validation loss = 6.2055  \n",
      "\n",
      "Fold: 9  Epoch: 38  Training loss = 0.9659  Validation loss = 6.2014  \n",
      "\n",
      "Fold: 9  Epoch: 39  Training loss = 0.8530  Validation loss = 6.0941  \n",
      "\n",
      "Fold: 9  Epoch: 40  Training loss = 0.8650  Validation loss = 5.8744  \n",
      "\n",
      "Fold: 9  Epoch: 41  Training loss = 0.8193  Validation loss = 5.8987  \n",
      "\n",
      "Fold: 9  Epoch: 42  Training loss = 0.8745  Validation loss = 5.9768  \n",
      "\n",
      "Fold: 9  Epoch: 43  Training loss = 0.8519  Validation loss = 6.0943  \n",
      "\n",
      "Fold: 9  Epoch: 44  Training loss = 0.8495  Validation loss = 6.1073  \n",
      "\n",
      "Fold: 9  Epoch: 45  Training loss = 0.8528  Validation loss = 6.1174  \n",
      "\n",
      "Fold: 9  Epoch: 46  Training loss = 0.8167  Validation loss = 6.1493  \n",
      "\n",
      "Fold: 9  Epoch: 47  Training loss = 0.8292  Validation loss = 6.0781  \n",
      "\n",
      "Fold: 9  Epoch: 48  Training loss = 0.8277  Validation loss = 6.0613  \n",
      "\n",
      "Fold: 9  Epoch: 49  Training loss = 0.8295  Validation loss = 6.0675  \n",
      "\n",
      "Fold: 9  Epoch: 50  Training loss = 0.8103  Validation loss = 6.0284  \n",
      "\n",
      "Fold: 9  Epoch: 51  Training loss = 0.8089  Validation loss = 6.0860  \n",
      "\n",
      "Fold: 9  Epoch: 52  Training loss = 0.7822  Validation loss = 6.0003  \n",
      "\n",
      "Fold: 9  Epoch: 53  Training loss = 0.7930  Validation loss = 5.9832  \n",
      "\n",
      "Fold: 9  Epoch: 54  Training loss = 0.9904  Validation loss = 5.8606  \n",
      "\n",
      "Fold: 9  Epoch: 55  Training loss = 0.7908  Validation loss = 5.8875  \n",
      "\n",
      "Fold: 9  Epoch: 56  Training loss = 0.7916  Validation loss = 6.0421  \n",
      "\n",
      "Fold: 9  Epoch: 57  Training loss = 0.8249  Validation loss = 5.9055  \n",
      "\n",
      "Fold: 9  Epoch: 58  Training loss = 0.8311  Validation loss = 5.9917  \n",
      "\n",
      "Fold: 9  Epoch: 59  Training loss = 0.7860  Validation loss = 5.6284  \n",
      "\n",
      "Fold: 9  Epoch: 60  Training loss = 0.8032  Validation loss = 5.7177  \n",
      "\n",
      "Fold: 9  Epoch: 61  Training loss = 0.7747  Validation loss = 5.5985  \n",
      "\n",
      "Fold: 9  Epoch: 62  Training loss = 0.7776  Validation loss = 5.7105  \n",
      "\n",
      "Fold: 9  Epoch: 63  Training loss = 0.7849  Validation loss = 5.7569  \n",
      "\n",
      "Fold: 9  Epoch: 64  Training loss = 0.7577  Validation loss = 5.8998  \n",
      "\n",
      "Fold: 9  Epoch: 65  Training loss = 0.8840  Validation loss = 5.8482  \n",
      "\n",
      "Fold: 9  Epoch: 66  Training loss = 1.1626  Validation loss = 5.8933  \n",
      "\n",
      "Fold: 9  Epoch: 67  Training loss = 0.7970  Validation loss = 5.7248  \n",
      "\n",
      "Fold: 9  Epoch: 68  Training loss = 0.7697  Validation loss = 5.7273  \n",
      "\n",
      "Fold: 9  Epoch: 69  Training loss = 0.7693  Validation loss = 5.7319  \n",
      "\n",
      "Fold: 9  Epoch: 70  Training loss = 0.7307  Validation loss = 5.6634  \n",
      "\n",
      "Fold: 9  Epoch: 71  Training loss = 0.7637  Validation loss = 5.7538  \n",
      "\n",
      "Fold: 9  Epoch: 72  Training loss = 0.7304  Validation loss = 5.7291  \n",
      "\n",
      "Fold: 9  Epoch: 73  Training loss = 0.7363  Validation loss = 5.7047  \n",
      "\n",
      "Fold: 9  Epoch: 74  Training loss = 0.7365  Validation loss = 5.7473  \n",
      "\n",
      "Fold: 9  Epoch: 75  Training loss = 0.8684  Validation loss = 5.8464  \n",
      "\n",
      "Fold: 9  Epoch: 76  Training loss = 0.7943  Validation loss = 5.7209  \n",
      "\n",
      "Fold: 9  Epoch: 77  Training loss = 0.7666  Validation loss = 5.7376  \n",
      "\n",
      "Fold: 9  Epoch: 78  Training loss = 0.7049  Validation loss = 5.6759  \n",
      "\n",
      "Fold: 9  Epoch: 79  Training loss = 0.7964  Validation loss = 5.7444  \n",
      "\n",
      "Fold: 9  Epoch: 80  Training loss = 0.7412  Validation loss = 5.7960  \n",
      "\n",
      "Fold: 9  Epoch: 81  Training loss = 0.8546  Validation loss = 5.7827  \n",
      "\n",
      "Fold: 9  Epoch: 82  Training loss = 0.8363  Validation loss = 5.8722  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 61  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 1.5113  Validation loss = 3.9998  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 1.4747  Validation loss = 4.1129  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 1.4202  Validation loss = 5.2447  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 1.3995  Validation loss = 4.7749  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 1.3462  Validation loss = 5.4618  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 1.3781  Validation loss = 5.1140  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 1.3332  Validation loss = 3.6414  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 1.4898  Validation loss = 2.3069  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 1.1700  Validation loss = 3.3860  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 1.4163  Validation loss = 2.8568  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 1.1261  Validation loss = 5.1856  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 1.2169  Validation loss = 2.6641  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 1.2762  Validation loss = 3.0056  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 1.1980  Validation loss = 2.7199  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 1.3053  Validation loss = 3.4366  \n",
      "\n",
      "Fold: 10  Epoch: 16  Training loss = 1.0296  Validation loss = 3.4457  \n",
      "\n",
      "Fold: 10  Epoch: 17  Training loss = 1.3791  Validation loss = 2.3134  \n",
      "\n",
      "Fold: 10  Epoch: 18  Training loss = 1.2564  Validation loss = 4.7371  \n",
      "\n",
      "Fold: 10  Epoch: 19  Training loss = 1.2018  Validation loss = 2.9811  \n",
      "\n",
      "Fold: 10  Epoch: 20  Training loss = 0.9934  Validation loss = 2.8822  \n",
      "\n",
      "Fold: 10  Epoch: 21  Training loss = 0.9733  Validation loss = 2.4448  \n",
      "\n",
      "Fold: 10  Epoch: 22  Training loss = 0.8792  Validation loss = 3.0808  \n",
      "\n",
      "Fold: 10  Epoch: 23  Training loss = 1.1502  Validation loss = 2.0602  \n",
      "\n",
      "Fold: 10  Epoch: 24  Training loss = 0.8819  Validation loss = 3.0102  \n",
      "\n",
      "Fold: 10  Epoch: 25  Training loss = 0.9748  Validation loss = 3.0978  \n",
      "\n",
      "Fold: 10  Epoch: 26  Training loss = 0.9366  Validation loss = 2.2449  \n",
      "\n",
      "Fold: 10  Epoch: 27  Training loss = 0.9395  Validation loss = 3.6187  \n",
      "\n",
      "Fold: 10  Epoch: 28  Training loss = 0.8063  Validation loss = 2.9251  \n",
      "\n",
      "Fold: 10  Epoch: 29  Training loss = 0.8499  Validation loss = 2.4876  \n",
      "\n",
      "Fold: 10  Epoch: 30  Training loss = 0.7718  Validation loss = 3.0723  \n",
      "\n",
      "Fold: 10  Epoch: 31  Training loss = 0.7997  Validation loss = 2.5048  \n",
      "\n",
      "Fold: 10  Epoch: 32  Training loss = 0.9774  Validation loss = 3.4398  \n",
      "\n",
      "Fold: 10  Epoch: 33  Training loss = 0.8522  Validation loss = 2.6285  \n",
      "\n",
      "Fold: 10  Epoch: 34  Training loss = 0.8133  Validation loss = 2.5807  \n",
      "\n",
      "Fold: 10  Epoch: 35  Training loss = 0.8684  Validation loss = 2.2207  \n",
      "\n",
      "Fold: 10  Epoch: 36  Training loss = 1.0701  Validation loss = 2.3197  \n",
      "\n",
      "Fold: 10  Epoch: 37  Training loss = 0.7764  Validation loss = 3.5188  \n",
      "\n",
      "Fold: 10  Epoch: 38  Training loss = 0.8668  Validation loss = 2.2477  \n",
      "\n",
      "Fold: 10  Epoch: 39  Training loss = 1.0164  Validation loss = 2.7546  \n",
      "\n",
      "Fold: 10  Epoch: 40  Training loss = 0.7836  Validation loss = 3.2698  \n",
      "\n",
      "Fold: 10  Epoch: 41  Training loss = 0.7842  Validation loss = 2.6230  \n",
      "\n",
      "Fold: 10  Epoch: 42  Training loss = 0.8732  Validation loss = 2.6666  \n",
      "\n",
      "Fold: 10  Epoch: 43  Training loss = 0.8350  Validation loss = 2.7330  \n",
      "\n",
      "Fold: 10  Epoch: 44  Training loss = 0.8278  Validation loss = 2.9189  \n",
      "\n",
      "Fold: 10  Epoch: 45  Training loss = 0.7689  Validation loss = 2.6895  \n",
      "\n",
      "Fold: 10  Epoch: 46  Training loss = 0.7594  Validation loss = 3.3125  \n",
      "\n",
      "Fold: 10  Epoch: 47  Training loss = 0.7535  Validation loss = 3.2528  \n",
      "\n",
      "Fold: 10  Epoch: 48  Training loss = 0.8715  Validation loss = 2.2820  \n",
      "\n",
      "Fold: 10  Epoch: 49  Training loss = 0.7342  Validation loss = 2.5335  \n",
      "\n",
      "Fold: 10  Epoch: 50  Training loss = 0.7572  Validation loss = 2.3735  \n",
      "\n",
      "Fold: 10  Epoch: 51  Training loss = 0.7590  Validation loss = 2.3429  \n",
      "\n",
      "Fold: 10  Epoch: 52  Training loss = 0.7255  Validation loss = 2.6237  \n",
      "\n",
      "Fold: 10  Epoch: 53  Training loss = 0.8213  Validation loss = 3.1092  \n",
      "\n",
      "Fold: 10  Epoch: 54  Training loss = 0.8501  Validation loss = 2.8235  \n",
      "\n",
      "Fold: 10  Epoch: 55  Training loss = 0.7158  Validation loss = 2.6431  \n",
      "\n",
      "Fold: 10  Epoch: 56  Training loss = 0.7454  Validation loss = 2.7290  \n",
      "\n",
      "Fold: 10  Epoch: 57  Training loss = 0.7145  Validation loss = 2.5481  \n",
      "\n",
      "Fold: 10  Epoch: 58  Training loss = 0.6652  Validation loss = 2.7093  \n",
      "\n",
      "Fold: 10  Epoch: 59  Training loss = 0.6460  Validation loss = 2.7458  \n",
      "\n",
      "Fold: 10  Epoch: 60  Training loss = 0.6881  Validation loss = 2.8525  \n",
      "\n",
      "Fold: 10  Epoch: 61  Training loss = 0.6953  Validation loss = 3.2346  \n",
      "\n",
      "Fold: 10  Epoch: 62  Training loss = 0.8341  Validation loss = 2.6246  \n",
      "\n",
      "Fold: 10  Epoch: 63  Training loss = 0.6669  Validation loss = 2.6502  \n",
      "\n",
      "Fold: 10  Epoch: 64  Training loss = 0.7146  Validation loss = 2.8300  \n",
      "\n",
      "Fold: 10  Epoch: 65  Training loss = 0.7472  Validation loss = 3.6930  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 23  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 1.0084  Validation loss = 2.9760  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 0.9221  Validation loss = 3.0191  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 1.2357  Validation loss = 3.3667  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 1.0040  Validation loss = 4.1896  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 0.9574  Validation loss = 3.4495  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 0.9344  Validation loss = 3.7330  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 0.8871  Validation loss = 3.4196  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 0.8523  Validation loss = 2.3753  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 0.8313  Validation loss = 2.6389  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 0.8305  Validation loss = 3.2153  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 0.8433  Validation loss = 2.8438  \n",
      "\n",
      "Fold: 11  Epoch: 12  Training loss = 0.9580  Validation loss = 4.9821  \n",
      "\n",
      "Fold: 11  Epoch: 13  Training loss = 0.8742  Validation loss = 3.3532  \n",
      "\n",
      "Fold: 11  Epoch: 14  Training loss = 1.0038  Validation loss = 3.9666  \n",
      "\n",
      "Fold: 11  Epoch: 15  Training loss = 0.8180  Validation loss = 2.6038  \n",
      "\n",
      "Fold: 11  Epoch: 16  Training loss = 0.8412  Validation loss = 2.3104  \n",
      "\n",
      "Fold: 11  Epoch: 17  Training loss = 0.8560  Validation loss = 3.2760  \n",
      "\n",
      "Fold: 11  Epoch: 18  Training loss = 0.7939  Validation loss = 2.6274  \n",
      "\n",
      "Fold: 11  Epoch: 19  Training loss = 0.8324  Validation loss = 2.2124  \n",
      "\n",
      "Fold: 11  Epoch: 20  Training loss = 0.7831  Validation loss = 2.5663  \n",
      "\n",
      "Fold: 11  Epoch: 21  Training loss = 0.9566  Validation loss = 4.7707  \n",
      "\n",
      "Fold: 11  Epoch: 22  Training loss = 0.7932  Validation loss = 3.2331  \n",
      "\n",
      "Fold: 11  Epoch: 23  Training loss = 0.8227  Validation loss = 3.7724  \n",
      "\n",
      "Fold: 11  Epoch: 24  Training loss = 0.7852  Validation loss = 2.6132  \n",
      "\n",
      "Fold: 11  Epoch: 25  Training loss = 0.7806  Validation loss = 3.1453  \n",
      "\n",
      "Fold: 11  Epoch: 26  Training loss = 0.8949  Validation loss = 2.2902  \n",
      "\n",
      "Fold: 11  Epoch: 27  Training loss = 0.9138  Validation loss = 4.3116  \n",
      "\n",
      "Fold: 11  Epoch: 28  Training loss = 0.7670  Validation loss = 2.4884  \n",
      "\n",
      "Fold: 11  Epoch: 29  Training loss = 0.9532  Validation loss = 4.2765  \n",
      "\n",
      "Fold: 11  Epoch: 30  Training loss = 0.7656  Validation loss = 2.8491  \n",
      "\n",
      "Fold: 11  Epoch: 31  Training loss = 0.7892  Validation loss = 3.4090  \n",
      "\n",
      "Fold: 11  Epoch: 32  Training loss = 0.7858  Validation loss = 3.2591  \n",
      "\n",
      "Fold: 11  Epoch: 33  Training loss = 0.7831  Validation loss = 3.1325  \n",
      "\n",
      "Fold: 11  Epoch: 34  Training loss = 0.7699  Validation loss = 2.5448  \n",
      "\n",
      "Fold: 11  Epoch: 35  Training loss = 0.7813  Validation loss = 2.9001  \n",
      "\n",
      "Fold: 11  Epoch: 36  Training loss = 0.7403  Validation loss = 2.4766  \n",
      "\n",
      "Fold: 11  Epoch: 37  Training loss = 0.7428  Validation loss = 3.0365  \n",
      "\n",
      "Fold: 11  Epoch: 38  Training loss = 0.7641  Validation loss = 3.3418  \n",
      "\n",
      "Fold: 11  Epoch: 39  Training loss = 0.7496  Validation loss = 3.0648  \n",
      "\n",
      "Fold: 11  Epoch: 40  Training loss = 0.7152  Validation loss = 3.1118  \n",
      "\n",
      "Fold: 11  Epoch: 41  Training loss = 0.7861  Validation loss = 2.9905  \n",
      "\n",
      "Fold: 11  Epoch: 42  Training loss = 0.7854  Validation loss = 4.1158  \n",
      "\n",
      "Fold: 11  Epoch: 43  Training loss = 0.8426  Validation loss = 2.7294  \n",
      "\n",
      "Fold: 11  Epoch: 44  Training loss = 0.7566  Validation loss = 3.0642  \n",
      "\n",
      "Fold: 11  Epoch: 45  Training loss = 0.7442  Validation loss = 2.9093  \n",
      "\n",
      "Fold: 11  Epoch: 46  Training loss = 0.7167  Validation loss = 3.1646  \n",
      "\n",
      "Fold: 11  Epoch: 47  Training loss = 0.7720  Validation loss = 3.5682  \n",
      "\n",
      "Fold: 11  Epoch: 48  Training loss = 0.7340  Validation loss = 3.5278  \n",
      "\n",
      "Fold: 11  Epoch: 49  Training loss = 0.7421  Validation loss = 3.1699  \n",
      "\n",
      "Fold: 11  Epoch: 50  Training loss = 0.7432  Validation loss = 2.3346  \n",
      "\n",
      "Fold: 11  Epoch: 51  Training loss = 0.7654  Validation loss = 2.3989  \n",
      "\n",
      "Fold: 11  Epoch: 52  Training loss = 0.7362  Validation loss = 3.1370  \n",
      "\n",
      "Fold: 11  Epoch: 53  Training loss = 0.7742  Validation loss = 3.4310  \n",
      "\n",
      "Fold: 11  Epoch: 54  Training loss = 0.7064  Validation loss = 2.4369  \n",
      "\n",
      "Fold: 11  Epoch: 55  Training loss = 0.7676  Validation loss = 2.9671  \n",
      "\n",
      "Fold: 11  Epoch: 56  Training loss = 0.7271  Validation loss = 3.4558  \n",
      "\n",
      "Fold: 11  Epoch: 57  Training loss = 0.7661  Validation loss = 3.2398  \n",
      "\n",
      "Fold: 11  Epoch: 58  Training loss = 0.7642  Validation loss = 3.0189  \n",
      "\n",
      "Fold: 11  Epoch: 59  Training loss = 0.7154  Validation loss = 3.2445  \n",
      "\n",
      "Fold: 11  Epoch: 60  Training loss = 0.8496  Validation loss = 3.9529  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 19  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 1.6922  Validation loss = 1.0521  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 1.6171  Validation loss = 0.9020  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 1.5166  Validation loss = 1.3954  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 1.5305  Validation loss = 1.5416  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 1.3592  Validation loss = 1.0710  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 1.3349  Validation loss = 1.9576  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 1.5037  Validation loss = 2.9890  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 1.2626  Validation loss = 1.5826  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 2.1276  Validation loss = 3.0598  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 1.2615  Validation loss = 0.8967  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 1.4279  Validation loss = 0.3845  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 1.3196  Validation loss = 1.9700  \n",
      "\n",
      "Fold: 12  Epoch: 13  Training loss = 1.2482  Validation loss = 0.6594  \n",
      "\n",
      "Fold: 12  Epoch: 14  Training loss = 1.2008  Validation loss = 0.5995  \n",
      "\n",
      "Fold: 12  Epoch: 15  Training loss = 1.2338  Validation loss = 0.6932  \n",
      "\n",
      "Fold: 12  Epoch: 16  Training loss = 1.0874  Validation loss = 0.6414  \n",
      "\n",
      "Fold: 12  Epoch: 17  Training loss = 1.2533  Validation loss = 0.7388  \n",
      "\n",
      "Fold: 12  Epoch: 18  Training loss = 1.2594  Validation loss = 0.7638  \n",
      "\n",
      "Fold: 12  Epoch: 19  Training loss = 1.2043  Validation loss = 1.0056  \n",
      "\n",
      "Fold: 12  Epoch: 20  Training loss = 1.3062  Validation loss = 1.4702  \n",
      "\n",
      "Fold: 12  Epoch: 21  Training loss = 1.1991  Validation loss = 2.0033  \n",
      "\n",
      "Fold: 12  Epoch: 22  Training loss = 1.1775  Validation loss = 1.2322  \n",
      "\n",
      "Fold: 12  Epoch: 23  Training loss = 1.1019  Validation loss = 0.8269  \n",
      "\n",
      "Fold: 12  Epoch: 24  Training loss = 1.3314  Validation loss = 0.7141  \n",
      "\n",
      "Fold: 12  Epoch: 25  Training loss = 1.0959  Validation loss = 1.1633  \n",
      "\n",
      "Fold: 12  Epoch: 26  Training loss = 1.1583  Validation loss = 0.6579  \n",
      "\n",
      "Fold: 12  Epoch: 27  Training loss = 1.2942  Validation loss = 0.8781  \n",
      "\n",
      "Fold: 12  Epoch: 28  Training loss = 1.2249  Validation loss = 1.5441  \n",
      "\n",
      "Fold: 12  Epoch: 29  Training loss = 1.0605  Validation loss = 0.7391  \n",
      "\n",
      "Fold: 12  Epoch: 30  Training loss = 1.0352  Validation loss = 0.7921  \n",
      "\n",
      "Fold: 12  Epoch: 31  Training loss = 1.1173  Validation loss = 0.7055  \n",
      "\n",
      "Fold: 12  Epoch: 32  Training loss = 1.0596  Validation loss = 0.7596  \n",
      "\n",
      "Fold: 12  Epoch: 33  Training loss = 1.1178  Validation loss = 0.5090  \n",
      "\n",
      "Fold: 12  Epoch: 34  Training loss = 1.0626  Validation loss = 0.4903  \n",
      "\n",
      "Fold: 12  Epoch: 35  Training loss = 1.0034  Validation loss = 0.6346  \n",
      "\n",
      "Fold: 12  Epoch: 36  Training loss = 1.2688  Validation loss = 0.8682  \n",
      "\n",
      "Fold: 12  Epoch: 37  Training loss = 1.0319  Validation loss = 0.6544  \n",
      "\n",
      "Fold: 12  Epoch: 38  Training loss = 1.1335  Validation loss = 0.7021  \n",
      "\n",
      "Fold: 12  Epoch: 39  Training loss = 1.1116  Validation loss = 1.0384  \n",
      "\n",
      "Fold: 12  Epoch: 40  Training loss = 1.0619  Validation loss = 1.1726  \n",
      "\n",
      "Fold: 12  Epoch: 41  Training loss = 1.0010  Validation loss = 0.5359  \n",
      "\n",
      "Fold: 12  Epoch: 42  Training loss = 1.0322  Validation loss = 0.4987  \n",
      "\n",
      "Fold: 12  Epoch: 43  Training loss = 1.0161  Validation loss = 0.6282  \n",
      "\n",
      "Fold: 12  Epoch: 44  Training loss = 1.1372  Validation loss = 0.2702  \n",
      "\n",
      "Fold: 12  Epoch: 45  Training loss = 1.0548  Validation loss = 0.3516  \n",
      "\n",
      "Fold: 12  Epoch: 46  Training loss = 1.0296  Validation loss = 0.3681  \n",
      "\n",
      "Fold: 12  Epoch: 47  Training loss = 1.0125  Validation loss = 0.4900  \n",
      "\n",
      "Fold: 12  Epoch: 48  Training loss = 1.1984  Validation loss = 0.8482  \n",
      "\n",
      "Fold: 12  Epoch: 49  Training loss = 1.0430  Validation loss = 1.1765  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 44  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 1.0084  Validation loss = 3.3330  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 0.9476  Validation loss = 3.1285  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 0.9664  Validation loss = 3.3451  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 0.9612  Validation loss = 3.3156  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 1.0008  Validation loss = 3.4353  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 0.9324  Validation loss = 3.0496  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 0.9578  Validation loss = 3.1864  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 0.9203  Validation loss = 3.1743  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 0.9157  Validation loss = 3.2053  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 0.9039  Validation loss = 3.0643  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 0.9599  Validation loss = 3.1893  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 0.8864  Validation loss = 2.7311  \n",
      "\n",
      "Fold: 13  Epoch: 13  Training loss = 0.9051  Validation loss = 3.1237  \n",
      "\n",
      "Fold: 13  Epoch: 14  Training loss = 0.8743  Validation loss = 2.7345  \n",
      "\n",
      "Fold: 13  Epoch: 15  Training loss = 0.9715  Validation loss = 2.9729  \n",
      "\n",
      "Fold: 13  Epoch: 16  Training loss = 0.9435  Validation loss = 2.7562  \n",
      "\n",
      "Fold: 13  Epoch: 17  Training loss = 0.8726  Validation loss = 2.8784  \n",
      "\n",
      "Fold: 13  Epoch: 18  Training loss = 0.8228  Validation loss = 2.8610  \n",
      "\n",
      "Fold: 13  Epoch: 19  Training loss = 0.9176  Validation loss = 2.4958  \n",
      "\n",
      "Fold: 13  Epoch: 20  Training loss = 0.8833  Validation loss = 3.1825  \n",
      "\n",
      "Fold: 13  Epoch: 21  Training loss = 0.9493  Validation loss = 2.5941  \n",
      "\n",
      "Fold: 13  Epoch: 22  Training loss = 0.8339  Validation loss = 2.8083  \n",
      "\n",
      "Fold: 13  Epoch: 23  Training loss = 0.8787  Validation loss = 2.9796  \n",
      "\n",
      "Fold: 13  Epoch: 24  Training loss = 0.9262  Validation loss = 3.1389  \n",
      "\n",
      "Fold: 13  Epoch: 25  Training loss = 0.8417  Validation loss = 2.8480  \n",
      "\n",
      "Fold: 13  Epoch: 26  Training loss = 0.8312  Validation loss = 2.9487  \n",
      "\n",
      "Fold: 13  Epoch: 27  Training loss = 0.8549  Validation loss = 3.2131  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 19  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 1.4809  Validation loss = 3.5209  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 1.3333  Validation loss = 3.7623  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 1.1499  Validation loss = 2.6094  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 1.0909  Validation loss = 2.5358  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 1.0938  Validation loss = 3.0422  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 1.0705  Validation loss = 3.3582  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 1.1931  Validation loss = 3.9633  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 1.0395  Validation loss = 2.5151  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 1.0417  Validation loss = 2.3401  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 1.1374  Validation loss = 2.9454  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 0.9798  Validation loss = 3.3562  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 1.0435  Validation loss = 2.0522  \n",
      "\n",
      "Fold: 14  Epoch: 13  Training loss = 0.9836  Validation loss = 3.4016  \n",
      "\n",
      "Fold: 14  Epoch: 14  Training loss = 0.9846  Validation loss = 2.2667  \n",
      "\n",
      "Fold: 14  Epoch: 15  Training loss = 0.9020  Validation loss = 2.3854  \n",
      "\n",
      "Fold: 14  Epoch: 16  Training loss = 0.9521  Validation loss = 2.4258  \n",
      "\n",
      "Fold: 14  Epoch: 17  Training loss = 0.9046  Validation loss = 2.9771  \n",
      "\n",
      "Fold: 14  Epoch: 18  Training loss = 0.8635  Validation loss = 2.1329  \n",
      "\n",
      "Fold: 14  Epoch: 19  Training loss = 1.0689  Validation loss = 2.3148  \n",
      "\n",
      "Fold: 14  Epoch: 20  Training loss = 0.9273  Validation loss = 2.9453  \n",
      "\n",
      "Fold: 14  Epoch: 21  Training loss = 0.8378  Validation loss = 2.4626  \n",
      "\n",
      "Fold: 14  Epoch: 22  Training loss = 0.8539  Validation loss = 2.6284  \n",
      "\n",
      "Fold: 14  Epoch: 23  Training loss = 1.0443  Validation loss = 2.7853  \n",
      "\n",
      "Fold: 14  Epoch: 24  Training loss = 0.8283  Validation loss = 2.5096  \n",
      "\n",
      "Fold: 14  Epoch: 25  Training loss = 0.9751  Validation loss = 3.0738  \n",
      "\n",
      "Fold: 14  Epoch: 26  Training loss = 0.8586  Validation loss = 2.7628  \n",
      "\n",
      "Fold: 14  Epoch: 27  Training loss = 1.1699  Validation loss = 3.9871  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 12  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 1.1751  Validation loss = 5.1090  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 1.2143  Validation loss = 5.2809  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 1.2122  Validation loss = 5.3881  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 1.5663  Validation loss = 5.5832  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 1.2175  Validation loss = 5.4652  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 1.3937  Validation loss = 5.4165  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 1.1445  Validation loss = 5.2163  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 1.3651  Validation loss = 5.3791  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 1.0484  Validation loss = 5.3456  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 1.9924  Validation loss = 5.3289  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 1.4916  Validation loss = 5.4348  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 1.1098  Validation loss = 5.2423  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 1.0415  Validation loss = 5.3785  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 1.2040  Validation loss = 5.2604  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 1.1346  Validation loss = 5.2389  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 1.1593  Validation loss = 5.4123  \n",
      "\n",
      "Fold: 15  Epoch: 17  Training loss = 1.0148  Validation loss = 5.3015  \n",
      "\n",
      "Fold: 15  Epoch: 18  Training loss = 0.9815  Validation loss = 5.3929  \n",
      "\n",
      "Fold: 15  Epoch: 19  Training loss = 1.0794  Validation loss = 5.2380  \n",
      "\n",
      "Fold: 15  Epoch: 20  Training loss = 0.9356  Validation loss = 5.2971  \n",
      "\n",
      "Fold: 15  Epoch: 21  Training loss = 1.0323  Validation loss = 5.4278  \n",
      "\n",
      "Fold: 15  Epoch: 22  Training loss = 1.0050  Validation loss = 5.3776  \n",
      "\n",
      "Fold: 15  Epoch: 23  Training loss = 0.9553  Validation loss = 5.2915  \n",
      "\n",
      "Fold: 15  Epoch: 24  Training loss = 0.9755  Validation loss = 5.5914  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 1  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 1.7062  Validation loss = 4.3342  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 1.5230  Validation loss = 4.0156  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 1.6169  Validation loss = 4.2891  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 1.4285  Validation loss = 3.9877  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 1.5364  Validation loss = 3.8496  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 1.4950  Validation loss = 3.9309  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 1.3714  Validation loss = 3.6078  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 1.5082  Validation loss = 3.8393  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 1.3445  Validation loss = 4.0197  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 1.4973  Validation loss = 3.9199  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 1.5041  Validation loss = 3.2640  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 1.4823  Validation loss = 3.5502  \n",
      "\n",
      "Fold: 16  Epoch: 13  Training loss = 1.4636  Validation loss = 3.5291  \n",
      "\n",
      "Fold: 16  Epoch: 14  Training loss = 1.4853  Validation loss = 3.6378  \n",
      "\n",
      "Fold: 16  Epoch: 15  Training loss = 1.5587  Validation loss = 3.6757  \n",
      "\n",
      "Fold: 16  Epoch: 16  Training loss = 1.4098  Validation loss = 3.6070  \n",
      "\n",
      "Fold: 16  Epoch: 17  Training loss = 1.4240  Validation loss = 3.7036  \n",
      "\n",
      "Fold: 16  Epoch: 18  Training loss = 1.5404  Validation loss = 3.5561  \n",
      "\n",
      "Fold: 16  Epoch: 19  Training loss = 1.4077  Validation loss = 3.7229  \n",
      "\n",
      "Fold: 16  Epoch: 20  Training loss = 1.4286  Validation loss = 3.7754  \n",
      "\n",
      "Fold: 16  Epoch: 21  Training loss = 1.5259  Validation loss = 3.8724  \n",
      "\n",
      "Fold: 16  Epoch: 22  Training loss = 1.4255  Validation loss = 3.8853  \n",
      "\n",
      "Fold: 16  Epoch: 23  Training loss = 1.5033  Validation loss = 4.0153  \n",
      "\n",
      "Fold: 16  Epoch: 24  Training loss = 1.5106  Validation loss = 3.9916  \n",
      "\n",
      "Fold: 16  Epoch: 25  Training loss = 1.3800  Validation loss = 3.8993  \n",
      "\n",
      "Fold: 16  Epoch: 26  Training loss = 1.3848  Validation loss = 3.8637  \n",
      "\n",
      "Fold: 16  Epoch: 27  Training loss = 1.3313  Validation loss = 3.8152  \n",
      "\n",
      "Fold: 16  Epoch: 28  Training loss = 1.4907  Validation loss = 4.1047  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 11  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 1.7964  Validation loss = 5.8560  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 1.5183  Validation loss = 6.1836  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 1.5039  Validation loss = 6.5135  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 1.5550  Validation loss = 6.1214  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 1.6253  Validation loss = 6.1816  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 1.5066  Validation loss = 6.1119  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 1.5984  Validation loss = 5.3259  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 1.4852  Validation loss = 5.7839  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 1.5154  Validation loss = 5.6428  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 1.4474  Validation loss = 5.8281  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 1.4320  Validation loss = 5.5587  \n",
      "\n",
      "Fold: 17  Epoch: 12  Training loss = 1.4500  Validation loss = 5.7762  \n",
      "\n",
      "Fold: 17  Epoch: 13  Training loss = 1.4426  Validation loss = 6.0296  \n",
      "\n",
      "Fold: 17  Epoch: 14  Training loss = 1.3902  Validation loss = 5.9615  \n",
      "\n",
      "Fold: 17  Epoch: 15  Training loss = 1.5129  Validation loss = 5.9684  \n",
      "\n",
      "Fold: 17  Epoch: 16  Training loss = 1.4795  Validation loss = 5.9548  \n",
      "\n",
      "Fold: 17  Epoch: 17  Training loss = 1.4375  Validation loss = 5.9376  \n",
      "\n",
      "Fold: 17  Epoch: 18  Training loss = 1.3785  Validation loss = 5.8900  \n",
      "\n",
      "Fold: 17  Epoch: 19  Training loss = 1.5874  Validation loss = 5.7051  \n",
      "\n",
      "Fold: 17  Epoch: 20  Training loss = 1.3530  Validation loss = 5.9085  \n",
      "\n",
      "Fold: 17  Epoch: 21  Training loss = 1.4081  Validation loss = 5.8381  \n",
      "\n",
      "Fold: 17  Epoch: 22  Training loss = 1.4129  Validation loss = 5.6549  \n",
      "\n",
      "Fold: 17  Epoch: 23  Training loss = 1.3696  Validation loss = 5.7811  \n",
      "\n",
      "Fold: 17  Epoch: 24  Training loss = 1.4037  Validation loss = 5.6615  \n",
      "\n",
      "Fold: 17  Epoch: 25  Training loss = 1.5209  Validation loss = 6.2769  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 7  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 1.8064  Validation loss = 3.0880  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 2.1142  Validation loss = 2.7186  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 1.8326  Validation loss = 1.6722  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 1.7444  Validation loss = 1.3580  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 1.7063  Validation loss = 1.6112  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 1.5508  Validation loss = 1.2204  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 1.8929  Validation loss = 1.1819  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 1.5518  Validation loss = 1.0686  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 1.4690  Validation loss = 1.2166  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 1.4560  Validation loss = 1.4279  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 1.3678  Validation loss = 1.5432  \n",
      "\n",
      "Fold: 18  Epoch: 12  Training loss = 1.4920  Validation loss = 1.3933  \n",
      "\n",
      "Fold: 18  Epoch: 13  Training loss = 1.8336  Validation loss = 1.9497  \n",
      "\n",
      "Fold: 18  Epoch: 14  Training loss = 1.4087  Validation loss = 1.6592  \n",
      "\n",
      "Fold: 18  Epoch: 15  Training loss = 1.4137  Validation loss = 1.5314  \n",
      "\n",
      "Fold: 18  Epoch: 16  Training loss = 1.3341  Validation loss = 1.3027  \n",
      "\n",
      "Fold: 18  Epoch: 17  Training loss = 1.7352  Validation loss = 0.2922  \n",
      "\n",
      "Fold: 18  Epoch: 18  Training loss = 1.3239  Validation loss = 0.5863  \n",
      "\n",
      "Fold: 18  Epoch: 19  Training loss = 1.3525  Validation loss = 1.5494  \n",
      "\n",
      "Fold: 18  Epoch: 20  Training loss = 1.3889  Validation loss = 0.6355  \n",
      "\n",
      "Fold: 18  Epoch: 21  Training loss = 1.5588  Validation loss = 0.8183  \n",
      "\n",
      "Fold: 18  Epoch: 22  Training loss = 1.2776  Validation loss = 1.5801  \n",
      "\n",
      "Fold: 18  Epoch: 23  Training loss = 1.3611  Validation loss = 0.7596  \n",
      "\n",
      "Fold: 18  Epoch: 24  Training loss = 1.3066  Validation loss = 0.8296  \n",
      "\n",
      "Fold: 18  Epoch: 25  Training loss = 1.3235  Validation loss = 0.6792  \n",
      "\n",
      "Fold: 18  Epoch: 26  Training loss = 1.3795  Validation loss = 0.8610  \n",
      "\n",
      "Fold: 18  Epoch: 27  Training loss = 1.3912  Validation loss = 0.9156  \n",
      "\n",
      "Fold: 18  Epoch: 28  Training loss = 1.3136  Validation loss = 0.8828  \n",
      "\n",
      "Fold: 18  Epoch: 29  Training loss = 1.6025  Validation loss = 0.8010  \n",
      "\n",
      "Fold: 18  Epoch: 30  Training loss = 1.4601  Validation loss = 1.3572  \n",
      "\n",
      "Fold: 18  Epoch: 31  Training loss = 1.5036  Validation loss = 1.3000  \n",
      "\n",
      "Fold: 18  Epoch: 32  Training loss = 1.3940  Validation loss = 1.9479  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 17  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 1.4427  Validation loss = 4.2722  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 1.4633  Validation loss = 4.0133  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 1.5392  Validation loss = 4.0894  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 1.5376  Validation loss = 4.1498  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 1.4194  Validation loss = 3.7390  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 1.3257  Validation loss = 3.3593  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 1.3323  Validation loss = 3.3497  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 1.3113  Validation loss = 3.6655  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 1.3483  Validation loss = 4.0800  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 1.3958  Validation loss = 4.2502  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 1.3449  Validation loss = 4.1793  \n",
      "\n",
      "Fold: 19  Epoch: 12  Training loss = 1.3634  Validation loss = 4.3092  \n",
      "\n",
      "Fold: 19  Epoch: 13  Training loss = 1.3469  Validation loss = 3.6920  \n",
      "\n",
      "Fold: 19  Epoch: 14  Training loss = 1.3251  Validation loss = 3.8540  \n",
      "\n",
      "Fold: 19  Epoch: 15  Training loss = 1.3894  Validation loss = 4.1175  \n",
      "\n",
      "Fold: 19  Epoch: 16  Training loss = 1.3422  Validation loss = 4.4015  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 7  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 1.6118  Validation loss = 1.4233  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 1.7374  Validation loss = 3.7505  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 1.7325  Validation loss = 2.6982  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 1.7572  Validation loss = 2.6634  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 1.7262  Validation loss = 2.8376  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 1.6852  Validation loss = 1.4888  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 1.8628  Validation loss = 2.8552  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 1.7520  Validation loss = 2.8566  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 1.8167  Validation loss = 2.3965  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 1.7417  Validation loss = 2.7962  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 1.5831  Validation loss = 3.6445  \n",
      "\n",
      "Fold: 20  Epoch: 12  Training loss = 1.8050  Validation loss = 2.9884  \n",
      "\n",
      "Fold: 20  Epoch: 13  Training loss = 1.6286  Validation loss = 3.2831  \n",
      "\n",
      "Fold: 20  Epoch: 14  Training loss = 1.5622  Validation loss = 3.6593  \n",
      "\n",
      "Fold: 20  Epoch: 15  Training loss = 1.5837  Validation loss = 2.7074  \n",
      "\n",
      "Fold: 20  Epoch: 16  Training loss = 1.5429  Validation loss = 2.6594  \n",
      "\n",
      "Fold: 20  Epoch: 17  Training loss = 1.5124  Validation loss = 2.9036  \n",
      "\n",
      "Fold: 20  Epoch: 18  Training loss = 1.4798  Validation loss = 2.9762  \n",
      "\n",
      "Fold: 20  Epoch: 19  Training loss = 1.5059  Validation loss = 2.9361  \n",
      "\n",
      "Fold: 20  Epoch: 20  Training loss = 1.4440  Validation loss = 2.9759  \n",
      "\n",
      "Fold: 20  Epoch: 21  Training loss = 1.4783  Validation loss = 2.8908  \n",
      "\n",
      "Fold: 20  Epoch: 22  Training loss = 1.4837  Validation loss = 2.9094  \n",
      "\n",
      "Fold: 20  Epoch: 23  Training loss = 1.4730  Validation loss = 2.5224  \n",
      "\n",
      "Fold: 20  Epoch: 24  Training loss = 1.4733  Validation loss = 3.0307  \n",
      "\n",
      "Fold: 20  Epoch: 25  Training loss = 1.5031  Validation loss = 2.8807  \n",
      "\n",
      "Fold: 20  Epoch: 26  Training loss = 1.5834  Validation loss = 2.7701  \n",
      "\n",
      "Fold: 20  Epoch: 27  Training loss = 1.4653  Validation loss = 2.8493  \n",
      "\n",
      "Fold: 20  Epoch: 28  Training loss = 1.4184  Validation loss = 2.7866  \n",
      "\n",
      "Fold: 20  Epoch: 29  Training loss = 1.4017  Validation loss = 2.6333  \n",
      "\n",
      "Fold: 20  Epoch: 30  Training loss = 1.4822  Validation loss = 2.7732  \n",
      "\n",
      "Fold: 20  Epoch: 31  Training loss = 1.7802  Validation loss = 2.4662  \n",
      "\n",
      "Fold: 20  Epoch: 32  Training loss = 1.5687  Validation loss = 3.0945  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 1  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 1.5648  Validation loss = 4.6958  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 1.5981  Validation loss = 4.8751  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 1.5250  Validation loss = 4.9091  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 1.5234  Validation loss = 4.8876  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 1.5355  Validation loss = 4.9876  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 1.5312  Validation loss = 5.4071  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 1.5173  Validation loss = 5.1771  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 1.5076  Validation loss = 5.0752  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 1.5092  Validation loss = 5.2591  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 1.5293  Validation loss = 4.9962  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 1.4832  Validation loss = 4.8457  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 1.4824  Validation loss = 5.0515  \n",
      "\n",
      "Fold: 21  Epoch: 13  Training loss = 1.4987  Validation loss = 5.5149  \n",
      "\n",
      "Fold: 21  Epoch: 14  Training loss = 1.4336  Validation loss = 5.2410  \n",
      "\n",
      "Fold: 21  Epoch: 15  Training loss = 1.4579  Validation loss = 5.3634  \n",
      "\n",
      "Fold: 21  Epoch: 16  Training loss = 1.4768  Validation loss = 5.4059  \n",
      "\n",
      "Fold: 21  Epoch: 17  Training loss = 1.4287  Validation loss = 5.0328  \n",
      "\n",
      "Fold: 21  Epoch: 18  Training loss = 1.4419  Validation loss = 4.9057  \n",
      "\n",
      "Fold: 21  Epoch: 19  Training loss = 1.4615  Validation loss = 5.2386  \n",
      "\n",
      "Fold: 21  Epoch: 20  Training loss = 1.4121  Validation loss = 5.1444  \n",
      "\n",
      "Fold: 21  Epoch: 21  Training loss = 1.4099  Validation loss = 5.2958  \n",
      "\n",
      "Fold: 21  Epoch: 22  Training loss = 1.5065  Validation loss = 5.1456  \n",
      "\n",
      "Fold: 21  Epoch: 23  Training loss = 1.4442  Validation loss = 5.1742  \n",
      "\n",
      "Fold: 21  Epoch: 24  Training loss = 1.4286  Validation loss = 5.2603  \n",
      "\n",
      "Fold: 21  Epoch: 25  Training loss = 1.3663  Validation loss = 5.7518  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 1  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 1.8298  Validation loss = 3.3640  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 1.8017  Validation loss = 3.5850  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 1.8193  Validation loss = 3.7834  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 1.7980  Validation loss = 3.3395  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 1.7798  Validation loss = 3.3729  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 1.8098  Validation loss = 2.9351  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 1.8017  Validation loss = 2.6598  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 1.7476  Validation loss = 2.6710  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 1.6904  Validation loss = 2.8250  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 1.6318  Validation loss = 2.6610  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 1.6104  Validation loss = 3.1831  \n",
      "\n",
      "Fold: 22  Epoch: 12  Training loss = 1.7200  Validation loss = 2.5268  \n",
      "\n",
      "Fold: 22  Epoch: 13  Training loss = 1.6898  Validation loss = 2.7434  \n",
      "\n",
      "Fold: 22  Epoch: 14  Training loss = 1.5800  Validation loss = 2.8724  \n",
      "\n",
      "Fold: 22  Epoch: 15  Training loss = 1.5965  Validation loss = 2.8158  \n",
      "\n",
      "Fold: 22  Epoch: 16  Training loss = 1.6997  Validation loss = 3.3781  \n",
      "\n",
      "Fold: 22  Epoch: 17  Training loss = 1.6813  Validation loss = 3.5281  \n",
      "\n",
      "Fold: 22  Epoch: 18  Training loss = 1.6560  Validation loss = 3.5374  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 12  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 1.7666  Validation loss = 4.6368  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 1.8397  Validation loss = 3.6430  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 1.8408  Validation loss = 3.2413  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 1.7132  Validation loss = 4.4149  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 1.7045  Validation loss = 3.6970  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 1.6810  Validation loss = 3.3456  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 1.6207  Validation loss = 4.0194  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 1.5994  Validation loss = 4.2708  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 1.6219  Validation loss = 4.5381  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 1.6086  Validation loss = 3.5847  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 1.6313  Validation loss = 4.5338  \n",
      "\n",
      "Fold: 23  Epoch: 12  Training loss = 1.6001  Validation loss = 4.5495  \n",
      "\n",
      "Fold: 23  Epoch: 13  Training loss = 1.6184  Validation loss = 4.4999  \n",
      "\n",
      "Fold: 23  Epoch: 14  Training loss = 1.5717  Validation loss = 4.3416  \n",
      "\n",
      "Fold: 23  Epoch: 15  Training loss = 1.6260  Validation loss = 4.3282  \n",
      "\n",
      "Fold: 23  Epoch: 16  Training loss = 1.5957  Validation loss = 4.6842  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 3  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 1.8487  Validation loss = 1.7123  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 2.3352  Validation loss = 2.4196  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 2.3776  Validation loss = 1.5364  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 2.2122  Validation loss = 1.5077  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 2.1732  Validation loss = 1.2985  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 1.8719  Validation loss = 0.9732  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 1.9541  Validation loss = 0.9477  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 2.2210  Validation loss = 2.1996  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 2.1757  Validation loss = 1.5317  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 2.1751  Validation loss = 2.0871  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 2.2706  Validation loss = 2.4824  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 2.1413  Validation loss = 1.6906  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 2.0532  Validation loss = 2.0511  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 2.1638  Validation loss = 2.0698  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 2.1245  Validation loss = 1.8767  \n",
      "\n",
      "Fold: 24  Epoch: 16  Training loss = 2.1185  Validation loss = 1.9930  \n",
      "\n",
      "Fold: 24  Epoch: 17  Training loss = 2.2594  Validation loss = 1.9326  \n",
      "\n",
      "Fold: 24  Epoch: 18  Training loss = 2.1663  Validation loss = 1.6051  \n",
      "\n",
      "Fold: 24  Epoch: 19  Training loss = 2.0811  Validation loss = 2.0834  \n",
      "\n",
      "Fold: 24  Epoch: 20  Training loss = 2.1415  Validation loss = 1.9551  \n",
      "\n",
      "Fold: 24  Epoch: 21  Training loss = 2.1111  Validation loss = 1.4742  \n",
      "\n",
      "Fold: 24  Epoch: 22  Training loss = 2.1557  Validation loss = 1.8366  \n",
      "\n",
      "Fold: 24  Epoch: 23  Training loss = 2.1251  Validation loss = 2.1955  \n",
      "\n",
      "Fold: 24  Epoch: 24  Training loss = 2.0892  Validation loss = 1.8641  \n",
      "\n",
      "Fold: 24  Epoch: 25  Training loss = 2.1207  Validation loss = 2.3058  \n",
      "\n",
      "Fold: 24  Epoch: 26  Training loss = 2.0428  Validation loss = 2.1387  \n",
      "\n",
      "Fold: 24  Epoch: 27  Training loss = 2.0885  Validation loss = 1.6679  \n",
      "\n",
      "Fold: 24  Epoch: 28  Training loss = 2.1250  Validation loss = 1.6001  \n",
      "\n",
      "Fold: 24  Epoch: 29  Training loss = 2.1550  Validation loss = 1.8996  \n",
      "\n",
      "Fold: 24  Epoch: 30  Training loss = 2.1229  Validation loss = 2.2254  \n",
      "\n",
      "Fold: 24  Epoch: 31  Training loss = 2.0545  Validation loss = 2.3506  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 7  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 2.1985  Validation loss = 2.8097  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 2.0899  Validation loss = 1.7277  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 2.0249  Validation loss = 1.8426  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 2.0379  Validation loss = 1.8172  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 2.1279  Validation loss = 2.3680  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 2.0098  Validation loss = 1.4921  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 2.1546  Validation loss = 1.6498  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 2.0587  Validation loss = 1.4975  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 2.0563  Validation loss = 1.4728  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 2.2933  Validation loss = 2.2653  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 2.0882  Validation loss = 3.5197  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 2.0865  Validation loss = 2.0364  \n",
      "\n",
      "Fold: 25  Epoch: 13  Training loss = 1.9679  Validation loss = 2.0467  \n",
      "\n",
      "Fold: 25  Epoch: 14  Training loss = 1.9734  Validation loss = 2.2302  \n",
      "\n",
      "Fold: 25  Epoch: 15  Training loss = 1.9378  Validation loss = 2.1560  \n",
      "\n",
      "Fold: 25  Epoch: 16  Training loss = 1.9685  Validation loss = 2.2097  \n",
      "\n",
      "Fold: 25  Epoch: 17  Training loss = 1.9392  Validation loss = 2.1430  \n",
      "\n",
      "Fold: 25  Epoch: 18  Training loss = 1.9492  Validation loss = 2.0358  \n",
      "\n",
      "Fold: 25  Epoch: 19  Training loss = 1.9913  Validation loss = 1.8302  \n",
      "\n",
      "Fold: 25  Epoch: 20  Training loss = 1.9479  Validation loss = 1.6807  \n",
      "\n",
      "Fold: 25  Epoch: 21  Training loss = 1.8806  Validation loss = 1.8507  \n",
      "\n",
      "Fold: 25  Epoch: 22  Training loss = 1.8198  Validation loss = 2.0429  \n",
      "\n",
      "Fold: 25  Epoch: 23  Training loss = 1.9234  Validation loss = 1.8573  \n",
      "\n",
      "Fold: 25  Epoch: 24  Training loss = 1.8312  Validation loss = 1.8866  \n",
      "\n",
      "Fold: 25  Epoch: 25  Training loss = 1.9197  Validation loss = 1.8294  \n",
      "\n",
      "Fold: 25  Epoch: 26  Training loss = 1.8104  Validation loss = 2.4312  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 9  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 1.9204  Validation loss = 1.6218  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 1.9635  Validation loss = 1.4840  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 1.9282  Validation loss = 1.6699  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 1.9113  Validation loss = 1.7819  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 1.8926  Validation loss = 1.8635  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 1.8823  Validation loss = 1.8514  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 1.8260  Validation loss = 1.9425  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 1.8680  Validation loss = 2.1212  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 1.8809  Validation loss = 2.2435  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 1.8671  Validation loss = 1.8428  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 1.7741  Validation loss = 1.8634  \n",
      "\n",
      "Fold: 26  Epoch: 12  Training loss = 1.7805  Validation loss = 1.9960  \n",
      "\n",
      "Fold: 26  Epoch: 13  Training loss = 1.8594  Validation loss = 1.8091  \n",
      "\n",
      "Fold: 26  Epoch: 14  Training loss = 1.7884  Validation loss = 1.8457  \n",
      "\n",
      "Fold: 26  Epoch: 15  Training loss = 1.7137  Validation loss = 1.2375  \n",
      "\n",
      "Fold: 26  Epoch: 16  Training loss = 1.7955  Validation loss = 1.2057  \n",
      "\n",
      "Fold: 26  Epoch: 17  Training loss = 1.7912  Validation loss = 1.9574  \n",
      "\n",
      "Fold: 26  Epoch: 18  Training loss = 1.7411  Validation loss = 1.4051  \n",
      "\n",
      "Fold: 26  Epoch: 19  Training loss = 1.5736  Validation loss = 1.4431  \n",
      "\n",
      "Fold: 26  Epoch: 20  Training loss = 1.6996  Validation loss = 2.0098  \n",
      "\n",
      "Fold: 26  Epoch: 21  Training loss = 1.7285  Validation loss = 1.4453  \n",
      "\n",
      "Fold: 26  Epoch: 22  Training loss = 1.7190  Validation loss = 1.4344  \n",
      "\n",
      "Fold: 26  Epoch: 23  Training loss = 1.9550  Validation loss = 1.1592  \n",
      "\n",
      "Fold: 26  Epoch: 24  Training loss = 1.8491  Validation loss = 1.4112  \n",
      "\n",
      "Fold: 26  Epoch: 25  Training loss = 1.7436  Validation loss = 1.7066  \n",
      "\n",
      "Fold: 26  Epoch: 26  Training loss = 1.7469  Validation loss = 1.7321  \n",
      "\n",
      "Fold: 26  Epoch: 27  Training loss = 1.7590  Validation loss = 1.7833  \n",
      "\n",
      "Fold: 26  Epoch: 28  Training loss = 1.7466  Validation loss = 1.5195  \n",
      "\n",
      "Fold: 26  Epoch: 29  Training loss = 1.7607  Validation loss = 1.7156  \n",
      "\n",
      "Fold: 26  Epoch: 30  Training loss = 1.7517  Validation loss = 1.7302  \n",
      "\n",
      "Fold: 26  Epoch: 31  Training loss = 1.7130  Validation loss = 1.7905  \n",
      "\n",
      "Fold: 26  Epoch: 32  Training loss = 1.6822  Validation loss = 1.9802  \n",
      "\n",
      "Fold: 26  Epoch: 33  Training loss = 1.7033  Validation loss = 1.7222  \n",
      "\n",
      "Fold: 26  Epoch: 34  Training loss = 1.7083  Validation loss = 1.7521  \n",
      "\n",
      "Fold: 26  Epoch: 35  Training loss = 1.7048  Validation loss = 1.7059  \n",
      "\n",
      "Fold: 26  Epoch: 36  Training loss = 1.8587  Validation loss = 1.9633  \n",
      "\n",
      "Fold: 26  Epoch: 37  Training loss = 1.6750  Validation loss = 1.9834  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 23  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 1.6993  Validation loss = 0.9557  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 1.6742  Validation loss = 1.0210  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 1.7451  Validation loss = 1.0051  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 1.6912  Validation loss = 1.1752  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 1.7032  Validation loss = 1.2325  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 1.6763  Validation loss = 1.0385  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 1.6844  Validation loss = 1.1448  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 4.3424  Validation loss = 2.2624  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 3.1443  Validation loss = 1.1146  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 2.7242  Validation loss = 0.9116  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 2.5667  Validation loss = 1.3724  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 2.4624  Validation loss = 1.4609  \n",
      "\n",
      "Fold: 27  Epoch: 13  Training loss = 2.4560  Validation loss = 1.1757  \n",
      "\n",
      "Fold: 27  Epoch: 14  Training loss = 2.5865  Validation loss = 0.9530  \n",
      "\n",
      "Fold: 27  Epoch: 15  Training loss = 2.5527  Validation loss = 1.0337  \n",
      "\n",
      "Fold: 27  Epoch: 16  Training loss = 2.4843  Validation loss = 1.0793  \n",
      "\n",
      "Fold: 27  Epoch: 17  Training loss = 2.3570  Validation loss = 1.0282  \n",
      "\n",
      "Fold: 27  Epoch: 18  Training loss = 2.3386  Validation loss = 1.0519  \n",
      "\n",
      "Fold: 27  Epoch: 19  Training loss = 2.3323  Validation loss = 0.9257  \n",
      "\n",
      "Fold: 27  Epoch: 20  Training loss = 2.2461  Validation loss = 1.0224  \n",
      "\n",
      "Fold: 27  Epoch: 21  Training loss = 2.2421  Validation loss = 0.9820  \n",
      "\n",
      "Fold: 27  Epoch: 22  Training loss = 2.1971  Validation loss = 1.1036  \n",
      "\n",
      "Fold: 27  Epoch: 23  Training loss = 2.1755  Validation loss = 1.1372  \n",
      "\n",
      "Fold: 27  Epoch: 24  Training loss = 2.1376  Validation loss = 1.0214  \n",
      "\n",
      "Fold: 27  Epoch: 25  Training loss = 2.1362  Validation loss = 1.0233  \n",
      "\n",
      "Fold: 27  Epoch: 26  Training loss = 2.0928  Validation loss = 1.4800  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 10  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 2.0593  Validation loss = 2.6368  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 2.0184  Validation loss = 2.5643  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 2.0084  Validation loss = 2.5824  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 1.9802  Validation loss = 2.4208  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 2.1763  Validation loss = 2.4299  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 2.0461  Validation loss = 2.1343  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 1.9746  Validation loss = 2.2218  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 1.9111  Validation loss = 2.0341  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 1.9639  Validation loss = 2.1127  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 1.8443  Validation loss = 2.1151  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 1.8608  Validation loss = 2.1208  \n",
      "\n",
      "Fold: 28  Epoch: 12  Training loss = 1.9042  Validation loss = 2.0455  \n",
      "\n",
      "Fold: 28  Epoch: 13  Training loss = 1.8808  Validation loss = 2.1083  \n",
      "\n",
      "Fold: 28  Epoch: 14  Training loss = 1.8112  Validation loss = 1.9089  \n",
      "\n",
      "Fold: 28  Epoch: 15  Training loss = 1.8756  Validation loss = 2.1455  \n",
      "\n",
      "Fold: 28  Epoch: 16  Training loss = 1.8010  Validation loss = 2.0976  \n",
      "\n",
      "Fold: 28  Epoch: 17  Training loss = 1.8123  Validation loss = 1.9413  \n",
      "\n",
      "Fold: 28  Epoch: 18  Training loss = 1.7928  Validation loss = 2.1215  \n",
      "\n",
      "Fold: 28  Epoch: 19  Training loss = 1.7536  Validation loss = 1.9943  \n",
      "\n",
      "Fold: 28  Epoch: 20  Training loss = 1.7405  Validation loss = 2.0414  \n",
      "\n",
      "Fold: 28  Epoch: 21  Training loss = 1.7765  Validation loss = 1.9912  \n",
      "\n",
      "Fold: 28  Epoch: 22  Training loss = 1.8078  Validation loss = 1.7017  \n",
      "\n",
      "Fold: 28  Epoch: 23  Training loss = 1.8432  Validation loss = 1.6332  \n",
      "\n",
      "Fold: 28  Epoch: 24  Training loss = 1.8818  Validation loss = 2.0910  \n",
      "\n",
      "Fold: 28  Epoch: 25  Training loss = 1.7450  Validation loss = 1.6938  \n",
      "\n",
      "Fold: 28  Epoch: 26  Training loss = 1.7580  Validation loss = 2.0326  \n",
      "\n",
      "Fold: 28  Epoch: 27  Training loss = 1.6931  Validation loss = 2.4995  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 23  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 1.7028  Validation loss = 1.0052  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 1.7532  Validation loss = 0.9936  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 1.6865  Validation loss = 0.8314  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 1.6590  Validation loss = 0.8197  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 1.7062  Validation loss = 0.6739  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 1.6610  Validation loss = 0.7532  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 1.6728  Validation loss = 0.6754  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 1.7460  Validation loss = 0.8510  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 1.6804  Validation loss = 0.6498  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 1.6370  Validation loss = 0.6637  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 1.6323  Validation loss = 0.7026  \n",
      "\n",
      "Fold: 29  Epoch: 12  Training loss = 1.6096  Validation loss = 0.7348  \n",
      "\n",
      "Fold: 29  Epoch: 13  Training loss = 1.6003  Validation loss = 0.7309  \n",
      "\n",
      "Fold: 29  Epoch: 14  Training loss = 1.6055  Validation loss = 0.6103  \n",
      "\n",
      "Fold: 29  Epoch: 15  Training loss = 1.6037  Validation loss = 0.6273  \n",
      "\n",
      "Fold: 29  Epoch: 16  Training loss = 1.6069  Validation loss = 0.6981  \n",
      "\n",
      "Fold: 29  Epoch: 17  Training loss = 1.5711  Validation loss = 0.7092  \n",
      "\n",
      "Fold: 29  Epoch: 18  Training loss = 1.5696  Validation loss = 0.7528  \n",
      "\n",
      "Fold: 29  Epoch: 19  Training loss = 1.5632  Validation loss = 0.7511  \n",
      "\n",
      "Fold: 29  Epoch: 20  Training loss = 1.5619  Validation loss = 0.6602  \n",
      "\n",
      "Fold: 29  Epoch: 21  Training loss = 1.5601  Validation loss = 0.9315  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 14  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 1.5891  Validation loss = 1.9373  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 1.5452  Validation loss = 2.0633  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 1.5530  Validation loss = 2.4773  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 1.5582  Validation loss = 2.4053  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 1.5520  Validation loss = 1.8273  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 1.5332  Validation loss = 1.7805  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 1.5254  Validation loss = 2.3157  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 1.5614  Validation loss = 2.1001  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 1.5732  Validation loss = 2.0922  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 1.5449  Validation loss = 1.9467  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 1.5217  Validation loss = 1.9742  \n",
      "\n",
      "Fold: 30  Epoch: 12  Training loss = 1.6191  Validation loss = 2.3002  \n",
      "\n",
      "Fold: 30  Epoch: 13  Training loss = 1.5796  Validation loss = 2.6171  \n",
      "\n",
      "Fold: 30  Epoch: 14  Training loss = 1.5705  Validation loss = 2.5979  \n",
      "\n",
      "Fold: 30  Epoch: 15  Training loss = 1.5807  Validation loss = 1.9835  \n",
      "\n",
      "Fold: 30  Epoch: 16  Training loss = 1.5344  Validation loss = 2.3904  \n",
      "\n",
      "Fold: 30  Epoch: 17  Training loss = 1.5419  Validation loss = 2.1429  \n",
      "\n",
      "Fold: 30  Epoch: 18  Training loss = 1.5197  Validation loss = 2.4097  \n",
      "\n",
      "Fold: 30  Epoch: 19  Training loss = 1.5161  Validation loss = 2.0160  \n",
      "\n",
      "Fold: 30  Epoch: 20  Training loss = 1.5119  Validation loss = 2.2242  \n",
      "\n",
      "Fold: 30  Epoch: 21  Training loss = 1.5542  Validation loss = 2.4510  \n",
      "\n",
      "Fold: 30  Epoch: 22  Training loss = 1.5211  Validation loss = 1.9625  \n",
      "\n",
      "Fold: 30  Epoch: 23  Training loss = 1.4957  Validation loss = 2.3605  \n",
      "\n",
      "Fold: 30  Epoch: 24  Training loss = 1.5181  Validation loss = 2.3711  \n",
      "\n",
      "Fold: 30  Epoch: 25  Training loss = 1.5164  Validation loss = 2.3253  \n",
      "\n",
      "Fold: 30  Epoch: 26  Training loss = 1.5222  Validation loss = 2.2184  \n",
      "\n",
      "Fold: 30  Epoch: 27  Training loss = 1.5331  Validation loss = 2.3795  \n",
      "\n",
      "Fold: 30  Epoch: 28  Training loss = 1.5102  Validation loss = 2.1813  \n",
      "\n",
      "Fold: 30  Epoch: 29  Training loss = 1.5038  Validation loss = 2.1927  \n",
      "\n",
      "Fold: 30  Epoch: 30  Training loss = 1.6757  Validation loss = 2.5551  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 6  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 1.5913  Validation loss = 2.6560  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 1.5718  Validation loss = 3.0168  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 1.5404  Validation loss = 2.9978  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 1.5644  Validation loss = 3.0786  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 1.5443  Validation loss = 2.9681  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 1.5419  Validation loss = 2.9063  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 1.5308  Validation loss = 2.9015  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 1.5507  Validation loss = 3.0573  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 1.5343  Validation loss = 2.9595  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 1.5537  Validation loss = 3.0984  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 1.5537  Validation loss = 2.9755  \n",
      "\n",
      "Fold: 31  Epoch: 12  Training loss = 1.5883  Validation loss = 3.1933  \n",
      "\n",
      "Fold: 31  Epoch: 13  Training loss = 1.5308  Validation loss = 2.8634  \n",
      "\n",
      "Fold: 31  Epoch: 14  Training loss = 1.5503  Validation loss = 2.7901  \n",
      "\n",
      "Fold: 31  Epoch: 15  Training loss = 1.5189  Validation loss = 2.7667  \n",
      "\n",
      "Fold: 31  Epoch: 16  Training loss = 1.5294  Validation loss = 2.7828  \n",
      "\n",
      "Fold: 31  Epoch: 17  Training loss = 1.5208  Validation loss = 2.8542  \n",
      "\n",
      "Fold: 31  Epoch: 18  Training loss = 1.5337  Validation loss = 2.8436  \n",
      "\n",
      "Fold: 31  Epoch: 19  Training loss = 1.5052  Validation loss = 2.8572  \n",
      "\n",
      "Fold: 31  Epoch: 20  Training loss = 1.5159  Validation loss = 2.7501  \n",
      "\n",
      "Fold: 31  Epoch: 21  Training loss = 4.2560  Validation loss = 5.1074  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 1  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 2.6126  Validation loss = 4.6902  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 2.1071  Validation loss = 4.1338  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 2.0009  Validation loss = 4.3693  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 1.9543  Validation loss = 3.8827  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 1.9059  Validation loss = 3.6499  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 1.8623  Validation loss = 3.4581  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 1.8129  Validation loss = 3.6541  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 1.7718  Validation loss = 3.4950  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 1.7754  Validation loss = 3.5556  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 1.6992  Validation loss = 3.7637  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 1.6806  Validation loss = 3.4512  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 1.8858  Validation loss = 2.9498  \n",
      "\n",
      "Fold: 32  Epoch: 13  Training loss = 1.9755  Validation loss = 4.0881  \n",
      "\n",
      "Fold: 32  Epoch: 14  Training loss = 1.8457  Validation loss = 4.0281  \n",
      "\n",
      "Fold: 32  Epoch: 15  Training loss = 1.7909  Validation loss = 3.7042  \n",
      "\n",
      "Fold: 32  Epoch: 16  Training loss = 1.7619  Validation loss = 3.6676  \n",
      "\n",
      "Fold: 32  Epoch: 17  Training loss = 1.7552  Validation loss = 3.6011  \n",
      "\n",
      "Fold: 32  Epoch: 18  Training loss = 1.7456  Validation loss = 3.5415  \n",
      "\n",
      "Fold: 32  Epoch: 19  Training loss = 1.6772  Validation loss = 3.2655  \n",
      "\n",
      "Fold: 32  Epoch: 20  Training loss = 1.6294  Validation loss = 3.1749  \n",
      "\n",
      "Fold: 32  Epoch: 21  Training loss = 1.5870  Validation loss = 3.1970  \n",
      "\n",
      "Fold: 32  Epoch: 22  Training loss = 1.5469  Validation loss = 2.9842  \n",
      "\n",
      "Fold: 32  Epoch: 23  Training loss = 1.5478  Validation loss = 3.1558  \n",
      "\n",
      "Fold: 32  Epoch: 24  Training loss = 1.5201  Validation loss = 2.5547  \n",
      "\n",
      "Fold: 32  Epoch: 25  Training loss = 1.5067  Validation loss = 2.5793  \n",
      "\n",
      "Fold: 32  Epoch: 26  Training loss = 1.5984  Validation loss = 2.4693  \n",
      "\n",
      "Fold: 32  Epoch: 27  Training loss = 1.5816  Validation loss = 2.1767  \n",
      "\n",
      "Fold: 32  Epoch: 28  Training loss = 1.5407  Validation loss = 2.7932  \n",
      "\n",
      "Fold: 32  Epoch: 29  Training loss = 1.5330  Validation loss = 2.4865  \n",
      "\n",
      "Fold: 32  Epoch: 30  Training loss = 1.5276  Validation loss = 2.3221  \n",
      "\n",
      "Fold: 32  Epoch: 31  Training loss = 1.5103  Validation loss = 2.4979  \n",
      "\n",
      "Fold: 32  Epoch: 32  Training loss = 1.4776  Validation loss = 2.6774  \n",
      "\n",
      "Fold: 32  Epoch: 33  Training loss = 1.4525  Validation loss = 2.6071  \n",
      "\n",
      "Fold: 32  Epoch: 34  Training loss = 1.4559  Validation loss = 2.6459  \n",
      "\n",
      "Fold: 32  Epoch: 35  Training loss = 1.4614  Validation loss = 2.2319  \n",
      "\n",
      "Fold: 32  Epoch: 36  Training loss = 1.4881  Validation loss = 2.7948  \n",
      "\n",
      "Fold: 32  Epoch: 37  Training loss = 1.4254  Validation loss = 2.5449  \n",
      "\n",
      "Fold: 32  Epoch: 38  Training loss = 1.4090  Validation loss = 2.7665  \n",
      "\n",
      "Fold: 32  Epoch: 39  Training loss = 1.4208  Validation loss = 2.9411  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 27  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 13\n",
      "Average validation error: 3.14782\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 1.3696  Test loss = 3.2199  \n",
      "\n",
      "Epoch: 2  Training loss = 1.3436  Test loss = 3.2044  \n",
      "\n",
      "Epoch: 3  Training loss = 1.3224  Test loss = 3.1910  \n",
      "\n",
      "Epoch: 4  Training loss = 1.3044  Test loss = 3.1792  \n",
      "\n",
      "Epoch: 5  Training loss = 1.2900  Test loss = 3.1684  \n",
      "\n",
      "Epoch: 6  Training loss = 1.2790  Test loss = 3.1584  \n",
      "\n",
      "Epoch: 7  Training loss = 1.2701  Test loss = 3.1490  \n",
      "\n",
      "Epoch: 8  Training loss = 1.2624  Test loss = 3.1402  \n",
      "\n",
      "Epoch: 9  Training loss = 1.2555  Test loss = 3.1319  \n",
      "\n",
      "Epoch: 10  Training loss = 1.2493  Test loss = 3.1242  \n",
      "\n",
      "Epoch: 11  Training loss = 1.2436  Test loss = 3.1170  \n",
      "\n",
      "Epoch: 12  Training loss = 1.2383  Test loss = 3.1102  \n",
      "\n",
      "Epoch: 13  Training loss = 1.2334  Test loss = 3.1037  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWWZc7GZmmXGxm5ll5j/bG59zg1PprAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc200e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXd4VFX+h9+TMkkIIYSE3kKRGpqEJlgABQsrq7AooKyK\nuPayRXddy7q7v12sYFkLoIKCiuKqrKKAUqQZDCAQooGEngJJIAlJSEg5vz/O3MnMZGrKTMp5nyfP\nzNy5c+/NlPM533qElBKNRqPRND8C/H0BGo1Go/EPWgA0Go2mmaIFQKPRaJopWgA0Go2mmaIFQKPR\naJopWgA0Go2mmaIFQKPRaJopXgmAEOIdIcRpIUSS1bY2Qoj1QohD5tsoJ6/9rXmfQ0KI39b2wjUa\njUZTO7y1AJYCV9tt+zPwnZTyIuA782MbhBBtgKeBUcBI4GlnQqHRaDQa3xDkzc5Syu+FELF2m6cC\nV5jvLwM2AY/Z7TMZWC+lPAMghFiPEpIPXZ0vJiZGxsban06j0Wg0zti1a1eOlLKtJ/t6JQBOaC+l\nzDTfzwLaO9inM3DC6vFJ87ZqCCHuAu4C6NatG4mJiXVwiRqNRtM8EEIc83TfOg0CS9VYqFbNhaSU\ni6SU8VLK+LZtPRIxjUaj0dSAuhCAU0KIjgDm29MO9kkHulo97mLeptFoNBo/URcCsBowsnp+C3zh\nYJ+1wCQhRJQ5+DvJvE2j0Wg0fsLbNNAPgR1AXyHESSHEXGA+cJUQ4hBwpfkxQoh4IcQSAHPw9x/A\nj+a/vxsBYY1Go9H4B9GQ1wOIj4+XOgis0Wg0niOE2CWljPdkX10JrNFoNM0ULQAajUbTTNECoNFo\nmjbJybBpk7+vokFSF4VgGo1G03D5298gMREOH/b3lTQ4tAWg0WiaNqdPQ3a2v6+iQaIFQKPRNG1y\ncqCwEEpL/X0lDQ4tABqNpmmTk6Nuc3P9ex0NEC0AGo2m6VJZqQXABVoANBpN0yU/Hyoq1H0tANXQ\nAqDRaJouxuwftAA4QAuARqNpumgBcIkWAI1G03SxFgDr+xpAC4BGo2nKaAvAJVoANBpN08UQgKgo\nLQAO0AKg0WiaLtnZEBIC3btrAXCAFgCNRtN0ycmBmBiIjtYC4AAtABqNpuliCEBMjA4CO0ALQGOg\nAa/aptE0aLQF4JJaC4AQoq8Q4iervwIhxMN2+1whhMi32uep2p632SAljBgB//iHv69Eo2l85ORA\n27ZKAM6eraoK1gB1sB6AlDIFGAoghAgE0oHPHOy6RUo5pbbna3YcOQK7dkG7dv6+Eo2m8WFtAUgJ\neXnqvgaoexfQRCBNSnmsjo/bfNmwQd0eOeLf69BoGhvl5WrWbwgAaDeQHXUtADcDHzp5bowQYq8Q\n4mshxMA6Pm/TZeNGdXv0qI4FaDTeYAz2RhAYdCDYjjoTACGECbge+MTB07uB7lLKIcCrwOcujnOX\nECJRCJGY3dxX8ZFSWQBBQVBSolY20mg0nmEM9toCcEpdWgDXALullKfsn5BSFkgpC8331wDBQogY\nRweRUi6SUsZLKePbtm1bsys5d65pzJZ/+QWysmCKOXSi3UAajedoAXBLXQrATJy4f4QQHYQQwnx/\npPm89fNJnDmjsmb++c96ObxPMfz/c+eq26NH/XYpGk2jQwuAW+pEAIQQ4cBVwH+ttt0thLjb/HA6\nkCSE2Au8AtwsZT1N0aOiYNQoeOop+PTTejmFz9iwQZWwjx+vHmsLQKPxHGsBaNVKuVK1ANhQ6zRQ\nACllERBtt+1Nq/uvAa/VxbncIgS89RYcOgRz5kDPnjBsmE9OXadUVsKmTTB1KoSHq1xmbQFoNJ5j\nLQBCKCtAB4FtaJqVwKGh8N//qg986lTlR29s7Nun3FkTJqjHPXpoAdBovCEnByIiVDM40NXADmia\nAgDQoQN88YX6wG+4QWXRNCYM/7/h/omN1S4gjcYbsrOr0j9BC4ADmq4AgHL9vPce/PADLFzo76vx\njg0boE8f6NxZPY6NhWPHlGtIo9G4x6gCNtACUI2mLQAA06ZBp04qJtBYKCuDzZur3D+gXEAXLkBm\npv+uS6NpTGgBcEvTFwBofMGfXbugsNBWAGJj1a2OA2g0nuFIAHJymkaNUB3RPAQgJqZxKb/h/7/i\niqptPXqoWy0AGo1n2AtATIyyrgsL/XdNDYzmIQCNzQLYuBEGD1apnwbduqlbHQjWaNxz/jwUFdn+\nhnQxWDWahwA0JgvgyBElANdcY7s9LExlNmkLoNEza9Ys7rnnHn9fRtPGuhGcgRaAajQPAYiOVjn1\njSGD5t//VhWLDz5Y/bkePbQFAHzzzTfs2LHD35dRI7Kysli5ciVJSUn+vpTGw+7dsH69d68xGkk6\nEoDG5A2oZ+qkErjBExOjBv+8PGjTxt9X45xjx2DpUvjd71Tmkj2xsZCQ4OuranA8+OCDdOnShQ1G\nrKQR8dFHH1FZWcmFCxf8fSmNh4cfVtlv3mTyWVcBG2gLoBrNwwJoLL3An31Wlaw/9pjj53v0gOPH\nm/WydlJKTp48SUpKir8vpUasWLECQAuAp5SWws6d3rdCdyQAxn0tABaahwA0BuU/eRLefhvuuAO6\ndHG8T2ysWuUoPd2nl9aQyMvL4/z582RkZHDu3Dl/X45XpKSkkJiYiBBCC4Cn/PijEoGCAlUH4ymO\nBCAqSt025HHAxzQPAWgMFsBzzyk31Z//7HwfXQtAupX4HTx40I9X4j0rVqxACMEVV1zhnQD873/w\n1Vf1d2ENmS1bqu578/vNyVHWtLXLNygIWrfWAmBF8xCAhm4BZGbCokXw29+q9s/OMGoBmnEguLEK\ngJSSFStWMGHCBGJjYz0XgFOnYOZMePLJ+r3Ahoq1AHizQmBOjhr8AwNttze2lPB6pnkIQEO1ACor\nITUV/vIX5dr5y19c79+1q5rVaAsAoFHFARISEjh8+DCzZ8/GZDJRWlrq2QufeUbls584Ub8X6IqK\nCvX99Md5t22D/v3VY28FIMbBooO6HYQNzUMAIiKU+ddQBOC559SiNa1awUUXwbJlcOed0KuX69eF\nhKjmcNoCoGPHjo1KAFasWEFISAg33ngjJpPJMwsgJUVZhhER6rt7/nz9X6gj7r5bddT1Nfv3K9//\njTeqx978fu07gRo0ppogH9A8BECIhvPBnz8Pf/2rWrd47lxYskSldr7+umevj41t9hZAdHQ0gwcP\nbjQCUFZWxsqVK/nVr35FZGSk5wLw+OOqAPCpp9Tjkyfr90KdceAAJCf7/ryG+8cQAG0B1DnNow4A\nGo7vb88eZU7/+99qsRpviY219Ys2M9LT0+ncuTN9+/Zl69atSCkxLzfdYPn222/Jzs5m9uzZAISE\nhLgXgB071KJGzzwDw4erbcePK4vR1+TkqEJKX/P996oFypAhahLnrQCMHFl9uxYAG5qHBQANxwIw\nCrkcfTk9oUcP5Q8uK6u7a2pEWAtAUVERGRkZ/r4ktyxfvpzWrVtzjbm9h8lkoqysDKfLYksJjz4K\n7dvD73+vYj/gvzhATo4qovRlHEBKNdG59FIVyPVmAielawugsFCllmrqTgCEEEeFEPuFED8JIRId\nPC+EEK8IIVKFEPuEEBfX1bk9oqFYADt3qh90x441e31srAoe+8sd4GcMAejTpw/Q8APB2dnZfPrp\np8yaNYsQ89KEJpMJUK4hh6xeDVu3wt/+Bi1bVtWF+EMAysvh7Fl137j1BampKgPq0kvV45gYzy2A\nc+fUBMmZAEDDmAw2AOraAhgvpRwqpYx38Nw1wEXmv7uAN+r43K5pSBbAqFE1f30zTgW9cOECp0+f\ntlgAUE+poOnpavCpAxYvXkxpaSn333+/ZZshANUygc6fh7//XaV99u2rYkSg1rhu184/AmDt+vHl\n78dwcxoC0Lat5wLgqAjMQAuADb50AU0F3pOKH4DWQogaToNrgOH78+diENnZauCujQA042KwTPNq\naJ07d6Zz5860aNGifiyAGTPguus8/q5IKR26c8rLy3njjTeYOHEi/Y1URqoEwBIHkBI+/VSlOz79\nNEyZopqfBQdXHaxrV/8IgLXV7GsBiI6uSgGtiQBYt4I20O0gbKhLAZDAOiHELiHEXQ6e7wxYf4NP\nmrfZIIS4SwiRKIRIzPYm6OOOmBiVV5yfX3fH9JadO9VtTf3/oNwBAQHNUgCMFNDOnTsTEBBAnz59\n6l4ApFRZL7t2wfbtHr3klltuYcqUKVTadZv94osvOHnyJA888IDN9moC8MADMH26SgveuBE+/rjK\n72/QtasKAvsafwrApZeq4C+o36+nLlxHnUANtAVgQ10KwDgp5cUoV899QojLanIQKeUiKWW8lDK+\nrSMFrykNoRgsIUEFtIysjpoQHKzcQPv31911NRKsBQCoHwHIza2aJLz6quN9KittrIMNGzawZs0a\nFixYYLPbq6++Svfu3ZkyZYrNdiMWcOHCBeWvfvttmD1btT22XgXOmuZkAWRmQlpalfsH1Gw+N9ez\nlu6euIAaQjywAVBnAiClTDffngY+A+ynuemA9bSmi3mbb2gIyp+QAHFxEB5eu+Ncdhls2tTsuoLa\nC0Dfvn05evSo+6ra9HTP3Qdpaeq2Xz/llrHPMqqogPHj1YANFBQUkJWVRYsWLXj88cfZt28fAPv2\n7WPz5s3ce++9BNq1I7CxAL78EkpKVLFVkIus7K5dlVj42oK1HiidpYIuXQrXXqvaNr/5pvpu1qZR\nn73/H5QAVFR4FohuaDGA8nL/FfG5oU4EQAgRLoSIMO4DkwD7FS9WA3PM2UCjgXwpZWZdnN8j/G0B\nSKlcQLVx/xhceaVKy9uzp/bHakSkp6cTEhJCtPlH3LdvXyorK0kzBm1n3HADDB3qWT9541jPPacG\nnDff5H//+x8bN25U219+WeWnf/IJ5OZagtALFy6kTZs2zJ49m5KSEv7zn/8QGhrKXCOQa4WNAHzy\niVr74ZJLXF+XsSSor60A8+9FBgQ4HzSXLVOuq8WL4Z57lEAOGACHD9fsnFu3qknSsGFV2wxvgCe/\n35wcJaatWlV/LixM/flSAB57DOId5cX4n7qyANoDW4UQe4GdwFdSym+EEHcLIe4277MGOAykAouB\ne+vo3J7hbwvg0CE1aNcmAGwwYYK6/e672h+rEZGenk6nTp0shV9GJpBLN5Dh08/IUO4VdyJgCMCV\nV8J111HxxhvMuekmHnvsMZWa+MQTSkzKy2HVKosAjB07lnfffZekpCTuu+8+li9fzuzZsy1iZY0l\nDfTMGfj6a5g2TcV1XOGnWoB9GzZQAOQK4fy3c+qUsgDOnVNxis8/h+JiJQQ1iVX98gsMHGhrERkT\nOE8sOaMGwFmBoK8zAteuVZXU/ojhuKFOBEBKeVhKOcT8N1BK+X/m7W9KKd8035dSyvuklL2klIOk\nlNVqBeoVf1sARgFYXQhAhw7qB9IMBcBw/wCWWgCXqaCZmWowuvtu1U9+/Hg1kDsjLU31WwoLgwce\nIDAnh+vOnyc5KQk5bx6YTKo1c79+8OGHpKSkEBAQQK9evbj66qu5//77eeeddyguLrZJ/bTGEIDw\njRuV+2fGDPf/vCEAPhpEpJQ89dRT/PTdd5wJCOB0RQXlzlJjT51SRWsBAeo6p06Fb79VfXzGj/f+\nmjMy1GdgjWEBeCIAp06ptFln+LIaOD+/qo3Gtm2+OacXNJ9K4MhIFYD1lwWwc6cq6rFKB6wVEycq\nU7khVjSWlXkVn9i+fTvJHvSasReAVq1a0aFDB9cWgDHY33CDEsySEmUJOBOBtDRLU77jffqQAjwS\nFMTs8+cRmzbBiy8ql82sWfD99+Ts2UNsbKwlsPvss88yaNAgJk2axNChQx2ewhCAqG+/9cz9A6pw\nMCDAJxZAWVkZd955J//4xz8Y1qULrXr2JBcocnTusjIVG2jf3nb7sGEqlfXsWSUC3hQu1lYA0tOr\nv94aXxaFJiRUJQxoAfAjQvi3GjghQfkB7fuT15SJE1VgqSEujj5unMqj97B1wE033cQf//hHl/tI\nKasJACg3kEsBMFw+vXvD4MGwYYOyCB55xPH+qakWAfj7P//JG4GBDC8vZyGQPXiwWrENVLGWlPTe\ntctiiQC0aNGCXbt2sXr1aqeXFBISQksgKiHBM/cPKHdI584+EYCHHnqId955h6eeeoq4jh0J7dKF\nXHBsARgDsr0AgPq+r12r9rnoIpg0CZ5/Hn76yXk2z/nzSjTs18T2xoJ3JwDt2inXlC9aW+zYocae\nESO0APgdfwlASYn60tdFANjg8svVwFEPbqDExESO1rTO4MQJZe2sXau6WbohMzOTkydP8vPPP7vc\n7+zZs5SUlFQTALepoKmpKnXWCKIOHqxcFD/+WH3foiLIyoJevTh06BBLly7FNG8eMiKCSuDjK6+s\n8iv37o0cMYIrsrIssQiD4OBgi0XgCJPJxBQg8MIF+M1vXP7fNvggFbSwsJBly5Zxxx138MwzzyBy\ncgjr2pVzwcEE5uVVf4EhCs5cLqNGqXqKu+9WM/tHH1XWgbP20uZiv2oCEBqqLGh3FkBZmVo/2P71\n1syYoT7njz5yfay6YMcO5a695hrYt6922VH1QPMSAH+1g9i7V30x68L/bxAZqWYVdSwAUkquu+46\nnqzpClTr1qnbyZPVbO/jj13unpioQkHHjh2juLjY6X72KaAGffv2JTc3l1xnn+uhQ6puwjqgOGyY\nGrgy7ZLQjKyV3r15+umnCQkJ4Q9/+xti5Uruat+erXYpofnXXcewykpGRUa6/B/tMZlM/AY4HxUF\nY8d6/kIfFIN98cUXFBcXc/vtt6sNOTmImBgC27cnzFEqoyEAjiwAg7g4WLAAkpLU7Py665xbrsZ7\n7GgA96QaOCtLuVxcWQDXXw+DBsE//1m/qdSVlfDDDzBmjPqcjccNiOYlAD6yAF588UU2b95ctaEu\nA8DWTJyoZtsFBXV2yOPHj3P69GmysrJqdoC1a9WP94svlG/7jjvUD98ao1kXVQIgpXQZzHUlAOAi\nEJyaWr2FsuGbt0+jNWcApUrJRx99xIMPPkj79u3hmmsoGDGC/XbFd0kDBlAJjPKyL1NoWRnXACdG\njfLM/WPQtavypddjO5MPPviAbt26cckll6j40rlzEBNDeJcuhFRWUllYaPsCTwTAmk6dYPRoNZA7\nEhRjxTdnAuDu92u83pUABASobK6UFFi1yrPrrgm//KKCwGPGqP9ZCI+ry31F8xIAH1gAx44d449/\n/CP33XdfVX+YhAT1hXb1pbQiLy/PsyUDJ05UM5jvv6/FFdtiDMhOZ9SuqKhQ2R+TJqnVyz75RK1m\n9etfq4DgE08oEWzdGu6913K+Fi1aAPDLL784PbQ7AXDoBpJSCUDv3rbbDQH46Sfb7WYB+PfHH9Oq\nVSv+9Kc/WZ4aNGgQKSkpNn389+fksBHounWr7aBcUaHcSU783K23biUMOOJtRXjXrmpQrssWKVZk\nZ2ezdu1aZs6cSYB13n9MDFFmET1h/56dPq1uPRUAqHLHOQoMu7IAPOkI6okAgIq99OunrABPqotr\ngmHljBmjahIGDWpwcYDmJQCGBVCPM6gPP/wQgAMHDvDjW2+pxbzXrPF49l9WVsbgwYN5+OGH3e98\nySXKN+qtG+hf/1I+2Y8/rvaDqpUAJCaqAN7kyepxp05qhnXsmBKF+fOVK6ZPH9i4ESklP/74I1On\nTkUIUSUA99yjfsCdOln+ZjzyCOlA7NixNtt7X3456UCnd9+tfj2ZmWogtrIAzp49y4uLF1PcqZNj\nCyAqioSDB5kwYQJt2rSxPBUXF0d5ebmNpXHw4EFWmUwEHzmiegclJKi+Ph06KH91YKAqaGrXTv0/\nXbpAly60feIJMoCT3bt79/7WtBjsiSfcrzcNfPLJJ1RUVFgWrrGuqO04cCAAaUY/K4NTp6r8855i\n/B+O3FkZGep4rVtXf84TF5ArC8KawEC1Ml9SkrJW64MdOyAqSn3fQbmBfvihQVXwN58VwUDNIMrL\nlVnrqEqwDli1fDmLO3Rg0unTdLvnHmVuXnpp1bJ+wKlTp2jdurXDQOG6des4ceIEH3zwAQsWLCA0\nNNT5yUJD1ZfKGwEoKlJ95svL4a231LbBg9VM6Fe/sghATk1cZWvXKjP3yiurto0dq6wCIx0wMlKt\nhvb445xMSiI7O5tx48aRkJCgBKCsTFWW9u9v0zPppy1bOHLkCHPs+uoI4MLSpQzcvbv69Ripnhdd\nRHp6OgsWLOCtt96isLCQEe3acZm9AJgzgM5mZNgM/qAEAGD//v2W+ykpKRT26QMHD6qgfHGxsnyu\nvx4uvlg9Li5W77lV7//S8+e594MPmOxtFop1MZin1kN6Ojz7rBp0//1vl7uuWLGCuLg4Bg0apDZY\nCUAXs5VWzQIwagC8WZXNVU2DkQLq6HiGAEjp/HwZGSro76gNhD0336x+C//4h7JS63pluR9+UK4f\nw803diy88Ybq4+UkRdjXND8BAPXFrgcBOPj557x94ABDgJTevZmbmsoTO3bQwyr758iRIwwZMoQp\nU6bwwQcfVDvG+++/T0BAAAUFBXz99dfc4G4x7okTVbbN6dOui18Mtm9Xg9GXXyqLaMMGeOklWLwY\nOWUKiYmJCCEoLi7m/PnzhIWFef4GrF2rBib7H9/ll9s+Ng9eR//7XwDi4+Pp16+fEoADB5Rv+E9/\nUj9QM89OmcLJkBDmLFpU7bQJO3dy0969SmSioqqeMKeALt64kfumTKGiooKbb76ZX375haSsLC5L\nS1PxE+O7kJYGI0dy9sABoqyPA/Tr14+goCCSrOIZKSkpjBgxQgU1d+1StQE33qhEzgWleXl88cEH\njPdkXWBralIM9uqrSuyPH1czTydpyEeOHGH79u3821okrAQgzOwmybZ30xkC4A2uFrjJyHA+e4+J\nURl1xcXO+2mlp6vXe5pa+/jjat2FNWvU5+iI5ctVf6O33vI8jdsoALMu8jMC/tu2NRgBaH4uIPA+\nDvDzz64DrVLCW28R+5vf0BHIW7GCiM2beS8oiFfMLiGAyspKbrvtNs6dO8fKlSurpVrm5eXx+eef\nM2/ePNq1a2dxJ7lk4kR1u2GDZ//Lpk3qS3z55Wp28vjj6ot55AhpaWnk5eUx3DxAe+UGystTLhDD\n/eMK8/ELN28mODiYIUOG0K9fP1JSUqg0/KZ2KbOOagAMgs0/rOyvvrJ9IjUVGRzM42++yZgxYzh0\n6BArVqygf//+7DL8vnv3qtuyMjh2jPLu3Tl//jyt7VwQJpOJPn36WASgtLSUo0ePqhqA+fNVjOP2\n290O/saxAM8WhrembVtlYXjqAiosVINWWJgSAfusJyuM79rNVqJrEYDoaMtvJ98+4F0TAQgJUW4y\nR0JmDOCO8KQYzF0NgD233grdu6veT8747DPVsdWNBWWDUQA2ZkzVtu7d1f/WgOIAzUsAatIOYuNG\n5SK57jrHwaLKSjXzu/tufggK4uHx42k9axadOnVixowZvPPOO5wz5/4uXLiQ77//nr///e8IIXjV\nrt3wqlWrKC0t5fbbb2fGjBn873//o8Bdhs/w4dCmjZrFXHMNvPCC8m07C2xt3KjSR619trGxcPQo\niebc+KuvvhrwUgA2bFAzzMmTycvLc5nSSXQ0dO9OaHIygwYNIiQkhH79+lFSUkLRxo3qczJWPjPj\nSgC6TZtGJXD6yy9tnzh0iAudO5OTl8cdd9xBz549AYiKimKHkYFiuIHMM+SiDh0s+9gTFxdnyQRK\nS0ujsrKyWg2AJzhdEcwdQkCXLqRt2sQzzzxTtf5ASYlqc2HP0qVKmB99VD12UtshpWTFihWMHTuW\nWGPBIaj6nbRpo/6Aypwc2+/k6dPeCwAol5S9AEjp2gLwVADc+f+tCQ6Gq6+uatfgCON38PTTVZ1K\n3WEUgFnH/oRQky17Afj8cxWX88NiVc1LALy1AA4eVNkCkZGq7cLrr1ff54UX4KOPOHbHHVxRUsK1\nRqUo8OCDD1JQUMCyZctITk7m8ccf5/rrr+eJJ55gxowZLFmyxObH9P7779OnTx9GjhzJzJkzKSkp\n4Qt3AarAQJV7f/vt6gf+pz8p/7OjoF9hoSqAGj/ednuPHlBYSPKWLYSEhHDZZZeZ3yYvBGDtWpXx\nM3o0EydO5KKLLuLrr792urscPpxu2dnEm7skWlbMMjqmWvljS0tLyc7OdioAg8eN4xchCLAPUKam\nkhURAcC4ceMsm6OiovglPx/Ztm1VJpA5AyjP/B1xJACDBg3iyJEjFBYWWoLBNRGAwMBAhBDeWwCA\n7NaN3L17+dvf/sZtt91GeXm5mpwMGFAVAAUlxgsWqBmoMat3IgD79u0jOTm5KvhrkJOjgrHBwRAS\nQnloKNFQ5QarrFSDsSeuR3scCcC5cype4k4AXE3gHLWRcEdMjGpn4WzSlJurLO2ePdVkz5PfhVEA\nZu9qHjtW/d9GBtR77ym34V//Cq+84t111wHNSwC8sQByc9UPKyhIDZqTJ8Of/2y7Fm9Cgvrgpk/n\nWZOJ0LAwpk6danl61KhRjBo1ildeeYU5c+YQERHBokWLEELwyCOPUFBQwDvvvAPA0aNH+f7775kz\nZw5CCMaMGUP37t09cwMNHw6vvaZcVSdPqiUFFy2q3ido2zblCrBfdMQ86zuVkMDQoUPpaF6w3uNA\nsJRKACZM4FxJCbt37yY3N5drr72WefPmObRizsTG0quykjEDBgDKx94SaHn8eDX3j1GT4EwATCYT\nh2Ni6HjiRNUsypwC+nNZGR06dLDM/kEN7hIoj4ursgDMApBjduE4swBAZXgZaafWbSA8RQiByWSq\nkQCcDQ+nQ1kZl112Ge+//z5PXXmlsr7S0uCqq6pmx6tXq8K2P/xBuR7AqQB88MEHBAUF8Rv7qmSj\nq6ZBdDTRwF7DbZabq4SmJhaAUdVsPet1lQIK7juCFhSoSY63AhAdrQZ/R5XOoP7P2FhYuVJZPLfd\n5nq2bl0AZo/R92n7dnj3XXWsiRNV4sAf/whbt1JZWclxHzX9a14C0Lq1Cg65U/ALF9TM32ht26OH\nGlADAuDpyciHAAAgAElEQVTOO9WHn5+v+sF06kTZf/7Dx598wtSpU4kwzzgNHnroIQ4dOsSuXbt4\n6623VGERMGLECMaNG8fLL79MRUUFy5cvB7DMwoQQzJw5k3Xr1uHV0pidO8N996kvs3kG/s0336hZ\n26ZNStDsq0/N7pbi5GTi4+MtLYw9tgAOHlSpnpMnWwaHFStW8Nhjj/HOO+8wePBgEoxiODNJ5gyo\ncebskpiYGCa2aoWQslrKrLMaAGuKBw2idXk5ZUY9QFYWFBWx/fRpxo0bZ2khDVWDe1GfPirofOGC\nGkBDQzllDh7axwAAS3ZMUlISKSkpdOjQgVY1TCYICQmpkQD8UlxMZ+Cj5ctZuHAhozZvJj84mJJP\nP1WTk8mT1XfzxRfV5/rrX6sYQLt26jNywMaNGxk3bhwx9sF7OwEIbNeODkFBVQLgbRGYNd26qWCu\n9SIzhgA4+5zduYA8rQGwx5VnQEq1PTpaWdbPP68SKB59VE0AHS1QY10AZs/QodCihco8mjtXifbq\n1coS6NGD0qlTue7ii7nssssoKSnx7v+oAc1LAAIClC/T3cz2vvtg82Z4550qxe7WTX34GzZULXxx\n/Dh88AHrfvyR3NxcZs2aVe1Q06dPp3///tx1113ceOONNs898sgjHD16lM8//5z33nuPyy+/3MYH\nO3PmTCoqKljlbbXilVeqH/zy5VRWVnLzzTdz9913K///yJHVMyjM5+xQUlIzAbBq/7DHPKMeM2YM\n8+fPZ8uWLQghmD59OoVWVaTfmWdbPawGgMnGrHvECJvDeyIAra66Su372WdqgzkDKOHMGRv3D2BJ\n8TzTrZsK/iYnW1JA88zWiiMLoEePHoSFhZGUlMTBgwdrNPs3qKkFkJCeTiDQEXho0iSmAgvLyrjr\n88/hv/9Vee1jxihr76GHqrJWzHEeeyorKzlw4ABDhgypfjI7ARDR0XQJC7OselajIjADR7UA7iyA\nVq2UO8rZ79fTGgB7XC0UX1SkJgiGSBjrN7/wgkqiaNNG/dYuvVSNCa+9pmIv4FgAgoPVbzApSYn1\nF19AWBgpWVk81KUL5WfO8PTPP/OPp56yxIrqk+YlAOC+F/jBg7BkiTKdzbPx2267jZEjRzJp1Sr2\nt2tH+T33wIcfsumKK/jPTz/x0ksv0aZNGyY7yIAJDg5m3759vGXk3FsxdepUevToYbES5syZY/P8\noEGDGDBggGduIGuCguCmm+DLLzm2bx/5+fns3bYNmZhY3f8PEBlJaXg4PVCWiclkIiIiwqkLaOHC\nhXz++edVG9auVdW2PXuye/du2rVrZ3EjXXLJJaxYsYKTJ0/yj3/8w/KS75OTyTSZCLTKKx8pBIcD\nA6t+bGYMAejk4ofdb/p0ioCC9evVBnMNwCGoJgDG4J5lvkb27LG0gT5rntE5EoCAgAAGDhzI/v37\nSUlJqZH/38BkMnkdBC4qKmKjsWDNiRNqEAoLI+PGG9m0aZNKAlixQrU4iIys6lwKTgXgyJEjFBcX\nW9xbNjhwAbUNDGTfvn0qAF1bCwBsBcAYwI3PxR4hXFcDu7MgnOFqnWDrTCjjGlauVO7W1avVpPDX\nv1bPffSREojnn1fX6WyC8Ic/wIMPquyi0FDef/99Bg4cyDs//sj6GTMYfeECtx44oKqx65nmJwAx\nMa4tgDffVCptbk+clZXFsmXLKC4u5lxhIb9v2ZJSKdkcEMDE777j/vvvZ8OGDcyePdupYgc5Wes1\nMDCQhx56iPT0dEJDQ5k+fbrN80IIZs2axZYtWzjhbfXn7NlQWkquOW9+HCAqKpwuOn66RQt6BQTQ\nr18/AKKjo51aAPPnz6/KFy8pUZaFWfz27NnDsGHDbFwul1xyCbfffjsvvfQSycnJVFZWsmvXLk51\n6aLy5830OXuWHRUVnLFbe9Z+KUhHdO/Vi33BwbQwApSHDlEeEEBuixbVZrfG4J7eooUyx/fsUf5y\nNwIASpQTEhLIycmptQB4awFs3ryZw0bxWEICvP8+zJ1Lt4sv5sSJExQVFanuomvWqEHK2h0ZG6sG\nW7tApxHQtRR/WeNAAFqVlVFUVMThw4fddwJ1haMVzjIy1CzfVVWxq2rgmrqAXFkAxjbr715AgGoj\n8atfqXFi0SKVHXTmjLqGdetUWrCzAXzKFLW0qLnIc+HChQwcOJDU1FR+vXKlEpGvvlLxjHqm+QmA\nq4ZwxcUqMDNtmspTBraZU7befvttduzYwfq0NMJPnODy4mKKS0o4deoUhw4d4sUXX6zR5dxxxx1E\nRUUxbdo0h/5kIy/7I29b144cCb16EbVmDYGBgczu1IkLgHRklgJpFRX0DQmxLGDuTADKy8s5ffo0\nu3btUumtW7eq9+2aaygtLeXAgQMMs17L1cyzzz5LREQE999/v6qgLSyk0linNz8f0tOJyM9nJ9V7\nAtkvBekIIQRZ3brRNSdHmeyHDnEyOJiRl1xSTYCNwf1sfj4MGaJiJcXFFgEIDw8nODjY4Xni4uIs\nrixfu4DWrVtHtlE9bnSy/P3vLUJ0yFj7YPLk6vUY3bur98WuyZ8hAAPMwXgLxcWqIM9OAEKKixGo\nzCFOnVLWphOxdIlR02DvAnLnvnEnAK1bK1H3BlcWgCMBcIYQ6vqvusrjQq/09HR2797NrFmzLPFB\nXnihagGpeqbWAiCE6CqE2CiESBZCHBBCPORgnyuEEPlCiJ/Mf085OpZPcNUQ7sMPVfD03qrlirdu\n3UpYWJjtoNa5M4SEEBISQrt27ejdu7fTAcMdERER7Nmzh9cdpZgCvXr1YtCgQXznbb8fIWD2bHoc\nO8YVF13E5JAQEoAfDxyotmt5eTl7CwrodOGCJbshJibGoQvo1KlTSCmpqKhg+/btavA0meCKKzhw\n4ADl5eUOBaBt27b861//YuPGjZYma9GTJqkn9+xRX3ggAccC4Mr/byBHjiRESgq2bKEiJYUDpaXV\n3D9QJQBnzpxRP1SjZYRZABwFgA2sZ8q+tgDWrVvHsCuuUDP7M2dUlWmPHpbrcNVMz4jz2LuB9u/f\nT2xsbLXkBesqYAvR0QgpaSOECgQb1ec1cVUYy0d6KwCuLHhvawAMXK0W6I0A1IA1a9YAcJ11FbLJ\nVG+tauypCwugHPiDlHIAMBq4TwgxwMF+W6SUQ81/f6+D89YMZw3hpIT//Ef1LrcaNLZu3cqoUaPq\nNSDTvXt3l9kko0ePZufOnVXdRT1l9mwCgPtDQog5doytgYG8//771XZLTk7mUHk5pooKi1nvzALI\ntKom/f777+Gbb1RVcXi4JQB88cUXO7ycefPmER8fz1dffUWLFi3oYqTM7toFO3cig4JIDg62GciO\nHz/ODz/84DhIaUf7668HIP3TT5GpqQ79/wBhYWGYTCbl7rEWq969ycvLc+r+gapU0KCgIHrYFat5\ng7dZQCdOnODnn39m0qRJVe4Tc4FX7969EUK4XhjHEAC7TKCkpCTn7h+wFQBz8Hx4bKwSgJpUAVtj\nrgVISEhQNSOe5PC7sgDMr9+wYQNLjUCsJ7haLdCqI2p98OWXX9K9e3cGmpvt+ZpaC4CUMlNKudt8\n/xzwM+ClE86HxMQoU7ioyHb7zp1qJnrvvZYipMLCQvbs2eNwEPElI0eO5OzZs1UmvodkRkSwE5iS\nnIyorKRs3Dg++ugjyqwak4F5BTDjgXmG6MwCyDAH2lq2bMkv69apDBpz5fCePXuIiIiwybm3JjAw\nkDfeeAMhBBdffDGBHTqowczcSVMMGUL3vn1tBODvf1dzhUeNalYXDL72WjKAkG++IaikhDQhGOWg\nC6sQgqioKFsBCAyE7t05e/asSwHo0KEDbdq0oWfPnjW2+sB7C2CdOdNq8uTJKpB/002Waw8LC6N7\n9+6uBcBBLcCFCxdISUlxHgCGahYAwGUDB/Ltt99SnpFRawGQJ04wa9YsbpoxA+mpCygvz6a5ngVz\nG4iHH36YefPmcdKbdYideQaMbXbNAeuCkpISvv32W6ZMmeLSvVmf1GkMQAgRCwxDWfL2jBFC7BVC\nfC2E8I/cgfNisNdfV6b1LbdYNiUkJFBRUeF3ATAGMftcenfs2bOHFUBQWRmYTMQ/8AA5OTmsXbvW\nZr/ExESyjdRQ8wARHR1NQUFBNbEwLIBf//rXtDc6cF5zDQC7d+9m6NChLrMX4uPjWbx4MU8Z3VGH\nD1eFdomJMGoU/fv3twhASkoK7777Lvfccw/djKwRF0S0asXPERHEGsV6vXvT0okf1SIAcXFq8O/W\nDYKD3QqAEIIpU6aomXgt8DYLaO3atXTu3Fn56l97rdpyhm7XRg4PV999KwE4ePAg5eXlXgvAzEmT\nKCoqoujw4ZoFgA26doWMDI4fPoypsBBx4YJnAgDVB+uKCsjKIickhP3791NeXs5rr73m+bU4yw7M\nzVUuIieJHLVh06ZNFBcXM8Wuw60vqTMBEEK0BD4FHpZS2pd+7ga6SymHAK8Cn9u/3uo4dwkhEoUQ\niV4VQHmKo6KPnByVNTFnjk3mxNatWwkICGCMk8CprxgwYAAtW7b0WgB2797NSkAGBMDo0Vx1/fXE\nxMTYuIGOHDnCunXriDbcNubB08i4sc/IycjIQAjBb37zG66qrKSkXTvo14+Kigr27t3r0P9vz9y5\nc7nKnLfP8OHKB3/uHIwcSb9+/Th8+DClpaU89dRThIWF8bgHawsb5PXrZ/lSt3ch3BYBCA1VvZ7M\nrSjcxQAAli1bVq2Pk7d4YwFUVFTw7bffMmnSJKczxb59+3Lw4EHXbsLYWBsXkNHXyGMXkPk70TMy\nkklXXUVofj7ltfGNd+uGqKxkVJcujDbcWp7EAKC6G+jUKaioYKc5E+jSSy+1tP72CBcuoIqoqJqv\nkW3G0efy5Zdf0qJFC65wkpnnC+pEAIQQwajBf4WU8r/2z0spC6SUheb7a4BgIYRDp5qUcpGUMl5K\nGd/WUPu6xJEF8M47qm3CPffY7Lp161YGDx5c42rPuiIwMJD4+PgaWQAte/VCLFgATzxBcHAwN998\nM6tXryYzM5Mnn3yS/v37k5mZybxHHlGzKysXEFRvB5GZmUm7du24fMwYrgSSunYFITh06BDFxcUe\nCYAN1n3tR42in1lMPvnkEz7++GMeeeQR2nkxywwz9zG6AAwwu6Yc0aZNG0vKJ59+qlL5wG0MoK7w\nRgASExM5e/asS6ujb9++FBYWWlx0DrGrBUhKSiIoKMhxMDsnRwVqrcXQavL0l/vuIwTY5Y2bxY5f\nzA0DH7rhBm41d7U96W51LmfVwOb/+5t9+xg3bhzPPvsseXl5LFu2zLOLceECOl5URHx8vEqzrQH/\n93//x9ChQ23ESErJl19+yZVXXul6zY96pi6ygATwNvCzlPIlJ/t0MO+HEGKk+bx+WJ2d6hbAmjWq\nH/7ll6vmTWbKy8vZsWOH390/BqNGjWLv3r1elYfv2bNHBWQffFClpgG33HILJSUl9O7dm3/+859M\nmzaNlJQUte5AbGw1C8A+EJyRkUHHjh2JPHCACOBz8/UYAeAaC0CrVtCnj6UO4YEHHiAqKoo/musx\nPCV2+nQqgSPAJWYxcITFAgDVMqFzZyoqKigoKGhwArBu3TqEEFxpvdCOHS6XxjTo3l1ZAObZaFJS\nEn369HGc4JCTo/ze1v3vIyOVKJw5w+Xmz2nVli1VXUnNfPrpp6xcudLt//WWuVXJlCFDmGR2Q33k\nbnlTswBcsG58B5YagO3HjzNjxgzGjBnD6NGjWbhwIRWerMDlLDkkN5dcKcnNzfUusGzF+vXr2bdv\nH7///e8t2w4cOMCxY8f86v6BurEAxgK3AhOs0jyvFULcLYS427zPdCBJCLEXeAW4WXqd0lJHmGe2\n5zdvVgUZ112nvtgLFtjstnfvXoqKihqUAJSVlVkGWnecPXuWI0eOVBuQR44cyYgRI+jXrx9btmxh\nxYoVdDEW6OjRwyYGANUFIDMzU1Xkfv01FQEBvJWaSmlpKXv27MFkMlXPJ3dHu3bKFzxyJAQEWHLr\n8/Ly+POf/0ykB/31rek7YgQ/BQRwODycDuZaDkfYCICZPHN7Cl8IgDdZQFu2bGHo0KHVe/VY4ZEA\nxMaqwj1zppfTDCCoXgQGVa1UcnMR5jYQe7Oy+NKqDfeCBQuYPn06v/vd71SnUiccPXqUJeZYVFh2\nNlHm9tyvf/aZ6wHbfE3P/ulPnLdeVN4sABnAtGnTANVqJTU11eb6XB7XWC3Qmtxcss0Ct2DBAs/E\nxI7k5GTCw8NZvHgxq1evBuAr89oV1157rdfHq0vqIgtoq5RSSCkHW6V5rpFSvimlfNO8z2tSyoFS\nyiFSytFSyu21v/SakVVaSiUQtmgRJevXU/Dkk2qJNruBcuvWrQCMtW+c5ie8DQT/ZG6xYC8AQggS\nEhLYtWtXdXEzfMSVlU5dQIYFwDffcGbAAHJKS/nxxx/ZvXs3gwYNqllmzCefqJWrUNlFXbt2pWPH\njtx///1eHyowMJBv7r+fo27iBlFRUeTl5dn8oA1BcBcDqAu8sQBOnTrlNgjeuXNnwsPDPU4FLSws\n5PDhw44DwOBYAKAqWGoWkcCOHXnuueeQUvLPf/6T3//+9/Tp04f8/Hx22rfntuKVV16hJDCQitat\nVS1ARgalEREcychgvdHOwxHmiUnFqVO2q5elp1MO9Bk3ztIy5MYbb6Rbt24ssJvcuTpuNTdQbi6n\nysuJjo4mLS3NMoB7SnZ2NtnZ2TzxxBMMGTKEO++8k1OnTvHll19y8cUXe1TfUp80u0rgFxYsYBWw\nITaWXuXltHvuOe59+GFOGWXtZrZu3UpsbGzV7NjPdOrUiS5dungsAK5cMk5Tznr0UCmymZkOLYCK\nigpOnz5N35YtYd8+Wpib223evNnSAqJGjBqlSuvNvPHGG3zyySe08Lai08zjL7/MPR4IAEB+fr5l\nm7s2EHWJN1lAubm5LttggPpM3WYCWaWCJpsXQPFaAMwWgNEIbtq997Jt2zZmzJjBk08+ya233mpJ\nnvjmm28cHjo/P58lS5YwY8YMAo0WFenpBMfGEh0dbWmR7ggZGMgZIWgnBPPnz7dkjOUlJ5MFTL/p\nJsu+QUFBPPTQQ2zevJldVi1HHOIoNnjhAhQUkFFayk033URsbKzXFf/G+zxs2DBWrFhBQUEBs2bN\nYvv27bbFX36iWQlAdnY2b7zxBv+75RYmHDnC9wcPMmfOHJYsWcKECRMsLgApJVu3bm0w7h+DUaNG\neSUAnTp1qiov9wRjhnjkCC1atCA0NNRGAE6fPk1lZSXx5m3h06YxaNAgVqxYwZkzZ2ouAHZcd911\n9W55WdpBWLmBfOkC8sYC8EQAwINUUCsBcNkDCDyzAITg5vvvJyoqilWrVnH33XezdOlS2rZty6hR\no6qlGxu8/fbbnDt3TvnEu3VT/YAyMgjo3JlbbrmFzz//3GkjwjNnznBaSsb27Ut4eDj33HMPUkpy\n9+0jnSr3j8HcuXOJiIjgpZcchidt/y+wtQDMGXDppaW0b9+ehx9+mG3btnmVjGEIwMCBAxk4cCDz\n589nw4YNVFZW+t3/D81MAF566SXOnz/PX//6V0C1WVi0aBFr167l0KFD3HjjjVy4cIHDhw+TlZXV\nIAXgyJEjHq0PUKMZuVHZ6qQYzMgw6XP4sErXGzSIyy67jJ9//hmoQQDYjzgSAF9bAJ4IQHFxMSUl\nJR4LwNGjR21949a0aqVm8MeOkZSURFhYmONqZik9E4CYGFq2bs3ixYt56aWXeP311y01IJMnT+ZH\nc5t020NLXn/9dS699FK19rSxMpi5COz222+nrKyMDz74wOG/kJqaSg7QOTiY+fPns2nTJpYvX45M\nT6c0OtrShdYgMjKSefPmsXLlStXAzhmO+gGZrz0HFRO74447iIyM9MoKOHDgAK1atbK4eh588EGu\nuuoqunbtalkNz580GwHIzc3ltddeY8aMGZZME4Px48ezZMkSNm7cyLx589hiXvezIQoAuI8DFBcX\n8/PPP3s/IBszRKtMIOsfcGZmJuFAh59+gmuvBSG4/PLLAdUqefDgwd6dz4+4EoCGFAMw3n9PBUBK\nSarR28gR5lTQ/fv3M3DgQMdFe+fOqUpbdwJgTs+dNm0ajzzyiI1rcfLkyUgpq/nzN2/eTFpaGvPm\nzVMbunZVlb2ZmdCpE0OGDOHiiy9mxYoVDi8/NTWV00BkTg7zbr+d0aNHc9999xFTWkprJwkIf/jD\nHwgKCmL+/PnO3xdHHUHN93NR739ERAS/+93v+PTTTzlivTKgC5KTkxkwYIDlvQkICGD16tXs3r3b\nJ+2e3eH/K/ARL7/8MoWFhTzxxBMOn58zZw7PPPMM7733Ho899hhRUVFV69Q2EIYPH05gYKBbAdi/\nfz+VlZXeC0BoqOrFbpUJZG8B3AgEFherojmwrB/c12ySNxaMRWH8ZQF4mgXkrQCAB6mgZheQS/cP\nOBeA4mI1a3fhXhwxYgRRUVHV3EBvv/02kZGRVa4aI7gtpaUIbPz48ezdu9dhxk1aWhofAsGZmQQs\nXsybb74JRUW0BmKdTNg6derE3LlzWbp0qfOlFlu3Vi1gHFgAhgCASk8OCAjg5Zdfdvq/W3PgwIFq\nmXGhoaEuM7p8SbMQgLy8PF5++WWmTZvmPOgFPPnkk9x2222cPn2asWPHNgiFtiY8PJy4uDi3AuCu\nKZtLrGoBYmJiqlkAvwVkz56Whnnt27dn5MiRjHe00EwDxpkFYDKZCAsLq/fzm0wmKisr3aYVeiMA\nRgqtu0wgeewYWVlZrgPA4FwAQC176EIAAgMDueqqq1i3bp2lCjYvL49Vq1Yxa9asqgC/dXaT2U0S\nFxdHaWkpacbiN1akpqays2tXtY7uE08wpFMnXvzDHwBoZWfZW/PYY48B8Nxzzzm74KoAt4GVABgD\ndpcuXZgxYwZLly6t1ibFnpycHE6fPu19arQPaVgjXD3xyiuvUFBQ4HT2byCE4K233uKee+7hvvvu\n89HVeceoUaPYuXNnteIba/bs2UNUVBTdDZeON9jVAlgLQElKChMB8dvfWhrmgeoK6umMqKFg0xLa\njFEF7IvGXEbxlbtMIG8EIDw8nC5durgVAFFcTAxuMoDAtQAUF7ttBDd58mQyMjIsAecPPviAkpIS\n7rzzzqqdrAXAbAEY12W8zprU1FR6X3QRvPKKWjDl8ceZZ+TSu0ip7NatG7/97W9ZsmSJTUfbav+b\nCxeQwYwZM8jPz7e4ip1hxMb81enTE5q8ABw7doyXXnqJ66+/nqEeLNJgMpl4/fXXudpFGwF/MmrU\nKPLz8zl48KDTffbs2cPQoUNrNpAZaXnl5cTExHDmzBnLLHWg0fzNbunKkJAQp6ueNVTCwsIICQmp\nZgH4wv0DVQLgzg3kjQCA55lAsbjJAALnaaAGbgTAaF1hpIMuWbKEoUOH2lqmHTtWVRubBaB///4I\nIZwLQO/eMGCAWvP47bfBWJ7UTR+hv/zlL5SXl/P888873sF+rYHcXMqDgijG9v032jd88cUXLs93\nwLz2hrYA/ERZWRkzZ86ksrLSfRpYI8FdIPj8+fP89NNPNc8w6NFDdVZMTyc6OhoppUqPlJLLjh5l\nb5s2VemijRz7amBPGsHVFd4KQBsP2xEbAuC00N782Q1s2dJ5pbQnFgC47QTapUsX4uLiWLt2Lbt3\n72bPnj22s39Qg3/nzqrK2Hy88PBwevToYRlADfLz88nOzlYCAPDUU0qEDOvTTVFVz549mT17Nm++\n+SanjQXt7f83OwugKDSU0NBQm5qU8PBwrrzySlavXu2y+V5ycrKlsLGh0qQF4Omnn2bHjh0sXryY\nXr16+fty6oR+/foRERHBDz/84PD5xMREysrKap5Hb1ULYFMMtn073UpLSWzA5qy3OBIAX1kAIeal\nHT0RgIiICI8XJOrbty/5+fmOBziwWACj27d3biHm5Kj2x46aIFoLgAc1JpMnT2bLli28+uqrhISE\nMGvWrOo7deumjmVlRcbFxVWzAIyYgEUAWrVSC7CDWj7Rg6aNjz/+OCUlJY4nhPYdQXNzKQgOJjo6\nutp7NXXqVI6as6mcYQSA/dXr3xOarACsX7+e+fPnc+edd3KTVXVgYycwMJBx48axadMmh88baxhf\ncsklNTuBVS2AdTuIynffpRBIHz26ZsdtgNgLgK86gYJ3FoCn7h9wnwmUXVZGHjDcVRaKUQPgaOCq\ngQBcuHCBpUuXMn36dMfv73XXgXk1N4O4uDgOHjxo8/4Y6a02k7nZs+Gyy8DD9Zn79u3LDTfcwLJl\ny6rP3o2OoMb23FzyAgIcvv/GIi6uWkMYKaANmSYpAFlZWdxyyy3079+/0QUnPWH8+PH88ssvDoNZ\n27Zto0+fPtS4lba5vbO1BZCXkQEff8wqIKaJuH/AvxaAvwTgu+++4ygwMDcXli+H7dtVTv+xY7Bq\nFTz2mFrn2dk5w8LUH3gkAJdeeqklq2ru3LmOd/rzn+HNN202xcXFUV5ebhPrcigAQqiOvk6qjh0x\nfvx4srKyqrfOjo5WzfLMbarJzbUUgdnToUMHRo0a5TQOcObMGbKyshp0ABiaoABUVlYyZ84cCgoK\nWLlyZY37yTRkjJRLeyugsrKS7du3166Ngsmk3AQvvUS/f/2La4CIb74h4Nw5lkK1SsvGjLUAVFZW\nkpeX5/MYgCdZQJ76/0Flu4SGhjoVgPXr1/N9SAhhaWlw660wdix06KBcf7/5jeqK26EDPPKI85MY\nA6IH6zSEhoYyefJk+vbtayka9ARj4LR2A6WmptKxY8fq9SbGamceYsTHEhMTbZ+wLwbLyeFURYXT\nnP3rr7+exMRE0u1bU1PVAkJbAD6moKCAkpISXnnlFZc5/42ZYcOGERkZycaNG222p6SkcObMmdr3\n0Vm1Cm6+mZYbN7IGuPTttylu147vwdJpsSlgvSjMuXPnqKysbPQWQEBAABdddJElBdEaozL3+ylT\nEC+lsIgAABWmSURBVMXFKpf/q69USuXrr6t1sc+dU0t0OputgxKAyEgwxzHcsWzZMkuDOE/p27cv\ngYGB1QTA4v+vBUOGDCEwMLC6AFj3A5ISzpwhs7TU6fs/depUAIftpq17ADVkmpwAtG7dmo0bN1bP\nNmhCBAYGcvnll7Nhwwab7Yb/v9YtLIYPh8WLISuLGwIC2B0Xx45f/xpJ07MA8vPzqaio8GkVMNSf\nAIBqYb5x40ZLczuDgwcPcuLECbUcZ2go9O2rWno88IBaDW/ECM8G9ehorxaDb9WqldeVryEhIfTp\n06deBCAsLIyBAwc6twByciA/HyoqOOmiD1P//v3p1auXQzfQgQMHCA8Pb9AZQNAEBQDUANmQI+91\nwfjx40lLS+PEiROWbdu2bSMmJsZSEVpbRGgoO9q25Y3Ro9lmTrFztchKY8MY7PPy8nzaCRQ8ywKq\nqKggLy/PawG48847OX/+fLWGakZfHst6zDXlzjtVDn49M3DgQEsqaFFREZmZmXUiAKDcQImJibaB\nYGsLwGgEJ6VT8RJCcP311/Pdd99VW3s4OTmZ/v37N7huAvY07KvTOMWIA1i7gbZt28Yll1xSp+Jn\ntIPIzMwkJibG43TExoB1OwhfNoIDzyyAs2fPIqX0WgCGDx/OsGHDWLRokc0At27dOnr27EnPnj1r\ndtEGM2fCvffW7hgeEBcXR1paGsXFxdVTQGtJfHw8OTk5tr2BrC0AJ1XA9kydOpULFy5U63mUnJzc\n4N0/oAWg0TJo0CCio6MtAnD69GkOHTpU5330jXYQGRkZTcr/D44FoCG5gLytArZm3rx57N271+Lm\nKCsrY9OmTbWf/fuQuLg4pJT8/PPPlgyguhQAsAsEG8F2KwvAnQCMHTuWNm3a8LlRjYyyKDMyMhp8\nABjqSACEEFcLIVKEEKlCiD87eD5ECLHS/HyCECK2Ls7bnAkICLDEAaSUbN+uVtmsDwHIyckhMzOz\nSfn/oWEIgKssoNoIgNFwbfHixYCqHD937lyjEwBQ/nSHKaC1YPDgwQQHB9uuFBYUpILbXghAUFAQ\nN954I8uXL2fu3Lnk5OQ0mgAw1IEACCECgf8A1wADgJlCCHvpmwuclVL2BhYAz9b2vBqYMGECx48f\n58iRI2zduhWTyaQW2ahDDBeQZS3gJoS1APg6BlDfFkBkZCQ33XQTH374IYWFhaxfv56AgAAmTJhQ\nswv2A7169cJkMpGUlERqaipt27YlMjKyTo4dEhLCoEGDHAeC7VxA7gLYCxcu5NFHH+W9996jX79+\nvPDCC0DDTwGFurEARgKpUsrDUsoLwEfAVLt9pgLLzPdXARNFU4/S+gDrOMC2bduIj48nNDS0Ts9h\nuIBOnTrV5F1AAQEBtGzZ0ifnrm8BAOUGKiws5KOPPmL9+vXEx8f7TODqgqCgIPr3728RgLpy/xg4\nDQSbLQApBHm4f//Dw8N59tln2bNnD/379+ezzz6jRYsWNevG62PqQgA6AyesHp80b3O4j5SyHMgH\navat1ljo378/7du35+uvv2bXrl31so5uTEwM5eXllJeXN1kL4MyZM5ZGcL7K2vAkC6i2AjB69Gji\n4uJYuHAhO3fubFTuH4O4uDiLC6iuBWD48OGcPXvWdnUvKwvgfGgoIjDQY6sjLi6OzZs389577/Ha\na681+AwgaIBBYCHEXUKIRCFEoidr3zZnhBBcccUVfPbZZ7VrAOcC68GnqVkAYWFhhIaGWiwAX86O\nPbUAgoKCaOVBkzNHCCGYN28eBw4coKKiotEKwPHjxzlx4kS9WABgFwi2sgDOmUy0adPGq6y6gIAA\nbr31Vm6//fY6vdb6oi4EIB2wrnboYt7mcB8hRBAQiXKvVUNKuUhKGS+ljK9xP5tmxIQJEyyLw9S4\nAZwLrAWgqVkAUNUOwl8C4C4I7O0AZM8tt9xCaGgo4eHhjBkzpsbH8RfWgdS6FoC4uDhMJpOtAFhZ\nAHmBgQ1m6cb6oi5W8fgRuEgI0QM10N8M2Pd8XQ38FtgBTAc2SFeNtDUeY8QBatUAzgXWP4CmZgFA\nlQD4sg8QeG4B1NT9Y9CmTRseffRRSkpKGmUNh3U7l7oWAJPJxJAhQ6pbAEVFkJ5OrhC1fv8bOrUW\nAClluRDifmAtEAi8I6U8IIT4O5AopVwNvA28L4RIBc6gREJTB/Tu3Zt+/fpZVl+qa6x/AE2pCtjA\n2gLwZdl+cHAwUP8CAPDMM8/U+hj+onv37oSHh1NUVFTnAgDKDbRixQoqKyuVz954v9PSyG7RQguA\nJ0gp1wBr7LY9ZXW/BPhNXZxLY4sQgsTExHqb3Rk/gOjoaEvgsikRFRXFyZMnfe4CCggIICgoyK0A\n1Lpqt5ETEBDAwIEDOXTokFddUT0lPj6eN954g7S0NC666KKqauALF8gKDm7yLqAGFwTWeE94eLhl\nRlnXGJkxTdH/D/6LAYDKBPKFBdDYmT17Nrfeemu9HLtaINjq/XbVCK6p0LhW8tb4nMDAQKKiopqk\n/x+UAGRmZlJWVuZzATCZTE4FQEqpBcDMgw8+WG/HHjBgAKGhoSQmJjJz5kybdQVOV1TQq4m//9oC\n0Lhl7Nix9ZJi2hBo06aNZRD2ZRAYlAA4ywIqLi6m1EUvek3dEBQUxNChQ/nxxx/VBqv3210biKaA\ntgA0bnG27F1TwHrW35AsgNoWgWk8Jz4+nnfffZeKigoC7QRAxwA0miaMFgDNiBEjKCoqUstohoSA\nuR1Ic7AAtABomjVaADRGINjeDaQFQKNp4lgP+r6OAbjKAtIC4Dv69u1Ly5YtqzKBzG4fLQAaTRNH\nWwCawMBALr74YhsL4EJwMKVQL7UHDQktAJpmjT8tAFdZQIYANPUBqKEwYsQIfvrpJ8rKyqBzZ/Ja\ntKB169YEBTXtPBktAJpmjSEArVq1IjAw0KfndmcBRERENMr+PY2R+Ph4SktLSUpKgn/9i+fGjm0W\n1pcWAE2zJjQ0lNDQUJ/P/sG9ADSHAaihMGLECMBcEdyhA/vKypp8CihoAdBoaNOmjV9WytIC0HDo\n2bMnUVFRljhAc3n/tQBomj1RUVF+EQB3WUDa/+87hBCWJSKh+QhA045waDQe8MADD9R41a3a4M4C\naO6dQH1NfHw8zz//PCUlJVoANJrmwu9+9zu/nNddFlBzGIAaEiNGjKC8vJyEhAQKCwt1DECj0dQf\nziyA8vJy8vLytAD4GCMQvHbtWqB51GBoAdBo/IQzATh79izQPAaghkTnzp1p374933zzDdA83n8t\nABqNn3AmALoK2D8IIRgxYgR79uwBmsf7rwVAo/ETzrKAtAD4D6MxHDT9VtBQyyCwEOJ54FfABSAN\nuF1Kmedgv6PAOaACKJdSxtvvo9E0N0wmE2VlZUgpEUJYtmsB8B9GHACax/tfWwtgPRAnpRwMHAT+\n4mLf8VLKoXrw12gURpsHeytAC4D/sLYAmsP7XysBkFKuk1KWmx/+AHSp/SVpNM0DLQANj3bt2tGt\nWzfCw8MJDQ319+XUO3UZA7gD+NrJcxJYJ4TYJYS4qw7PqdE0WlwJQFBQkF+K0zQwevRoOnXq5O/L\n8AluYwBCiG+BDg6e+quU8gvzPn8FyoEVTg4zTkqZLoRoB6wXQvwipfzeyfnuAu4C6Natmwf/gkbT\nOHElAG3atLGJC2h8x8svv2xJxW3quBUAKeWVrp4XQtwGTAEmSimlk2Okm29PCyE+A0YCDgVASrkI\nWAQQHx/v8HgaTVMgJCQEcCwA2v3jPzp06ECHDo7mvE2PWrmAhBBXA48C10spi53sEy6EiDDuA5OA\npNqcV6NpCjizAM6cOaMFQOMTahsDeA2IQLl1fhJCvAkghOgkhFhj3qc9sFUIsRfYCXwlpfymlufV\naBo9hgDY9wPKy8vzy/oEmuZHreoApJS9nWzPAK413z8MDKnNeTSapogzCyAvL4+4uDh/XJKmmaEr\ngTUaP+FMAPLz84mMjPTHJWmaGVoANBo/4UgApJRaADQ+QwuARuMnHGUBFRYWUllZqWMAGp+gBUCj\n8ROOLID8/HwAbQFofIIWAI3GTzjKAsrLU70UtQWg8QVaADQaP6EtAI2/0QKg0fgJRwKgLQCNL9EC\noNH4CW0BaPyNFgCNxk84ygLSFoDGl2gB0Gj8hKMgsLYANL5EC4BG4yecuYBMJlOzWIxE43+0AGg0\nfsJZEDgyMlKvBaDxCVoANBo/ERgYiBCimgWg/f8aX6EFQKPxE0IITCaTQwtAo/EFWgA0Gj8SEhJS\nzQLQAqDxFVoANBo/YjKZqrWC0C4gja/QAqDR+BF7F5C2ADS+RAuARuNHHMUAtAWg8RW1XRT+b0KI\ndPN6wD8JIa51st/VQogUIUSqEOLPtTmnRtOUsBaAsrIyiouLtQWg8Rm1WhPYzAIp5QvOnhRCBAL/\nAa4CTgI/CiFWSymT6+DcGk2jxloACgoKAN0GQuM7fOECGgmkSikPSykvAB8BU31wXo2mwWOdBWT0\nAdIWgMZX1IUA3C+E2CeEeEcIEeXg+c7ACavHJ83bNJpmj3UWkNEHSFsAGl/hVgCEEN8KIZIc/E0F\n3gB6AUOBTODF2l6QEOIuIUSiECIxOzu7tofTaBo01i4gbQFofI3bGICU8kpPDiSEWAx86eCpdKCr\n1eMu5m3OzrcIWAQQHx8vPTm3RtNYMZlMFBYWAtoC0Pie2mYBdbR6eAOQ5GC3H4GLhBA9hBAm4GZg\ndW3Oq9E0FbQFoPEntc0Cek4IMRSQwFHgdwBCiE7AEinltVLKciHE/cBaIBB4R0p5oJbn1WiaBNYC\noC0Aja+plQBIKW91sj0DuNbq8RpgTW3OpdE0RRxlAbVq1cqfl6RpRuhKYI3Gj9hnAbVs2ZLAwEA/\nX5WmuaAFQKPxI/YuIO3+0fgSLQAajR+xDwLrALDGl2gB0Gj8iLYANP5EC4BG40e0BfD/7d1fjFxl\nHcbx71Oms9qVbkGwVkssRlrSC7vApkKsRiqS0hiaGII0XmBC0hsuILExNCQmXnrhHy6MSeO/G1OJ\nKEKqQUolIXrRuoVWF9pS0BpaoQvGFatJ49CfF+c9Om62u21nMu/bOc8nOZnzZzrzZE67z77vOdu1\nnFwAZhl13wXkEYANmgvALKN2u83Zs2fpdDoeAdjAuQDMMmq32wCcOXPGIwAbOBeAWUZ1AczMzNDp\ndDwCsIFyAZhlVBdA/T/fegRgg+QCMMtodgF4BGCD5AIwy2hkZASA6elpwCMAGywXgFlG9QigLgCP\nAGyQXABmGbkALCcXgFlGvghsObkAzDLyCMBycgGYZdQ9Ami1WixZsiRzImsSF4BZRt13AY2NjSEp\ncyJrEheAWUbdU0Ce/7dB6+l3Akt6FFiTNpcBMxExPsfzjgP/AN4BOhEx0cv7mg2LugBOnz7N6tWr\nM6expun1l8J/vl6X9HXg7/M8/daIeKuX9zMbNnUBgO8AssHrqQBqqiYu7wY29uP1zJqiuwB8B5AN\nWr+uAXwCOBURx85xPICnJR2QtG2+F5K0TdKkpMn63mizYeURgOW04AhA0jPA++c49HBEPJHWtwK7\n5nmZDRFxUtL7gD2SjkTEc3M9MSJ2AjsBJiYmYqF8Zpey+i4g8AjABm/BAoiI2+Y7LqkFfA64aZ7X\nOJkepyU9DqwH5iwAsybxCMBy6scU0G3AkYg4MddBSaOSLq/XgduBqT68r9klz9cALKd+FMA9zJr+\nkfQBSb9Mm8uB30g6BOwHfhERT/Xhfc0ueYsXL/7vugvABq3nu4Ai4otz7PsLsDmt/xFY1+v7mA2j\nRYsW0Wq16HQ6ngKygfNPAptlVk8DeQRgg+YCMMusvhPIIwAbNBeAWWYeAVguLgCzzOoC8AjABs0F\nYJZZXQBLly7NnMSaxgVgllm73WZ0dPT/bgk1GwQXgFlm7Xbb8/+WhQvALLORkRHP/1sWLgCzzDwC\nsFz68vsAzOzibd++PXcEaygXgFlmW7ZsyR3BGspTQGZmDeUCMDNrKBeAmVlDuQDMzBrKBWBm1lAu\nADOzhnIBmJk1lAvAzKyhFBG5M5yTpDeBP1/kH78KeKuPcfrN+XrjfL1xvt6UnO9DEXH1+Tyx6ALo\nhaTJiJjIneNcnK83ztcb5+tN6fnOl6eAzMwaygVgZtZQw1wAO3MHWIDz9cb5euN8vSk933kZ2msA\nZmY2v2EeAZiZ2TyGrgAkbZJ0VNIrkh7KnQdA0vclTUua6tp3paQ9ko6lxysyZbtG0rOSXpL0oqQH\nCsv3Lkn7JR1K+b6a9l8raV86z49KaufI15XzMkkvSNpdaL7jkv4g6aCkybSviHOcsiyT9JikI5IO\nS7qllHyS1qTPrV7elvRgKfl6MVQFIOky4NvAHcBaYKuktXlTAfBDYNOsfQ8BeyPiOmBv2s6hA3wp\nItYCNwP3p8+slHxngI0RsQ4YBzZJuhn4GvDNiPgI8Dfgvkz5ag8Ah7u2S8sHcGtEjHfdvljKOQZ4\nBHgqIq4H1lF9lkXki4ij6XMbB24C/gU8Xkq+nkTE0CzALcCvurZ3ADty50pZVgFTXdtHgRVpfQVw\nNHfGlOUJ4DMl5gOWAM8DH6P6IZzWXOc9Q66VVF8ANgK7AZWUL2U4Dlw1a18R5xgYA/5EuiZZWr5Z\nmW4HfltqvgtdhmoEAHwQeK1r+0TaV6LlEfF6Wn8DWJ4zDICkVcANwD4KypemVw4C08Ae4FVgJiI6\n6Sm5z/O3gC8DZ9P2eykrH0AAT0s6IGlb2lfKOb4WeBP4QZpG+66k0YLydbsH2JXWS8x3QYatAC5J\nUX0LkfV2LEnvAX4KPBgRb3cfy50vIt6Javi9ElgPXJ8ry2ySPgtMR8SB3FkWsCEibqSaHr1f0ie7\nD2Y+xy3gRuA7EXED8E9mTafk/jsIkK7j3An8ZPaxEvJdjGErgJPANV3bK9O+Ep2StAIgPU7nCiJp\nMdUX/x9FxM9Ky1eLiBngWaoplWWSWulQzvP8ceBOSceBH1NNAz1COfkAiIiT6XGaav56PeWc4xPA\niYjYl7YfoyqEUvLV7gCej4hTabu0fBds2Argd8B16Q6MNtVw7cnMmc7lSeDetH4v1dz7wEkS8D3g\ncER8o+tQKfmulrQsrb+b6vrEYaoiuCt3vojYERErI2IV1d+3X0fEF0rJByBpVNLl9TrVPPYUhZzj\niHgDeE3SmrTr08BLFJKvy1b+N/0D5eW7cLkvQvR7ATYDL1PNEz+cO0/KtAt4Hfg31Xc791HNE+8F\njgHPAFdmyraBauj6e+BgWjYXlO+jwAsp3xTwlbT/w8B+4BWqIflIAef5U8Du0vKlLIfS8mL976KU\nc5yyjAOT6Tz/HLiisHyjwF+Bsa59xeS72MU/CWxm1lDDNgVkZmbnyQVgZtZQLgAzs4ZyAZiZNZQL\nwMysoVwAZmYN5QIwM2soF4CZWUP9Bwit1gtwSWvaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc6b5a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 3.10370961025 \n",
      "Fixed scheme MAE:  2.22433025707\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 1.2334  Test loss = 6.4054  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 1.4662  Test loss = 5.4682  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 1.6154  Test loss = 0.0634  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 1.6104  Test loss = 0.3263  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 1.1793  Test loss = 1.4642  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 1.1875  Test loss = 0.0842  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 1.1850  Test loss = 0.1568  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 1.1845  Test loss = 0.1971  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 1.0717  Test loss = 0.4931  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 1.0708  Test loss = 0.1506  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 1.0301  Test loss = 0.9449  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 1.0367  Test loss = 1.7401  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 0.9814  Test loss = 1.1056  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 0.9851  Test loss = 3.8945  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 1.0972  Test loss = 3.2832  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 1.1695  Test loss = 5.8445  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 1.0734  Test loss = 1.7551  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 1.0951  Test loss = 0.5238  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 1.0963  Test loss = 1.0026  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 1.0969  Test loss = 0.7342  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 0.9706  Test loss = 3.5554  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 1.0651  Test loss = 1.8298  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 1.0887  Test loss = 1.2120  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 1.0988  Test loss = 2.4510  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 1.0500  Test loss = 0.5459  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 1.0466  Test loss = 0.1752  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 1.0467  Test loss = 0.3292  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 1.0472  Test loss = 3.3407  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 1.0312  Test loss = 0.8059  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 1.0257  Test loss = 0.1109  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 1.0178  Test loss = 3.3560  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 1.0960  Test loss = 0.9410  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 0.9948  Test loss = 0.8984  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 1.0010  Test loss = 0.1486  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 0.8861  Test loss = 0.8633  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 0.8904  Test loss = 5.3772  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 1.0639  Test loss = 0.2362  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 1.0149  Test loss = 2.0137  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 1.0347  Test loss = 0.5349  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 1.0354  Test loss = 2.1037  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 0.7967  Test loss = 2.8612  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 0.8711  Test loss = 2.2179  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 0.9088  Test loss = 3.2455  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 0.9918  Test loss = 11.8993  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 1.7319  Test loss = 6.3281  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 1.9015  Test loss = 1.2009  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 1.9073  Test loss = 1.2773  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 1.9137  Test loss = 0.1098  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 1.7484  Test loss = 1.8880  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 1.7638  Test loss = 3.3089  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 1.8109  Test loss = 1.2374  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 1.8174  Test loss = 3.4653  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 1.8079  Test loss = 0.9496  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 1.8103  Test loss = 3.9186  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 1.8744  Test loss = 1.4252  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 1.8684  Test loss = 0.3631  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 1.7608  Test loss = 0.9447  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 1.7641  Test loss = 0.4607  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 1.7630  Test loss = 1.3717  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 1.7638  Test loss = 1.3281  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.7655  Test loss = 0.5949  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.7644  Test loss = 5.3200  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 1.8818  Test loss = 1.4052  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 1.8897  Test loss = 1.6542  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 1.8040  Test loss = 2.2651  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 1.8215  Test loss = 5.3535  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.9386  Test loss = 0.1951  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 1.9387  Test loss = 1.8719  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 1.7977  Test loss = 2.3479  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 1.8211  Test loss = 0.6487  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 1.8225  Test loss = 1.0307  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 1.8264  Test loss = 2.4744  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 1.7824  Test loss = 2.6733  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 1.8109  Test loss = 1.2042  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 1.8168  Test loss = 0.1156  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 1.8169  Test loss = 1.5282  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 1.7786  Test loss = 2.4011  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMxaygVgZtZS/wPwVaq29JJRQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd28b710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.77035942676 \n",
      "Updating scheme MAE:  1.93963486274\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
