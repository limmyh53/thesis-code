{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"1Q/16_units/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 500\n",
    "learning_rate = 1e-4\n",
    "batch_size = 5\n",
    "early_stop_iters = 10\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 16 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag1',\n",
    "                                       'inflation.lag2',\n",
    "                                       'inflation.lag3',\n",
    "                                       'inflation.lag4']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag1',\n",
    "                                   'unemp.lag2',\n",
    "                                   'unemp.lag3',\n",
    "                                   'unemp.lag4']])\n",
    "train_4lag_oil = np.array(train[['oil.lag1',\n",
    "                                 'oil.lag2',\n",
    "                                 'oil.lag3',\n",
    "                                 'oil.lag4']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag1',\n",
    "                                     'inflation.lag2',\n",
    "                                     'inflation.lag3',\n",
    "                                     'inflation.lag4']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag1',\n",
    "                                 'unemp.lag2',\n",
    "                                 'unemp.lag3',\n",
    "                                 'unemp.lag4']])\n",
    "test_4lag_oil = np.array(test[['oil.lag1',\n",
    "                               'oil.lag2',\n",
    "                               'oil.lag3',\n",
    "                               'oil.lag4']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 16 \n",
      "Learning rate = 0.0001 \n",
      "Epochs = 500 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 10 \n",
      "Learning rate = 0.0001\n",
      "Fold: 1  Epoch: 1  Training loss = 3.2582  Validation loss = 3.4933  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 3.2464  Validation loss = 3.4679  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 3.2328  Validation loss = 3.4393  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 3.2204  Validation loss = 3.4111  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 3.2075  Validation loss = 3.3834  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 3.1990  Validation loss = 3.3648  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 3.1911  Validation loss = 3.3466  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 3.1815  Validation loss = 3.3259  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 3.1712  Validation loss = 3.3023  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 3.1625  Validation loss = 3.2823  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 3.1541  Validation loss = 3.2621  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 3.1477  Validation loss = 3.2478  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 3.1417  Validation loss = 3.2346  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 3.1343  Validation loss = 3.2175  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 3.1250  Validation loss = 3.1955  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 3.1139  Validation loss = 3.1683  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 3.1063  Validation loss = 3.1504  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 3.0962  Validation loss = 3.1263  \n",
      "\n",
      "Fold: 1  Epoch: 19  Training loss = 3.0904  Validation loss = 3.1122  \n",
      "\n",
      "Fold: 1  Epoch: 20  Training loss = 3.0830  Validation loss = 3.0944  \n",
      "\n",
      "Fold: 1  Epoch: 21  Training loss = 3.0744  Validation loss = 3.0738  \n",
      "\n",
      "Fold: 1  Epoch: 22  Training loss = 3.0652  Validation loss = 3.0505  \n",
      "\n",
      "Fold: 1  Epoch: 23  Training loss = 3.0577  Validation loss = 3.0320  \n",
      "\n",
      "Fold: 1  Epoch: 24  Training loss = 3.0504  Validation loss = 3.0141  \n",
      "\n",
      "Fold: 1  Epoch: 25  Training loss = 3.0447  Validation loss = 3.0001  \n",
      "\n",
      "Fold: 1  Epoch: 26  Training loss = 3.0418  Validation loss = 2.9930  \n",
      "\n",
      "Fold: 1  Epoch: 27  Training loss = 3.0372  Validation loss = 2.9815  \n",
      "\n",
      "Fold: 1  Epoch: 28  Training loss = 3.0312  Validation loss = 2.9669  \n",
      "\n",
      "Fold: 1  Epoch: 29  Training loss = 3.0288  Validation loss = 2.9605  \n",
      "\n",
      "Fold: 1  Epoch: 30  Training loss = 3.0234  Validation loss = 2.9471  \n",
      "\n",
      "Fold: 1  Epoch: 31  Training loss = 3.0142  Validation loss = 2.9247  \n",
      "\n",
      "Fold: 1  Epoch: 32  Training loss = 3.0088  Validation loss = 2.9122  \n",
      "\n",
      "Fold: 1  Epoch: 33  Training loss = 3.0038  Validation loss = 2.9002  \n",
      "\n",
      "Fold: 1  Epoch: 34  Training loss = 2.9989  Validation loss = 2.8876  \n",
      "\n",
      "Fold: 1  Epoch: 35  Training loss = 2.9914  Validation loss = 2.8696  \n",
      "\n",
      "Fold: 1  Epoch: 36  Training loss = 2.9874  Validation loss = 2.8592  \n",
      "\n",
      "Fold: 1  Epoch: 37  Training loss = 2.9805  Validation loss = 2.8426  \n",
      "\n",
      "Fold: 1  Epoch: 38  Training loss = 2.9737  Validation loss = 2.8251  \n",
      "\n",
      "Fold: 1  Epoch: 39  Training loss = 2.9683  Validation loss = 2.8119  \n",
      "\n",
      "Fold: 1  Epoch: 40  Training loss = 2.9637  Validation loss = 2.8007  \n",
      "\n",
      "Fold: 1  Epoch: 41  Training loss = 2.9606  Validation loss = 2.7931  \n",
      "\n",
      "Fold: 1  Epoch: 42  Training loss = 2.9570  Validation loss = 2.7845  \n",
      "\n",
      "Fold: 1  Epoch: 43  Training loss = 2.9515  Validation loss = 2.7714  \n",
      "\n",
      "Fold: 1  Epoch: 44  Training loss = 2.9485  Validation loss = 2.7635  \n",
      "\n",
      "Fold: 1  Epoch: 45  Training loss = 2.9428  Validation loss = 2.7495  \n",
      "\n",
      "Fold: 1  Epoch: 46  Training loss = 2.9356  Validation loss = 2.7308  \n",
      "\n",
      "Fold: 1  Epoch: 47  Training loss = 2.9285  Validation loss = 2.7129  \n",
      "\n",
      "Fold: 1  Epoch: 48  Training loss = 2.9253  Validation loss = 2.7048  \n",
      "\n",
      "Fold: 1  Epoch: 49  Training loss = 2.9235  Validation loss = 2.6999  \n",
      "\n",
      "Fold: 1  Epoch: 50  Training loss = 2.9175  Validation loss = 2.6845  \n",
      "\n",
      "Fold: 1  Epoch: 51  Training loss = 2.9148  Validation loss = 2.6773  \n",
      "\n",
      "Fold: 1  Epoch: 52  Training loss = 2.9101  Validation loss = 2.6655  \n",
      "\n",
      "Fold: 1  Epoch: 53  Training loss = 2.9052  Validation loss = 2.6527  \n",
      "\n",
      "Fold: 1  Epoch: 54  Training loss = 2.8999  Validation loss = 2.6388  \n",
      "\n",
      "Fold: 1  Epoch: 55  Training loss = 2.8965  Validation loss = 2.6296  \n",
      "\n",
      "Fold: 1  Epoch: 56  Training loss = 2.8914  Validation loss = 2.6162  \n",
      "\n",
      "Fold: 1  Epoch: 57  Training loss = 2.8873  Validation loss = 2.6055  \n",
      "\n",
      "Fold: 1  Epoch: 58  Training loss = 2.8837  Validation loss = 2.5956  \n",
      "\n",
      "Fold: 1  Epoch: 59  Training loss = 2.8811  Validation loss = 2.5882  \n",
      "\n",
      "Fold: 1  Epoch: 60  Training loss = 2.8760  Validation loss = 2.5739  \n",
      "\n",
      "Fold: 1  Epoch: 61  Training loss = 2.8726  Validation loss = 2.5641  \n",
      "\n",
      "Fold: 1  Epoch: 62  Training loss = 2.8696  Validation loss = 2.5560  \n",
      "\n",
      "Fold: 1  Epoch: 63  Training loss = 2.8657  Validation loss = 2.5446  \n",
      "\n",
      "Fold: 1  Epoch: 64  Training loss = 2.8621  Validation loss = 2.5357  \n",
      "\n",
      "Fold: 1  Epoch: 65  Training loss = 2.8594  Validation loss = 2.5278  \n",
      "\n",
      "Fold: 1  Epoch: 66  Training loss = 2.8557  Validation loss = 2.5177  \n",
      "\n",
      "Fold: 1  Epoch: 67  Training loss = 2.8524  Validation loss = 2.5085  \n",
      "\n",
      "Fold: 1  Epoch: 68  Training loss = 2.8493  Validation loss = 2.5005  \n",
      "\n",
      "Fold: 1  Epoch: 69  Training loss = 2.8454  Validation loss = 2.4896  \n",
      "\n",
      "Fold: 1  Epoch: 70  Training loss = 2.8418  Validation loss = 2.4798  \n",
      "\n",
      "Fold: 1  Epoch: 71  Training loss = 2.8386  Validation loss = 2.4703  \n",
      "\n",
      "Fold: 1  Epoch: 72  Training loss = 2.8346  Validation loss = 2.4585  \n",
      "\n",
      "Fold: 1  Epoch: 73  Training loss = 2.8312  Validation loss = 2.4480  \n",
      "\n",
      "Fold: 1  Epoch: 74  Training loss = 2.8281  Validation loss = 2.4382  \n",
      "\n",
      "Fold: 1  Epoch: 75  Training loss = 2.8240  Validation loss = 2.4270  \n",
      "\n",
      "Fold: 1  Epoch: 76  Training loss = 2.8205  Validation loss = 2.4153  \n",
      "\n",
      "Fold: 1  Epoch: 77  Training loss = 2.8180  Validation loss = 2.4081  \n",
      "\n",
      "Fold: 1  Epoch: 78  Training loss = 2.8147  Validation loss = 2.3965  \n",
      "\n",
      "Fold: 1  Epoch: 79  Training loss = 2.8115  Validation loss = 2.3869  \n",
      "\n",
      "Fold: 1  Epoch: 80  Training loss = 2.8101  Validation loss = 2.3822  \n",
      "\n",
      "Fold: 1  Epoch: 81  Training loss = 2.8067  Validation loss = 2.3721  \n",
      "\n",
      "Fold: 1  Epoch: 82  Training loss = 2.8041  Validation loss = 2.3633  \n",
      "\n",
      "Fold: 1  Epoch: 83  Training loss = 2.8024  Validation loss = 2.3584  \n",
      "\n",
      "Fold: 1  Epoch: 84  Training loss = 2.7982  Validation loss = 2.3440  \n",
      "\n",
      "Fold: 1  Epoch: 85  Training loss = 2.7968  Validation loss = 2.3393  \n",
      "\n",
      "Fold: 1  Epoch: 86  Training loss = 2.7936  Validation loss = 2.3280  \n",
      "\n",
      "Fold: 1  Epoch: 87  Training loss = 2.7910  Validation loss = 2.3183  \n",
      "\n",
      "Fold: 1  Epoch: 88  Training loss = 2.7886  Validation loss = 2.3096  \n",
      "\n",
      "Fold: 1  Epoch: 89  Training loss = 2.7860  Validation loss = 2.3001  \n",
      "\n",
      "Fold: 1  Epoch: 90  Training loss = 2.7853  Validation loss = 2.2972  \n",
      "\n",
      "Fold: 1  Epoch: 91  Training loss = 2.7818  Validation loss = 2.2837  \n",
      "\n",
      "Fold: 1  Epoch: 92  Training loss = 2.7803  Validation loss = 2.2781  \n",
      "\n",
      "Fold: 1  Epoch: 93  Training loss = 2.7795  Validation loss = 2.2748  \n",
      "\n",
      "Fold: 1  Epoch: 94  Training loss = 2.7772  Validation loss = 2.2668  \n",
      "\n",
      "Fold: 1  Epoch: 95  Training loss = 2.7761  Validation loss = 2.2628  \n",
      "\n",
      "Fold: 1  Epoch: 96  Training loss = 2.7740  Validation loss = 2.2546  \n",
      "\n",
      "Fold: 1  Epoch: 97  Training loss = 2.7730  Validation loss = 2.2509  \n",
      "\n",
      "Fold: 1  Epoch: 98  Training loss = 2.7708  Validation loss = 2.2422  \n",
      "\n",
      "Fold: 1  Epoch: 99  Training loss = 2.7703  Validation loss = 2.2404  \n",
      "\n",
      "Fold: 1  Epoch: 100  Training loss = 2.7695  Validation loss = 2.2369  \n",
      "\n",
      "Fold: 1  Epoch: 101  Training loss = 2.7682  Validation loss = 2.2316  \n",
      "\n",
      "Fold: 1  Epoch: 102  Training loss = 2.7665  Validation loss = 2.2246  \n",
      "\n",
      "Fold: 1  Epoch: 103  Training loss = 2.7668  Validation loss = 2.2261  \n",
      "\n",
      "Fold: 1  Epoch: 104  Training loss = 2.7647  Validation loss = 2.2177  \n",
      "\n",
      "Fold: 1  Epoch: 105  Training loss = 2.7633  Validation loss = 2.2119  \n",
      "\n",
      "Fold: 1  Epoch: 106  Training loss = 2.7631  Validation loss = 2.2107  \n",
      "\n",
      "Fold: 1  Epoch: 107  Training loss = 2.7605  Validation loss = 2.2000  \n",
      "\n",
      "Fold: 1  Epoch: 108  Training loss = 2.7587  Validation loss = 2.1923  \n",
      "\n",
      "Fold: 1  Epoch: 109  Training loss = 2.7572  Validation loss = 2.1852  \n",
      "\n",
      "Fold: 1  Epoch: 110  Training loss = 2.7554  Validation loss = 2.1774  \n",
      "\n",
      "Fold: 1  Epoch: 111  Training loss = 2.7541  Validation loss = 2.1715  \n",
      "\n",
      "Fold: 1  Epoch: 112  Training loss = 2.7533  Validation loss = 2.1683  \n",
      "\n",
      "Fold: 1  Epoch: 113  Training loss = 2.7509  Validation loss = 2.1571  \n",
      "\n",
      "Fold: 1  Epoch: 114  Training loss = 2.7493  Validation loss = 2.1492  \n",
      "\n",
      "Fold: 1  Epoch: 115  Training loss = 2.7486  Validation loss = 2.1463  \n",
      "\n",
      "Fold: 1  Epoch: 116  Training loss = 2.7477  Validation loss = 2.1426  \n",
      "\n",
      "Fold: 1  Epoch: 117  Training loss = 2.7467  Validation loss = 2.1378  \n",
      "\n",
      "Fold: 1  Epoch: 118  Training loss = 2.7453  Validation loss = 2.1313  \n",
      "\n",
      "Fold: 1  Epoch: 119  Training loss = 2.7437  Validation loss = 2.1244  \n",
      "\n",
      "Fold: 1  Epoch: 120  Training loss = 2.7420  Validation loss = 2.1160  \n",
      "\n",
      "Fold: 1  Epoch: 121  Training loss = 2.7402  Validation loss = 2.1072  \n",
      "\n",
      "Fold: 1  Epoch: 122  Training loss = 2.7390  Validation loss = 2.1017  \n",
      "\n",
      "Fold: 1  Epoch: 123  Training loss = 2.7383  Validation loss = 2.0981  \n",
      "\n",
      "Fold: 1  Epoch: 124  Training loss = 2.7379  Validation loss = 2.0964  \n",
      "\n",
      "Fold: 1  Epoch: 125  Training loss = 2.7372  Validation loss = 2.0932  \n",
      "\n",
      "Fold: 1  Epoch: 126  Training loss = 2.7347  Validation loss = 2.0802  \n",
      "\n",
      "Fold: 1  Epoch: 127  Training loss = 2.7328  Validation loss = 2.0705  \n",
      "\n",
      "Fold: 1  Epoch: 128  Training loss = 2.7325  Validation loss = 2.0690  \n",
      "\n",
      "Fold: 1  Epoch: 129  Training loss = 2.7323  Validation loss = 2.0684  \n",
      "\n",
      "Fold: 1  Epoch: 130  Training loss = 2.7313  Validation loss = 2.0631  \n",
      "\n",
      "Fold: 1  Epoch: 131  Training loss = 2.7310  Validation loss = 2.0620  \n",
      "\n",
      "Fold: 1  Epoch: 132  Training loss = 2.7309  Validation loss = 2.0625  \n",
      "\n",
      "Fold: 1  Epoch: 133  Training loss = 2.7295  Validation loss = 2.0556  \n",
      "\n",
      "Fold: 1  Epoch: 134  Training loss = 2.7288  Validation loss = 2.0522  \n",
      "\n",
      "Fold: 1  Epoch: 135  Training loss = 2.7286  Validation loss = 2.0517  \n",
      "\n",
      "Fold: 1  Epoch: 136  Training loss = 2.7282  Validation loss = 2.0496  \n",
      "\n",
      "Fold: 1  Epoch: 137  Training loss = 2.7276  Validation loss = 2.0461  \n",
      "\n",
      "Fold: 1  Epoch: 138  Training loss = 2.7261  Validation loss = 2.0385  \n",
      "\n",
      "Fold: 1  Epoch: 139  Training loss = 2.7261  Validation loss = 2.0392  \n",
      "\n",
      "Fold: 1  Epoch: 140  Training loss = 2.7248  Validation loss = 2.0318  \n",
      "\n",
      "Fold: 1  Epoch: 141  Training loss = 2.7237  Validation loss = 2.0256  \n",
      "\n",
      "Fold: 1  Epoch: 142  Training loss = 2.7230  Validation loss = 2.0218  \n",
      "\n",
      "Fold: 1  Epoch: 143  Training loss = 2.7219  Validation loss = 2.0158  \n",
      "\n",
      "Fold: 1  Epoch: 144  Training loss = 2.7209  Validation loss = 2.0104  \n",
      "\n",
      "Fold: 1  Epoch: 145  Training loss = 2.7197  Validation loss = 2.0039  \n",
      "\n",
      "Fold: 1  Epoch: 146  Training loss = 2.7184  Validation loss = 1.9964  \n",
      "\n",
      "Fold: 1  Epoch: 147  Training loss = 2.7181  Validation loss = 1.9953  \n",
      "\n",
      "Fold: 1  Epoch: 148  Training loss = 2.7177  Validation loss = 1.9934  \n",
      "\n",
      "Fold: 1  Epoch: 149  Training loss = 2.7167  Validation loss = 1.9877  \n",
      "\n",
      "Fold: 1  Epoch: 150  Training loss = 2.7167  Validation loss = 1.9884  \n",
      "\n",
      "Fold: 1  Epoch: 151  Training loss = 2.7154  Validation loss = 1.9804  \n",
      "\n",
      "Fold: 1  Epoch: 152  Training loss = 2.7142  Validation loss = 1.9731  \n",
      "\n",
      "Fold: 1  Epoch: 153  Training loss = 2.7135  Validation loss = 1.9689  \n",
      "\n",
      "Fold: 1  Epoch: 154  Training loss = 2.7116  Validation loss = 1.9570  \n",
      "\n",
      "Fold: 1  Epoch: 155  Training loss = 2.7103  Validation loss = 1.9483  \n",
      "\n",
      "Fold: 1  Epoch: 156  Training loss = 2.7090  Validation loss = 1.9403  \n",
      "\n",
      "Fold: 1  Epoch: 157  Training loss = 2.7081  Validation loss = 1.9352  \n",
      "\n",
      "Fold: 1  Epoch: 158  Training loss = 2.7064  Validation loss = 1.9236  \n",
      "\n",
      "Fold: 1  Epoch: 159  Training loss = 2.7061  Validation loss = 1.9211  \n",
      "\n",
      "Fold: 1  Epoch: 160  Training loss = 2.7045  Validation loss = 1.9103  \n",
      "\n",
      "Fold: 1  Epoch: 161  Training loss = 2.7037  Validation loss = 1.9045  \n",
      "\n",
      "Fold: 1  Epoch: 162  Training loss = 2.7025  Validation loss = 1.8968  \n",
      "\n",
      "Fold: 1  Epoch: 163  Training loss = 2.7024  Validation loss = 1.8974  \n",
      "\n",
      "Fold: 1  Epoch: 164  Training loss = 2.7019  Validation loss = 1.8938  \n",
      "\n",
      "Fold: 1  Epoch: 165  Training loss = 2.7017  Validation loss = 1.8940  \n",
      "\n",
      "Fold: 1  Epoch: 166  Training loss = 2.7008  Validation loss = 1.8876  \n",
      "\n",
      "Fold: 1  Epoch: 167  Training loss = 2.7004  Validation loss = 1.8839  \n",
      "\n",
      "Fold: 1  Epoch: 168  Training loss = 2.7006  Validation loss = 1.8875  \n",
      "\n",
      "Fold: 1  Epoch: 169  Training loss = 2.6990  Validation loss = 1.8752  \n",
      "\n",
      "Fold: 1  Epoch: 170  Training loss = 2.6981  Validation loss = 1.8692  \n",
      "\n",
      "Fold: 1  Epoch: 171  Training loss = 2.6970  Validation loss = 1.8612  \n",
      "\n",
      "Fold: 1  Epoch: 172  Training loss = 2.6965  Validation loss = 1.8570  \n",
      "\n",
      "Fold: 1  Epoch: 173  Training loss = 2.6956  Validation loss = 1.8509  \n",
      "\n",
      "Fold: 1  Epoch: 174  Training loss = 2.6952  Validation loss = 1.8470  \n",
      "\n",
      "Fold: 1  Epoch: 175  Training loss = 2.6939  Validation loss = 1.8363  \n",
      "\n",
      "Fold: 1  Epoch: 176  Training loss = 2.6932  Validation loss = 1.8309  \n",
      "\n",
      "Fold: 1  Epoch: 177  Training loss = 2.6923  Validation loss = 1.8233  \n",
      "\n",
      "Fold: 1  Epoch: 178  Training loss = 2.6913  Validation loss = 1.8141  \n",
      "\n",
      "Fold: 1  Epoch: 179  Training loss = 2.6911  Validation loss = 1.8144  \n",
      "\n",
      "Fold: 1  Epoch: 180  Training loss = 2.6909  Validation loss = 1.8142  \n",
      "\n",
      "Fold: 1  Epoch: 181  Training loss = 2.6902  Validation loss = 1.8081  \n",
      "\n",
      "Fold: 1  Epoch: 182  Training loss = 2.6900  Validation loss = 1.8074  \n",
      "\n",
      "Fold: 1  Epoch: 183  Training loss = 2.6896  Validation loss = 1.8035  \n",
      "\n",
      "Fold: 1  Epoch: 184  Training loss = 2.6891  Validation loss = 1.7999  \n",
      "\n",
      "Fold: 1  Epoch: 185  Training loss = 2.6889  Validation loss = 1.7979  \n",
      "\n",
      "Fold: 1  Epoch: 186  Training loss = 2.6882  Validation loss = 1.7919  \n",
      "\n",
      "Fold: 1  Epoch: 187  Training loss = 2.6879  Validation loss = 1.7888  \n",
      "\n",
      "Fold: 1  Epoch: 188  Training loss = 2.6876  Validation loss = 1.7870  \n",
      "\n",
      "Fold: 1  Epoch: 189  Training loss = 2.6875  Validation loss = 1.7883  \n",
      "\n",
      "Fold: 1  Epoch: 190  Training loss = 2.6866  Validation loss = 1.7796  \n",
      "\n",
      "Fold: 1  Epoch: 191  Training loss = 2.6863  Validation loss = 1.7771  \n",
      "\n",
      "Fold: 1  Epoch: 192  Training loss = 2.6854  Validation loss = 1.7690  \n",
      "\n",
      "Fold: 1  Epoch: 193  Training loss = 2.6851  Validation loss = 1.7662  \n",
      "\n",
      "Fold: 1  Epoch: 194  Training loss = 2.6847  Validation loss = 1.7638  \n",
      "\n",
      "Fold: 1  Epoch: 195  Training loss = 2.6841  Validation loss = 1.7569  \n",
      "\n",
      "Fold: 1  Epoch: 196  Training loss = 2.6837  Validation loss = 1.7513  \n",
      "\n",
      "Fold: 1  Epoch: 197  Training loss = 2.6832  Validation loss = 1.7482  \n",
      "\n",
      "Fold: 1  Epoch: 198  Training loss = 2.6834  Validation loss = 1.7514  \n",
      "\n",
      "Fold: 1  Epoch: 199  Training loss = 2.6831  Validation loss = 1.7488  \n",
      "\n",
      "Fold: 1  Epoch: 200  Training loss = 2.6829  Validation loss = 1.7482  \n",
      "\n",
      "Fold: 1  Epoch: 201  Training loss = 2.6820  Validation loss = 1.7373  \n",
      "\n",
      "Fold: 1  Epoch: 202  Training loss = 2.6814  Validation loss = 1.7323  \n",
      "\n",
      "Fold: 1  Epoch: 203  Training loss = 2.6810  Validation loss = 1.7281  \n",
      "\n",
      "Fold: 1  Epoch: 204  Training loss = 2.6809  Validation loss = 1.7285  \n",
      "\n",
      "Fold: 1  Epoch: 205  Training loss = 2.6806  Validation loss = 1.7275  \n",
      "\n",
      "Fold: 1  Epoch: 206  Training loss = 2.6803  Validation loss = 1.7259  \n",
      "\n",
      "Fold: 1  Epoch: 207  Training loss = 2.6804  Validation loss = 1.7281  \n",
      "\n",
      "Fold: 1  Epoch: 208  Training loss = 2.6793  Validation loss = 1.7165  \n",
      "\n",
      "Fold: 1  Epoch: 209  Training loss = 2.6791  Validation loss = 1.7160  \n",
      "\n",
      "Fold: 1  Epoch: 210  Training loss = 2.6786  Validation loss = 1.7099  \n",
      "\n",
      "Fold: 1  Epoch: 211  Training loss = 2.6785  Validation loss = 1.7091  \n",
      "\n",
      "Fold: 1  Epoch: 212  Training loss = 2.6782  Validation loss = 1.7072  \n",
      "\n",
      "Fold: 1  Epoch: 213  Training loss = 2.6782  Validation loss = 1.7102  \n",
      "\n",
      "Fold: 1  Epoch: 214  Training loss = 2.6776  Validation loss = 1.7043  \n",
      "\n",
      "Fold: 1  Epoch: 215  Training loss = 2.6774  Validation loss = 1.7051  \n",
      "\n",
      "Fold: 1  Epoch: 216  Training loss = 2.6774  Validation loss = 1.7085  \n",
      "\n",
      "Fold: 1  Epoch: 217  Training loss = 2.6767  Validation loss = 1.6992  \n",
      "\n",
      "Fold: 1  Epoch: 218  Training loss = 2.6762  Validation loss = 1.6937  \n",
      "\n",
      "Fold: 1  Epoch: 219  Training loss = 2.6758  Validation loss = 1.6907  \n",
      "\n",
      "Fold: 1  Epoch: 220  Training loss = 2.6758  Validation loss = 1.6942  \n",
      "\n",
      "Fold: 1  Epoch: 221  Training loss = 2.6757  Validation loss = 1.6964  \n",
      "\n",
      "Fold: 1  Epoch: 222  Training loss = 2.6755  Validation loss = 1.6952  \n",
      "\n",
      "Fold: 1  Epoch: 223  Training loss = 2.6751  Validation loss = 1.6920  \n",
      "\n",
      "Fold: 1  Epoch: 224  Training loss = 2.6749  Validation loss = 1.6917  \n",
      "\n",
      "Fold: 1  Epoch: 225  Training loss = 2.6746  Validation loss = 1.6883  \n",
      "\n",
      "Fold: 1  Epoch: 226  Training loss = 2.6744  Validation loss = 1.6840  \n",
      "\n",
      "Fold: 1  Epoch: 227  Training loss = 2.6743  Validation loss = 1.6853  \n",
      "\n",
      "Fold: 1  Epoch: 228  Training loss = 2.6741  Validation loss = 1.6814  \n",
      "\n",
      "Fold: 1  Epoch: 229  Training loss = 2.6741  Validation loss = 1.6840  \n",
      "\n",
      "Fold: 1  Epoch: 230  Training loss = 2.6740  Validation loss = 1.6833  \n",
      "\n",
      "Fold: 1  Epoch: 231  Training loss = 2.6738  Validation loss = 1.6828  \n",
      "\n",
      "Fold: 1  Epoch: 232  Training loss = 2.6734  Validation loss = 1.6762  \n",
      "\n",
      "Fold: 1  Epoch: 233  Training loss = 2.6732  Validation loss = 1.6773  \n",
      "\n",
      "Fold: 1  Epoch: 234  Training loss = 2.6729  Validation loss = 1.6758  \n",
      "\n",
      "Fold: 1  Epoch: 235  Training loss = 2.6730  Validation loss = 1.6777  \n",
      "\n",
      "Fold: 1  Epoch: 236  Training loss = 2.6731  Validation loss = 1.6810  \n",
      "\n",
      "Fold: 1  Epoch: 237  Training loss = 2.6727  Validation loss = 1.6750  \n",
      "\n",
      "Fold: 1  Epoch: 238  Training loss = 2.6723  Validation loss = 1.6711  \n",
      "\n",
      "Fold: 1  Epoch: 239  Training loss = 2.6721  Validation loss = 1.6707  \n",
      "\n",
      "Fold: 1  Epoch: 240  Training loss = 2.6719  Validation loss = 1.6682  \n",
      "\n",
      "Fold: 1  Epoch: 241  Training loss = 2.6715  Validation loss = 1.6624  \n",
      "\n",
      "Fold: 1  Epoch: 242  Training loss = 2.6713  Validation loss = 1.6623  \n",
      "\n",
      "Fold: 1  Epoch: 243  Training loss = 2.6710  Validation loss = 1.6583  \n",
      "\n",
      "Fold: 1  Epoch: 244  Training loss = 2.6708  Validation loss = 1.6598  \n",
      "\n",
      "Fold: 1  Epoch: 245  Training loss = 2.6706  Validation loss = 1.6613  \n",
      "\n",
      "Fold: 1  Epoch: 246  Training loss = 2.6700  Validation loss = 1.6536  \n",
      "\n",
      "Fold: 1  Epoch: 247  Training loss = 2.6701  Validation loss = 1.6601  \n",
      "\n",
      "Fold: 1  Epoch: 248  Training loss = 2.6695  Validation loss = 1.6534  \n",
      "\n",
      "Fold: 1  Epoch: 249  Training loss = 2.6693  Validation loss = 1.6509  \n",
      "\n",
      "Fold: 1  Epoch: 250  Training loss = 2.6690  Validation loss = 1.6476  \n",
      "\n",
      "Fold: 1  Epoch: 251  Training loss = 2.6688  Validation loss = 1.6471  \n",
      "\n",
      "Fold: 1  Epoch: 252  Training loss = 2.6687  Validation loss = 1.6477  \n",
      "\n",
      "Fold: 1  Epoch: 253  Training loss = 2.6681  Validation loss = 1.6401  \n",
      "\n",
      "Fold: 1  Epoch: 254  Training loss = 2.6678  Validation loss = 1.6398  \n",
      "\n",
      "Fold: 1  Epoch: 255  Training loss = 2.6677  Validation loss = 1.6417  \n",
      "\n",
      "Fold: 1  Epoch: 256  Training loss = 2.6677  Validation loss = 1.6427  \n",
      "\n",
      "Fold: 1  Epoch: 257  Training loss = 2.6674  Validation loss = 1.6423  \n",
      "\n",
      "Fold: 1  Epoch: 258  Training loss = 2.6670  Validation loss = 1.6365  \n",
      "\n",
      "Fold: 1  Epoch: 259  Training loss = 2.6668  Validation loss = 1.6351  \n",
      "\n",
      "Fold: 1  Epoch: 260  Training loss = 2.6666  Validation loss = 1.6345  \n",
      "\n",
      "Fold: 1  Epoch: 261  Training loss = 2.6663  Validation loss = 1.6337  \n",
      "\n",
      "Fold: 1  Epoch: 262  Training loss = 2.6660  Validation loss = 1.6268  \n",
      "\n",
      "Fold: 1  Epoch: 263  Training loss = 2.6657  Validation loss = 1.6275  \n",
      "\n",
      "Fold: 1  Epoch: 264  Training loss = 2.6655  Validation loss = 1.6274  \n",
      "\n",
      "Fold: 1  Epoch: 265  Training loss = 2.6653  Validation loss = 1.6257  \n",
      "\n",
      "Fold: 1  Epoch: 266  Training loss = 2.6653  Validation loss = 1.6325  \n",
      "\n",
      "Fold: 1  Epoch: 267  Training loss = 2.6650  Validation loss = 1.6294  \n",
      "\n",
      "Fold: 1  Epoch: 268  Training loss = 2.6648  Validation loss = 1.6283  \n",
      "\n",
      "Fold: 1  Epoch: 269  Training loss = 2.6645  Validation loss = 1.6233  \n",
      "\n",
      "Fold: 1  Epoch: 270  Training loss = 2.6644  Validation loss = 1.6285  \n",
      "\n",
      "Fold: 1  Epoch: 271  Training loss = 2.6639  Validation loss = 1.6193  \n",
      "\n",
      "Fold: 1  Epoch: 272  Training loss = 2.6637  Validation loss = 1.6223  \n",
      "\n",
      "Fold: 1  Epoch: 273  Training loss = 2.6633  Validation loss = 1.6159  \n",
      "\n",
      "Fold: 1  Epoch: 274  Training loss = 2.6631  Validation loss = 1.6129  \n",
      "\n",
      "Fold: 1  Epoch: 275  Training loss = 2.6630  Validation loss = 1.6186  \n",
      "\n",
      "Fold: 1  Epoch: 276  Training loss = 2.6628  Validation loss = 1.6210  \n",
      "\n",
      "Fold: 1  Epoch: 277  Training loss = 2.6624  Validation loss = 1.6178  \n",
      "\n",
      "Fold: 1  Epoch: 278  Training loss = 2.6621  Validation loss = 1.6157  \n",
      "\n",
      "Fold: 1  Epoch: 279  Training loss = 2.6619  Validation loss = 1.6172  \n",
      "\n",
      "Fold: 1  Epoch: 280  Training loss = 2.6615  Validation loss = 1.6120  \n",
      "\n",
      "Fold: 1  Epoch: 281  Training loss = 2.6613  Validation loss = 1.6080  \n",
      "\n",
      "Fold: 1  Epoch: 282  Training loss = 2.6613  Validation loss = 1.6146  \n",
      "\n",
      "Fold: 1  Epoch: 283  Training loss = 2.6609  Validation loss = 1.6060  \n",
      "\n",
      "Fold: 1  Epoch: 284  Training loss = 2.6608  Validation loss = 1.6089  \n",
      "\n",
      "Fold: 1  Epoch: 285  Training loss = 2.6606  Validation loss = 1.6080  \n",
      "\n",
      "Fold: 1  Epoch: 286  Training loss = 2.6604  Validation loss = 1.6042  \n",
      "\n",
      "Fold: 1  Epoch: 287  Training loss = 2.6601  Validation loss = 1.5961  \n",
      "\n",
      "Fold: 1  Epoch: 288  Training loss = 2.6600  Validation loss = 1.5967  \n",
      "\n",
      "Fold: 1  Epoch: 289  Training loss = 2.6595  Validation loss = 1.5913  \n",
      "\n",
      "Fold: 1  Epoch: 290  Training loss = 2.6591  Validation loss = 1.5888  \n",
      "\n",
      "Fold: 1  Epoch: 291  Training loss = 2.6587  Validation loss = 1.5881  \n",
      "\n",
      "Fold: 1  Epoch: 292  Training loss = 2.6585  Validation loss = 1.5878  \n",
      "\n",
      "Fold: 1  Epoch: 293  Training loss = 2.6584  Validation loss = 1.5959  \n",
      "\n",
      "Fold: 1  Epoch: 294  Training loss = 2.6583  Validation loss = 1.5971  \n",
      "\n",
      "Fold: 1  Epoch: 295  Training loss = 2.6581  Validation loss = 1.5968  \n",
      "\n",
      "Fold: 1  Epoch: 296  Training loss = 2.6579  Validation loss = 1.5968  \n",
      "\n",
      "Fold: 1  Epoch: 297  Training loss = 2.6576  Validation loss = 1.5883  \n",
      "\n",
      "Fold: 1  Epoch: 298  Training loss = 2.6575  Validation loss = 1.5876  \n",
      "\n",
      "Fold: 1  Epoch: 299  Training loss = 2.6575  Validation loss = 1.5880  \n",
      "\n",
      "Fold: 1  Epoch: 300  Training loss = 2.6572  Validation loss = 1.5838  \n",
      "\n",
      "Fold: 1  Epoch: 301  Training loss = 2.6571  Validation loss = 1.5811  \n",
      "\n",
      "Fold: 1  Epoch: 302  Training loss = 2.6569  Validation loss = 1.5882  \n",
      "\n",
      "Fold: 1  Epoch: 303  Training loss = 2.6566  Validation loss = 1.5829  \n",
      "\n",
      "Fold: 1  Epoch: 304  Training loss = 2.6565  Validation loss = 1.5809  \n",
      "\n",
      "Fold: 1  Epoch: 305  Training loss = 2.6565  Validation loss = 1.5796  \n",
      "\n",
      "Fold: 1  Epoch: 306  Training loss = 2.6562  Validation loss = 1.5828  \n",
      "\n",
      "Fold: 1  Epoch: 307  Training loss = 2.6561  Validation loss = 1.5909  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 305  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.5493  Validation loss = 1.7805  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.5491  Validation loss = 1.7812  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.5489  Validation loss = 1.7806  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.5486  Validation loss = 1.7786  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.5482  Validation loss = 1.7741  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.5481  Validation loss = 1.7759  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.5480  Validation loss = 1.7741  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.5473  Validation loss = 1.7665  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.5472  Validation loss = 1.7653  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.5472  Validation loss = 1.7646  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.5469  Validation loss = 1.7646  \n",
      "\n",
      "Fold: 2  Epoch: 12  Training loss = 2.5466  Validation loss = 1.7628  \n",
      "\n",
      "Fold: 2  Epoch: 13  Training loss = 2.5465  Validation loss = 1.7624  \n",
      "\n",
      "Fold: 2  Epoch: 14  Training loss = 2.5462  Validation loss = 1.7597  \n",
      "\n",
      "Fold: 2  Epoch: 15  Training loss = 2.5461  Validation loss = 1.7600  \n",
      "\n",
      "Fold: 2  Epoch: 16  Training loss = 2.5460  Validation loss = 1.7631  \n",
      "\n",
      "Fold: 2  Epoch: 17  Training loss = 2.5457  Validation loss = 1.7586  \n",
      "\n",
      "Fold: 2  Epoch: 18  Training loss = 2.5454  Validation loss = 1.7559  \n",
      "\n",
      "Fold: 2  Epoch: 19  Training loss = 2.5450  Validation loss = 1.7562  \n",
      "\n",
      "Fold: 2  Epoch: 20  Training loss = 2.5447  Validation loss = 1.7575  \n",
      "\n",
      "Fold: 2  Epoch: 21  Training loss = 2.5447  Validation loss = 1.7597  \n",
      "\n",
      "Fold: 2  Epoch: 22  Training loss = 2.5445  Validation loss = 1.7581  \n",
      "\n",
      "Fold: 2  Epoch: 23  Training loss = 2.5442  Validation loss = 1.7585  \n",
      "\n",
      "Fold: 2  Epoch: 24  Training loss = 2.5441  Validation loss = 1.7601  \n",
      "\n",
      "Fold: 2  Epoch: 25  Training loss = 2.5437  Validation loss = 1.7572  \n",
      "\n",
      "Fold: 2  Epoch: 26  Training loss = 2.5432  Validation loss = 1.7523  \n",
      "\n",
      "Fold: 2  Epoch: 27  Training loss = 2.5430  Validation loss = 1.7503  \n",
      "\n",
      "Fold: 2  Epoch: 28  Training loss = 2.5429  Validation loss = 1.7515  \n",
      "\n",
      "Fold: 2  Epoch: 29  Training loss = 2.5425  Validation loss = 1.7484  \n",
      "\n",
      "Fold: 2  Epoch: 30  Training loss = 2.5422  Validation loss = 1.7484  \n",
      "\n",
      "Fold: 2  Epoch: 31  Training loss = 2.5420  Validation loss = 1.7498  \n",
      "\n",
      "Fold: 2  Epoch: 32  Training loss = 2.5416  Validation loss = 1.7501  \n",
      "\n",
      "Fold: 2  Epoch: 33  Training loss = 2.5414  Validation loss = 1.7511  \n",
      "\n",
      "Fold: 2  Epoch: 34  Training loss = 2.5411  Validation loss = 1.7491  \n",
      "\n",
      "Fold: 2  Epoch: 35  Training loss = 2.5409  Validation loss = 1.7463  \n",
      "\n",
      "Fold: 2  Epoch: 36  Training loss = 2.5408  Validation loss = 1.7468  \n",
      "\n",
      "Fold: 2  Epoch: 37  Training loss = 2.5407  Validation loss = 1.7487  \n",
      "\n",
      "Fold: 2  Epoch: 38  Training loss = 2.5402  Validation loss = 1.7464  \n",
      "\n",
      "Fold: 2  Epoch: 39  Training loss = 2.5397  Validation loss = 1.7462  \n",
      "\n",
      "Fold: 2  Epoch: 40  Training loss = 2.5394  Validation loss = 1.7429  \n",
      "\n",
      "Fold: 2  Epoch: 41  Training loss = 2.5391  Validation loss = 1.7434  \n",
      "\n",
      "Fold: 2  Epoch: 42  Training loss = 2.5388  Validation loss = 1.7417  \n",
      "\n",
      "Fold: 2  Epoch: 43  Training loss = 2.5385  Validation loss = 1.7387  \n",
      "\n",
      "Fold: 2  Epoch: 44  Training loss = 2.5381  Validation loss = 1.7395  \n",
      "\n",
      "Fold: 2  Epoch: 45  Training loss = 2.5378  Validation loss = 1.7405  \n",
      "\n",
      "Fold: 2  Epoch: 46  Training loss = 2.5377  Validation loss = 1.7404  \n",
      "\n",
      "Fold: 2  Epoch: 47  Training loss = 2.5371  Validation loss = 1.7373  \n",
      "\n",
      "Fold: 2  Epoch: 48  Training loss = 2.5369  Validation loss = 1.7387  \n",
      "\n",
      "Fold: 2  Epoch: 49  Training loss = 2.5366  Validation loss = 1.7400  \n",
      "\n",
      "Fold: 2  Epoch: 50  Training loss = 2.5363  Validation loss = 1.7412  \n",
      "\n",
      "Fold: 2  Epoch: 51  Training loss = 2.5360  Validation loss = 1.7380  \n",
      "\n",
      "Fold: 2  Epoch: 52  Training loss = 2.5357  Validation loss = 1.7394  \n",
      "\n",
      "Fold: 2  Epoch: 53  Training loss = 2.5355  Validation loss = 1.7401  \n",
      "\n",
      "Fold: 2  Epoch: 54  Training loss = 2.5349  Validation loss = 1.7373  \n",
      "\n",
      "Fold: 2  Epoch: 55  Training loss = 2.5347  Validation loss = 1.7367  \n",
      "\n",
      "Fold: 2  Epoch: 56  Training loss = 2.5342  Validation loss = 1.7332  \n",
      "\n",
      "Fold: 2  Epoch: 57  Training loss = 2.5339  Validation loss = 1.7324  \n",
      "\n",
      "Fold: 2  Epoch: 58  Training loss = 2.5337  Validation loss = 1.7316  \n",
      "\n",
      "Fold: 2  Epoch: 59  Training loss = 2.5335  Validation loss = 1.7350  \n",
      "\n",
      "Fold: 2  Epoch: 60  Training loss = 2.5331  Validation loss = 1.7334  \n",
      "\n",
      "Fold: 2  Epoch: 61  Training loss = 2.5327  Validation loss = 1.7309  \n",
      "\n",
      "Fold: 2  Epoch: 62  Training loss = 2.5324  Validation loss = 1.7324  \n",
      "\n",
      "Fold: 2  Epoch: 63  Training loss = 2.5316  Validation loss = 1.7302  \n",
      "\n",
      "Fold: 2  Epoch: 64  Training loss = 2.5313  Validation loss = 1.7285  \n",
      "\n",
      "Fold: 2  Epoch: 65  Training loss = 2.5309  Validation loss = 1.7277  \n",
      "\n",
      "Fold: 2  Epoch: 66  Training loss = 2.5305  Validation loss = 1.7273  \n",
      "\n",
      "Fold: 2  Epoch: 67  Training loss = 2.5300  Validation loss = 1.7241  \n",
      "\n",
      "Fold: 2  Epoch: 68  Training loss = 2.5294  Validation loss = 1.7219  \n",
      "\n",
      "Fold: 2  Epoch: 69  Training loss = 2.5293  Validation loss = 1.7226  \n",
      "\n",
      "Fold: 2  Epoch: 70  Training loss = 2.5287  Validation loss = 1.7228  \n",
      "\n",
      "Fold: 2  Epoch: 71  Training loss = 2.5284  Validation loss = 1.7231  \n",
      "\n",
      "Fold: 2  Epoch: 72  Training loss = 2.5275  Validation loss = 1.7203  \n",
      "\n",
      "Fold: 2  Epoch: 73  Training loss = 2.5271  Validation loss = 1.7212  \n",
      "\n",
      "Fold: 2  Epoch: 74  Training loss = 2.5270  Validation loss = 1.7180  \n",
      "\n",
      "Fold: 2  Epoch: 75  Training loss = 2.5270  Validation loss = 1.7192  \n",
      "\n",
      "Fold: 2  Epoch: 76  Training loss = 2.5264  Validation loss = 1.7165  \n",
      "\n",
      "Fold: 2  Epoch: 77  Training loss = 2.5259  Validation loss = 1.7128  \n",
      "\n",
      "Fold: 2  Epoch: 78  Training loss = 2.5258  Validation loss = 1.7101  \n",
      "\n",
      "Fold: 2  Epoch: 79  Training loss = 2.5249  Validation loss = 1.7114  \n",
      "\n",
      "Fold: 2  Epoch: 80  Training loss = 2.5247  Validation loss = 1.7088  \n",
      "\n",
      "Fold: 2  Epoch: 81  Training loss = 2.5240  Validation loss = 1.7087  \n",
      "\n",
      "Fold: 2  Epoch: 82  Training loss = 2.5235  Validation loss = 1.7090  \n",
      "\n",
      "Fold: 2  Epoch: 83  Training loss = 2.5229  Validation loss = 1.7056  \n",
      "\n",
      "Fold: 2  Epoch: 84  Training loss = 2.5224  Validation loss = 1.7025  \n",
      "\n",
      "Fold: 2  Epoch: 85  Training loss = 2.5221  Validation loss = 1.7023  \n",
      "\n",
      "Fold: 2  Epoch: 86  Training loss = 2.5215  Validation loss = 1.7023  \n",
      "\n",
      "Fold: 2  Epoch: 87  Training loss = 2.5209  Validation loss = 1.6992  \n",
      "\n",
      "Fold: 2  Epoch: 88  Training loss = 2.5207  Validation loss = 1.6976  \n",
      "\n",
      "Fold: 2  Epoch: 89  Training loss = 2.5201  Validation loss = 1.6985  \n",
      "\n",
      "Fold: 2  Epoch: 90  Training loss = 2.5198  Validation loss = 1.6975  \n",
      "\n",
      "Fold: 2  Epoch: 91  Training loss = 2.5194  Validation loss = 1.6964  \n",
      "\n",
      "Fold: 2  Epoch: 92  Training loss = 2.5192  Validation loss = 1.6973  \n",
      "\n",
      "Fold: 2  Epoch: 93  Training loss = 2.5187  Validation loss = 1.6965  \n",
      "\n",
      "Fold: 2  Epoch: 94  Training loss = 2.5184  Validation loss = 1.6985  \n",
      "\n",
      "Fold: 2  Epoch: 95  Training loss = 2.5181  Validation loss = 1.7013  \n",
      "\n",
      "Fold: 2  Epoch: 96  Training loss = 2.5178  Validation loss = 1.7040  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 91  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.4863  Validation loss = 2.5952  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.4818  Validation loss = 2.6002  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.4787  Validation loss = 2.6048  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.4789  Validation loss = 2.5963  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.4763  Validation loss = 2.6007  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.4738  Validation loss = 2.6013  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.4711  Validation loss = 2.6049  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.4695  Validation loss = 2.6019  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.4689  Validation loss = 2.6013  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.4682  Validation loss = 2.5990  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.4668  Validation loss = 2.5926  \n",
      "\n",
      "Fold: 3  Epoch: 12  Training loss = 1.4654  Validation loss = 2.5927  \n",
      "\n",
      "Fold: 3  Epoch: 13  Training loss = 1.4641  Validation loss = 2.5944  \n",
      "\n",
      "Fold: 3  Epoch: 14  Training loss = 1.4639  Validation loss = 2.5892  \n",
      "\n",
      "Fold: 3  Epoch: 15  Training loss = 1.4621  Validation loss = 2.5955  \n",
      "\n",
      "Fold: 3  Epoch: 16  Training loss = 1.4611  Validation loss = 2.5971  \n",
      "\n",
      "Fold: 3  Epoch: 17  Training loss = 1.4598  Validation loss = 2.5945  \n",
      "\n",
      "Fold: 3  Epoch: 18  Training loss = 1.4591  Validation loss = 2.5922  \n",
      "\n",
      "Fold: 3  Epoch: 19  Training loss = 1.4594  Validation loss = 2.5852  \n",
      "\n",
      "Fold: 3  Epoch: 20  Training loss = 1.4597  Validation loss = 2.5773  \n",
      "\n",
      "Fold: 3  Epoch: 21  Training loss = 1.4585  Validation loss = 2.5826  \n",
      "\n",
      "Fold: 3  Epoch: 22  Training loss = 1.4577  Validation loss = 2.5822  \n",
      "\n",
      "Fold: 3  Epoch: 23  Training loss = 1.4569  Validation loss = 2.5805  \n",
      "\n",
      "Fold: 3  Epoch: 24  Training loss = 1.4564  Validation loss = 2.5837  \n",
      "\n",
      "Fold: 3  Epoch: 25  Training loss = 1.4557  Validation loss = 2.5823  \n",
      "\n",
      "Fold: 3  Epoch: 26  Training loss = 1.4556  Validation loss = 2.5767  \n",
      "\n",
      "Fold: 3  Epoch: 27  Training loss = 1.4552  Validation loss = 2.5719  \n",
      "\n",
      "Fold: 3  Epoch: 28  Training loss = 1.4547  Validation loss = 2.5673  \n",
      "\n",
      "Fold: 3  Epoch: 29  Training loss = 1.4533  Validation loss = 2.5663  \n",
      "\n",
      "Fold: 3  Epoch: 30  Training loss = 1.4525  Validation loss = 2.5695  \n",
      "\n",
      "Fold: 3  Epoch: 31  Training loss = 1.4518  Validation loss = 2.5642  \n",
      "\n",
      "Fold: 3  Epoch: 32  Training loss = 1.4514  Validation loss = 2.5546  \n",
      "\n",
      "Fold: 3  Epoch: 33  Training loss = 1.4505  Validation loss = 2.5552  \n",
      "\n",
      "Fold: 3  Epoch: 34  Training loss = 1.4494  Validation loss = 2.5579  \n",
      "\n",
      "Fold: 3  Epoch: 35  Training loss = 1.4492  Validation loss = 2.5518  \n",
      "\n",
      "Fold: 3  Epoch: 36  Training loss = 1.4484  Validation loss = 2.5514  \n",
      "\n",
      "Fold: 3  Epoch: 37  Training loss = 1.4481  Validation loss = 2.5490  \n",
      "\n",
      "Fold: 3  Epoch: 38  Training loss = 1.4478  Validation loss = 2.5462  \n",
      "\n",
      "Fold: 3  Epoch: 39  Training loss = 1.4471  Validation loss = 2.5480  \n",
      "\n",
      "Fold: 3  Epoch: 40  Training loss = 1.4469  Validation loss = 2.5414  \n",
      "\n",
      "Fold: 3  Epoch: 41  Training loss = 1.4460  Validation loss = 2.5443  \n",
      "\n",
      "Fold: 3  Epoch: 42  Training loss = 1.4455  Validation loss = 2.5430  \n",
      "\n",
      "Fold: 3  Epoch: 43  Training loss = 1.4451  Validation loss = 2.5413  \n",
      "\n",
      "Fold: 3  Epoch: 44  Training loss = 1.4446  Validation loss = 2.5429  \n",
      "\n",
      "Fold: 3  Epoch: 45  Training loss = 1.4440  Validation loss = 2.5429  \n",
      "\n",
      "Fold: 3  Epoch: 46  Training loss = 1.4434  Validation loss = 2.5393  \n",
      "\n",
      "Fold: 3  Epoch: 47  Training loss = 1.4427  Validation loss = 2.5343  \n",
      "\n",
      "Fold: 3  Epoch: 48  Training loss = 1.4423  Validation loss = 2.5325  \n",
      "\n",
      "Fold: 3  Epoch: 49  Training loss = 1.4421  Validation loss = 2.5374  \n",
      "\n",
      "Fold: 3  Epoch: 50  Training loss = 1.4414  Validation loss = 2.5413  \n",
      "\n",
      "Fold: 3  Epoch: 51  Training loss = 1.4408  Validation loss = 2.5429  \n",
      "\n",
      "Fold: 3  Epoch: 52  Training loss = 1.4405  Validation loss = 2.5421  \n",
      "\n",
      "Fold: 3  Epoch: 53  Training loss = 1.4402  Validation loss = 2.5379  \n",
      "\n",
      "Fold: 3  Epoch: 54  Training loss = 1.4398  Validation loss = 2.5382  \n",
      "\n",
      "Fold: 3  Epoch: 55  Training loss = 1.4393  Validation loss = 2.5381  \n",
      "\n",
      "Fold: 3  Epoch: 56  Training loss = 1.4388  Validation loss = 2.5351  \n",
      "\n",
      "Fold: 3  Epoch: 57  Training loss = 1.4385  Validation loss = 2.5372  \n",
      "\n",
      "Fold: 3  Epoch: 58  Training loss = 1.4383  Validation loss = 2.5368  \n",
      "\n",
      "Fold: 3  Epoch: 59  Training loss = 1.4375  Validation loss = 2.5328  \n",
      "\n",
      "Fold: 3  Epoch: 60  Training loss = 1.4371  Validation loss = 2.5325  \n",
      "\n",
      "Fold: 3  Epoch: 61  Training loss = 1.4369  Validation loss = 2.5332  \n",
      "\n",
      "Fold: 3  Epoch: 62  Training loss = 1.4365  Validation loss = 2.5312  \n",
      "\n",
      "Fold: 3  Epoch: 63  Training loss = 1.4361  Validation loss = 2.5253  \n",
      "\n",
      "Fold: 3  Epoch: 64  Training loss = 1.4358  Validation loss = 2.5238  \n",
      "\n",
      "Fold: 3  Epoch: 65  Training loss = 1.4353  Validation loss = 2.5168  \n",
      "\n",
      "Fold: 3  Epoch: 66  Training loss = 1.4347  Validation loss = 2.5119  \n",
      "\n",
      "Fold: 3  Epoch: 67  Training loss = 1.4345  Validation loss = 2.5096  \n",
      "\n",
      "Fold: 3  Epoch: 68  Training loss = 1.4341  Validation loss = 2.5100  \n",
      "\n",
      "Fold: 3  Epoch: 69  Training loss = 1.4336  Validation loss = 2.5035  \n",
      "\n",
      "Fold: 3  Epoch: 70  Training loss = 1.4330  Validation loss = 2.4975  \n",
      "\n",
      "Fold: 3  Epoch: 71  Training loss = 1.4326  Validation loss = 2.4994  \n",
      "\n",
      "Fold: 3  Epoch: 72  Training loss = 1.4324  Validation loss = 2.4957  \n",
      "\n",
      "Fold: 3  Epoch: 73  Training loss = 1.4320  Validation loss = 2.4947  \n",
      "\n",
      "Fold: 3  Epoch: 74  Training loss = 1.4318  Validation loss = 2.4920  \n",
      "\n",
      "Fold: 3  Epoch: 75  Training loss = 1.4315  Validation loss = 2.4909  \n",
      "\n",
      "Fold: 3  Epoch: 76  Training loss = 1.4312  Validation loss = 2.4873  \n",
      "\n",
      "Fold: 3  Epoch: 77  Training loss = 1.4310  Validation loss = 2.4830  \n",
      "\n",
      "Fold: 3  Epoch: 78  Training loss = 1.4304  Validation loss = 2.4815  \n",
      "\n",
      "Fold: 3  Epoch: 79  Training loss = 1.4300  Validation loss = 2.4788  \n",
      "\n",
      "Fold: 3  Epoch: 80  Training loss = 1.4294  Validation loss = 2.4743  \n",
      "\n",
      "Fold: 3  Epoch: 81  Training loss = 1.4289  Validation loss = 2.4681  \n",
      "\n",
      "Fold: 3  Epoch: 82  Training loss = 1.4287  Validation loss = 2.4615  \n",
      "\n",
      "Fold: 3  Epoch: 83  Training loss = 1.4285  Validation loss = 2.4598  \n",
      "\n",
      "Fold: 3  Epoch: 84  Training loss = 1.4282  Validation loss = 2.4579  \n",
      "\n",
      "Fold: 3  Epoch: 85  Training loss = 1.4277  Validation loss = 2.4530  \n",
      "\n",
      "Fold: 3  Epoch: 86  Training loss = 1.4271  Validation loss = 2.4497  \n",
      "\n",
      "Fold: 3  Epoch: 87  Training loss = 1.4267  Validation loss = 2.4493  \n",
      "\n",
      "Fold: 3  Epoch: 88  Training loss = 1.4262  Validation loss = 2.4486  \n",
      "\n",
      "Fold: 3  Epoch: 89  Training loss = 1.4260  Validation loss = 2.4433  \n",
      "\n",
      "Fold: 3  Epoch: 90  Training loss = 1.4254  Validation loss = 2.4392  \n",
      "\n",
      "Fold: 3  Epoch: 91  Training loss = 1.4250  Validation loss = 2.4409  \n",
      "\n",
      "Fold: 3  Epoch: 92  Training loss = 1.4247  Validation loss = 2.4430  \n",
      "\n",
      "Fold: 3  Epoch: 93  Training loss = 1.4247  Validation loss = 2.4495  \n",
      "\n",
      "Fold: 3  Epoch: 94  Training loss = 1.4242  Validation loss = 2.4450  \n",
      "\n",
      "Fold: 3  Epoch: 95  Training loss = 1.4238  Validation loss = 2.4393  \n",
      "\n",
      "Fold: 3  Epoch: 96  Training loss = 1.4235  Validation loss = 2.4329  \n",
      "\n",
      "Fold: 3  Epoch: 97  Training loss = 1.4231  Validation loss = 2.4337  \n",
      "\n",
      "Fold: 3  Epoch: 98  Training loss = 1.4229  Validation loss = 2.4375  \n",
      "\n",
      "Fold: 3  Epoch: 99  Training loss = 1.4227  Validation loss = 2.4352  \n",
      "\n",
      "Fold: 3  Epoch: 100  Training loss = 1.4226  Validation loss = 2.4356  \n",
      "\n",
      "Fold: 3  Epoch: 101  Training loss = 1.4221  Validation loss = 2.4339  \n",
      "\n",
      "Fold: 3  Epoch: 102  Training loss = 1.4216  Validation loss = 2.4310  \n",
      "\n",
      "Fold: 3  Epoch: 103  Training loss = 1.4212  Validation loss = 2.4256  \n",
      "\n",
      "Fold: 3  Epoch: 104  Training loss = 1.4212  Validation loss = 2.4319  \n",
      "\n",
      "Fold: 3  Epoch: 105  Training loss = 1.4210  Validation loss = 2.4337  \n",
      "\n",
      "Fold: 3  Epoch: 106  Training loss = 1.4203  Validation loss = 2.4293  \n",
      "\n",
      "Fold: 3  Epoch: 107  Training loss = 1.4200  Validation loss = 2.4273  \n",
      "\n",
      "Fold: 3  Epoch: 108  Training loss = 1.4201  Validation loss = 2.4328  \n",
      "\n",
      "Fold: 3  Epoch: 109  Training loss = 1.4199  Validation loss = 2.4317  \n",
      "\n",
      "Fold: 3  Epoch: 110  Training loss = 1.4193  Validation loss = 2.4269  \n",
      "\n",
      "Fold: 3  Epoch: 111  Training loss = 1.4188  Validation loss = 2.4200  \n",
      "\n",
      "Fold: 3  Epoch: 112  Training loss = 1.4181  Validation loss = 2.4110  \n",
      "\n",
      "Fold: 3  Epoch: 113  Training loss = 1.4177  Validation loss = 2.4047  \n",
      "\n",
      "Fold: 3  Epoch: 114  Training loss = 1.4174  Validation loss = 2.4046  \n",
      "\n",
      "Fold: 3  Epoch: 115  Training loss = 1.4170  Validation loss = 2.4044  \n",
      "\n",
      "Fold: 3  Epoch: 116  Training loss = 1.4169  Validation loss = 2.4077  \n",
      "\n",
      "Fold: 3  Epoch: 117  Training loss = 1.4165  Validation loss = 2.4000  \n",
      "\n",
      "Fold: 3  Epoch: 118  Training loss = 1.4162  Validation loss = 2.4020  \n",
      "\n",
      "Fold: 3  Epoch: 119  Training loss = 1.4160  Validation loss = 2.4033  \n",
      "\n",
      "Fold: 3  Epoch: 120  Training loss = 1.4154  Validation loss = 2.3887  \n",
      "\n",
      "Fold: 3  Epoch: 121  Training loss = 1.4152  Validation loss = 2.3874  \n",
      "\n",
      "Fold: 3  Epoch: 122  Training loss = 1.4148  Validation loss = 2.3857  \n",
      "\n",
      "Fold: 3  Epoch: 123  Training loss = 1.4144  Validation loss = 2.3835  \n",
      "\n",
      "Fold: 3  Epoch: 124  Training loss = 1.4142  Validation loss = 2.3840  \n",
      "\n",
      "Fold: 3  Epoch: 125  Training loss = 1.4140  Validation loss = 2.3844  \n",
      "\n",
      "Fold: 3  Epoch: 126  Training loss = 1.4137  Validation loss = 2.3823  \n",
      "\n",
      "Fold: 3  Epoch: 127  Training loss = 1.4137  Validation loss = 2.3859  \n",
      "\n",
      "Fold: 3  Epoch: 128  Training loss = 1.4133  Validation loss = 2.3817  \n",
      "\n",
      "Fold: 3  Epoch: 129  Training loss = 1.4131  Validation loss = 2.3800  \n",
      "\n",
      "Fold: 3  Epoch: 130  Training loss = 1.4126  Validation loss = 2.3734  \n",
      "\n",
      "Fold: 3  Epoch: 131  Training loss = 1.4122  Validation loss = 2.3682  \n",
      "\n",
      "Fold: 3  Epoch: 132  Training loss = 1.4121  Validation loss = 2.3741  \n",
      "\n",
      "Fold: 3  Epoch: 133  Training loss = 1.4118  Validation loss = 2.3691  \n",
      "\n",
      "Fold: 3  Epoch: 134  Training loss = 1.4115  Validation loss = 2.3655  \n",
      "\n",
      "Fold: 3  Epoch: 135  Training loss = 1.4113  Validation loss = 2.3558  \n",
      "\n",
      "Fold: 3  Epoch: 136  Training loss = 1.4111  Validation loss = 2.3510  \n",
      "\n",
      "Fold: 3  Epoch: 137  Training loss = 1.4107  Validation loss = 2.3513  \n",
      "\n",
      "Fold: 3  Epoch: 138  Training loss = 1.4106  Validation loss = 2.3497  \n",
      "\n",
      "Fold: 3  Epoch: 139  Training loss = 1.4102  Validation loss = 2.3414  \n",
      "\n",
      "Fold: 3  Epoch: 140  Training loss = 1.4100  Validation loss = 2.3435  \n",
      "\n",
      "Fold: 3  Epoch: 141  Training loss = 1.4100  Validation loss = 2.3372  \n",
      "\n",
      "Fold: 3  Epoch: 142  Training loss = 1.4097  Validation loss = 2.3297  \n",
      "\n",
      "Fold: 3  Epoch: 143  Training loss = 1.4091  Validation loss = 2.3351  \n",
      "\n",
      "Fold: 3  Epoch: 144  Training loss = 1.4089  Validation loss = 2.3274  \n",
      "\n",
      "Fold: 3  Epoch: 145  Training loss = 1.4089  Validation loss = 2.3201  \n",
      "\n",
      "Fold: 3  Epoch: 146  Training loss = 1.4084  Validation loss = 2.3233  \n",
      "\n",
      "Fold: 3  Epoch: 147  Training loss = 1.4079  Validation loss = 2.3231  \n",
      "\n",
      "Fold: 3  Epoch: 148  Training loss = 1.4077  Validation loss = 2.3178  \n",
      "\n",
      "Fold: 3  Epoch: 149  Training loss = 1.4072  Validation loss = 2.3135  \n",
      "\n",
      "Fold: 3  Epoch: 150  Training loss = 1.4071  Validation loss = 2.3134  \n",
      "\n",
      "Fold: 3  Epoch: 151  Training loss = 1.4068  Validation loss = 2.3120  \n",
      "\n",
      "Fold: 3  Epoch: 152  Training loss = 1.4066  Validation loss = 2.3095  \n",
      "\n",
      "Fold: 3  Epoch: 153  Training loss = 1.4065  Validation loss = 2.3086  \n",
      "\n",
      "Fold: 3  Epoch: 154  Training loss = 1.4061  Validation loss = 2.3110  \n",
      "\n",
      "Fold: 3  Epoch: 155  Training loss = 1.4056  Validation loss = 2.3090  \n",
      "\n",
      "Fold: 3  Epoch: 156  Training loss = 1.4051  Validation loss = 2.3011  \n",
      "\n",
      "Fold: 3  Epoch: 157  Training loss = 1.4043  Validation loss = 2.3029  \n",
      "\n",
      "Fold: 3  Epoch: 158  Training loss = 1.4039  Validation loss = 2.3055  \n",
      "\n",
      "Fold: 3  Epoch: 159  Training loss = 1.4036  Validation loss = 2.2999  \n",
      "\n",
      "Fold: 3  Epoch: 160  Training loss = 1.4031  Validation loss = 2.2906  \n",
      "\n",
      "Fold: 3  Epoch: 161  Training loss = 1.4029  Validation loss = 2.2839  \n",
      "\n",
      "Fold: 3  Epoch: 162  Training loss = 1.4027  Validation loss = 2.2808  \n",
      "\n",
      "Fold: 3  Epoch: 163  Training loss = 1.4023  Validation loss = 2.2817  \n",
      "\n",
      "Fold: 3  Epoch: 164  Training loss = 1.4019  Validation loss = 2.2783  \n",
      "\n",
      "Fold: 3  Epoch: 165  Training loss = 1.4016  Validation loss = 2.2732  \n",
      "\n",
      "Fold: 3  Epoch: 166  Training loss = 1.4010  Validation loss = 2.2735  \n",
      "\n",
      "Fold: 3  Epoch: 167  Training loss = 1.4003  Validation loss = 2.2755  \n",
      "\n",
      "Fold: 3  Epoch: 168  Training loss = 1.4001  Validation loss = 2.2720  \n",
      "\n",
      "Fold: 3  Epoch: 169  Training loss = 1.3998  Validation loss = 2.2757  \n",
      "\n",
      "Fold: 3  Epoch: 170  Training loss = 1.3991  Validation loss = 2.2686  \n",
      "\n",
      "Fold: 3  Epoch: 171  Training loss = 1.3989  Validation loss = 2.2715  \n",
      "\n",
      "Fold: 3  Epoch: 172  Training loss = 1.3987  Validation loss = 2.2659  \n",
      "\n",
      "Fold: 3  Epoch: 173  Training loss = 1.3984  Validation loss = 2.2701  \n",
      "\n",
      "Fold: 3  Epoch: 174  Training loss = 1.3981  Validation loss = 2.2680  \n",
      "\n",
      "Fold: 3  Epoch: 175  Training loss = 1.3980  Validation loss = 2.2611  \n",
      "\n",
      "Fold: 3  Epoch: 176  Training loss = 1.3976  Validation loss = 2.2643  \n",
      "\n",
      "Fold: 3  Epoch: 177  Training loss = 1.3973  Validation loss = 2.2671  \n",
      "\n",
      "Fold: 3  Epoch: 178  Training loss = 1.3972  Validation loss = 2.2638  \n",
      "\n",
      "Fold: 3  Epoch: 179  Training loss = 1.3967  Validation loss = 2.2509  \n",
      "\n",
      "Fold: 3  Epoch: 180  Training loss = 1.3964  Validation loss = 2.2522  \n",
      "\n",
      "Fold: 3  Epoch: 181  Training loss = 1.3960  Validation loss = 2.2512  \n",
      "\n",
      "Fold: 3  Epoch: 182  Training loss = 1.3957  Validation loss = 2.2521  \n",
      "\n",
      "Fold: 3  Epoch: 183  Training loss = 1.3955  Validation loss = 2.2518  \n",
      "\n",
      "Fold: 3  Epoch: 184  Training loss = 1.3951  Validation loss = 2.2482  \n",
      "\n",
      "Fold: 3  Epoch: 185  Training loss = 1.3948  Validation loss = 2.2522  \n",
      "\n",
      "Fold: 3  Epoch: 186  Training loss = 1.3945  Validation loss = 2.2536  \n",
      "\n",
      "Fold: 3  Epoch: 187  Training loss = 1.3944  Validation loss = 2.2526  \n",
      "\n",
      "Fold: 3  Epoch: 188  Training loss = 1.3942  Validation loss = 2.2514  \n",
      "\n",
      "Fold: 3  Epoch: 189  Training loss = 1.3941  Validation loss = 2.2504  \n",
      "\n",
      "Fold: 3  Epoch: 190  Training loss = 1.3940  Validation loss = 2.2585  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 184  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.4103  Validation loss = 3.6617  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.4100  Validation loss = 3.6612  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.4092  Validation loss = 3.6562  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.4092  Validation loss = 3.6589  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.4082  Validation loss = 3.6538  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.4079  Validation loss = 3.6524  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.4071  Validation loss = 3.6473  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.4060  Validation loss = 3.6403  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.4053  Validation loss = 3.6366  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 1.4042  Validation loss = 3.6287  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.4034  Validation loss = 3.6215  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 1.4029  Validation loss = 3.6219  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.4025  Validation loss = 3.6208  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 1.4020  Validation loss = 3.6201  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.4012  Validation loss = 3.6149  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 1.4013  Validation loss = 3.6199  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 1.4002  Validation loss = 3.6120  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 1.3996  Validation loss = 3.6100  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 1.3988  Validation loss = 3.6041  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 1.3984  Validation loss = 3.6030  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 1.3980  Validation loss = 3.6014  \n",
      "\n",
      "Fold: 4  Epoch: 22  Training loss = 1.3978  Validation loss = 3.6010  \n",
      "\n",
      "Fold: 4  Epoch: 23  Training loss = 1.3973  Validation loss = 3.5989  \n",
      "\n",
      "Fold: 4  Epoch: 24  Training loss = 1.3962  Validation loss = 3.5925  \n",
      "\n",
      "Fold: 4  Epoch: 25  Training loss = 1.3954  Validation loss = 3.5888  \n",
      "\n",
      "Fold: 4  Epoch: 26  Training loss = 1.3950  Validation loss = 3.5875  \n",
      "\n",
      "Fold: 4  Epoch: 27  Training loss = 1.3944  Validation loss = 3.5818  \n",
      "\n",
      "Fold: 4  Epoch: 28  Training loss = 1.3937  Validation loss = 3.5788  \n",
      "\n",
      "Fold: 4  Epoch: 29  Training loss = 1.3931  Validation loss = 3.5761  \n",
      "\n",
      "Fold: 4  Epoch: 30  Training loss = 1.3926  Validation loss = 3.5737  \n",
      "\n",
      "Fold: 4  Epoch: 31  Training loss = 1.3910  Validation loss = 3.5600  \n",
      "\n",
      "Fold: 4  Epoch: 32  Training loss = 1.3901  Validation loss = 3.5556  \n",
      "\n",
      "Fold: 4  Epoch: 33  Training loss = 1.3895  Validation loss = 3.5492  \n",
      "\n",
      "Fold: 4  Epoch: 34  Training loss = 1.3891  Validation loss = 3.5476  \n",
      "\n",
      "Fold: 4  Epoch: 35  Training loss = 1.3887  Validation loss = 3.5457  \n",
      "\n",
      "Fold: 4  Epoch: 36  Training loss = 1.3879  Validation loss = 3.5372  \n",
      "\n",
      "Fold: 4  Epoch: 37  Training loss = 1.3873  Validation loss = 3.5352  \n",
      "\n",
      "Fold: 4  Epoch: 38  Training loss = 1.3867  Validation loss = 3.5339  \n",
      "\n",
      "Fold: 4  Epoch: 39  Training loss = 1.3861  Validation loss = 3.5293  \n",
      "\n",
      "Fold: 4  Epoch: 40  Training loss = 1.3856  Validation loss = 3.5280  \n",
      "\n",
      "Fold: 4  Epoch: 41  Training loss = 1.3854  Validation loss = 3.5301  \n",
      "\n",
      "Fold: 4  Epoch: 42  Training loss = 1.3848  Validation loss = 3.5260  \n",
      "\n",
      "Fold: 4  Epoch: 43  Training loss = 1.3841  Validation loss = 3.5229  \n",
      "\n",
      "Fold: 4  Epoch: 44  Training loss = 1.3836  Validation loss = 3.5204  \n",
      "\n",
      "Fold: 4  Epoch: 45  Training loss = 1.3830  Validation loss = 3.5170  \n",
      "\n",
      "Fold: 4  Epoch: 46  Training loss = 1.3825  Validation loss = 3.5131  \n",
      "\n",
      "Fold: 4  Epoch: 47  Training loss = 1.3822  Validation loss = 3.5136  \n",
      "\n",
      "Fold: 4  Epoch: 48  Training loss = 1.3816  Validation loss = 3.5118  \n",
      "\n",
      "Fold: 4  Epoch: 49  Training loss = 1.3810  Validation loss = 3.5044  \n",
      "\n",
      "Fold: 4  Epoch: 50  Training loss = 1.3803  Validation loss = 3.4983  \n",
      "\n",
      "Fold: 4  Epoch: 51  Training loss = 1.3796  Validation loss = 3.4955  \n",
      "\n",
      "Fold: 4  Epoch: 52  Training loss = 1.3794  Validation loss = 3.4943  \n",
      "\n",
      "Fold: 4  Epoch: 53  Training loss = 1.3789  Validation loss = 3.4870  \n",
      "\n",
      "Fold: 4  Epoch: 54  Training loss = 1.3785  Validation loss = 3.4835  \n",
      "\n",
      "Fold: 4  Epoch: 55  Training loss = 1.3779  Validation loss = 3.4820  \n",
      "\n",
      "Fold: 4  Epoch: 56  Training loss = 1.3776  Validation loss = 3.4818  \n",
      "\n",
      "Fold: 4  Epoch: 57  Training loss = 1.3770  Validation loss = 3.4787  \n",
      "\n",
      "Fold: 4  Epoch: 58  Training loss = 1.3765  Validation loss = 3.4809  \n",
      "\n",
      "Fold: 4  Epoch: 59  Training loss = 1.3764  Validation loss = 3.4858  \n",
      "\n",
      "Fold: 4  Epoch: 60  Training loss = 1.3758  Validation loss = 3.4838  \n",
      "\n",
      "Fold: 4  Epoch: 61  Training loss = 1.3754  Validation loss = 3.4772  \n",
      "\n",
      "Fold: 4  Epoch: 62  Training loss = 1.3750  Validation loss = 3.4786  \n",
      "\n",
      "Fold: 4  Epoch: 63  Training loss = 1.3746  Validation loss = 3.4758  \n",
      "\n",
      "Fold: 4  Epoch: 64  Training loss = 1.3741  Validation loss = 3.4792  \n",
      "\n",
      "Fold: 4  Epoch: 65  Training loss = 1.3734  Validation loss = 3.4749  \n",
      "\n",
      "Fold: 4  Epoch: 66  Training loss = 1.3731  Validation loss = 3.4730  \n",
      "\n",
      "Fold: 4  Epoch: 67  Training loss = 1.3724  Validation loss = 3.4678  \n",
      "\n",
      "Fold: 4  Epoch: 68  Training loss = 1.3720  Validation loss = 3.4658  \n",
      "\n",
      "Fold: 4  Epoch: 69  Training loss = 1.3716  Validation loss = 3.4685  \n",
      "\n",
      "Fold: 4  Epoch: 70  Training loss = 1.3713  Validation loss = 3.4706  \n",
      "\n",
      "Fold: 4  Epoch: 71  Training loss = 1.3706  Validation loss = 3.4650  \n",
      "\n",
      "Fold: 4  Epoch: 72  Training loss = 1.3701  Validation loss = 3.4618  \n",
      "\n",
      "Fold: 4  Epoch: 73  Training loss = 1.3695  Validation loss = 3.4557  \n",
      "\n",
      "Fold: 4  Epoch: 74  Training loss = 1.3689  Validation loss = 3.4595  \n",
      "\n",
      "Fold: 4  Epoch: 75  Training loss = 1.3682  Validation loss = 3.4539  \n",
      "\n",
      "Fold: 4  Epoch: 76  Training loss = 1.3674  Validation loss = 3.4505  \n",
      "\n",
      "Fold: 4  Epoch: 77  Training loss = 1.3668  Validation loss = 3.4462  \n",
      "\n",
      "Fold: 4  Epoch: 78  Training loss = 1.3660  Validation loss = 3.4396  \n",
      "\n",
      "Fold: 4  Epoch: 79  Training loss = 1.3655  Validation loss = 3.4378  \n",
      "\n",
      "Fold: 4  Epoch: 80  Training loss = 1.3647  Validation loss = 3.4326  \n",
      "\n",
      "Fold: 4  Epoch: 81  Training loss = 1.3644  Validation loss = 3.4371  \n",
      "\n",
      "Fold: 4  Epoch: 82  Training loss = 1.3635  Validation loss = 3.4267  \n",
      "\n",
      "Fold: 4  Epoch: 83  Training loss = 1.3632  Validation loss = 3.4258  \n",
      "\n",
      "Fold: 4  Epoch: 84  Training loss = 1.3628  Validation loss = 3.4286  \n",
      "\n",
      "Fold: 4  Epoch: 85  Training loss = 1.3627  Validation loss = 3.4322  \n",
      "\n",
      "Fold: 4  Epoch: 86  Training loss = 1.3621  Validation loss = 3.4277  \n",
      "\n",
      "Fold: 4  Epoch: 87  Training loss = 1.3616  Validation loss = 3.4295  \n",
      "\n",
      "Fold: 4  Epoch: 88  Training loss = 1.3610  Validation loss = 3.4260  \n",
      "\n",
      "Fold: 4  Epoch: 89  Training loss = 1.3600  Validation loss = 3.4163  \n",
      "\n",
      "Fold: 4  Epoch: 90  Training loss = 1.3596  Validation loss = 3.4138  \n",
      "\n",
      "Fold: 4  Epoch: 91  Training loss = 1.3594  Validation loss = 3.4144  \n",
      "\n",
      "Fold: 4  Epoch: 92  Training loss = 1.3589  Validation loss = 3.4142  \n",
      "\n",
      "Fold: 4  Epoch: 93  Training loss = 1.3587  Validation loss = 3.4153  \n",
      "\n",
      "Fold: 4  Epoch: 94  Training loss = 1.3582  Validation loss = 3.4134  \n",
      "\n",
      "Fold: 4  Epoch: 95  Training loss = 1.3576  Validation loss = 3.4076  \n",
      "\n",
      "Fold: 4  Epoch: 96  Training loss = 1.3569  Validation loss = 3.4000  \n",
      "\n",
      "Fold: 4  Epoch: 97  Training loss = 1.3563  Validation loss = 3.3974  \n",
      "\n",
      "Fold: 4  Epoch: 98  Training loss = 1.3555  Validation loss = 3.3903  \n",
      "\n",
      "Fold: 4  Epoch: 99  Training loss = 1.3547  Validation loss = 3.3808  \n",
      "\n",
      "Fold: 4  Epoch: 100  Training loss = 1.3543  Validation loss = 3.3768  \n",
      "\n",
      "Fold: 4  Epoch: 101  Training loss = 1.3536  Validation loss = 3.3689  \n",
      "\n",
      "Fold: 4  Epoch: 102  Training loss = 1.3532  Validation loss = 3.3659  \n",
      "\n",
      "Fold: 4  Epoch: 103  Training loss = 1.3526  Validation loss = 3.3642  \n",
      "\n",
      "Fold: 4  Epoch: 104  Training loss = 1.3522  Validation loss = 3.3638  \n",
      "\n",
      "Fold: 4  Epoch: 105  Training loss = 1.3520  Validation loss = 3.3628  \n",
      "\n",
      "Fold: 4  Epoch: 106  Training loss = 1.3516  Validation loss = 3.3642  \n",
      "\n",
      "Fold: 4  Epoch: 107  Training loss = 1.3510  Validation loss = 3.3663  \n",
      "\n",
      "Fold: 4  Epoch: 108  Training loss = 1.3504  Validation loss = 3.3626  \n",
      "\n",
      "Fold: 4  Epoch: 109  Training loss = 1.3499  Validation loss = 3.3612  \n",
      "\n",
      "Fold: 4  Epoch: 110  Training loss = 1.3491  Validation loss = 3.3566  \n",
      "\n",
      "Fold: 4  Epoch: 111  Training loss = 1.3489  Validation loss = 3.3605  \n",
      "\n",
      "Fold: 4  Epoch: 112  Training loss = 1.3479  Validation loss = 3.3464  \n",
      "\n",
      "Fold: 4  Epoch: 113  Training loss = 1.3478  Validation loss = 3.3464  \n",
      "\n",
      "Fold: 4  Epoch: 114  Training loss = 1.3473  Validation loss = 3.3416  \n",
      "\n",
      "Fold: 4  Epoch: 115  Training loss = 1.3467  Validation loss = 3.3386  \n",
      "\n",
      "Fold: 4  Epoch: 116  Training loss = 1.3462  Validation loss = 3.3312  \n",
      "\n",
      "Fold: 4  Epoch: 117  Training loss = 1.3456  Validation loss = 3.3337  \n",
      "\n",
      "Fold: 4  Epoch: 118  Training loss = 1.3454  Validation loss = 3.3366  \n",
      "\n",
      "Fold: 4  Epoch: 119  Training loss = 1.3449  Validation loss = 3.3333  \n",
      "\n",
      "Fold: 4  Epoch: 120  Training loss = 1.3445  Validation loss = 3.3348  \n",
      "\n",
      "Fold: 4  Epoch: 121  Training loss = 1.3441  Validation loss = 3.3343  \n",
      "\n",
      "Fold: 4  Epoch: 122  Training loss = 1.3438  Validation loss = 3.3357  \n",
      "\n",
      "Fold: 4  Epoch: 123  Training loss = 1.3432  Validation loss = 3.3276  \n",
      "\n",
      "Fold: 4  Epoch: 124  Training loss = 1.3427  Validation loss = 3.3198  \n",
      "\n",
      "Fold: 4  Epoch: 125  Training loss = 1.3424  Validation loss = 3.3093  \n",
      "\n",
      "Fold: 4  Epoch: 126  Training loss = 1.3419  Validation loss = 3.3055  \n",
      "\n",
      "Fold: 4  Epoch: 127  Training loss = 1.3412  Validation loss = 3.3008  \n",
      "\n",
      "Fold: 4  Epoch: 128  Training loss = 1.3406  Validation loss = 3.3019  \n",
      "\n",
      "Fold: 4  Epoch: 129  Training loss = 1.3401  Validation loss = 3.2972  \n",
      "\n",
      "Fold: 4  Epoch: 130  Training loss = 1.3393  Validation loss = 3.2973  \n",
      "\n",
      "Fold: 4  Epoch: 131  Training loss = 1.3394  Validation loss = 3.2926  \n",
      "\n",
      "Fold: 4  Epoch: 132  Training loss = 1.3389  Validation loss = 3.2869  \n",
      "\n",
      "Fold: 4  Epoch: 133  Training loss = 1.3381  Validation loss = 3.2851  \n",
      "\n",
      "Fold: 4  Epoch: 134  Training loss = 1.3377  Validation loss = 3.2835  \n",
      "\n",
      "Fold: 4  Epoch: 135  Training loss = 1.3375  Validation loss = 3.2799  \n",
      "\n",
      "Fold: 4  Epoch: 136  Training loss = 1.3370  Validation loss = 3.2784  \n",
      "\n",
      "Fold: 4  Epoch: 137  Training loss = 1.3363  Validation loss = 3.2797  \n",
      "\n",
      "Fold: 4  Epoch: 138  Training loss = 1.3360  Validation loss = 3.2777  \n",
      "\n",
      "Fold: 4  Epoch: 139  Training loss = 1.3358  Validation loss = 3.2763  \n",
      "\n",
      "Fold: 4  Epoch: 140  Training loss = 1.3357  Validation loss = 3.2716  \n",
      "\n",
      "Fold: 4  Epoch: 141  Training loss = 1.3351  Validation loss = 3.2697  \n",
      "\n",
      "Fold: 4  Epoch: 142  Training loss = 1.3348  Validation loss = 3.2734  \n",
      "\n",
      "Fold: 4  Epoch: 143  Training loss = 1.3342  Validation loss = 3.2745  \n",
      "\n",
      "Fold: 4  Epoch: 144  Training loss = 1.3338  Validation loss = 3.2711  \n",
      "\n",
      "Fold: 4  Epoch: 145  Training loss = 1.3333  Validation loss = 3.2648  \n",
      "\n",
      "Fold: 4  Epoch: 146  Training loss = 1.3327  Validation loss = 3.2640  \n",
      "\n",
      "Fold: 4  Epoch: 147  Training loss = 1.3322  Validation loss = 3.2631  \n",
      "\n",
      "Fold: 4  Epoch: 148  Training loss = 1.3318  Validation loss = 3.2644  \n",
      "\n",
      "Fold: 4  Epoch: 149  Training loss = 1.3314  Validation loss = 3.2616  \n",
      "\n",
      "Fold: 4  Epoch: 150  Training loss = 1.3311  Validation loss = 3.2628  \n",
      "\n",
      "Fold: 4  Epoch: 151  Training loss = 1.3307  Validation loss = 3.2571  \n",
      "\n",
      "Fold: 4  Epoch: 152  Training loss = 1.3304  Validation loss = 3.2502  \n",
      "\n",
      "Fold: 4  Epoch: 153  Training loss = 1.3299  Validation loss = 3.2523  \n",
      "\n",
      "Fold: 4  Epoch: 154  Training loss = 1.3297  Validation loss = 3.2471  \n",
      "\n",
      "Fold: 4  Epoch: 155  Training loss = 1.3295  Validation loss = 3.2423  \n",
      "\n",
      "Fold: 4  Epoch: 156  Training loss = 1.3294  Validation loss = 3.2369  \n",
      "\n",
      "Fold: 4  Epoch: 157  Training loss = 1.3289  Validation loss = 3.2390  \n",
      "\n",
      "Fold: 4  Epoch: 158  Training loss = 1.3287  Validation loss = 3.2334  \n",
      "\n",
      "Fold: 4  Epoch: 159  Training loss = 1.3284  Validation loss = 3.2319  \n",
      "\n",
      "Fold: 4  Epoch: 160  Training loss = 1.3277  Validation loss = 3.2333  \n",
      "\n",
      "Fold: 4  Epoch: 161  Training loss = 1.3273  Validation loss = 3.2323  \n",
      "\n",
      "Fold: 4  Epoch: 162  Training loss = 1.3268  Validation loss = 3.2300  \n",
      "\n",
      "Fold: 4  Epoch: 163  Training loss = 1.3264  Validation loss = 3.2333  \n",
      "\n",
      "Fold: 4  Epoch: 164  Training loss = 1.3262  Validation loss = 3.2328  \n",
      "\n",
      "Fold: 4  Epoch: 165  Training loss = 1.3260  Validation loss = 3.2327  \n",
      "\n",
      "Fold: 4  Epoch: 166  Training loss = 1.3258  Validation loss = 3.2244  \n",
      "\n",
      "Fold: 4  Epoch: 167  Training loss = 1.3255  Validation loss = 3.2224  \n",
      "\n",
      "Fold: 4  Epoch: 168  Training loss = 1.3249  Validation loss = 3.2218  \n",
      "\n",
      "Fold: 4  Epoch: 169  Training loss = 1.3246  Validation loss = 3.2196  \n",
      "\n",
      "Fold: 4  Epoch: 170  Training loss = 1.3243  Validation loss = 3.2156  \n",
      "\n",
      "Fold: 4  Epoch: 171  Training loss = 1.3238  Validation loss = 3.2157  \n",
      "\n",
      "Fold: 4  Epoch: 172  Training loss = 1.3233  Validation loss = 3.2162  \n",
      "\n",
      "Fold: 4  Epoch: 173  Training loss = 1.3229  Validation loss = 3.2147  \n",
      "\n",
      "Fold: 4  Epoch: 174  Training loss = 1.3227  Validation loss = 3.2202  \n",
      "\n",
      "Fold: 4  Epoch: 175  Training loss = 1.3223  Validation loss = 3.2182  \n",
      "\n",
      "Fold: 4  Epoch: 176  Training loss = 1.3219  Validation loss = 3.2166  \n",
      "\n",
      "Fold: 4  Epoch: 177  Training loss = 1.3215  Validation loss = 3.2157  \n",
      "\n",
      "Fold: 4  Epoch: 178  Training loss = 1.3213  Validation loss = 3.2101  \n",
      "\n",
      "Fold: 4  Epoch: 179  Training loss = 1.3208  Validation loss = 3.2077  \n",
      "\n",
      "Fold: 4  Epoch: 180  Training loss = 1.3203  Validation loss = 3.2075  \n",
      "\n",
      "Fold: 4  Epoch: 181  Training loss = 1.3199  Validation loss = 3.2073  \n",
      "\n",
      "Fold: 4  Epoch: 182  Training loss = 1.3197  Validation loss = 3.2029  \n",
      "\n",
      "Fold: 4  Epoch: 183  Training loss = 1.3195  Validation loss = 3.2036  \n",
      "\n",
      "Fold: 4  Epoch: 184  Training loss = 1.3192  Validation loss = 3.2000  \n",
      "\n",
      "Fold: 4  Epoch: 185  Training loss = 1.3187  Validation loss = 3.1954  \n",
      "\n",
      "Fold: 4  Epoch: 186  Training loss = 1.3185  Validation loss = 3.1903  \n",
      "\n",
      "Fold: 4  Epoch: 187  Training loss = 1.3178  Validation loss = 3.1930  \n",
      "\n",
      "Fold: 4  Epoch: 188  Training loss = 1.3176  Validation loss = 3.1888  \n",
      "\n",
      "Fold: 4  Epoch: 189  Training loss = 1.3171  Validation loss = 3.1935  \n",
      "\n",
      "Fold: 4  Epoch: 190  Training loss = 1.3169  Validation loss = 3.1948  \n",
      "\n",
      "Fold: 4  Epoch: 191  Training loss = 1.3167  Validation loss = 3.1973  \n",
      "\n",
      "Fold: 4  Epoch: 192  Training loss = 1.3163  Validation loss = 3.1937  \n",
      "\n",
      "Fold: 4  Epoch: 193  Training loss = 1.3160  Validation loss = 3.1930  \n",
      "\n",
      "Fold: 4  Epoch: 194  Training loss = 1.3156  Validation loss = 3.1921  \n",
      "\n",
      "Fold: 4  Epoch: 195  Training loss = 1.3154  Validation loss = 3.1922  \n",
      "\n",
      "Fold: 4  Epoch: 196  Training loss = 1.3149  Validation loss = 3.1853  \n",
      "\n",
      "Fold: 4  Epoch: 197  Training loss = 1.3146  Validation loss = 3.1852  \n",
      "\n",
      "Fold: 4  Epoch: 198  Training loss = 1.3141  Validation loss = 3.1821  \n",
      "\n",
      "Fold: 4  Epoch: 199  Training loss = 1.3138  Validation loss = 3.1793  \n",
      "\n",
      "Fold: 4  Epoch: 200  Training loss = 1.3136  Validation loss = 3.1806  \n",
      "\n",
      "Fold: 4  Epoch: 201  Training loss = 1.3133  Validation loss = 3.1777  \n",
      "\n",
      "Fold: 4  Epoch: 202  Training loss = 1.3130  Validation loss = 3.1769  \n",
      "\n",
      "Fold: 4  Epoch: 203  Training loss = 1.3129  Validation loss = 3.1786  \n",
      "\n",
      "Fold: 4  Epoch: 204  Training loss = 1.3128  Validation loss = 3.1786  \n",
      "\n",
      "Fold: 4  Epoch: 205  Training loss = 1.3123  Validation loss = 3.1725  \n",
      "\n",
      "Fold: 4  Epoch: 206  Training loss = 1.3121  Validation loss = 3.1726  \n",
      "\n",
      "Fold: 4  Epoch: 207  Training loss = 1.3117  Validation loss = 3.1708  \n",
      "\n",
      "Fold: 4  Epoch: 208  Training loss = 1.3112  Validation loss = 3.1632  \n",
      "\n",
      "Fold: 4  Epoch: 209  Training loss = 1.3109  Validation loss = 3.1597  \n",
      "\n",
      "Fold: 4  Epoch: 210  Training loss = 1.3106  Validation loss = 3.1517  \n",
      "\n",
      "Fold: 4  Epoch: 211  Training loss = 1.3102  Validation loss = 3.1499  \n",
      "\n",
      "Fold: 4  Epoch: 212  Training loss = 1.3100  Validation loss = 3.1544  \n",
      "\n",
      "Fold: 4  Epoch: 213  Training loss = 1.3098  Validation loss = 3.1548  \n",
      "\n",
      "Fold: 4  Epoch: 214  Training loss = 1.3094  Validation loss = 3.1492  \n",
      "\n",
      "Fold: 4  Epoch: 215  Training loss = 1.3092  Validation loss = 3.1444  \n",
      "\n",
      "Fold: 4  Epoch: 216  Training loss = 1.3089  Validation loss = 3.1435  \n",
      "\n",
      "Fold: 4  Epoch: 217  Training loss = 1.3085  Validation loss = 3.1470  \n",
      "\n",
      "Fold: 4  Epoch: 218  Training loss = 1.3082  Validation loss = 3.1468  \n",
      "\n",
      "Fold: 4  Epoch: 219  Training loss = 1.3077  Validation loss = 3.1428  \n",
      "\n",
      "Fold: 4  Epoch: 220  Training loss = 1.3073  Validation loss = 3.1403  \n",
      "\n",
      "Fold: 4  Epoch: 221  Training loss = 1.3070  Validation loss = 3.1383  \n",
      "\n",
      "Fold: 4  Epoch: 222  Training loss = 1.3067  Validation loss = 3.1410  \n",
      "\n",
      "Fold: 4  Epoch: 223  Training loss = 1.3065  Validation loss = 3.1439  \n",
      "\n",
      "Fold: 4  Epoch: 224  Training loss = 1.3060  Validation loss = 3.1367  \n",
      "\n",
      "Fold: 4  Epoch: 225  Training loss = 1.3054  Validation loss = 3.1301  \n",
      "\n",
      "Fold: 4  Epoch: 226  Training loss = 1.3053  Validation loss = 3.1313  \n",
      "\n",
      "Fold: 4  Epoch: 227  Training loss = 1.3051  Validation loss = 3.1197  \n",
      "\n",
      "Fold: 4  Epoch: 228  Training loss = 1.3046  Validation loss = 3.1198  \n",
      "\n",
      "Fold: 4  Epoch: 229  Training loss = 1.3043  Validation loss = 3.1230  \n",
      "\n",
      "Fold: 4  Epoch: 230  Training loss = 1.3041  Validation loss = 3.1259  \n",
      "\n",
      "Fold: 4  Epoch: 231  Training loss = 1.3038  Validation loss = 3.1227  \n",
      "\n",
      "Fold: 4  Epoch: 232  Training loss = 1.3034  Validation loss = 3.1180  \n",
      "\n",
      "Fold: 4  Epoch: 233  Training loss = 1.3027  Validation loss = 3.1133  \n",
      "\n",
      "Fold: 4  Epoch: 234  Training loss = 1.3024  Validation loss = 3.1087  \n",
      "\n",
      "Fold: 4  Epoch: 235  Training loss = 1.3020  Validation loss = 3.1070  \n",
      "\n",
      "Fold: 4  Epoch: 236  Training loss = 1.3019  Validation loss = 3.1012  \n",
      "\n",
      "Fold: 4  Epoch: 237  Training loss = 1.3019  Validation loss = 3.0962  \n",
      "\n",
      "Fold: 4  Epoch: 238  Training loss = 1.3014  Validation loss = 3.0944  \n",
      "\n",
      "Fold: 4  Epoch: 239  Training loss = 1.3011  Validation loss = 3.0902  \n",
      "\n",
      "Fold: 4  Epoch: 240  Training loss = 1.3010  Validation loss = 3.0886  \n",
      "\n",
      "Fold: 4  Epoch: 241  Training loss = 1.3010  Validation loss = 3.0819  \n",
      "\n",
      "Fold: 4  Epoch: 242  Training loss = 1.3004  Validation loss = 3.0821  \n",
      "\n",
      "Fold: 4  Epoch: 243  Training loss = 1.3003  Validation loss = 3.0812  \n",
      "\n",
      "Fold: 4  Epoch: 244  Training loss = 1.2998  Validation loss = 3.0813  \n",
      "\n",
      "Fold: 4  Epoch: 245  Training loss = 1.2994  Validation loss = 3.0778  \n",
      "\n",
      "Fold: 4  Epoch: 246  Training loss = 1.2988  Validation loss = 3.0801  \n",
      "\n",
      "Fold: 4  Epoch: 247  Training loss = 1.2980  Validation loss = 3.0833  \n",
      "\n",
      "Fold: 4  Epoch: 248  Training loss = 1.2977  Validation loss = 3.0812  \n",
      "\n",
      "Fold: 4  Epoch: 249  Training loss = 1.2977  Validation loss = 3.0746  \n",
      "\n",
      "Fold: 4  Epoch: 250  Training loss = 1.2979  Validation loss = 3.0698  \n",
      "\n",
      "Fold: 4  Epoch: 251  Training loss = 1.2974  Validation loss = 3.0721  \n",
      "\n",
      "Fold: 4  Epoch: 252  Training loss = 1.2969  Validation loss = 3.0717  \n",
      "\n",
      "Fold: 4  Epoch: 253  Training loss = 1.2968  Validation loss = 3.0645  \n",
      "\n",
      "Fold: 4  Epoch: 254  Training loss = 1.2964  Validation loss = 3.0658  \n",
      "\n",
      "Fold: 4  Epoch: 255  Training loss = 1.2962  Validation loss = 3.0629  \n",
      "\n",
      "Fold: 4  Epoch: 256  Training loss = 1.2960  Validation loss = 3.0567  \n",
      "\n",
      "Fold: 4  Epoch: 257  Training loss = 1.2958  Validation loss = 3.0544  \n",
      "\n",
      "Fold: 4  Epoch: 258  Training loss = 1.2952  Validation loss = 3.0586  \n",
      "\n",
      "Fold: 4  Epoch: 259  Training loss = 1.2946  Validation loss = 3.0562  \n",
      "\n",
      "Fold: 4  Epoch: 260  Training loss = 1.2941  Validation loss = 3.0629  \n",
      "\n",
      "Fold: 4  Epoch: 261  Training loss = 1.2937  Validation loss = 3.0592  \n",
      "\n",
      "Fold: 4  Epoch: 262  Training loss = 1.2936  Validation loss = 3.0597  \n",
      "\n",
      "Fold: 4  Epoch: 263  Training loss = 1.2935  Validation loss = 3.0598  \n",
      "\n",
      "Fold: 4  Epoch: 264  Training loss = 1.2933  Validation loss = 3.0603  \n",
      "\n",
      "Fold: 4  Epoch: 265  Training loss = 1.2930  Validation loss = 3.0585  \n",
      "\n",
      "Fold: 4  Epoch: 266  Training loss = 1.2928  Validation loss = 3.0578  \n",
      "\n",
      "Fold: 4  Epoch: 267  Training loss = 1.2927  Validation loss = 3.0617  \n",
      "\n",
      "Fold: 4  Epoch: 268  Training loss = 1.2927  Validation loss = 3.0642  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 257  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.4214  Validation loss = 2.8732  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.4181  Validation loss = 2.8621  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.4157  Validation loss = 2.8542  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.4146  Validation loss = 2.8505  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.4123  Validation loss = 2.8426  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 1.4108  Validation loss = 2.8378  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 1.4090  Validation loss = 2.8307  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.4078  Validation loss = 2.8262  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 1.4074  Validation loss = 2.8248  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.4045  Validation loss = 2.8125  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 1.4022  Validation loss = 2.8030  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 1.4011  Validation loss = 2.7979  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 1.4001  Validation loss = 2.7936  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.3976  Validation loss = 2.7824  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 1.3961  Validation loss = 2.7773  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 1.3948  Validation loss = 2.7720  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 1.3939  Validation loss = 2.7697  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 1.3928  Validation loss = 2.7663  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 1.3912  Validation loss = 2.7578  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 1.3892  Validation loss = 2.7490  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 1.3880  Validation loss = 2.7438  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 1.3869  Validation loss = 2.7378  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 1.3854  Validation loss = 2.7307  \n",
      "\n",
      "Fold: 5  Epoch: 24  Training loss = 1.3844  Validation loss = 2.7272  \n",
      "\n",
      "Fold: 5  Epoch: 25  Training loss = 1.3836  Validation loss = 2.7239  \n",
      "\n",
      "Fold: 5  Epoch: 26  Training loss = 1.3819  Validation loss = 2.7167  \n",
      "\n",
      "Fold: 5  Epoch: 27  Training loss = 1.3814  Validation loss = 2.7163  \n",
      "\n",
      "Fold: 5  Epoch: 28  Training loss = 1.3804  Validation loss = 2.7115  \n",
      "\n",
      "Fold: 5  Epoch: 29  Training loss = 1.3790  Validation loss = 2.7031  \n",
      "\n",
      "Fold: 5  Epoch: 30  Training loss = 1.3785  Validation loss = 2.7035  \n",
      "\n",
      "Fold: 5  Epoch: 31  Training loss = 1.3771  Validation loss = 2.6985  \n",
      "\n",
      "Fold: 5  Epoch: 32  Training loss = 1.3763  Validation loss = 2.6950  \n",
      "\n",
      "Fold: 5  Epoch: 33  Training loss = 1.3750  Validation loss = 2.6884  \n",
      "\n",
      "Fold: 5  Epoch: 34  Training loss = 1.3726  Validation loss = 2.6775  \n",
      "\n",
      "Fold: 5  Epoch: 35  Training loss = 1.3718  Validation loss = 2.6752  \n",
      "\n",
      "Fold: 5  Epoch: 36  Training loss = 1.3716  Validation loss = 2.6751  \n",
      "\n",
      "Fold: 5  Epoch: 37  Training loss = 1.3712  Validation loss = 2.6764  \n",
      "\n",
      "Fold: 5  Epoch: 38  Training loss = 1.3704  Validation loss = 2.6726  \n",
      "\n",
      "Fold: 5  Epoch: 39  Training loss = 1.3695  Validation loss = 2.6696  \n",
      "\n",
      "Fold: 5  Epoch: 40  Training loss = 1.3685  Validation loss = 2.6644  \n",
      "\n",
      "Fold: 5  Epoch: 41  Training loss = 1.3679  Validation loss = 2.6632  \n",
      "\n",
      "Fold: 5  Epoch: 42  Training loss = 1.3666  Validation loss = 2.6571  \n",
      "\n",
      "Fold: 5  Epoch: 43  Training loss = 1.3657  Validation loss = 2.6545  \n",
      "\n",
      "Fold: 5  Epoch: 44  Training loss = 1.3644  Validation loss = 2.6490  \n",
      "\n",
      "Fold: 5  Epoch: 45  Training loss = 1.3634  Validation loss = 2.6446  \n",
      "\n",
      "Fold: 5  Epoch: 46  Training loss = 1.3623  Validation loss = 2.6398  \n",
      "\n",
      "Fold: 5  Epoch: 47  Training loss = 1.3613  Validation loss = 2.6360  \n",
      "\n",
      "Fold: 5  Epoch: 48  Training loss = 1.3607  Validation loss = 2.6332  \n",
      "\n",
      "Fold: 5  Epoch: 49  Training loss = 1.3591  Validation loss = 2.6254  \n",
      "\n",
      "Fold: 5  Epoch: 50  Training loss = 1.3577  Validation loss = 2.6196  \n",
      "\n",
      "Fold: 5  Epoch: 51  Training loss = 1.3570  Validation loss = 2.6175  \n",
      "\n",
      "Fold: 5  Epoch: 52  Training loss = 1.3557  Validation loss = 2.6111  \n",
      "\n",
      "Fold: 5  Epoch: 53  Training loss = 1.3551  Validation loss = 2.6081  \n",
      "\n",
      "Fold: 5  Epoch: 54  Training loss = 1.3542  Validation loss = 2.6039  \n",
      "\n",
      "Fold: 5  Epoch: 55  Training loss = 1.3528  Validation loss = 2.5968  \n",
      "\n",
      "Fold: 5  Epoch: 56  Training loss = 1.3518  Validation loss = 2.5940  \n",
      "\n",
      "Fold: 5  Epoch: 57  Training loss = 1.3502  Validation loss = 2.5857  \n",
      "\n",
      "Fold: 5  Epoch: 58  Training loss = 1.3494  Validation loss = 2.5839  \n",
      "\n",
      "Fold: 5  Epoch: 59  Training loss = 1.3485  Validation loss = 2.5788  \n",
      "\n",
      "Fold: 5  Epoch: 60  Training loss = 1.3477  Validation loss = 2.5723  \n",
      "\n",
      "Fold: 5  Epoch: 61  Training loss = 1.3473  Validation loss = 2.5712  \n",
      "\n",
      "Fold: 5  Epoch: 62  Training loss = 1.3461  Validation loss = 2.5651  \n",
      "\n",
      "Fold: 5  Epoch: 63  Training loss = 1.3459  Validation loss = 2.5653  \n",
      "\n",
      "Fold: 5  Epoch: 64  Training loss = 1.3449  Validation loss = 2.5612  \n",
      "\n",
      "Fold: 5  Epoch: 65  Training loss = 1.3437  Validation loss = 2.5563  \n",
      "\n",
      "Fold: 5  Epoch: 66  Training loss = 1.3431  Validation loss = 2.5554  \n",
      "\n",
      "Fold: 5  Epoch: 67  Training loss = 1.3423  Validation loss = 2.5531  \n",
      "\n",
      "Fold: 5  Epoch: 68  Training loss = 1.3414  Validation loss = 2.5490  \n",
      "\n",
      "Fold: 5  Epoch: 69  Training loss = 1.3401  Validation loss = 2.5413  \n",
      "\n",
      "Fold: 5  Epoch: 70  Training loss = 1.3384  Validation loss = 2.5321  \n",
      "\n",
      "Fold: 5  Epoch: 71  Training loss = 1.3378  Validation loss = 2.5305  \n",
      "\n",
      "Fold: 5  Epoch: 72  Training loss = 1.3363  Validation loss = 2.5230  \n",
      "\n",
      "Fold: 5  Epoch: 73  Training loss = 1.3353  Validation loss = 2.5156  \n",
      "\n",
      "Fold: 5  Epoch: 74  Training loss = 1.3337  Validation loss = 2.5066  \n",
      "\n",
      "Fold: 5  Epoch: 75  Training loss = 1.3329  Validation loss = 2.5063  \n",
      "\n",
      "Fold: 5  Epoch: 76  Training loss = 1.3326  Validation loss = 2.5057  \n",
      "\n",
      "Fold: 5  Epoch: 77  Training loss = 1.3311  Validation loss = 2.4972  \n",
      "\n",
      "Fold: 5  Epoch: 78  Training loss = 1.3301  Validation loss = 2.4869  \n",
      "\n",
      "Fold: 5  Epoch: 79  Training loss = 1.3288  Validation loss = 2.4811  \n",
      "\n",
      "Fold: 5  Epoch: 80  Training loss = 1.3281  Validation loss = 2.4765  \n",
      "\n",
      "Fold: 5  Epoch: 81  Training loss = 1.3271  Validation loss = 2.4721  \n",
      "\n",
      "Fold: 5  Epoch: 82  Training loss = 1.3264  Validation loss = 2.4718  \n",
      "\n",
      "Fold: 5  Epoch: 83  Training loss = 1.3258  Validation loss = 2.4744  \n",
      "\n",
      "Fold: 5  Epoch: 84  Training loss = 1.3252  Validation loss = 2.4718  \n",
      "\n",
      "Fold: 5  Epoch: 85  Training loss = 1.3248  Validation loss = 2.4706  \n",
      "\n",
      "Fold: 5  Epoch: 86  Training loss = 1.3247  Validation loss = 2.4715  \n",
      "\n",
      "Fold: 5  Epoch: 87  Training loss = 1.3241  Validation loss = 2.4720  \n",
      "\n",
      "Fold: 5  Epoch: 88  Training loss = 1.3246  Validation loss = 2.4770  \n",
      "\n",
      "Fold: 5  Epoch: 89  Training loss = 1.3233  Validation loss = 2.4703  \n",
      "\n",
      "Fold: 5  Epoch: 90  Training loss = 1.3231  Validation loss = 2.4709  \n",
      "\n",
      "Fold: 5  Epoch: 91  Training loss = 1.3232  Validation loss = 2.4718  \n",
      "\n",
      "Fold: 5  Epoch: 92  Training loss = 1.3221  Validation loss = 2.4663  \n",
      "\n",
      "Fold: 5  Epoch: 93  Training loss = 1.3207  Validation loss = 2.4606  \n",
      "\n",
      "Fold: 5  Epoch: 94  Training loss = 1.3191  Validation loss = 2.4514  \n",
      "\n",
      "Fold: 5  Epoch: 95  Training loss = 1.3180  Validation loss = 2.4456  \n",
      "\n",
      "Fold: 5  Epoch: 96  Training loss = 1.3174  Validation loss = 2.4429  \n",
      "\n",
      "Fold: 5  Epoch: 97  Training loss = 1.3165  Validation loss = 2.4378  \n",
      "\n",
      "Fold: 5  Epoch: 98  Training loss = 1.3156  Validation loss = 2.4309  \n",
      "\n",
      "Fold: 5  Epoch: 99  Training loss = 1.3154  Validation loss = 2.4309  \n",
      "\n",
      "Fold: 5  Epoch: 100  Training loss = 1.3147  Validation loss = 2.4301  \n",
      "\n",
      "Fold: 5  Epoch: 101  Training loss = 1.3135  Validation loss = 2.4231  \n",
      "\n",
      "Fold: 5  Epoch: 102  Training loss = 1.3132  Validation loss = 2.4196  \n",
      "\n",
      "Fold: 5  Epoch: 103  Training loss = 1.3124  Validation loss = 2.4168  \n",
      "\n",
      "Fold: 5  Epoch: 104  Training loss = 1.3119  Validation loss = 2.4141  \n",
      "\n",
      "Fold: 5  Epoch: 105  Training loss = 1.3114  Validation loss = 2.4131  \n",
      "\n",
      "Fold: 5  Epoch: 106  Training loss = 1.3109  Validation loss = 2.4132  \n",
      "\n",
      "Fold: 5  Epoch: 107  Training loss = 1.3102  Validation loss = 2.4099  \n",
      "\n",
      "Fold: 5  Epoch: 108  Training loss = 1.3097  Validation loss = 2.4097  \n",
      "\n",
      "Fold: 5  Epoch: 109  Training loss = 1.3096  Validation loss = 2.4114  \n",
      "\n",
      "Fold: 5  Epoch: 110  Training loss = 1.3088  Validation loss = 2.4063  \n",
      "\n",
      "Fold: 5  Epoch: 111  Training loss = 1.3081  Validation loss = 2.4032  \n",
      "\n",
      "Fold: 5  Epoch: 112  Training loss = 1.3072  Validation loss = 2.3994  \n",
      "\n",
      "Fold: 5  Epoch: 113  Training loss = 1.3072  Validation loss = 2.4022  \n",
      "\n",
      "Fold: 5  Epoch: 114  Training loss = 1.3069  Validation loss = 2.4026  \n",
      "\n",
      "Fold: 5  Epoch: 115  Training loss = 1.3065  Validation loss = 2.4009  \n",
      "\n",
      "Fold: 5  Epoch: 116  Training loss = 1.3054  Validation loss = 2.3953  \n",
      "\n",
      "Fold: 5  Epoch: 117  Training loss = 1.3047  Validation loss = 2.3926  \n",
      "\n",
      "Fold: 5  Epoch: 118  Training loss = 1.3036  Validation loss = 2.3861  \n",
      "\n",
      "Fold: 5  Epoch: 119  Training loss = 1.3030  Validation loss = 2.3839  \n",
      "\n",
      "Fold: 5  Epoch: 120  Training loss = 1.3023  Validation loss = 2.3788  \n",
      "\n",
      "Fold: 5  Epoch: 121  Training loss = 1.3018  Validation loss = 2.3767  \n",
      "\n",
      "Fold: 5  Epoch: 122  Training loss = 1.3012  Validation loss = 2.3725  \n",
      "\n",
      "Fold: 5  Epoch: 123  Training loss = 1.3008  Validation loss = 2.3700  \n",
      "\n",
      "Fold: 5  Epoch: 124  Training loss = 1.3001  Validation loss = 2.3649  \n",
      "\n",
      "Fold: 5  Epoch: 125  Training loss = 1.2995  Validation loss = 2.3594  \n",
      "\n",
      "Fold: 5  Epoch: 126  Training loss = 1.2991  Validation loss = 2.3584  \n",
      "\n",
      "Fold: 5  Epoch: 127  Training loss = 1.2988  Validation loss = 2.3602  \n",
      "\n",
      "Fold: 5  Epoch: 128  Training loss = 1.2984  Validation loss = 2.3601  \n",
      "\n",
      "Fold: 5  Epoch: 129  Training loss = 1.2982  Validation loss = 2.3592  \n",
      "\n",
      "Fold: 5  Epoch: 130  Training loss = 1.2975  Validation loss = 2.3552  \n",
      "\n",
      "Fold: 5  Epoch: 131  Training loss = 1.2968  Validation loss = 2.3491  \n",
      "\n",
      "Fold: 5  Epoch: 132  Training loss = 1.2969  Validation loss = 2.3551  \n",
      "\n",
      "Fold: 5  Epoch: 133  Training loss = 1.2964  Validation loss = 2.3523  \n",
      "\n",
      "Fold: 5  Epoch: 134  Training loss = 1.2952  Validation loss = 2.3444  \n",
      "\n",
      "Fold: 5  Epoch: 135  Training loss = 1.2945  Validation loss = 2.3361  \n",
      "\n",
      "Fold: 5  Epoch: 136  Training loss = 1.2940  Validation loss = 2.3358  \n",
      "\n",
      "Fold: 5  Epoch: 137  Training loss = 1.2934  Validation loss = 2.3337  \n",
      "\n",
      "Fold: 5  Epoch: 138  Training loss = 1.2928  Validation loss = 2.3313  \n",
      "\n",
      "Fold: 5  Epoch: 139  Training loss = 1.2923  Validation loss = 2.3289  \n",
      "\n",
      "Fold: 5  Epoch: 140  Training loss = 1.2912  Validation loss = 2.3230  \n",
      "\n",
      "Fold: 5  Epoch: 141  Training loss = 1.2915  Validation loss = 2.3278  \n",
      "\n",
      "Fold: 5  Epoch: 142  Training loss = 1.2906  Validation loss = 2.3242  \n",
      "\n",
      "Fold: 5  Epoch: 143  Training loss = 1.2900  Validation loss = 2.3218  \n",
      "\n",
      "Fold: 5  Epoch: 144  Training loss = 1.2900  Validation loss = 2.3238  \n",
      "\n",
      "Fold: 5  Epoch: 145  Training loss = 1.2900  Validation loss = 2.3231  \n",
      "\n",
      "Fold: 5  Epoch: 146  Training loss = 1.2891  Validation loss = 2.3209  \n",
      "\n",
      "Fold: 5  Epoch: 147  Training loss = 1.2877  Validation loss = 2.3147  \n",
      "\n",
      "Fold: 5  Epoch: 148  Training loss = 1.2866  Validation loss = 2.3064  \n",
      "\n",
      "Fold: 5  Epoch: 149  Training loss = 1.2861  Validation loss = 2.3046  \n",
      "\n",
      "Fold: 5  Epoch: 150  Training loss = 1.2853  Validation loss = 2.3010  \n",
      "\n",
      "Fold: 5  Epoch: 151  Training loss = 1.2854  Validation loss = 2.3017  \n",
      "\n",
      "Fold: 5  Epoch: 152  Training loss = 1.2847  Validation loss = 2.2954  \n",
      "\n",
      "Fold: 5  Epoch: 153  Training loss = 1.2843  Validation loss = 2.2909  \n",
      "\n",
      "Fold: 5  Epoch: 154  Training loss = 1.2839  Validation loss = 2.2882  \n",
      "\n",
      "Fold: 5  Epoch: 155  Training loss = 1.2832  Validation loss = 2.2865  \n",
      "\n",
      "Fold: 5  Epoch: 156  Training loss = 1.2829  Validation loss = 2.2842  \n",
      "\n",
      "Fold: 5  Epoch: 157  Training loss = 1.2823  Validation loss = 2.2807  \n",
      "\n",
      "Fold: 5  Epoch: 158  Training loss = 1.2823  Validation loss = 2.2831  \n",
      "\n",
      "Fold: 5  Epoch: 159  Training loss = 1.2816  Validation loss = 2.2797  \n",
      "\n",
      "Fold: 5  Epoch: 160  Training loss = 1.2817  Validation loss = 2.2808  \n",
      "\n",
      "Fold: 5  Epoch: 161  Training loss = 1.2805  Validation loss = 2.2754  \n",
      "\n",
      "Fold: 5  Epoch: 162  Training loss = 1.2795  Validation loss = 2.2711  \n",
      "\n",
      "Fold: 5  Epoch: 163  Training loss = 1.2799  Validation loss = 2.2731  \n",
      "\n",
      "Fold: 5  Epoch: 164  Training loss = 1.2792  Validation loss = 2.2693  \n",
      "\n",
      "Fold: 5  Epoch: 165  Training loss = 1.2780  Validation loss = 2.2599  \n",
      "\n",
      "Fold: 5  Epoch: 166  Training loss = 1.2779  Validation loss = 2.2582  \n",
      "\n",
      "Fold: 5  Epoch: 167  Training loss = 1.2774  Validation loss = 2.2546  \n",
      "\n",
      "Fold: 5  Epoch: 168  Training loss = 1.2764  Validation loss = 2.2484  \n",
      "\n",
      "Fold: 5  Epoch: 169  Training loss = 1.2762  Validation loss = 2.2465  \n",
      "\n",
      "Fold: 5  Epoch: 170  Training loss = 1.2763  Validation loss = 2.2508  \n",
      "\n",
      "Fold: 5  Epoch: 171  Training loss = 1.2758  Validation loss = 2.2503  \n",
      "\n",
      "Fold: 5  Epoch: 172  Training loss = 1.2750  Validation loss = 2.2427  \n",
      "\n",
      "Fold: 5  Epoch: 173  Training loss = 1.2742  Validation loss = 2.2395  \n",
      "\n",
      "Fold: 5  Epoch: 174  Training loss = 1.2736  Validation loss = 2.2386  \n",
      "\n",
      "Fold: 5  Epoch: 175  Training loss = 1.2732  Validation loss = 2.2345  \n",
      "\n",
      "Fold: 5  Epoch: 176  Training loss = 1.2722  Validation loss = 2.2273  \n",
      "\n",
      "Fold: 5  Epoch: 177  Training loss = 1.2711  Validation loss = 2.2200  \n",
      "\n",
      "Fold: 5  Epoch: 178  Training loss = 1.2708  Validation loss = 2.2220  \n",
      "\n",
      "Fold: 5  Epoch: 179  Training loss = 1.2707  Validation loss = 2.2243  \n",
      "\n",
      "Fold: 5  Epoch: 180  Training loss = 1.2697  Validation loss = 2.2168  \n",
      "\n",
      "Fold: 5  Epoch: 181  Training loss = 1.2699  Validation loss = 2.2187  \n",
      "\n",
      "Fold: 5  Epoch: 182  Training loss = 1.2696  Validation loss = 2.2203  \n",
      "\n",
      "Fold: 5  Epoch: 183  Training loss = 1.2690  Validation loss = 2.2160  \n",
      "\n",
      "Fold: 5  Epoch: 184  Training loss = 1.2683  Validation loss = 2.2124  \n",
      "\n",
      "Fold: 5  Epoch: 185  Training loss = 1.2674  Validation loss = 2.2079  \n",
      "\n",
      "Fold: 5  Epoch: 186  Training loss = 1.2668  Validation loss = 2.2064  \n",
      "\n",
      "Fold: 5  Epoch: 187  Training loss = 1.2661  Validation loss = 2.2006  \n",
      "\n",
      "Fold: 5  Epoch: 188  Training loss = 1.2657  Validation loss = 2.2016  \n",
      "\n",
      "Fold: 5  Epoch: 189  Training loss = 1.2645  Validation loss = 2.1904  \n",
      "\n",
      "Fold: 5  Epoch: 190  Training loss = 1.2643  Validation loss = 2.1925  \n",
      "\n",
      "Fold: 5  Epoch: 191  Training loss = 1.2643  Validation loss = 2.1942  \n",
      "\n",
      "Fold: 5  Epoch: 192  Training loss = 1.2641  Validation loss = 2.1950  \n",
      "\n",
      "Fold: 5  Epoch: 193  Training loss = 1.2637  Validation loss = 2.1917  \n",
      "\n",
      "Fold: 5  Epoch: 194  Training loss = 1.2635  Validation loss = 2.1905  \n",
      "\n",
      "Fold: 5  Epoch: 195  Training loss = 1.2632  Validation loss = 2.1913  \n",
      "\n",
      "Fold: 5  Epoch: 196  Training loss = 1.2632  Validation loss = 2.1916  \n",
      "\n",
      "Fold: 5  Epoch: 197  Training loss = 1.2625  Validation loss = 2.1880  \n",
      "\n",
      "Fold: 5  Epoch: 198  Training loss = 1.2615  Validation loss = 2.1810  \n",
      "\n",
      "Fold: 5  Epoch: 199  Training loss = 1.2610  Validation loss = 2.1774  \n",
      "\n",
      "Fold: 5  Epoch: 200  Training loss = 1.2612  Validation loss = 2.1797  \n",
      "\n",
      "Fold: 5  Epoch: 201  Training loss = 1.2601  Validation loss = 2.1726  \n",
      "\n",
      "Fold: 5  Epoch: 202  Training loss = 1.2599  Validation loss = 2.1695  \n",
      "\n",
      "Fold: 5  Epoch: 203  Training loss = 1.2596  Validation loss = 2.1644  \n",
      "\n",
      "Fold: 5  Epoch: 204  Training loss = 1.2590  Validation loss = 2.1563  \n",
      "\n",
      "Fold: 5  Epoch: 205  Training loss = 1.2583  Validation loss = 2.1555  \n",
      "\n",
      "Fold: 5  Epoch: 206  Training loss = 1.2576  Validation loss = 2.1545  \n",
      "\n",
      "Fold: 5  Epoch: 207  Training loss = 1.2569  Validation loss = 2.1499  \n",
      "\n",
      "Fold: 5  Epoch: 208  Training loss = 1.2563  Validation loss = 2.1455  \n",
      "\n",
      "Fold: 5  Epoch: 209  Training loss = 1.2562  Validation loss = 2.1494  \n",
      "\n",
      "Fold: 5  Epoch: 210  Training loss = 1.2557  Validation loss = 2.1459  \n",
      "\n",
      "Fold: 5  Epoch: 211  Training loss = 1.2550  Validation loss = 2.1415  \n",
      "\n",
      "Fold: 5  Epoch: 212  Training loss = 1.2547  Validation loss = 2.1405  \n",
      "\n",
      "Fold: 5  Epoch: 213  Training loss = 1.2543  Validation loss = 2.1419  \n",
      "\n",
      "Fold: 5  Epoch: 214  Training loss = 1.2542  Validation loss = 2.1416  \n",
      "\n",
      "Fold: 5  Epoch: 215  Training loss = 1.2535  Validation loss = 2.1341  \n",
      "\n",
      "Fold: 5  Epoch: 216  Training loss = 1.2529  Validation loss = 2.1288  \n",
      "\n",
      "Fold: 5  Epoch: 217  Training loss = 1.2525  Validation loss = 2.1309  \n",
      "\n",
      "Fold: 5  Epoch: 218  Training loss = 1.2520  Validation loss = 2.1279  \n",
      "\n",
      "Fold: 5  Epoch: 219  Training loss = 1.2517  Validation loss = 2.1245  \n",
      "\n",
      "Fold: 5  Epoch: 220  Training loss = 1.2513  Validation loss = 2.1280  \n",
      "\n",
      "Fold: 5  Epoch: 221  Training loss = 1.2509  Validation loss = 2.1235  \n",
      "\n",
      "Fold: 5  Epoch: 222  Training loss = 1.2503  Validation loss = 2.1214  \n",
      "\n",
      "Fold: 5  Epoch: 223  Training loss = 1.2502  Validation loss = 2.1189  \n",
      "\n",
      "Fold: 5  Epoch: 224  Training loss = 1.2496  Validation loss = 2.1060  \n",
      "\n",
      "Fold: 5  Epoch: 225  Training loss = 1.2489  Validation loss = 2.1040  \n",
      "\n",
      "Fold: 5  Epoch: 226  Training loss = 1.2485  Validation loss = 2.1049  \n",
      "\n",
      "Fold: 5  Epoch: 227  Training loss = 1.2483  Validation loss = 2.0929  \n",
      "\n",
      "Fold: 5  Epoch: 228  Training loss = 1.2476  Validation loss = 2.0932  \n",
      "\n",
      "Fold: 5  Epoch: 229  Training loss = 1.2473  Validation loss = 2.0935  \n",
      "\n",
      "Fold: 5  Epoch: 230  Training loss = 1.2469  Validation loss = 2.0925  \n",
      "\n",
      "Fold: 5  Epoch: 231  Training loss = 1.2465  Validation loss = 2.0868  \n",
      "\n",
      "Fold: 5  Epoch: 232  Training loss = 1.2461  Validation loss = 2.0864  \n",
      "\n",
      "Fold: 5  Epoch: 233  Training loss = 1.2457  Validation loss = 2.0831  \n",
      "\n",
      "Fold: 5  Epoch: 234  Training loss = 1.2454  Validation loss = 2.0773  \n",
      "\n",
      "Fold: 5  Epoch: 235  Training loss = 1.2449  Validation loss = 2.0706  \n",
      "\n",
      "Fold: 5  Epoch: 236  Training loss = 1.2448  Validation loss = 2.0638  \n",
      "\n",
      "Fold: 5  Epoch: 237  Training loss = 1.2444  Validation loss = 2.0622  \n",
      "\n",
      "Fold: 5  Epoch: 238  Training loss = 1.2444  Validation loss = 2.0562  \n",
      "\n",
      "Fold: 5  Epoch: 239  Training loss = 1.2440  Validation loss = 2.0504  \n",
      "\n",
      "Fold: 5  Epoch: 240  Training loss = 1.2440  Validation loss = 2.0502  \n",
      "\n",
      "Fold: 5  Epoch: 241  Training loss = 1.2433  Validation loss = 2.0491  \n",
      "\n",
      "Fold: 5  Epoch: 242  Training loss = 1.2426  Validation loss = 2.0507  \n",
      "\n",
      "Fold: 5  Epoch: 243  Training loss = 1.2419  Validation loss = 2.0571  \n",
      "\n",
      "Fold: 5  Epoch: 244  Training loss = 1.2416  Validation loss = 2.0548  \n",
      "\n",
      "Fold: 5  Epoch: 245  Training loss = 1.2410  Validation loss = 2.0525  \n",
      "\n",
      "Fold: 5  Epoch: 246  Training loss = 1.2406  Validation loss = 2.0524  \n",
      "\n",
      "Fold: 5  Epoch: 247  Training loss = 1.2402  Validation loss = 2.0452  \n",
      "\n",
      "Fold: 5  Epoch: 248  Training loss = 1.2400  Validation loss = 2.0381  \n",
      "\n",
      "Fold: 5  Epoch: 249  Training loss = 1.2397  Validation loss = 2.0330  \n",
      "\n",
      "Fold: 5  Epoch: 250  Training loss = 1.2397  Validation loss = 2.0269  \n",
      "\n",
      "Fold: 5  Epoch: 251  Training loss = 1.2397  Validation loss = 2.0216  \n",
      "\n",
      "Fold: 5  Epoch: 252  Training loss = 1.2388  Validation loss = 2.0199  \n",
      "\n",
      "Fold: 5  Epoch: 253  Training loss = 1.2379  Validation loss = 2.0216  \n",
      "\n",
      "Fold: 5  Epoch: 254  Training loss = 1.2373  Validation loss = 2.0227  \n",
      "\n",
      "Fold: 5  Epoch: 255  Training loss = 1.2370  Validation loss = 2.0268  \n",
      "\n",
      "Fold: 5  Epoch: 256  Training loss = 1.2366  Validation loss = 2.0303  \n",
      "\n",
      "Fold: 5  Epoch: 257  Training loss = 1.2362  Validation loss = 2.0294  \n",
      "\n",
      "Fold: 5  Epoch: 258  Training loss = 1.2358  Validation loss = 2.0270  \n",
      "\n",
      "Fold: 5  Epoch: 259  Training loss = 1.2355  Validation loss = 2.0279  \n",
      "\n",
      "Fold: 5  Epoch: 260  Training loss = 1.2351  Validation loss = 2.0302  \n",
      "\n",
      "Fold: 5  Epoch: 261  Training loss = 1.2350  Validation loss = 2.0203  \n",
      "\n",
      "Fold: 5  Epoch: 262  Training loss = 1.2345  Validation loss = 2.0236  \n",
      "\n",
      "Fold: 5  Epoch: 263  Training loss = 1.2340  Validation loss = 2.0272  \n",
      "\n",
      "Fold: 5  Epoch: 264  Training loss = 1.2338  Validation loss = 2.0280  \n",
      "\n",
      "Fold: 5  Epoch: 265  Training loss = 1.2333  Validation loss = 2.0238  \n",
      "\n",
      "Fold: 5  Epoch: 266  Training loss = 1.2331  Validation loss = 2.0245  \n",
      "\n",
      "Fold: 5  Epoch: 267  Training loss = 1.2328  Validation loss = 2.0218  \n",
      "\n",
      "Fold: 5  Epoch: 268  Training loss = 1.2324  Validation loss = 2.0222  \n",
      "\n",
      "Fold: 5  Epoch: 269  Training loss = 1.2320  Validation loss = 2.0267  \n",
      "\n",
      "Fold: 5  Epoch: 270  Training loss = 1.2316  Validation loss = 2.0258  \n",
      "\n",
      "Fold: 5  Epoch: 271  Training loss = 1.2315  Validation loss = 2.0295  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 252  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 1.2989  Validation loss = 0.7803  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 1.2967  Validation loss = 0.7892  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 1.2948  Validation loss = 0.7954  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 1.2928  Validation loss = 0.7957  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 1.2895  Validation loss = 0.8046  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 1.2892  Validation loss = 0.8022  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 1.2869  Validation loss = 0.8093  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 1.2836  Validation loss = 0.8151  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 1.2794  Validation loss = 0.8232  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 1.2756  Validation loss = 0.8298  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 1.2747  Validation loss = 0.8323  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 1  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 1.2334  Validation loss = 0.9592  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 1.2291  Validation loss = 0.9702  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 1.2242  Validation loss = 0.9646  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.2205  Validation loss = 0.9682  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 1.2179  Validation loss = 0.9502  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 1.2135  Validation loss = 0.9304  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 1.2111  Validation loss = 0.9299  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 1.2089  Validation loss = 0.9257  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 1.2072  Validation loss = 0.9269  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 1.2066  Validation loss = 0.9227  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 1.2051  Validation loss = 0.9379  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 1.2033  Validation loss = 0.9533  \n",
      "\n",
      "Fold: 7  Epoch: 13  Training loss = 1.2014  Validation loss = 0.9582  \n",
      "\n",
      "Fold: 7  Epoch: 14  Training loss = 1.2002  Validation loss = 0.9622  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 10  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 1.1402  Validation loss = 5.0238  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 1.1386  Validation loss = 5.0117  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 1.1375  Validation loss = 5.0039  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 1.1363  Validation loss = 4.9984  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 1.1353  Validation loss = 5.0008  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 1.1343  Validation loss = 5.0050  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 1.1335  Validation loss = 4.9881  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 1.1328  Validation loss = 4.9860  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 1.1318  Validation loss = 4.9987  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 1.1312  Validation loss = 5.0188  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 1.1302  Validation loss = 5.0007  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 1.1294  Validation loss = 4.9886  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 1.1291  Validation loss = 4.9799  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 1.1284  Validation loss = 4.9829  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 1.1278  Validation loss = 4.9846  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 1.1275  Validation loss = 4.9705  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 1.1270  Validation loss = 4.9672  \n",
      "\n",
      "Fold: 8  Epoch: 18  Training loss = 1.1264  Validation loss = 4.9588  \n",
      "\n",
      "Fold: 8  Epoch: 19  Training loss = 1.1261  Validation loss = 4.9509  \n",
      "\n",
      "Fold: 8  Epoch: 20  Training loss = 1.1256  Validation loss = 4.9517  \n",
      "\n",
      "Fold: 8  Epoch: 21  Training loss = 1.1251  Validation loss = 4.9513  \n",
      "\n",
      "Fold: 8  Epoch: 22  Training loss = 1.1249  Validation loss = 4.9455  \n",
      "\n",
      "Fold: 8  Epoch: 23  Training loss = 1.1239  Validation loss = 4.9650  \n",
      "\n",
      "Fold: 8  Epoch: 24  Training loss = 1.1234  Validation loss = 4.9498  \n",
      "\n",
      "Fold: 8  Epoch: 25  Training loss = 1.1223  Validation loss = 4.9635  \n",
      "\n",
      "Fold: 8  Epoch: 26  Training loss = 1.1212  Validation loss = 5.0004  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 22  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 1.6505  Validation loss = 8.4574  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 1.6438  Validation loss = 8.4452  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 1.6345  Validation loss = 8.4301  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 1.6342  Validation loss = 8.4213  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 1.6304  Validation loss = 8.4064  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 1.6243  Validation loss = 8.4112  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 1.6214  Validation loss = 8.3978  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 1.6164  Validation loss = 8.3907  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 1.6171  Validation loss = 8.3865  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 1.6156  Validation loss = 8.3929  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 1.6105  Validation loss = 8.3779  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 1.6094  Validation loss = 8.3818  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 1.6068  Validation loss = 8.3794  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 1.5978  Validation loss = 8.3370  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 1.5954  Validation loss = 8.3347  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 1.5935  Validation loss = 8.3406  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 1.5912  Validation loss = 8.3184  \n",
      "\n",
      "Fold: 9  Epoch: 18  Training loss = 1.5913  Validation loss = 8.3352  \n",
      "\n",
      "Fold: 9  Epoch: 19  Training loss = 1.5888  Validation loss = 8.3135  \n",
      "\n",
      "Fold: 9  Epoch: 20  Training loss = 1.5868  Validation loss = 8.3108  \n",
      "\n",
      "Fold: 9  Epoch: 21  Training loss = 1.5839  Validation loss = 8.3103  \n",
      "\n",
      "Fold: 9  Epoch: 22  Training loss = 1.5813  Validation loss = 8.3105  \n",
      "\n",
      "Fold: 9  Epoch: 23  Training loss = 1.5798  Validation loss = 8.3169  \n",
      "\n",
      "Fold: 9  Epoch: 24  Training loss = 1.5781  Validation loss = 8.3013  \n",
      "\n",
      "Fold: 9  Epoch: 25  Training loss = 1.5763  Validation loss = 8.2806  \n",
      "\n",
      "Fold: 9  Epoch: 26  Training loss = 1.5751  Validation loss = 8.2916  \n",
      "\n",
      "Fold: 9  Epoch: 27  Training loss = 1.5724  Validation loss = 8.2476  \n",
      "\n",
      "Fold: 9  Epoch: 28  Training loss = 1.5708  Validation loss = 8.2300  \n",
      "\n",
      "Fold: 9  Epoch: 29  Training loss = 1.5690  Validation loss = 8.2215  \n",
      "\n",
      "Fold: 9  Epoch: 30  Training loss = 1.5672  Validation loss = 8.2213  \n",
      "\n",
      "Fold: 9  Epoch: 31  Training loss = 1.5668  Validation loss = 8.2173  \n",
      "\n",
      "Fold: 9  Epoch: 32  Training loss = 1.5660  Validation loss = 8.2143  \n",
      "\n",
      "Fold: 9  Epoch: 33  Training loss = 1.5653  Validation loss = 8.2272  \n",
      "\n",
      "Fold: 9  Epoch: 34  Training loss = 1.5632  Validation loss = 8.2172  \n",
      "\n",
      "Fold: 9  Epoch: 35  Training loss = 1.5620  Validation loss = 8.2112  \n",
      "\n",
      "Fold: 9  Epoch: 36  Training loss = 1.5602  Validation loss = 8.2384  \n",
      "\n",
      "Fold: 9  Epoch: 37  Training loss = 1.5584  Validation loss = 8.2578  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 35  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 2.4781  Validation loss = 3.6070  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 2.4665  Validation loss = 3.5893  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 2.4560  Validation loss = 3.5761  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 2.4452  Validation loss = 3.5600  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 2.4409  Validation loss = 3.5546  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 2.4408  Validation loss = 3.5556  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 2.4379  Validation loss = 3.5502  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 2.4259  Validation loss = 3.5317  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 2.4128  Validation loss = 3.5115  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 2.4092  Validation loss = 3.5075  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 2.4010  Validation loss = 3.4978  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 2.3936  Validation loss = 3.4916  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 2.3929  Validation loss = 3.4903  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 2.3831  Validation loss = 3.4744  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 2.3776  Validation loss = 3.4665  \n",
      "\n",
      "Fold: 10  Epoch: 16  Training loss = 2.3679  Validation loss = 3.3260  \n",
      "\n",
      "Fold: 10  Epoch: 17  Training loss = 2.3633  Validation loss = 3.3178  \n",
      "\n",
      "Fold: 10  Epoch: 18  Training loss = 2.3594  Validation loss = 3.3122  \n",
      "\n",
      "Fold: 10  Epoch: 19  Training loss = 2.3535  Validation loss = 3.3066  \n",
      "\n",
      "Fold: 10  Epoch: 20  Training loss = 2.3438  Validation loss = 3.2900  \n",
      "\n",
      "Fold: 10  Epoch: 21  Training loss = 2.3372  Validation loss = 3.2814  \n",
      "\n",
      "Fold: 10  Epoch: 22  Training loss = 2.3261  Validation loss = 3.2645  \n",
      "\n",
      "Fold: 10  Epoch: 23  Training loss = 2.3195  Validation loss = 3.2574  \n",
      "\n",
      "Fold: 10  Epoch: 24  Training loss = 2.3166  Validation loss = 3.2536  \n",
      "\n",
      "Fold: 10  Epoch: 25  Training loss = 2.3118  Validation loss = 3.2457  \n",
      "\n",
      "Fold: 10  Epoch: 26  Training loss = 2.3070  Validation loss = 3.2357  \n",
      "\n",
      "Fold: 10  Epoch: 27  Training loss = 2.3056  Validation loss = 3.2343  \n",
      "\n",
      "Fold: 10  Epoch: 28  Training loss = 2.2979  Validation loss = 3.2173  \n",
      "\n",
      "Fold: 10  Epoch: 29  Training loss = 2.2946  Validation loss = 3.2130  \n",
      "\n",
      "Fold: 10  Epoch: 30  Training loss = 2.2907  Validation loss = 3.2057  \n",
      "\n",
      "Fold: 10  Epoch: 31  Training loss = 2.2847  Validation loss = 3.1903  \n",
      "\n",
      "Fold: 10  Epoch: 32  Training loss = 2.2818  Validation loss = 3.1822  \n",
      "\n",
      "Fold: 10  Epoch: 33  Training loss = 2.2781  Validation loss = 3.1733  \n",
      "\n",
      "Fold: 10  Epoch: 34  Training loss = 2.2747  Validation loss = 3.1636  \n",
      "\n",
      "Fold: 10  Epoch: 35  Training loss = 2.2697  Validation loss = 3.1487  \n",
      "\n",
      "Fold: 10  Epoch: 36  Training loss = 2.2668  Validation loss = 3.1422  \n",
      "\n",
      "Fold: 10  Epoch: 37  Training loss = 2.2635  Validation loss = 3.1350  \n",
      "\n",
      "Fold: 10  Epoch: 38  Training loss = 2.2626  Validation loss = 3.1374  \n",
      "\n",
      "Fold: 10  Epoch: 39  Training loss = 2.2588  Validation loss = 3.1296  \n",
      "\n",
      "Fold: 10  Epoch: 40  Training loss = 2.2559  Validation loss = 3.1212  \n",
      "\n",
      "Fold: 10  Epoch: 41  Training loss = 2.2552  Validation loss = 3.1258  \n",
      "\n",
      "Fold: 10  Epoch: 42  Training loss = 2.2526  Validation loss = 3.1157  \n",
      "\n",
      "Fold: 10  Epoch: 43  Training loss = 2.2482  Validation loss = 3.1021  \n",
      "\n",
      "Fold: 10  Epoch: 44  Training loss = 2.2466  Validation loss = 3.1066  \n",
      "\n",
      "Fold: 10  Epoch: 45  Training loss = 2.2427  Validation loss = 3.1007  \n",
      "\n",
      "Fold: 10  Epoch: 46  Training loss = 2.2384  Validation loss = 3.0901  \n",
      "\n",
      "Fold: 10  Epoch: 47  Training loss = 2.2359  Validation loss = 3.0849  \n",
      "\n",
      "Fold: 10  Epoch: 48  Training loss = 2.2326  Validation loss = 3.0770  \n",
      "\n",
      "Fold: 10  Epoch: 49  Training loss = 2.2303  Validation loss = 3.0729  \n",
      "\n",
      "Fold: 10  Epoch: 50  Training loss = 2.2216  Validation loss = 3.0440  \n",
      "\n",
      "Fold: 10  Epoch: 51  Training loss = 2.2227  Validation loss = 3.0531  \n",
      "\n",
      "Fold: 10  Epoch: 52  Training loss = 2.2182  Validation loss = 3.0412  \n",
      "\n",
      "Fold: 10  Epoch: 53  Training loss = 2.2145  Validation loss = 3.0361  \n",
      "\n",
      "Fold: 10  Epoch: 54  Training loss = 2.2129  Validation loss = 3.0319  \n",
      "\n",
      "Fold: 10  Epoch: 55  Training loss = 2.2130  Validation loss = 3.0221  \n",
      "\n",
      "Fold: 10  Epoch: 56  Training loss = 2.2105  Validation loss = 3.0200  \n",
      "\n",
      "Fold: 10  Epoch: 57  Training loss = 2.2043  Validation loss = 3.0061  \n",
      "\n",
      "Fold: 10  Epoch: 58  Training loss = 2.2013  Validation loss = 2.9998  \n",
      "\n",
      "Fold: 10  Epoch: 59  Training loss = 2.1965  Validation loss = 2.9918  \n",
      "\n",
      "Fold: 10  Epoch: 60  Training loss = 2.1944  Validation loss = 2.9766  \n",
      "\n",
      "Fold: 10  Epoch: 61  Training loss = 2.1912  Validation loss = 2.9700  \n",
      "\n",
      "Fold: 10  Epoch: 62  Training loss = 2.1888  Validation loss = 2.9655  \n",
      "\n",
      "Fold: 10  Epoch: 63  Training loss = 2.1841  Validation loss = 2.9554  \n",
      "\n",
      "Fold: 10  Epoch: 64  Training loss = 2.1815  Validation loss = 2.9560  \n",
      "\n",
      "Fold: 10  Epoch: 65  Training loss = 2.1846  Validation loss = 2.9728  \n",
      "\n",
      "Fold: 10  Epoch: 66  Training loss = 2.1785  Validation loss = 2.9536  \n",
      "\n",
      "Fold: 10  Epoch: 67  Training loss = 2.1830  Validation loss = 2.9347  \n",
      "\n",
      "Fold: 10  Epoch: 68  Training loss = 2.1809  Validation loss = 2.9256  \n",
      "\n",
      "Fold: 10  Epoch: 69  Training loss = 2.1784  Validation loss = 2.9092  \n",
      "\n",
      "Fold: 10  Epoch: 70  Training loss = 2.1742  Validation loss = 2.8987  \n",
      "\n",
      "Fold: 10  Epoch: 71  Training loss = 2.1707  Validation loss = 2.8946  \n",
      "\n",
      "Fold: 10  Epoch: 72  Training loss = 2.1687  Validation loss = 2.8901  \n",
      "\n",
      "Fold: 10  Epoch: 73  Training loss = 2.1649  Validation loss = 2.8874  \n",
      "\n",
      "Fold: 10  Epoch: 74  Training loss = 2.1619  Validation loss = 2.8819  \n",
      "\n",
      "Fold: 10  Epoch: 75  Training loss = 2.1571  Validation loss = 2.8736  \n",
      "\n",
      "Fold: 10  Epoch: 76  Training loss = 2.1545  Validation loss = 2.8707  \n",
      "\n",
      "Fold: 10  Epoch: 77  Training loss = 2.1503  Validation loss = 2.8761  \n",
      "\n",
      "Fold: 10  Epoch: 78  Training loss = 2.1448  Validation loss = 2.8762  \n",
      "\n",
      "Fold: 10  Epoch: 79  Training loss = 2.1384  Validation loss = 2.8668  \n",
      "\n",
      "Fold: 10  Epoch: 80  Training loss = 2.1340  Validation loss = 2.8605  \n",
      "\n",
      "Fold: 10  Epoch: 81  Training loss = 2.1463  Validation loss = 2.8628  \n",
      "\n",
      "Fold: 10  Epoch: 82  Training loss = 2.1437  Validation loss = 2.8683  \n",
      "\n",
      "Fold: 10  Epoch: 83  Training loss = 2.1368  Validation loss = 2.8581  \n",
      "\n",
      "Fold: 10  Epoch: 84  Training loss = 2.1331  Validation loss = 2.8439  \n",
      "\n",
      "Fold: 10  Epoch: 85  Training loss = 2.1248  Validation loss = 2.8460  \n",
      "\n",
      "Fold: 10  Epoch: 86  Training loss = 2.1213  Validation loss = 2.8403  \n",
      "\n",
      "Fold: 10  Epoch: 87  Training loss = 2.1188  Validation loss = 2.8333  \n",
      "\n",
      "Fold: 10  Epoch: 88  Training loss = 2.1131  Validation loss = 2.8490  \n",
      "\n",
      "Fold: 10  Epoch: 89  Training loss = 2.1103  Validation loss = 2.8511  \n",
      "\n",
      "Fold: 10  Epoch: 90  Training loss = 2.1079  Validation loss = 2.8520  \n",
      "\n",
      "Fold: 10  Epoch: 91  Training loss = 2.1057  Validation loss = 2.8665  \n",
      "\n",
      "Fold: 10  Epoch: 92  Training loss = 2.1041  Validation loss = 2.8721  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 87  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 2.2063  Validation loss = 1.3981  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 2.2048  Validation loss = 1.3949  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 2.2055  Validation loss = 1.3646  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 2.1979  Validation loss = 1.3709  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 2.1942  Validation loss = 1.3739  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 2.1902  Validation loss = 1.3817  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 2.1871  Validation loss = 1.3876  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 2.1833  Validation loss = 1.3894  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 2.1821  Validation loss = 1.3880  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 2.1786  Validation loss = 1.3899  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 2.1756  Validation loss = 1.3893  \n",
      "\n",
      "Fold: 11  Epoch: 12  Training loss = 2.1448  Validation loss = 1.3902  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 3  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 2.1425  Validation loss = 1.9946  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 2.1399  Validation loss = 2.0199  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 2.1355  Validation loss = 1.9870  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 2.1335  Validation loss = 1.9820  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 2.1305  Validation loss = 1.9869  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 2.1370  Validation loss = 2.0538  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 2.1449  Validation loss = 2.0894  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 2.1354  Validation loss = 1.2467  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 2.1337  Validation loss = 1.2492  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 2.1310  Validation loss = 1.2870  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 2.1279  Validation loss = 1.4070  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 2.1250  Validation loss = 1.5466  \n",
      "\n",
      "Fold: 12  Epoch: 13  Training loss = 2.1210  Validation loss = 1.6090  \n",
      "\n",
      "Fold: 12  Epoch: 14  Training loss = 2.1184  Validation loss = 1.6987  \n",
      "\n",
      "Fold: 12  Epoch: 15  Training loss = 2.1144  Validation loss = 1.7746  \n",
      "\n",
      "Fold: 12  Epoch: 16  Training loss = 2.1137  Validation loss = 1.8508  \n",
      "\n",
      "Fold: 12  Epoch: 17  Training loss = 2.1096  Validation loss = 1.8919  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 8  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 2.1182  Validation loss = 3.2552  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 2.1121  Validation loss = 3.1794  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 2.1093  Validation loss = 3.1621  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 2.1056  Validation loss = 3.1155  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 2.1022  Validation loss = 3.0858  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 2.0990  Validation loss = 3.0801  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 2.0952  Validation loss = 3.0943  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 2.0924  Validation loss = 3.1042  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 2.0900  Validation loss = 3.0872  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 2.0871  Validation loss = 3.0913  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 2.0864  Validation loss = 3.1244  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 2.0822  Validation loss = 3.0654  \n",
      "\n",
      "Fold: 13  Epoch: 13  Training loss = 2.0793  Validation loss = 3.0578  \n",
      "\n",
      "Fold: 13  Epoch: 14  Training loss = 2.0793  Validation loss = 3.1112  \n",
      "\n",
      "Fold: 13  Epoch: 15  Training loss = 2.0811  Validation loss = 3.1840  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 13  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 2.2118  Validation loss = 6.5146  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 2.2169  Validation loss = 6.5125  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 2.2002  Validation loss = 6.4907  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 2.1956  Validation loss = 6.4656  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 2.1981  Validation loss = 6.4665  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 2.1919  Validation loss = 6.4506  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 2.1882  Validation loss = 6.4398  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 2.1849  Validation loss = 6.4277  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 2.1830  Validation loss = 6.4137  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 2.1797  Validation loss = 6.3904  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 2.1789  Validation loss = 6.3814  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 2.1751  Validation loss = 6.3551  \n",
      "\n",
      "Fold: 14  Epoch: 13  Training loss = 2.1719  Validation loss = 6.3511  \n",
      "\n",
      "Fold: 14  Epoch: 14  Training loss = 2.1677  Validation loss = 6.3304  \n",
      "\n",
      "Fold: 14  Epoch: 15  Training loss = 2.1675  Validation loss = 6.3212  \n",
      "\n",
      "Fold: 14  Epoch: 16  Training loss = 2.1650  Validation loss = 6.3138  \n",
      "\n",
      "Fold: 14  Epoch: 17  Training loss = 2.1683  Validation loss = 6.3177  \n",
      "\n",
      "Fold: 14  Epoch: 18  Training loss = 2.1587  Validation loss = 6.3038  \n",
      "\n",
      "Fold: 14  Epoch: 19  Training loss = 2.1569  Validation loss = 6.3000  \n",
      "\n",
      "Fold: 14  Epoch: 20  Training loss = 2.1542  Validation loss = 6.2864  \n",
      "\n",
      "Fold: 14  Epoch: 21  Training loss = 2.1523  Validation loss = 6.2717  \n",
      "\n",
      "Fold: 14  Epoch: 22  Training loss = 2.1500  Validation loss = 6.2667  \n",
      "\n",
      "Fold: 14  Epoch: 23  Training loss = 2.1453  Validation loss = 6.2492  \n",
      "\n",
      "Fold: 14  Epoch: 24  Training loss = 2.1445  Validation loss = 6.2480  \n",
      "\n",
      "Fold: 14  Epoch: 25  Training loss = 2.1430  Validation loss = 6.2360  \n",
      "\n",
      "Fold: 14  Epoch: 26  Training loss = 2.1391  Validation loss = 6.2246  \n",
      "\n",
      "Fold: 14  Epoch: 27  Training loss = 2.1364  Validation loss = 6.1993  \n",
      "\n",
      "Fold: 14  Epoch: 28  Training loss = 2.1358  Validation loss = 6.1788  \n",
      "\n",
      "Fold: 14  Epoch: 29  Training loss = 2.1306  Validation loss = 6.1778  \n",
      "\n",
      "Fold: 14  Epoch: 30  Training loss = 2.1296  Validation loss = 6.1875  \n",
      "\n",
      "Fold: 14  Epoch: 31  Training loss = 2.1283  Validation loss = 6.1750  \n",
      "\n",
      "Fold: 14  Epoch: 32  Training loss = 2.1274  Validation loss = 6.1716  \n",
      "\n",
      "Fold: 14  Epoch: 33  Training loss = 2.1257  Validation loss = 6.1647  \n",
      "\n",
      "Fold: 14  Epoch: 34  Training loss = 2.1282  Validation loss = 6.1720  \n",
      "\n",
      "Fold: 14  Epoch: 35  Training loss = 2.1225  Validation loss = 6.1621  \n",
      "\n",
      "Fold: 14  Epoch: 36  Training loss = 2.1216  Validation loss = 6.1437  \n",
      "\n",
      "Fold: 14  Epoch: 37  Training loss = 2.1186  Validation loss = 6.1268  \n",
      "\n",
      "Fold: 14  Epoch: 38  Training loss = 2.1189  Validation loss = 6.1248  \n",
      "\n",
      "Fold: 14  Epoch: 39  Training loss = 2.1284  Validation loss = 6.1242  \n",
      "\n",
      "Fold: 14  Epoch: 40  Training loss = 2.1114  Validation loss = 6.1099  \n",
      "\n",
      "Fold: 14  Epoch: 41  Training loss = 2.1164  Validation loss = 6.0998  \n",
      "\n",
      "Fold: 14  Epoch: 42  Training loss = 2.1057  Validation loss = 6.0853  \n",
      "\n",
      "Fold: 14  Epoch: 43  Training loss = 2.1033  Validation loss = 6.0641  \n",
      "\n",
      "Fold: 14  Epoch: 44  Training loss = 2.1034  Validation loss = 6.0679  \n",
      "\n",
      "Fold: 14  Epoch: 45  Training loss = 2.1045  Validation loss = 6.0805  \n",
      "\n",
      "Fold: 14  Epoch: 46  Training loss = 2.1008  Validation loss = 6.0543  \n",
      "\n",
      "Fold: 14  Epoch: 47  Training loss = 2.0973  Validation loss = 6.0499  \n",
      "\n",
      "Fold: 14  Epoch: 48  Training loss = 2.0979  Validation loss = 6.0461  \n",
      "\n",
      "Fold: 14  Epoch: 49  Training loss = 2.0935  Validation loss = 6.0335  \n",
      "\n",
      "Fold: 14  Epoch: 50  Training loss = 2.0920  Validation loss = 6.0255  \n",
      "\n",
      "Fold: 14  Epoch: 51  Training loss = 2.0907  Validation loss = 6.0268  \n",
      "\n",
      "Fold: 14  Epoch: 52  Training loss = 2.0893  Validation loss = 6.0284  \n",
      "\n",
      "Fold: 14  Epoch: 53  Training loss = 2.0899  Validation loss = 6.0021  \n",
      "\n",
      "Fold: 14  Epoch: 54  Training loss = 2.0873  Validation loss = 6.0068  \n",
      "\n",
      "Fold: 14  Epoch: 55  Training loss = 2.0915  Validation loss = 5.9883  \n",
      "\n",
      "Fold: 14  Epoch: 56  Training loss = 2.0823  Validation loss = 5.9969  \n",
      "\n",
      "Fold: 14  Epoch: 57  Training loss = 2.0847  Validation loss = 5.9962  \n",
      "\n",
      "Fold: 14  Epoch: 58  Training loss = 2.0811  Validation loss = 5.9959  \n",
      "\n",
      "Fold: 14  Epoch: 59  Training loss = 2.0917  Validation loss = 6.0090  \n",
      "\n",
      "Fold: 14  Epoch: 60  Training loss = 2.0863  Validation loss = 5.9977  \n",
      "\n",
      "Fold: 14  Epoch: 61  Training loss = 2.0770  Validation loss = 5.9718  \n",
      "\n",
      "Fold: 14  Epoch: 62  Training loss = 2.0742  Validation loss = 5.9752  \n",
      "\n",
      "Fold: 14  Epoch: 63  Training loss = 2.0736  Validation loss = 5.9771  \n",
      "\n",
      "Fold: 14  Epoch: 64  Training loss = 2.0732  Validation loss = 5.9755  \n",
      "\n",
      "Fold: 14  Epoch: 65  Training loss = 2.0745  Validation loss = 5.9511  \n",
      "\n",
      "Fold: 14  Epoch: 66  Training loss = 2.0704  Validation loss = 5.9506  \n",
      "\n",
      "Fold: 14  Epoch: 67  Training loss = 2.0722  Validation loss = 5.9372  \n",
      "\n",
      "Fold: 14  Epoch: 68  Training loss = 2.0682  Validation loss = 5.9354  \n",
      "\n",
      "Fold: 14  Epoch: 69  Training loss = 2.0676  Validation loss = 5.9358  \n",
      "\n",
      "Fold: 14  Epoch: 70  Training loss = 2.0655  Validation loss = 5.9317  \n",
      "\n",
      "Fold: 14  Epoch: 71  Training loss = 2.0642  Validation loss = 5.9297  \n",
      "\n",
      "Fold: 14  Epoch: 72  Training loss = 2.0623  Validation loss = 5.9253  \n",
      "\n",
      "Fold: 14  Epoch: 73  Training loss = 2.0649  Validation loss = 5.9211  \n",
      "\n",
      "Fold: 14  Epoch: 74  Training loss = 2.0611  Validation loss = 5.9255  \n",
      "\n",
      "Fold: 14  Epoch: 75  Training loss = 2.0580  Validation loss = 5.9096  \n",
      "\n",
      "Fold: 14  Epoch: 76  Training loss = 2.0568  Validation loss = 5.8951  \n",
      "\n",
      "Fold: 14  Epoch: 77  Training loss = 2.0576  Validation loss = 5.8903  \n",
      "\n",
      "Fold: 14  Epoch: 78  Training loss = 2.0562  Validation loss = 5.8785  \n",
      "\n",
      "Fold: 14  Epoch: 79  Training loss = 2.0538  Validation loss = 5.8779  \n",
      "\n",
      "Fold: 14  Epoch: 80  Training loss = 2.0539  Validation loss = 5.8663  \n",
      "\n",
      "Fold: 14  Epoch: 81  Training loss = 2.0510  Validation loss = 5.8604  \n",
      "\n",
      "Fold: 14  Epoch: 82  Training loss = 2.0543  Validation loss = 5.8475  \n",
      "\n",
      "Fold: 14  Epoch: 83  Training loss = 2.0478  Validation loss = 5.8335  \n",
      "\n",
      "Fold: 14  Epoch: 84  Training loss = 2.0456  Validation loss = 5.8289  \n",
      "\n",
      "Fold: 14  Epoch: 85  Training loss = 2.0553  Validation loss = 5.8026  \n",
      "\n",
      "Fold: 14  Epoch: 86  Training loss = 2.0524  Validation loss = 5.7962  \n",
      "\n",
      "Fold: 14  Epoch: 87  Training loss = 2.0447  Validation loss = 5.8034  \n",
      "\n",
      "Fold: 14  Epoch: 88  Training loss = 2.0420  Validation loss = 5.7948  \n",
      "\n",
      "Fold: 14  Epoch: 89  Training loss = 2.0406  Validation loss = 5.7996  \n",
      "\n",
      "Fold: 14  Epoch: 90  Training loss = 2.0409  Validation loss = 5.8132  \n",
      "\n",
      "Fold: 14  Epoch: 91  Training loss = 2.0402  Validation loss = 5.8152  \n",
      "\n",
      "Fold: 14  Epoch: 92  Training loss = 2.0393  Validation loss = 5.8132  \n",
      "\n",
      "Fold: 14  Epoch: 93  Training loss = 2.0391  Validation loss = 5.8087  \n",
      "\n",
      "Fold: 14  Epoch: 94  Training loss = 2.0364  Validation loss = 5.7987  \n",
      "\n",
      "Fold: 14  Epoch: 95  Training loss = 2.0378  Validation loss = 5.7887  \n",
      "\n",
      "Fold: 14  Epoch: 96  Training loss = 2.0355  Validation loss = 5.7850  \n",
      "\n",
      "Fold: 14  Epoch: 97  Training loss = 2.0326  Validation loss = 5.7783  \n",
      "\n",
      "Fold: 14  Epoch: 98  Training loss = 2.0332  Validation loss = 5.7719  \n",
      "\n",
      "Fold: 14  Epoch: 99  Training loss = 2.0297  Validation loss = 5.7674  \n",
      "\n",
      "Fold: 14  Epoch: 100  Training loss = 2.0369  Validation loss = 5.7869  \n",
      "\n",
      "Fold: 14  Epoch: 101  Training loss = 2.0307  Validation loss = 5.7710  \n",
      "\n",
      "Fold: 14  Epoch: 102  Training loss = 2.0279  Validation loss = 5.7597  \n",
      "\n",
      "Fold: 14  Epoch: 103  Training loss = 2.0305  Validation loss = 5.7621  \n",
      "\n",
      "Fold: 14  Epoch: 104  Training loss = 2.0260  Validation loss = 5.7544  \n",
      "\n",
      "Fold: 14  Epoch: 105  Training loss = 2.0288  Validation loss = 5.7588  \n",
      "\n",
      "Fold: 14  Epoch: 106  Training loss = 2.0234  Validation loss = 5.7375  \n",
      "\n",
      "Fold: 14  Epoch: 107  Training loss = 2.0237  Validation loss = 5.7327  \n",
      "\n",
      "Fold: 14  Epoch: 108  Training loss = 2.0205  Validation loss = 5.7234  \n",
      "\n",
      "Fold: 14  Epoch: 109  Training loss = 2.0206  Validation loss = 5.7205  \n",
      "\n",
      "Fold: 14  Epoch: 110  Training loss = 2.0192  Validation loss = 5.7171  \n",
      "\n",
      "Fold: 14  Epoch: 111  Training loss = 2.0186  Validation loss = 5.7176  \n",
      "\n",
      "Fold: 14  Epoch: 112  Training loss = 2.0173  Validation loss = 5.7129  \n",
      "\n",
      "Fold: 14  Epoch: 113  Training loss = 2.0148  Validation loss = 5.6864  \n",
      "\n",
      "Fold: 14  Epoch: 114  Training loss = 2.0189  Validation loss = 5.6837  \n",
      "\n",
      "Fold: 14  Epoch: 115  Training loss = 2.0169  Validation loss = 5.6649  \n",
      "\n",
      "Fold: 14  Epoch: 116  Training loss = 2.0104  Validation loss = 5.6856  \n",
      "\n",
      "Fold: 14  Epoch: 117  Training loss = 2.0115  Validation loss = 5.6869  \n",
      "\n",
      "Fold: 14  Epoch: 118  Training loss = 2.0081  Validation loss = 5.6702  \n",
      "\n",
      "Fold: 14  Epoch: 119  Training loss = 2.0094  Validation loss = 5.6722  \n",
      "\n",
      "Fold: 14  Epoch: 120  Training loss = 2.0074  Validation loss = 5.6782  \n",
      "\n",
      "Fold: 14  Epoch: 121  Training loss = 2.0098  Validation loss = 5.6722  \n",
      "\n",
      "Fold: 14  Epoch: 122  Training loss = 2.0109  Validation loss = 5.6689  \n",
      "\n",
      "Fold: 14  Epoch: 123  Training loss = 2.0081  Validation loss = 5.6518  \n",
      "\n",
      "Fold: 14  Epoch: 124  Training loss = 2.0123  Validation loss = 5.6451  \n",
      "\n",
      "Fold: 14  Epoch: 125  Training loss = 2.0035  Validation loss = 5.6404  \n",
      "\n",
      "Fold: 14  Epoch: 126  Training loss = 2.0015  Validation loss = 5.6262  \n",
      "\n",
      "Fold: 14  Epoch: 127  Training loss = 2.0050  Validation loss = 5.6180  \n",
      "\n",
      "Fold: 14  Epoch: 128  Training loss = 2.0088  Validation loss = 5.6169  \n",
      "\n",
      "Fold: 14  Epoch: 129  Training loss = 2.0055  Validation loss = 5.6005  \n",
      "\n",
      "Fold: 14  Epoch: 130  Training loss = 1.9970  Validation loss = 5.5900  \n",
      "\n",
      "Fold: 14  Epoch: 131  Training loss = 1.9980  Validation loss = 5.6018  \n",
      "\n",
      "Fold: 14  Epoch: 132  Training loss = 1.9955  Validation loss = 5.5928  \n",
      "\n",
      "Fold: 14  Epoch: 133  Training loss = 1.9960  Validation loss = 5.5908  \n",
      "\n",
      "Fold: 14  Epoch: 134  Training loss = 1.9925  Validation loss = 5.5776  \n",
      "\n",
      "Fold: 14  Epoch: 135  Training loss = 1.9911  Validation loss = 5.5776  \n",
      "\n",
      "Fold: 14  Epoch: 136  Training loss = 1.9912  Validation loss = 5.5516  \n",
      "\n",
      "Fold: 14  Epoch: 137  Training loss = 1.9886  Validation loss = 5.5540  \n",
      "\n",
      "Fold: 14  Epoch: 138  Training loss = 1.9887  Validation loss = 5.5453  \n",
      "\n",
      "Fold: 14  Epoch: 139  Training loss = 1.9869  Validation loss = 5.5518  \n",
      "\n",
      "Fold: 14  Epoch: 140  Training loss = 1.9910  Validation loss = 5.5566  \n",
      "\n",
      "Fold: 14  Epoch: 141  Training loss = 1.9847  Validation loss = 5.5411  \n",
      "\n",
      "Fold: 14  Epoch: 142  Training loss = 1.9829  Validation loss = 5.5419  \n",
      "\n",
      "Fold: 14  Epoch: 143  Training loss = 1.9922  Validation loss = 5.5254  \n",
      "\n",
      "Fold: 14  Epoch: 144  Training loss = 1.9815  Validation loss = 5.5215  \n",
      "\n",
      "Fold: 14  Epoch: 145  Training loss = 1.9810  Validation loss = 5.5085  \n",
      "\n",
      "Fold: 14  Epoch: 146  Training loss = 1.9816  Validation loss = 5.5000  \n",
      "\n",
      "Fold: 14  Epoch: 147  Training loss = 1.9775  Validation loss = 5.4976  \n",
      "\n",
      "Fold: 14  Epoch: 148  Training loss = 1.9840  Validation loss = 5.4876  \n",
      "\n",
      "Fold: 14  Epoch: 149  Training loss = 1.9744  Validation loss = 5.4993  \n",
      "\n",
      "Fold: 14  Epoch: 150  Training loss = 1.9736  Validation loss = 5.5023  \n",
      "\n",
      "Fold: 14  Epoch: 151  Training loss = 1.9772  Validation loss = 5.5110  \n",
      "\n",
      "Fold: 14  Epoch: 152  Training loss = 1.9742  Validation loss = 5.4830  \n",
      "\n",
      "Fold: 14  Epoch: 153  Training loss = 1.9817  Validation loss = 5.4683  \n",
      "\n",
      "Fold: 14  Epoch: 154  Training loss = 1.9682  Validation loss = 5.4741  \n",
      "\n",
      "Fold: 14  Epoch: 155  Training loss = 1.9728  Validation loss = 5.4809  \n",
      "\n",
      "Fold: 14  Epoch: 156  Training loss = 1.9752  Validation loss = 5.4623  \n",
      "\n",
      "Fold: 14  Epoch: 157  Training loss = 1.9693  Validation loss = 5.4593  \n",
      "\n",
      "Fold: 14  Epoch: 158  Training loss = 1.9628  Validation loss = 5.4534  \n",
      "\n",
      "Fold: 14  Epoch: 159  Training loss = 1.9674  Validation loss = 5.4382  \n",
      "\n",
      "Fold: 14  Epoch: 160  Training loss = 1.9636  Validation loss = 5.4403  \n",
      "\n",
      "Fold: 14  Epoch: 161  Training loss = 1.9634  Validation loss = 5.4417  \n",
      "\n",
      "Fold: 14  Epoch: 162  Training loss = 1.9695  Validation loss = 5.4566  \n",
      "\n",
      "Fold: 14  Epoch: 163  Training loss = 1.9609  Validation loss = 5.4579  \n",
      "\n",
      "Fold: 14  Epoch: 164  Training loss = 1.9517  Validation loss = 5.4489  \n",
      "\n",
      "Fold: 14  Epoch: 165  Training loss = 1.9520  Validation loss = 5.4415  \n",
      "\n",
      "Fold: 14  Epoch: 166  Training loss = 1.9485  Validation loss = 5.4385  \n",
      "\n",
      "Fold: 14  Epoch: 167  Training loss = 1.9479  Validation loss = 5.4395  \n",
      "\n",
      "Fold: 14  Epoch: 168  Training loss = 1.9483  Validation loss = 5.4275  \n",
      "\n",
      "Fold: 14  Epoch: 169  Training loss = 1.9457  Validation loss = 5.4154  \n",
      "\n",
      "Fold: 14  Epoch: 170  Training loss = 1.9448  Validation loss = 5.4151  \n",
      "\n",
      "Fold: 14  Epoch: 171  Training loss = 1.9461  Validation loss = 5.4250  \n",
      "\n",
      "Fold: 14  Epoch: 172  Training loss = 1.9426  Validation loss = 5.4155  \n",
      "\n",
      "Fold: 14  Epoch: 173  Training loss = 1.9420  Validation loss = 5.4070  \n",
      "\n",
      "Fold: 14  Epoch: 174  Training loss = 1.9415  Validation loss = 5.4147  \n",
      "\n",
      "Fold: 14  Epoch: 175  Training loss = 1.9376  Validation loss = 5.4035  \n",
      "\n",
      "Fold: 14  Epoch: 176  Training loss = 1.9346  Validation loss = 5.4061  \n",
      "\n",
      "Fold: 14  Epoch: 177  Training loss = 1.9299  Validation loss = 5.3963  \n",
      "\n",
      "Fold: 14  Epoch: 178  Training loss = 1.9247  Validation loss = 5.3981  \n",
      "\n",
      "Fold: 14  Epoch: 179  Training loss = 1.9387  Validation loss = 5.3751  \n",
      "\n",
      "Fold: 14  Epoch: 180  Training loss = 1.9404  Validation loss = 5.3613  \n",
      "\n",
      "Fold: 14  Epoch: 181  Training loss = 1.9191  Validation loss = 5.3720  \n",
      "\n",
      "Fold: 14  Epoch: 182  Training loss = 1.9236  Validation loss = 5.3705  \n",
      "\n",
      "Fold: 14  Epoch: 183  Training loss = 1.9162  Validation loss = 5.3436  \n",
      "\n",
      "Fold: 14  Epoch: 184  Training loss = 1.9131  Validation loss = 5.3382  \n",
      "\n",
      "Fold: 14  Epoch: 185  Training loss = 1.9115  Validation loss = 5.3270  \n",
      "\n",
      "Fold: 14  Epoch: 186  Training loss = 1.9116  Validation loss = 5.3288  \n",
      "\n",
      "Fold: 14  Epoch: 187  Training loss = 1.9143  Validation loss = 5.3323  \n",
      "\n",
      "Fold: 14  Epoch: 188  Training loss = 1.9069  Validation loss = 5.3112  \n",
      "\n",
      "Fold: 14  Epoch: 189  Training loss = 1.9096  Validation loss = 5.3173  \n",
      "\n",
      "Fold: 14  Epoch: 190  Training loss = 1.9076  Validation loss = 5.3208  \n",
      "\n",
      "Fold: 14  Epoch: 191  Training loss = 1.9087  Validation loss = 5.3011  \n",
      "\n",
      "Fold: 14  Epoch: 192  Training loss = 1.9063  Validation loss = 5.2906  \n",
      "\n",
      "Fold: 14  Epoch: 193  Training loss = 1.8933  Validation loss = 5.2907  \n",
      "\n",
      "Fold: 14  Epoch: 194  Training loss = 1.8921  Validation loss = 5.2939  \n",
      "\n",
      "Fold: 14  Epoch: 195  Training loss = 1.8909  Validation loss = 5.2983  \n",
      "\n",
      "Fold: 14  Epoch: 196  Training loss = 1.8942  Validation loss = 5.2960  \n",
      "\n",
      "Fold: 14  Epoch: 197  Training loss = 1.9082  Validation loss = 5.3080  \n",
      "\n",
      "Fold: 14  Epoch: 198  Training loss = 1.8813  Validation loss = 5.2709  \n",
      "\n",
      "Fold: 14  Epoch: 199  Training loss = 1.8862  Validation loss = 5.2849  \n",
      "\n",
      "Fold: 14  Epoch: 200  Training loss = 1.8834  Validation loss = 5.2893  \n",
      "\n",
      "Fold: 14  Epoch: 201  Training loss = 1.8827  Validation loss = 5.2907  \n",
      "\n",
      "Fold: 14  Epoch: 202  Training loss = 1.8766  Validation loss = 5.2866  \n",
      "\n",
      "Fold: 14  Epoch: 203  Training loss = 1.8758  Validation loss = 5.2779  \n",
      "\n",
      "Fold: 14  Epoch: 204  Training loss = 1.8732  Validation loss = 5.2746  \n",
      "\n",
      "Fold: 14  Epoch: 205  Training loss = 1.8819  Validation loss = 5.2873  \n",
      "\n",
      "Fold: 14  Epoch: 206  Training loss = 1.8739  Validation loss = 5.2689  \n",
      "\n",
      "Fold: 14  Epoch: 207  Training loss = 1.8719  Validation loss = 5.2557  \n",
      "\n",
      "Fold: 14  Epoch: 208  Training loss = 1.8691  Validation loss = 5.2483  \n",
      "\n",
      "Fold: 14  Epoch: 209  Training loss = 1.8664  Validation loss = 5.2410  \n",
      "\n",
      "Fold: 14  Epoch: 210  Training loss = 1.8766  Validation loss = 5.2512  \n",
      "\n",
      "Fold: 14  Epoch: 211  Training loss = 1.8682  Validation loss = 5.2384  \n",
      "\n",
      "Fold: 14  Epoch: 212  Training loss = 1.8620  Validation loss = 5.2353  \n",
      "\n",
      "Fold: 14  Epoch: 213  Training loss = 1.8740  Validation loss = 5.2292  \n",
      "\n",
      "Fold: 14  Epoch: 214  Training loss = 1.8667  Validation loss = 5.2099  \n",
      "\n",
      "Fold: 14  Epoch: 215  Training loss = 1.8572  Validation loss = 5.2004  \n",
      "\n",
      "Fold: 14  Epoch: 216  Training loss = 1.8482  Validation loss = 5.1852  \n",
      "\n",
      "Fold: 14  Epoch: 217  Training loss = 1.8461  Validation loss = 5.1941  \n",
      "\n",
      "Fold: 14  Epoch: 218  Training loss = 1.8458  Validation loss = 5.1771  \n",
      "\n",
      "Fold: 14  Epoch: 219  Training loss = 1.8430  Validation loss = 5.1670  \n",
      "\n",
      "Fold: 14  Epoch: 220  Training loss = 1.8430  Validation loss = 5.1376  \n",
      "\n",
      "Fold: 14  Epoch: 221  Training loss = 1.8400  Validation loss = 5.1322  \n",
      "\n",
      "Fold: 14  Epoch: 222  Training loss = 1.8406  Validation loss = 5.1202  \n",
      "\n",
      "Fold: 14  Epoch: 223  Training loss = 1.8386  Validation loss = 5.1208  \n",
      "\n",
      "Fold: 14  Epoch: 224  Training loss = 1.8349  Validation loss = 5.1075  \n",
      "\n",
      "Fold: 14  Epoch: 225  Training loss = 1.8351  Validation loss = 5.0934  \n",
      "\n",
      "Fold: 14  Epoch: 226  Training loss = 1.8375  Validation loss = 5.1026  \n",
      "\n",
      "Fold: 14  Epoch: 227  Training loss = 1.8324  Validation loss = 5.1001  \n",
      "\n",
      "Fold: 14  Epoch: 228  Training loss = 1.8274  Validation loss = 5.0979  \n",
      "\n",
      "Fold: 14  Epoch: 229  Training loss = 1.8303  Validation loss = 5.0790  \n",
      "\n",
      "Fold: 14  Epoch: 230  Training loss = 1.8252  Validation loss = 5.0937  \n",
      "\n",
      "Fold: 14  Epoch: 231  Training loss = 1.8285  Validation loss = 5.0825  \n",
      "\n",
      "Fold: 14  Epoch: 232  Training loss = 1.8204  Validation loss = 5.0879  \n",
      "\n",
      "Fold: 14  Epoch: 233  Training loss = 1.8177  Validation loss = 5.0773  \n",
      "\n",
      "Fold: 14  Epoch: 234  Training loss = 1.8198  Validation loss = 5.0818  \n",
      "\n",
      "Fold: 14  Epoch: 235  Training loss = 1.8141  Validation loss = 5.0770  \n",
      "\n",
      "Fold: 14  Epoch: 236  Training loss = 1.8173  Validation loss = 5.0870  \n",
      "\n",
      "Fold: 14  Epoch: 237  Training loss = 1.8222  Validation loss = 5.0473  \n",
      "\n",
      "Fold: 14  Epoch: 238  Training loss = 1.8109  Validation loss = 5.0736  \n",
      "\n",
      "Fold: 14  Epoch: 239  Training loss = 1.8094  Validation loss = 5.0870  \n",
      "\n",
      "Fold: 14  Epoch: 240  Training loss = 1.8090  Validation loss = 5.0734  \n",
      "\n",
      "Fold: 14  Epoch: 241  Training loss = 1.8223  Validation loss = 5.1010  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 237  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 2.1938  Validation loss = 5.7567  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 2.1832  Validation loss = 5.7454  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 2.1945  Validation loss = 5.7387  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 2.1785  Validation loss = 5.7154  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 2.1744  Validation loss = 5.7064  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 2.1716  Validation loss = 5.6974  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 2.1701  Validation loss = 5.6901  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 2.1647  Validation loss = 5.6739  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 2.1644  Validation loss = 5.6640  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 2.1613  Validation loss = 5.6532  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 2.1623  Validation loss = 5.6282  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 2.1564  Validation loss = 5.6228  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 2.1490  Validation loss = 5.6069  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 2.1426  Validation loss = 5.5858  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 2.1434  Validation loss = 5.5659  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 2.1521  Validation loss = 5.5490  \n",
      "\n",
      "Fold: 15  Epoch: 17  Training loss = 2.1473  Validation loss = 5.5520  \n",
      "\n",
      "Fold: 15  Epoch: 18  Training loss = 2.1293  Validation loss = 5.5523  \n",
      "\n",
      "Fold: 15  Epoch: 19  Training loss = 2.1352  Validation loss = 5.5344  \n",
      "\n",
      "Fold: 15  Epoch: 20  Training loss = 2.1248  Validation loss = 5.5228  \n",
      "\n",
      "Fold: 15  Epoch: 21  Training loss = 2.1200  Validation loss = 5.5014  \n",
      "\n",
      "Fold: 15  Epoch: 22  Training loss = 2.1148  Validation loss = 5.4855  \n",
      "\n",
      "Fold: 15  Epoch: 23  Training loss = 2.1082  Validation loss = 5.4663  \n",
      "\n",
      "Fold: 15  Epoch: 24  Training loss = 2.1136  Validation loss = 5.4578  \n",
      "\n",
      "Fold: 15  Epoch: 25  Training loss = 2.1191  Validation loss = 5.4527  \n",
      "\n",
      "Fold: 15  Epoch: 26  Training loss = 2.1054  Validation loss = 5.4338  \n",
      "\n",
      "Fold: 15  Epoch: 27  Training loss = 2.1158  Validation loss = 5.4176  \n",
      "\n",
      "Fold: 15  Epoch: 28  Training loss = 2.1053  Validation loss = 5.4067  \n",
      "\n",
      "Fold: 15  Epoch: 29  Training loss = 2.0891  Validation loss = 5.3789  \n",
      "\n",
      "Fold: 15  Epoch: 30  Training loss = 2.0938  Validation loss = 5.3582  \n",
      "\n",
      "Fold: 15  Epoch: 31  Training loss = 2.0822  Validation loss = 5.3466  \n",
      "\n",
      "Fold: 15  Epoch: 32  Training loss = 2.0814  Validation loss = 5.3442  \n",
      "\n",
      "Fold: 15  Epoch: 33  Training loss = 2.0797  Validation loss = 5.3244  \n",
      "\n",
      "Fold: 15  Epoch: 34  Training loss = 2.0778  Validation loss = 5.3091  \n",
      "\n",
      "Fold: 15  Epoch: 35  Training loss = 2.0749  Validation loss = 5.3083  \n",
      "\n",
      "Fold: 15  Epoch: 36  Training loss = 2.0732  Validation loss = 5.3050  \n",
      "\n",
      "Fold: 15  Epoch: 37  Training loss = 2.0838  Validation loss = 5.3047  \n",
      "\n",
      "Fold: 15  Epoch: 38  Training loss = 2.0687  Validation loss = 5.2944  \n",
      "\n",
      "Fold: 15  Epoch: 39  Training loss = 2.0627  Validation loss = 5.2818  \n",
      "\n",
      "Fold: 15  Epoch: 40  Training loss = 2.0608  Validation loss = 5.2724  \n",
      "\n",
      "Fold: 15  Epoch: 41  Training loss = 2.0588  Validation loss = 5.2622  \n",
      "\n",
      "Fold: 15  Epoch: 42  Training loss = 2.0528  Validation loss = 5.2423  \n",
      "\n",
      "Fold: 15  Epoch: 43  Training loss = 2.0567  Validation loss = 5.2358  \n",
      "\n",
      "Fold: 15  Epoch: 44  Training loss = 2.0474  Validation loss = 5.2261  \n",
      "\n",
      "Fold: 15  Epoch: 45  Training loss = 2.0450  Validation loss = 5.2123  \n",
      "\n",
      "Fold: 15  Epoch: 46  Training loss = 2.0397  Validation loss = 5.1970  \n",
      "\n",
      "Fold: 15  Epoch: 47  Training loss = 2.0376  Validation loss = 5.1935  \n",
      "\n",
      "Fold: 15  Epoch: 48  Training loss = 2.0343  Validation loss = 5.1759  \n",
      "\n",
      "Fold: 15  Epoch: 49  Training loss = 2.0307  Validation loss = 5.1660  \n",
      "\n",
      "Fold: 15  Epoch: 50  Training loss = 2.0291  Validation loss = 5.1612  \n",
      "\n",
      "Fold: 15  Epoch: 51  Training loss = 2.0277  Validation loss = 5.1538  \n",
      "\n",
      "Fold: 15  Epoch: 52  Training loss = 2.0334  Validation loss = 5.1510  \n",
      "\n",
      "Fold: 15  Epoch: 53  Training loss = 2.0247  Validation loss = 5.1441  \n",
      "\n",
      "Fold: 15  Epoch: 54  Training loss = 2.0228  Validation loss = 5.1500  \n",
      "\n",
      "Fold: 15  Epoch: 55  Training loss = 2.0323  Validation loss = 5.1370  \n",
      "\n",
      "Fold: 15  Epoch: 56  Training loss = 2.0182  Validation loss = 5.1237  \n",
      "\n",
      "Fold: 15  Epoch: 57  Training loss = 2.0609  Validation loss = 5.1162  \n",
      "\n",
      "Fold: 15  Epoch: 58  Training loss = 2.0235  Validation loss = 5.1072  \n",
      "\n",
      "Fold: 15  Epoch: 59  Training loss = 2.0142  Validation loss = 5.1022  \n",
      "\n",
      "Fold: 15  Epoch: 60  Training loss = 2.0170  Validation loss = 5.0955  \n",
      "\n",
      "Fold: 15  Epoch: 61  Training loss = 2.0098  Validation loss = 5.0826  \n",
      "\n",
      "Fold: 15  Epoch: 62  Training loss = 2.0330  Validation loss = 5.0831  \n",
      "\n",
      "Fold: 15  Epoch: 63  Training loss = 2.0055  Validation loss = 5.0983  \n",
      "\n",
      "Fold: 15  Epoch: 64  Training loss = 2.0048  Validation loss = 5.0890  \n",
      "\n",
      "Fold: 15  Epoch: 65  Training loss = 2.0003  Validation loss = 5.0777  \n",
      "\n",
      "Fold: 15  Epoch: 66  Training loss = 2.0005  Validation loss = 5.0773  \n",
      "\n",
      "Fold: 15  Epoch: 67  Training loss = 2.0055  Validation loss = 5.0858  \n",
      "\n",
      "Fold: 15  Epoch: 68  Training loss = 2.0091  Validation loss = 5.0646  \n",
      "\n",
      "Fold: 15  Epoch: 69  Training loss = 1.9985  Validation loss = 5.0615  \n",
      "\n",
      "Fold: 15  Epoch: 70  Training loss = 2.0128  Validation loss = 5.0597  \n",
      "\n",
      "Fold: 15  Epoch: 71  Training loss = 2.0088  Validation loss = 5.0538  \n",
      "\n",
      "Fold: 15  Epoch: 72  Training loss = 2.0307  Validation loss = 5.0454  \n",
      "\n",
      "Fold: 15  Epoch: 73  Training loss = 2.0027  Validation loss = 5.0350  \n",
      "\n",
      "Fold: 15  Epoch: 74  Training loss = 1.9944  Validation loss = 5.0296  \n",
      "\n",
      "Fold: 15  Epoch: 75  Training loss = 1.9859  Validation loss = 5.0229  \n",
      "\n",
      "Fold: 15  Epoch: 76  Training loss = 1.9831  Validation loss = 5.0155  \n",
      "\n",
      "Fold: 15  Epoch: 77  Training loss = 1.9802  Validation loss = 5.0093  \n",
      "\n",
      "Fold: 15  Epoch: 78  Training loss = 1.9787  Validation loss = 5.0080  \n",
      "\n",
      "Fold: 15  Epoch: 79  Training loss = 1.9748  Validation loss = 4.9956  \n",
      "\n",
      "Fold: 15  Epoch: 80  Training loss = 1.9937  Validation loss = 4.9945  \n",
      "\n",
      "Fold: 15  Epoch: 81  Training loss = 1.9722  Validation loss = 4.9859  \n",
      "\n",
      "Fold: 15  Epoch: 82  Training loss = 1.9700  Validation loss = 4.9867  \n",
      "\n",
      "Fold: 15  Epoch: 83  Training loss = 1.9687  Validation loss = 4.9602  \n",
      "\n",
      "Fold: 15  Epoch: 84  Training loss = 1.9682  Validation loss = 4.9605  \n",
      "\n",
      "Fold: 15  Epoch: 85  Training loss = 2.0407  Validation loss = 4.9556  \n",
      "\n",
      "Fold: 15  Epoch: 86  Training loss = 2.0017  Validation loss = 4.9623  \n",
      "\n",
      "Fold: 15  Epoch: 87  Training loss = 1.9845  Validation loss = 4.9662  \n",
      "\n",
      "Fold: 15  Epoch: 88  Training loss = 1.9656  Validation loss = 4.9475  \n",
      "\n",
      "Fold: 15  Epoch: 89  Training loss = 1.9638  Validation loss = 4.9394  \n",
      "\n",
      "Fold: 15  Epoch: 90  Training loss = 1.9636  Validation loss = 4.9286  \n",
      "\n",
      "Fold: 15  Epoch: 91  Training loss = 1.9606  Validation loss = 4.9176  \n",
      "\n",
      "Fold: 15  Epoch: 92  Training loss = 1.9572  Validation loss = 4.9182  \n",
      "\n",
      "Fold: 15  Epoch: 93  Training loss = 1.9593  Validation loss = 4.9110  \n",
      "\n",
      "Fold: 15  Epoch: 94  Training loss = 1.9536  Validation loss = 4.9119  \n",
      "\n",
      "Fold: 15  Epoch: 95  Training loss = 1.9509  Validation loss = 4.9042  \n",
      "\n",
      "Fold: 15  Epoch: 96  Training loss = 1.9552  Validation loss = 4.9147  \n",
      "\n",
      "Fold: 15  Epoch: 97  Training loss = 1.9548  Validation loss = 4.8945  \n",
      "\n",
      "Fold: 15  Epoch: 98  Training loss = 1.9464  Validation loss = 4.8842  \n",
      "\n",
      "Fold: 15  Epoch: 99  Training loss = 1.9487  Validation loss = 4.9012  \n",
      "\n",
      "Fold: 15  Epoch: 100  Training loss = 1.9541  Validation loss = 4.9199  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 98  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 2.2825  Validation loss = 3.6939  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 2.2742  Validation loss = 3.6650  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 2.2789  Validation loss = 3.6207  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 2.2754  Validation loss = 3.6252  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 2.2652  Validation loss = 3.6295  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 2.2610  Validation loss = 3.6395  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 2.2593  Validation loss = 3.6141  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 2.2724  Validation loss = 3.5990  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 2.2589  Validation loss = 3.6047  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 2.2567  Validation loss = 3.6058  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 2.2458  Validation loss = 3.6475  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 2.2396  Validation loss = 3.6377  \n",
      "\n",
      "Fold: 16  Epoch: 13  Training loss = 2.2365  Validation loss = 3.6485  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 8  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 2.4000  Validation loss = 2.7379  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 2.3967  Validation loss = 2.7571  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 2.3898  Validation loss = 2.7562  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 2.3919  Validation loss = 2.7811  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 2.3872  Validation loss = 2.7523  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 2.3847  Validation loss = 2.6944  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 2.3810  Validation loss = 2.6877  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 2.3711  Validation loss = 2.7633  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 2.3680  Validation loss = 2.7043  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 2.3625  Validation loss = 2.7170  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 2.3579  Validation loss = 2.8393  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 7  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 2.4662  Validation loss = 0.9144  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 2.4422  Validation loss = 0.9255  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 2.4336  Validation loss = 0.9210  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 2.4287  Validation loss = 0.9733  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 2.4224  Validation loss = 0.9337  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 2.4232  Validation loss = 0.9195  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 2.4143  Validation loss = 0.9314  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 2.4135  Validation loss = 0.9317  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 2.4164  Validation loss = 0.9308  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 2.4052  Validation loss = 0.9322  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 2.3935  Validation loss = 0.9238  \n",
      "\n",
      "Fold: 18  Epoch: 12  Training loss = 2.3878  Validation loss = 0.9210  \n",
      "\n",
      "Fold: 18  Epoch: 13  Training loss = 2.3876  Validation loss = 0.9332  \n",
      "\n",
      "Fold: 18  Epoch: 14  Training loss = 2.3855  Validation loss = 0.9212  \n",
      "\n",
      "Fold: 18  Epoch: 15  Training loss = 2.3846  Validation loss = 0.9281  \n",
      "\n",
      "Fold: 18  Epoch: 16  Training loss = 2.3771  Validation loss = 0.9170  \n",
      "\n",
      "Fold: 18  Epoch: 17  Training loss = 2.3708  Validation loss = 0.9255  \n",
      "\n",
      "Fold: 18  Epoch: 18  Training loss = 2.3687  Validation loss = 0.9276  \n",
      "\n",
      "Fold: 18  Epoch: 19  Training loss = 2.3784  Validation loss = 1.0460  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 1  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 2.3285  Validation loss = 2.1566  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 2.3260  Validation loss = 2.2736  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 2.3278  Validation loss = 2.2813  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 2.3207  Validation loss = 2.3533  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 2.3279  Validation loss = 2.4100  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 2.3118  Validation loss = 2.2736  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 2.3067  Validation loss = 2.3196  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 2.3083  Validation loss = 2.2722  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 2.3007  Validation loss = 2.3212  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 2.3000  Validation loss = 2.3558  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 2.3402  Validation loss = 2.4961  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 1  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 2.3858  Validation loss = 0.9456  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 2.3681  Validation loss = 0.9174  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 2.6803  Validation loss = 0.9325  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 2.3338  Validation loss = 0.9485  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 2.3352  Validation loss = 0.9551  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 2.3392  Validation loss = 0.8551  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 2.3395  Validation loss = 0.9026  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 2.3323  Validation loss = 0.9378  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 2.4622  Validation loss = 1.0186  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 2.3682  Validation loss = 1.1532  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 2.3339  Validation loss = 1.1466  \n",
      "\n",
      "Fold: 20  Epoch: 12  Training loss = 2.3433  Validation loss = 1.0933  \n",
      "\n",
      "Fold: 20  Epoch: 13  Training loss = 2.3366  Validation loss = 1.0606  \n",
      "\n",
      "Fold: 20  Epoch: 14  Training loss = 2.3174  Validation loss = 1.1102  \n",
      "\n",
      "Fold: 20  Epoch: 15  Training loss = 2.3116  Validation loss = 1.0586  \n",
      "\n",
      "Fold: 20  Epoch: 16  Training loss = 2.3090  Validation loss = 1.0785  \n",
      "\n",
      "Fold: 20  Epoch: 17  Training loss = 2.3058  Validation loss = 0.9927  \n",
      "\n",
      "Fold: 20  Epoch: 18  Training loss = 2.2975  Validation loss = 1.0222  \n",
      "\n",
      "Fold: 20  Epoch: 19  Training loss = 2.2872  Validation loss = 0.9919  \n",
      "\n",
      "Fold: 20  Epoch: 20  Training loss = 2.2947  Validation loss = 0.8973  \n",
      "\n",
      "Fold: 20  Epoch: 21  Training loss = 2.2830  Validation loss = 0.9219  \n",
      "\n",
      "Fold: 20  Epoch: 22  Training loss = 2.2934  Validation loss = 0.8474  \n",
      "\n",
      "Fold: 20  Epoch: 23  Training loss = 2.2920  Validation loss = 0.8084  \n",
      "\n",
      "Fold: 20  Epoch: 24  Training loss = 2.2858  Validation loss = 0.8728  \n",
      "\n",
      "Fold: 20  Epoch: 25  Training loss = 2.2813  Validation loss = 0.8649  \n",
      "\n",
      "Fold: 20  Epoch: 26  Training loss = 2.2821  Validation loss = 0.7769  \n",
      "\n",
      "Fold: 20  Epoch: 27  Training loss = 2.2695  Validation loss = 0.8347  \n",
      "\n",
      "Fold: 20  Epoch: 28  Training loss = 2.2806  Validation loss = 0.8878  \n",
      "\n",
      "Fold: 20  Epoch: 29  Training loss = 2.2607  Validation loss = 0.9784  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 26  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 2.2397  Validation loss = 3.0520  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 2.2354  Validation loss = 2.8917  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 2.2329  Validation loss = 3.0964  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 2.2269  Validation loss = 2.9175  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 2.2212  Validation loss = 2.9552  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 2.2201  Validation loss = 2.8488  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 2.2165  Validation loss = 3.1184  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 2.2907  Validation loss = 2.9475  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 2.2110  Validation loss = 2.8684  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 2.2099  Validation loss = 2.8124  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 2.2261  Validation loss = 3.0268  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 2.1961  Validation loss = 2.8931  \n",
      "\n",
      "Fold: 21  Epoch: 13  Training loss = 2.1959  Validation loss = 3.0372  \n",
      "\n",
      "Fold: 21  Epoch: 14  Training loss = 2.1929  Validation loss = 2.8244  \n",
      "\n",
      "Fold: 21  Epoch: 15  Training loss = 2.1868  Validation loss = 2.8691  \n",
      "\n",
      "Fold: 21  Epoch: 16  Training loss = 2.1802  Validation loss = 3.0070  \n",
      "\n",
      "Fold: 21  Epoch: 17  Training loss = 2.1913  Validation loss = 2.9097  \n",
      "\n",
      "Fold: 21  Epoch: 18  Training loss = 2.1905  Validation loss = 2.8060  \n",
      "\n",
      "Fold: 21  Epoch: 19  Training loss = 2.1839  Validation loss = 2.8187  \n",
      "\n",
      "Fold: 21  Epoch: 20  Training loss = 2.1760  Validation loss = 2.9685  \n",
      "\n",
      "Fold: 21  Epoch: 21  Training loss = 2.1727  Validation loss = 3.0063  \n",
      "\n",
      "Fold: 21  Epoch: 22  Training loss = 2.1811  Validation loss = 2.8843  \n",
      "\n",
      "Fold: 21  Epoch: 23  Training loss = 2.1613  Validation loss = 2.9583  \n",
      "\n",
      "Fold: 21  Epoch: 24  Training loss = 2.1532  Validation loss = 2.9145  \n",
      "\n",
      "Fold: 21  Epoch: 25  Training loss = 2.1540  Validation loss = 3.0336  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 18  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 2.1540  Validation loss = 2.2751  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 2.1598  Validation loss = 2.1680  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 2.1432  Validation loss = 2.1766  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 2.1750  Validation loss = 2.9547  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 2.1315  Validation loss = 2.2579  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 2.1334  Validation loss = 2.5845  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 2.1706  Validation loss = 3.4904  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 2.1251  Validation loss = 2.7373  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 2.1304  Validation loss = 2.6748  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 2.1301  Validation loss = 2.6764  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 2.1404  Validation loss = 2.0499  \n",
      "\n",
      "Fold: 22  Epoch: 12  Training loss = 2.1159  Validation loss = 2.8195  \n",
      "\n",
      "Fold: 22  Epoch: 13  Training loss = 2.1338  Validation loss = 2.0985  \n",
      "\n",
      "Fold: 22  Epoch: 14  Training loss = 2.4347  Validation loss = 2.7433  \n",
      "\n",
      "Fold: 22  Epoch: 15  Training loss = 2.1818  Validation loss = 2.7522  \n",
      "\n",
      "Fold: 22  Epoch: 16  Training loss = 2.1615  Validation loss = 3.3539  \n",
      "\n",
      "Fold: 22  Epoch: 17  Training loss = 2.1487  Validation loss = 3.1717  \n",
      "\n",
      "Fold: 22  Epoch: 18  Training loss = 2.1188  Validation loss = 3.0981  \n",
      "\n",
      "Fold: 22  Epoch: 19  Training loss = 2.1406  Validation loss = 2.5900  \n",
      "\n",
      "Fold: 22  Epoch: 20  Training loss = 2.1496  Validation loss = 2.0486  \n",
      "\n",
      "Fold: 22  Epoch: 21  Training loss = 2.0921  Validation loss = 2.9601  \n",
      "\n",
      "Fold: 22  Epoch: 22  Training loss = 2.1036  Validation loss = 2.6775  \n",
      "\n",
      "Fold: 22  Epoch: 23  Training loss = 2.0843  Validation loss = 1.9347  \n",
      "\n",
      "Fold: 22  Epoch: 24  Training loss = 2.0875  Validation loss = 1.9352  \n",
      "\n",
      "Fold: 22  Epoch: 25  Training loss = 2.1041  Validation loss = 2.8750  \n",
      "\n",
      "Fold: 22  Epoch: 26  Training loss = 2.1001  Validation loss = 1.8235  \n",
      "\n",
      "Fold: 22  Epoch: 27  Training loss = 2.0899  Validation loss = 2.5557  \n",
      "\n",
      "Fold: 22  Epoch: 28  Training loss = 2.0970  Validation loss = 3.6099  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 26  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 2.1091  Validation loss = 1.4655  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 2.0798  Validation loss = 1.7110  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 2.1345  Validation loss = 1.4234  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 2.0811  Validation loss = 1.4637  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 2.0897  Validation loss = 1.7619  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 2.0684  Validation loss = 1.6950  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 2.1177  Validation loss = 2.1292  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 2.2018  Validation loss = 2.5233  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 2.0559  Validation loss = 1.3918  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 2.0552  Validation loss = 1.7044  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 2.0520  Validation loss = 1.4212  \n",
      "\n",
      "Fold: 23  Epoch: 12  Training loss = 2.0893  Validation loss = 1.9000  \n",
      "\n",
      "Fold: 23  Epoch: 13  Training loss = 2.1439  Validation loss = 1.9642  \n",
      "\n",
      "Fold: 23  Epoch: 14  Training loss = 2.1188  Validation loss = 2.2246  \n",
      "\n",
      "Fold: 23  Epoch: 15  Training loss = 2.0460  Validation loss = 1.5695  \n",
      "\n",
      "Fold: 23  Epoch: 16  Training loss = 2.0419  Validation loss = 1.4865  \n",
      "\n",
      "Fold: 23  Epoch: 17  Training loss = 2.0558  Validation loss = 1.8711  \n",
      "\n",
      "Fold: 23  Epoch: 18  Training loss = 2.0441  Validation loss = 1.7388  \n",
      "\n",
      "Fold: 23  Epoch: 19  Training loss = 2.0735  Validation loss = 1.8905  \n",
      "\n",
      "Fold: 23  Epoch: 20  Training loss = 2.0329  Validation loss = 1.3855  \n",
      "\n",
      "Fold: 23  Epoch: 21  Training loss = 2.0372  Validation loss = 1.3812  \n",
      "\n",
      "Fold: 23  Epoch: 22  Training loss = 2.0290  Validation loss = 1.3934  \n",
      "\n",
      "Fold: 23  Epoch: 23  Training loss = 2.1419  Validation loss = 2.3664  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 21  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 3.0188  Validation loss = 1.5095  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 2.7743  Validation loss = 1.4541  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 2.4768  Validation loss = 1.5868  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 2.2920  Validation loss = 1.6325  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 2.1787  Validation loss = 1.6543  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 2.1212  Validation loss = 1.7666  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 2.1826  Validation loss = 1.3979  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 2.3495  Validation loss = 1.6313  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 2.1451  Validation loss = 1.7053  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 2.1423  Validation loss = 1.6385  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 2.1315  Validation loss = 1.7023  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 2.1233  Validation loss = 1.7177  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 2.1291  Validation loss = 1.6719  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 2.1603  Validation loss = 1.7865  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 7  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 2.0903  Validation loss = 2.1894  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 2.4302  Validation loss = 2.1724  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 2.0901  Validation loss = 2.4481  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 2.0991  Validation loss = 2.0989  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 2.0710  Validation loss = 2.1260  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 2.0938  Validation loss = 2.1218  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 2.1398  Validation loss = 2.2604  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 2.3107  Validation loss = 2.1206  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 2.0624  Validation loss = 2.3699  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 2.0465  Validation loss = 2.2023  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 2.0564  Validation loss = 2.1777  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 2.0462  Validation loss = 2.1524  \n",
      "\n",
      "Fold: 25  Epoch: 13  Training loss = 2.0445  Validation loss = 2.1743  \n",
      "\n",
      "Fold: 25  Epoch: 14  Training loss = 2.0524  Validation loss = 2.1752  \n",
      "\n",
      "Fold: 25  Epoch: 15  Training loss = 2.0969  Validation loss = 2.2085  \n",
      "\n",
      "Fold: 25  Epoch: 16  Training loss = 2.0859  Validation loss = 2.1569  \n",
      "\n",
      "Fold: 25  Epoch: 17  Training loss = 2.0386  Validation loss = 2.1637  \n",
      "\n",
      "Fold: 25  Epoch: 18  Training loss = 2.0384  Validation loss = 2.2361  \n",
      "\n",
      "Fold: 25  Epoch: 19  Training loss = 2.0344  Validation loss = 2.0862  \n",
      "\n",
      "Fold: 25  Epoch: 20  Training loss = 2.0356  Validation loss = 2.2206  \n",
      "\n",
      "Fold: 25  Epoch: 21  Training loss = 2.0667  Validation loss = 2.3939  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 19  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 2.0868  Validation loss = 2.5614  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 2.0992  Validation loss = 1.6010  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 2.2184  Validation loss = 1.3796  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 2.2047  Validation loss = 1.2844  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 2.1178  Validation loss = 1.3892  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 2.1572  Validation loss = 1.2648  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 2.1910  Validation loss = 2.9257  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 2.2788  Validation loss = 2.6839  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 2.0748  Validation loss = 1.8644  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 2.0732  Validation loss = 1.6845  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 2.0917  Validation loss = 1.6485  \n",
      "\n",
      "Fold: 26  Epoch: 12  Training loss = 2.2800  Validation loss = 1.5162  \n",
      "\n",
      "Fold: 26  Epoch: 13  Training loss = 2.1039  Validation loss = 2.9112  \n",
      "\n",
      "Fold: 26  Epoch: 14  Training loss = 2.2523  Validation loss = 1.3912  \n",
      "\n",
      "Fold: 26  Epoch: 15  Training loss = 2.0884  Validation loss = 2.6334  \n",
      "\n",
      "Fold: 26  Epoch: 16  Training loss = 2.0564  Validation loss = 2.6295  \n",
      "\n",
      "Fold: 26  Epoch: 17  Training loss = 2.1655  Validation loss = 1.3538  \n",
      "\n",
      "Fold: 26  Epoch: 18  Training loss = 2.0360  Validation loss = 2.8941  \n",
      "\n",
      "Fold: 26  Epoch: 19  Training loss = 2.0467  Validation loss = 2.3287  \n",
      "\n",
      "Fold: 26  Epoch: 20  Training loss = 2.0321  Validation loss = 1.9082  \n",
      "\n",
      "Fold: 26  Epoch: 21  Training loss = 2.1093  Validation loss = 1.0902  \n",
      "\n",
      "Fold: 26  Epoch: 22  Training loss = 2.1019  Validation loss = 1.5162  \n",
      "\n",
      "Fold: 26  Epoch: 23  Training loss = 1.9612  Validation loss = 2.1389  \n",
      "\n",
      "Fold: 26  Epoch: 24  Training loss = 1.9783  Validation loss = 1.8137  \n",
      "\n",
      "Fold: 26  Epoch: 25  Training loss = 2.7958  Validation loss = 3.4923  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 21  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 2.1092  Validation loss = 0.5867  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 2.0289  Validation loss = 0.6592  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 1.9250  Validation loss = 0.6861  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 2.1333  Validation loss = 0.6601  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 1.8773  Validation loss = 0.7272  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 1.9093  Validation loss = 0.7359  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 2.3521  Validation loss = 0.7347  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 2.2764  Validation loss = 0.7158  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 2.0750  Validation loss = 0.6725  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 1.8460  Validation loss = 0.7064  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 1.8519  Validation loss = 0.7210  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 1.8942  Validation loss = 0.6694  \n",
      "\n",
      "Fold: 27  Epoch: 13  Training loss = 2.0769  Validation loss = 0.6949  \n",
      "\n",
      "Fold: 27  Epoch: 14  Training loss = 1.8294  Validation loss = 0.7530  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 1  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 1.9371  Validation loss = 0.6462  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 1.8165  Validation loss = 0.5798  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 1.8225  Validation loss = 0.5653  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 1.8038  Validation loss = 0.5425  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 1.8111  Validation loss = 0.5865  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 1.9084  Validation loss = 0.6013  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 1.7592  Validation loss = 0.5887  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 1.7924  Validation loss = 0.5769  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 1.7902  Validation loss = 0.6098  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 1.7767  Validation loss = 0.5893  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 1.8244  Validation loss = 0.6680  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 4  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 1.8601  Validation loss = 1.2511  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 1.8148  Validation loss = 1.1830  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 1.8745  Validation loss = 1.1961  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 2.0224  Validation loss = 1.1826  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 1.7435  Validation loss = 1.2090  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 1.7838  Validation loss = 1.1339  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 1.7407  Validation loss = 1.1302  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 1.7571  Validation loss = 1.1457  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 1.8261  Validation loss = 1.1617  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 1.7534  Validation loss = 1.1957  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 1.7284  Validation loss = 1.1794  \n",
      "\n",
      "Fold: 29  Epoch: 12  Training loss = 1.7244  Validation loss = 1.1557  \n",
      "\n",
      "Fold: 29  Epoch: 13  Training loss = 1.8239  Validation loss = 1.2202  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 7  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 1.7415  Validation loss = 1.2738  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 1.7664  Validation loss = 1.1300  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 1.7751  Validation loss = 1.1631  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 1.7407  Validation loss = 1.1775  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 1.8512  Validation loss = 1.1339  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 1.7134  Validation loss = 1.1574  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 1.7185  Validation loss = 1.1577  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 1.8122  Validation loss = 1.2226  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 1.7735  Validation loss = 1.0907  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 1.7278  Validation loss = 1.1369  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 1.7148  Validation loss = 1.1632  \n",
      "\n",
      "Fold: 30  Epoch: 12  Training loss = 1.7472  Validation loss = 1.1568  \n",
      "\n",
      "Fold: 30  Epoch: 13  Training loss = 1.6997  Validation loss = 1.1424  \n",
      "\n",
      "Fold: 30  Epoch: 14  Training loss = 1.7664  Validation loss = 1.1759  \n",
      "\n",
      "Fold: 30  Epoch: 15  Training loss = 1.7282  Validation loss = 1.1380  \n",
      "\n",
      "Fold: 30  Epoch: 16  Training loss = 1.7686  Validation loss = 1.0600  \n",
      "\n",
      "Fold: 30  Epoch: 17  Training loss = 1.7034  Validation loss = 1.1032  \n",
      "\n",
      "Fold: 30  Epoch: 18  Training loss = 1.6986  Validation loss = 1.1055  \n",
      "\n",
      "Fold: 30  Epoch: 19  Training loss = 1.7117  Validation loss = 1.1002  \n",
      "\n",
      "Fold: 30  Epoch: 20  Training loss = 1.7318  Validation loss = 1.0768  \n",
      "\n",
      "Fold: 30  Epoch: 21  Training loss = 1.7368  Validation loss = 1.1587  \n",
      "\n",
      "Fold: 30  Epoch: 22  Training loss = 1.8957  Validation loss = 1.0159  \n",
      "\n",
      "Fold: 30  Epoch: 23  Training loss = 1.7652  Validation loss = 1.0243  \n",
      "\n",
      "Fold: 30  Epoch: 24  Training loss = 2.1440  Validation loss = 1.3786  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 22  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 1.7793  Validation loss = 0.8192  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 2.0193  Validation loss = 0.7275  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 1.7331  Validation loss = 0.6893  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 1.7116  Validation loss = 0.5330  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 1.7002  Validation loss = 0.6278  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 1.7037  Validation loss = 0.6281  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 1.7014  Validation loss = 0.7633  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 1.6763  Validation loss = 0.6702  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 1.6691  Validation loss = 0.7045  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 1.6620  Validation loss = 0.6936  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 1.6631  Validation loss = 0.6691  \n",
      "\n",
      "Fold: 31  Epoch: 12  Training loss = 1.6795  Validation loss = 0.6196  \n",
      "\n",
      "Fold: 31  Epoch: 13  Training loss = 1.7024  Validation loss = 0.5816  \n",
      "\n",
      "Fold: 31  Epoch: 14  Training loss = 1.8022  Validation loss = 0.5317  \n",
      "\n",
      "Fold: 31  Epoch: 15  Training loss = 1.6888  Validation loss = 0.6212  \n",
      "\n",
      "Fold: 31  Epoch: 16  Training loss = 1.6793  Validation loss = 0.5528  \n",
      "\n",
      "Fold: 31  Epoch: 17  Training loss = 1.7205  Validation loss = 0.6553  \n",
      "\n",
      "Fold: 31  Epoch: 18  Training loss = 1.6591  Validation loss = 0.5369  \n",
      "\n",
      "Fold: 31  Epoch: 19  Training loss = 2.0367  Validation loss = 0.5299  \n",
      "\n",
      "Fold: 31  Epoch: 20  Training loss = 1.7456  Validation loss = 0.5126  \n",
      "\n",
      "Fold: 31  Epoch: 21  Training loss = 1.7004  Validation loss = 0.4980  \n",
      "\n",
      "Fold: 31  Epoch: 22  Training loss = 1.6709  Validation loss = 0.5352  \n",
      "\n",
      "Fold: 31  Epoch: 23  Training loss = 1.6771  Validation loss = 0.5571  \n",
      "\n",
      "Fold: 31  Epoch: 24  Training loss = 1.6642  Validation loss = 0.5740  \n",
      "\n",
      "Fold: 31  Epoch: 25  Training loss = 1.6702  Validation loss = 0.6214  \n",
      "\n",
      "Fold: 31  Epoch: 26  Training loss = 1.7088  Validation loss = 0.6275  \n",
      "\n",
      "Fold: 31  Epoch: 27  Training loss = 1.7502  Validation loss = 0.7615  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 21  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 1.4131  Validation loss = 2.1743  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 1.4328  Validation loss = 2.0998  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 1.4114  Validation loss = 2.3791  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 1.4138  Validation loss = 2.3802  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 1.4117  Validation loss = 2.2450  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 1.4113  Validation loss = 2.1976  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 1.4072  Validation loss = 2.2310  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 1.4065  Validation loss = 2.2450  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 1.4043  Validation loss = 2.1493  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 1.4092  Validation loss = 2.1001  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 1.3976  Validation loss = 2.1746  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 1.4010  Validation loss = 2.1706  \n",
      "\n",
      "Fold: 32  Epoch: 13  Training loss = 1.3970  Validation loss = 2.1817  \n",
      "\n",
      "Fold: 32  Epoch: 14  Training loss = 1.3936  Validation loss = 2.2575  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 2  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    # Load model with lowest validation error for each fold\n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 56\n",
      "Average validation error: 2.4934\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 1.4288  Test loss = 2.5625  \n",
      "\n",
      "Epoch: 2  Training loss = 1.4254  Test loss = 2.5582  \n",
      "\n",
      "Epoch: 3  Training loss = 1.4224  Test loss = 2.5543  \n",
      "\n",
      "Epoch: 4  Training loss = 1.4197  Test loss = 2.5508  \n",
      "\n",
      "Epoch: 5  Training loss = 1.4172  Test loss = 2.5476  \n",
      "\n",
      "Epoch: 6  Training loss = 1.4149  Test loss = 2.5446  \n",
      "\n",
      "Epoch: 7  Training loss = 1.4128  Test loss = 2.5420  \n",
      "\n",
      "Epoch: 8  Training loss = 1.4108  Test loss = 2.5395  \n",
      "\n",
      "Epoch: 9  Training loss = 1.4089  Test loss = 2.5373  \n",
      "\n",
      "Epoch: 10  Training loss = 1.4072  Test loss = 2.5352  \n",
      "\n",
      "Epoch: 11  Training loss = 1.4055  Test loss = 2.5334  \n",
      "\n",
      "Epoch: 12  Training loss = 1.4038  Test loss = 2.5317  \n",
      "\n",
      "Epoch: 13  Training loss = 1.4023  Test loss = 2.5301  \n",
      "\n",
      "Epoch: 14  Training loss = 1.4007  Test loss = 2.5288  \n",
      "\n",
      "Epoch: 15  Training loss = 1.3992  Test loss = 2.5275  \n",
      "\n",
      "Epoch: 16  Training loss = 1.3978  Test loss = 2.5265  \n",
      "\n",
      "Epoch: 17  Training loss = 1.3964  Test loss = 2.5256  \n",
      "\n",
      "Epoch: 18  Training loss = 1.3950  Test loss = 2.5248  \n",
      "\n",
      "Epoch: 19  Training loss = 1.3936  Test loss = 2.5243  \n",
      "\n",
      "Epoch: 20  Training loss = 1.3922  Test loss = 2.5239  \n",
      "\n",
      "Epoch: 21  Training loss = 1.3908  Test loss = 2.5238  \n",
      "\n",
      "Epoch: 22  Training loss = 1.3895  Test loss = 2.5238  \n",
      "\n",
      "Epoch: 23  Training loss = 1.3881  Test loss = 2.5242  \n",
      "\n",
      "Epoch: 24  Training loss = 1.3867  Test loss = 2.5248  \n",
      "\n",
      "Epoch: 25  Training loss = 1.3853  Test loss = 2.5259  \n",
      "\n",
      "Epoch: 26  Training loss = 1.3839  Test loss = 2.5274  \n",
      "\n",
      "Epoch: 27  Training loss = 1.3824  Test loss = 2.5294  \n",
      "\n",
      "Epoch: 28  Training loss = 1.3809  Test loss = 2.5322  \n",
      "\n",
      "Epoch: 29  Training loss = 1.3793  Test loss = 2.5358  \n",
      "\n",
      "Epoch: 30  Training loss = 1.3776  Test loss = 2.5405  \n",
      "\n",
      "Epoch: 31  Training loss = 1.3759  Test loss = 2.5466  \n",
      "\n",
      "Epoch: 32  Training loss = 1.3740  Test loss = 2.5544  \n",
      "\n",
      "Epoch: 33  Training loss = 1.3721  Test loss = 2.5639  \n",
      "\n",
      "Epoch: 34  Training loss = 1.3702  Test loss = 2.5747  \n",
      "\n",
      "Epoch: 35  Training loss = 1.3684  Test loss = 2.5857  \n",
      "\n",
      "Epoch: 36  Training loss = 1.3668  Test loss = 2.5944  \n",
      "\n",
      "Epoch: 37  Training loss = 1.3654  Test loss = 2.5992  \n",
      "\n",
      "Epoch: 38  Training loss = 1.3641  Test loss = 2.6008  \n",
      "\n",
      "Epoch: 39  Training loss = 1.3628  Test loss = 2.6012  \n",
      "\n",
      "Epoch: 40  Training loss = 1.3616  Test loss = 2.6012  \n",
      "\n",
      "Epoch: 41  Training loss = 1.3604  Test loss = 2.6011  \n",
      "\n",
      "Epoch: 42  Training loss = 1.3593  Test loss = 2.6010  \n",
      "\n",
      "Epoch: 43  Training loss = 1.3582  Test loss = 2.6009  \n",
      "\n",
      "Epoch: 44  Training loss = 1.3571  Test loss = 2.6008  \n",
      "\n",
      "Epoch: 45  Training loss = 1.3561  Test loss = 2.6006  \n",
      "\n",
      "Epoch: 46  Training loss = 1.3551  Test loss = 2.6004  \n",
      "\n",
      "Epoch: 47  Training loss = 1.3541  Test loss = 2.6002  \n",
      "\n",
      "Epoch: 48  Training loss = 1.3531  Test loss = 2.6000  \n",
      "\n",
      "Epoch: 49  Training loss = 1.3522  Test loss = 2.5999  \n",
      "\n",
      "Epoch: 50  Training loss = 1.3512  Test loss = 2.5997  \n",
      "\n",
      "Epoch: 51  Training loss = 1.3503  Test loss = 2.5995  \n",
      "\n",
      "Epoch: 52  Training loss = 1.3494  Test loss = 2.5994  \n",
      "\n",
      "Epoch: 53  Training loss = 1.3484  Test loss = 2.5993  \n",
      "\n",
      "Epoch: 54  Training loss = 1.3475  Test loss = 2.5993  \n",
      "\n",
      "Epoch: 55  Training loss = 1.3466  Test loss = 2.5993  \n",
      "\n",
      "Epoch: 56  Training loss = 1.3457  Test loss = 2.5993  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmnFhNzOrGRd2M7OacWE3M6uZfwAAkaFvoY1c4AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc0a8400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcabfd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 2.59928016114 \n",
      "Fixed scheme MAE:  1.68797623194\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 1.3457  Test loss = 2.9942  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 1.3925  Test loss = 2.2639  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 1.3796  Test loss = 0.1912  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 1.3798  Test loss = 0.0385  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 1.2195  Test loss = 0.7857  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 1.1931  Test loss = 0.4699  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 1.1524  Test loss = 0.3037  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 1.1214  Test loss = 0.1465  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 1.0729  Test loss = 0.8063  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 1.0761  Test loss = 1.8566  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 1.0542  Test loss = 1.0459  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 1.0578  Test loss = 1.4167  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 1.0315  Test loss = 0.1555  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 1.0316  Test loss = 1.0373  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 1.0360  Test loss = 2.4668  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 1.0749  Test loss = 3.3801  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 1.0920  Test loss = 0.9080  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 1.0940  Test loss = 0.1662  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 1.0897  Test loss = 0.8965  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 0.9976  Test loss = 0.5906  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 0.9628  Test loss = 1.5704  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 0.9788  Test loss = 2.8892  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 1.0338  Test loss = 1.5142  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 1.0388  Test loss = 0.3795  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 0.9923  Test loss = 2.6535  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 1.0453  Test loss = 0.9455  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 1.0504  Test loss = 1.0200  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 1.0577  Test loss = 1.1633  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 1.0390  Test loss = 1.0890  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 1.0461  Test loss = 0.5559  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 1.0464  Test loss = 3.2935  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 1.1065  Test loss = 0.2025  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 1.0749  Test loss = 1.1151  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 1.0835  Test loss = 0.5721  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 1.0397  Test loss = 0.3029  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 1.0340  Test loss = 5.0381  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 1.1614  Test loss = 1.9135  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 1.1775  Test loss = 2.4694  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 1.1953  Test loss = 0.2472  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 1.1955  Test loss = 2.0841  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 1.1213  Test loss = 0.1646  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 1.1209  Test loss = 0.0025  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 1.1175  Test loss = 1.6120  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 1.1332  Test loss = 13.0636  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 1.9930  Test loss = 7.7916  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.2116  Test loss = 1.7642  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.2221  Test loss = 0.0078  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.2204  Test loss = 0.5970  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 1.9190  Test loss = 1.4982  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 1.9212  Test loss = 3.1605  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 1.9607  Test loss = 1.1530  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 1.9642  Test loss = 1.2318  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 1.9377  Test loss = 2.4666  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 1.9613  Test loss = 2.7072  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 1.9878  Test loss = 0.6947  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 1.9873  Test loss = 0.9367  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 1.9557  Test loss = 0.9693  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 1.9592  Test loss = 2.2972  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 1.9785  Test loss = 0.7117  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 1.9793  Test loss = 0.1850  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.9451  Test loss = 0.9635  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.9472  Test loss = 2.6537  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 1.9723  Test loss = 0.3043  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 1.9658  Test loss = 0.3193  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 1.9366  Test loss = 0.5552  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 1.9378  Test loss = 0.4121  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.9353  Test loss = 1.3033  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 1.9415  Test loss = 2.1574  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 1.9434  Test loss = 4.7726  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 2.0310  Test loss = 0.1770  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 2.0308  Test loss = 2.3732  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 2.0424  Test loss = 3.9165  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 2.0413  Test loss = 3.4110  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 2.0847  Test loss = 0.2945  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 2.0848  Test loss = 1.4626  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 2.0921  Test loss = 0.4881  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 2.0186  Test loss = 1.4573  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyMjCniRJMmRkYU+SJBky/gGx29MYCvs3GwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd4ce160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.46833185131 \n",
      "Updating scheme MAE:  1.59708996859\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_test_fixed.csv\", yhat_test_fixed)\n",
    "np.savetxt(MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + \"yhat_update.csv\", yhat_update)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
