{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Directories\n",
    "MODEL_FOLDER = \"../../../models/lstm/\"\n",
    "MODEL_FILENAME = MODEL_FOLDER + \"1Q/1_cell/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: model saver\n",
    "def saveModel(sess, MODEL_FILENAME):\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    #print('Saving model at: ' + MODEL_FILENAME)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, MODEL_FILENAME)\n",
    "    #print('Model successfully saved.\\n')\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: model loader\n",
    "def loadModel(sess, MODEL_FILENAME):\n",
    "    if os.path.exists(MODEL_FILENAME + \".index\"):\n",
    "        print('Loading save model from: ' + MODEL_FILENAME)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, MODEL_FILENAME)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + MODEL_FILENAME + '>> does not exists!')\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Hyperparameters\n",
    "epochs = 750\n",
    "learning_rate = 1e-3\n",
    "batch_size = 5\n",
    "early_stop_iters = 15\n",
    "folds = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 3  # data input\n",
    "n_steps = 4  # timesteps\n",
    "n_hidden = 12 # dimension of recurrent unit\n",
    "\n",
    "# (REPRODUCIBILITY) set random seeds\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "# Define weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.truncated_normal([n_hidden, 1], stddev = 1.0 / tf.sqrt(float(n_hidden))),\n",
    "        name='out_weight')\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.zeros([1]),\n",
    "        name='out_bias')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Helper fct: Build model\n",
    "def RNN(X, weights, biases, n_hidden):\n",
    "\n",
    "    # Define a LSTM cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, state_is_tuple=True)\n",
    "    stacked_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell]*3, state_is_tuple = True)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(lstm_cell, X, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    out_layer = tf.matmul(outputs[:,-1,:], weights['out']) + biases['out']\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper fct: select batch\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Finished: data loaded. Stats below: \n",
      "Nr of training samples: 193\n",
      "Nr of testing  samples: 77\n",
      "Number of variables: 3\n",
      "Number of lags: 4\n",
      "Window length: 65\n",
      "Number of validation folds: 32\n"
     ]
    }
   ],
   "source": [
    "# =================================\n",
    "# 0. Load dataset\n",
    "# =================================\n",
    "print(\"Loading the data...\")\n",
    "# Training set pre-processing\n",
    "train = pd.read_csv('../../../Data/train.csv')\n",
    "train_4lag_inflation = np.array(train[['inflation.lag1',\n",
    "                                       'inflation.lag2',\n",
    "                                       'inflation.lag3',\n",
    "                                       'inflation.lag4']])\n",
    "train_4lag_unemp = np.array(train[['unemp.lag1',\n",
    "                                   'unemp.lag2',\n",
    "                                   'unemp.lag3',\n",
    "                                   'unemp.lag4']])\n",
    "train_4lag_oil = np.array(train[['oil.lag1',\n",
    "                                 'oil.lag2',\n",
    "                                 'oil.lag3',\n",
    "                                 'oil.lag4']])\n",
    "train_features = np.concatenate((train_4lag_inflation[:,:,np.newaxis], \n",
    "\t                             train_4lag_unemp[:,:,np.newaxis],\n",
    "\t                             train_4lag_oil[:,:,np.newaxis]),\n",
    "\t                             axis=2)\n",
    "train_target = np.array(train['inflation.target'])\n",
    "\n",
    "# Test set pre-processing\n",
    "test = pd.read_csv('../../../Data/test.csv')\n",
    "test_4lag_inflation = np.array(test[['inflation.lag1',\n",
    "                                     'inflation.lag2',\n",
    "                                     'inflation.lag3',\n",
    "                                     'inflation.lag4']])\n",
    "test_4lag_unemp = np.array(test[['unemp.lag1',\n",
    "                                 'unemp.lag2',\n",
    "                                 'unemp.lag3',\n",
    "                                 'unemp.lag4']])\n",
    "test_4lag_oil = np.array(test[['oil.lag1',\n",
    "                               'oil.lag2',\n",
    "                               'oil.lag3',\n",
    "                               'oil.lag4']])\n",
    "test_features = np.concatenate((test_4lag_inflation[:,:,np.newaxis], \n",
    "\t                            test_4lag_unemp[:,:,np.newaxis],\n",
    "\t                            test_4lag_oil[:,:,np.newaxis]),\n",
    "\t                            axis=2)\n",
    "test_target = np.array(test['inflation.target'])\n",
    "\n",
    "(nrTrainSamples, timesteps, variables) = train_features.shape\n",
    "(nrTestSamples, _, _) = test_features.shape\n",
    "\n",
    "# Window length and validation fold index\n",
    "window_length = 65\n",
    "valIndex = np.linspace(start=window_length, stop=nrTrainSamples, \n",
    "                       endpoint=True, num=folds+1, dtype=np.int)\n",
    "\n",
    "print(\"Finished: data loaded. Stats below: \")\n",
    "print(\"Nr of training samples: %d\" % nrTrainSamples)\n",
    "print(\"Nr of testing  samples: %d\" % nrTestSamples)\n",
    "print(\"Number of variables: %d\" % variables)\n",
    "print(\"Number of lags: %d\" % timesteps)\n",
    "print(\"Window length: %d\" % window_length)\n",
    "print(\"Number of validation folds: %d\" % folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# 1. Build model\n",
    "# ==================================\n",
    "# tf graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input], name='Batch')\n",
    "y = tf.placeholder(\"float\", name='True_labels_of_batch')\n",
    "lr = tf.placeholder(\"float\", name='Learning_rate')\n",
    "\n",
    "# Make predictions with the model\n",
    "pred = RNN(x, weights, biases, n_hidden)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.diag_part(tf.square(tf.subtract(x=pred, y=y))))\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training \n",
      "Hyperparameters: \n",
      "Dimension of recurrent unit = 12 \n",
      "Learning rate = 0.001 \n",
      "Epochs = 750 \n",
      "Batch size = 5 \n",
      "Early stopping epochs = 15 \n",
      "Learning rate = 0.001\n",
      "Fold: 1  Epoch: 1  Training loss = 3.0673  Validation loss = 3.0386  \n",
      "\n",
      "Fold: 1  Epoch: 2  Training loss = 3.0191  Validation loss = 2.9303  \n",
      "\n",
      "Fold: 1  Epoch: 3  Training loss = 2.9715  Validation loss = 2.8144  \n",
      "\n",
      "Fold: 1  Epoch: 4  Training loss = 2.9249  Validation loss = 2.6981  \n",
      "\n",
      "Fold: 1  Epoch: 5  Training loss = 2.8989  Validation loss = 2.6255  \n",
      "\n",
      "Fold: 1  Epoch: 6  Training loss = 2.8702  Validation loss = 2.5453  \n",
      "\n",
      "Fold: 1  Epoch: 7  Training loss = 2.8537  Validation loss = 2.4952  \n",
      "\n",
      "Fold: 1  Epoch: 8  Training loss = 2.8243  Validation loss = 2.4020  \n",
      "\n",
      "Fold: 1  Epoch: 9  Training loss = 2.8105  Validation loss = 2.3537  \n",
      "\n",
      "Fold: 1  Epoch: 10  Training loss = 2.7886  Validation loss = 2.2732  \n",
      "\n",
      "Fold: 1  Epoch: 11  Training loss = 2.7658  Validation loss = 2.1779  \n",
      "\n",
      "Fold: 1  Epoch: 12  Training loss = 2.7572  Validation loss = 2.1364  \n",
      "\n",
      "Fold: 1  Epoch: 13  Training loss = 2.7475  Validation loss = 2.0905  \n",
      "\n",
      "Fold: 1  Epoch: 14  Training loss = 2.7358  Validation loss = 2.0327  \n",
      "\n",
      "Fold: 1  Epoch: 15  Training loss = 2.7285  Validation loss = 1.9966  \n",
      "\n",
      "Fold: 1  Epoch: 16  Training loss = 2.7227  Validation loss = 1.9695  \n",
      "\n",
      "Fold: 1  Epoch: 17  Training loss = 2.7174  Validation loss = 1.9401  \n",
      "\n",
      "Fold: 1  Epoch: 18  Training loss = 2.7131  Validation loss = 1.9128  \n",
      "\n",
      "Fold: 1  Epoch: 19  Training loss = 2.7057  Validation loss = 1.8610  \n",
      "\n",
      "Fold: 1  Epoch: 20  Training loss = 2.7019  Validation loss = 1.8283  \n",
      "\n",
      "Fold: 1  Epoch: 21  Training loss = 2.6986  Validation loss = 1.8254  \n",
      "\n",
      "Fold: 1  Epoch: 22  Training loss = 2.6908  Validation loss = 1.7589  \n",
      "\n",
      "Fold: 1  Epoch: 23  Training loss = 2.6890  Validation loss = 1.7482  \n",
      "\n",
      "Fold: 1  Epoch: 24  Training loss = 2.6888  Validation loss = 1.7657  \n",
      "\n",
      "Fold: 1  Epoch: 25  Training loss = 2.6834  Validation loss = 1.7191  \n",
      "\n",
      "Fold: 1  Epoch: 26  Training loss = 2.6820  Validation loss = 1.7098  \n",
      "\n",
      "Fold: 1  Epoch: 27  Training loss = 2.6829  Validation loss = 1.7391  \n",
      "\n",
      "Fold: 1  Epoch: 28  Training loss = 2.6809  Validation loss = 1.7273  \n",
      "\n",
      "Fold: 1  Epoch: 29  Training loss = 2.6796  Validation loss = 1.7168  \n",
      "\n",
      "Fold: 1  Epoch: 30  Training loss = 2.6814  Validation loss = 1.7486  \n",
      "\n",
      "Fold: 1  Epoch: 31  Training loss = 2.6735  Validation loss = 1.6751  \n",
      "\n",
      "Fold: 1  Epoch: 32  Training loss = 2.6724  Validation loss = 1.6862  \n",
      "\n",
      "Fold: 1  Epoch: 33  Training loss = 2.6667  Validation loss = 1.6098  \n",
      "\n",
      "Fold: 1  Epoch: 34  Training loss = 2.6649  Validation loss = 1.6050  \n",
      "\n",
      "Fold: 1  Epoch: 35  Training loss = 2.6643  Validation loss = 1.6073  \n",
      "\n",
      "Fold: 1  Epoch: 36  Training loss = 2.6643  Validation loss = 1.6377  \n",
      "\n",
      "Fold: 1  Epoch: 37  Training loss = 2.6635  Validation loss = 1.6474  \n",
      "\n",
      "Fold: 1  Epoch: 38  Training loss = 2.6619  Validation loss = 1.6435  \n",
      "\n",
      "Fold: 1  Epoch: 39  Training loss = 2.6580  Validation loss = 1.5714  \n",
      "\n",
      "Fold: 1  Epoch: 40  Training loss = 2.6555  Validation loss = 1.5411  \n",
      "\n",
      "Fold: 1  Epoch: 41  Training loss = 2.6549  Validation loss = 1.5755  \n",
      "\n",
      "Fold: 1  Epoch: 42  Training loss = 2.6524  Validation loss = 1.5210  \n",
      "\n",
      "Fold: 1  Epoch: 43  Training loss = 2.6509  Validation loss = 1.5116  \n",
      "\n",
      "Fold: 1  Epoch: 44  Training loss = 2.6488  Validation loss = 1.5499  \n",
      "\n",
      "Fold: 1  Epoch: 45  Training loss = 2.6467  Validation loss = 1.5296  \n",
      "\n",
      "Fold: 1  Epoch: 46  Training loss = 2.6461  Validation loss = 1.6033  \n",
      "\n",
      "Fold: 1  Epoch: 47  Training loss = 2.6450  Validation loss = 1.6305  \n",
      "\n",
      "Fold: 1  Epoch: 48  Training loss = 2.6444  Validation loss = 1.6402  \n",
      "\n",
      "Fold: 1  Epoch: 49  Training loss = 2.6374  Validation loss = 1.5550  \n",
      "\n",
      "Fold: 1  Epoch: 50  Training loss = 2.6356  Validation loss = 1.5246  \n",
      "\n",
      "Fold: 1  Epoch: 51  Training loss = 2.6328  Validation loss = 1.5503  \n",
      "\n",
      "Fold: 1  Epoch: 52  Training loss = 2.6291  Validation loss = 1.5338  \n",
      "\n",
      "Fold: 1  Epoch: 53  Training loss = 2.6289  Validation loss = 1.5432  \n",
      "\n",
      "Fold: 1  Epoch: 54  Training loss = 2.6263  Validation loss = 1.5278  \n",
      "\n",
      "Fold: 1  Epoch: 55  Training loss = 2.6285  Validation loss = 1.5383  \n",
      "\n",
      "Fold: 1  Epoch: 56  Training loss = 2.6217  Validation loss = 1.5023  \n",
      "\n",
      "Fold: 1  Epoch: 57  Training loss = 2.6175  Validation loss = 1.5476  \n",
      "\n",
      "Fold: 1  Epoch: 58  Training loss = 2.6154  Validation loss = 1.5619  \n",
      "\n",
      "Fold: 1  Epoch: 59  Training loss = 2.6154  Validation loss = 1.6080  \n",
      "\n",
      "Fold: 1  Epoch: 60  Training loss = 2.6140  Validation loss = 1.6011  \n",
      "\n",
      "Fold: 1  Epoch: 61  Training loss = 2.6144  Validation loss = 1.6177  \n",
      "\n",
      "Fold: 1  Epoch: 62  Training loss = 2.6141  Validation loss = 1.6192  \n",
      "\n",
      "Fold: 1  Epoch: 63  Training loss = 2.6165  Validation loss = 1.6717  \n",
      "\n",
      "Check model:  Fold: 1  Optimal epoch: 56  \n",
      "\n",
      "Fold: 2  Epoch: 1  Training loss = 2.5055  Validation loss = 2.1177  \n",
      "\n",
      "Fold: 2  Epoch: 2  Training loss = 2.4969  Validation loss = 2.0963  \n",
      "\n",
      "Fold: 2  Epoch: 3  Training loss = 2.4915  Validation loss = 2.0763  \n",
      "\n",
      "Fold: 2  Epoch: 4  Training loss = 2.4920  Validation loss = 2.1191  \n",
      "\n",
      "Fold: 2  Epoch: 5  Training loss = 2.4874  Validation loss = 2.1114  \n",
      "\n",
      "Fold: 2  Epoch: 6  Training loss = 2.4813  Validation loss = 2.1264  \n",
      "\n",
      "Fold: 2  Epoch: 7  Training loss = 2.4810  Validation loss = 2.1816  \n",
      "\n",
      "Fold: 2  Epoch: 8  Training loss = 2.4707  Validation loss = 2.1444  \n",
      "\n",
      "Fold: 2  Epoch: 9  Training loss = 2.4567  Validation loss = 2.1007  \n",
      "\n",
      "Fold: 2  Epoch: 10  Training loss = 2.4498  Validation loss = 2.1237  \n",
      "\n",
      "Fold: 2  Epoch: 11  Training loss = 2.4520  Validation loss = 2.1612  \n",
      "\n",
      "Fold: 2  Epoch: 12  Training loss = 2.4467  Validation loss = 2.0182  \n",
      "\n",
      "Fold: 2  Epoch: 13  Training loss = 2.4332  Validation loss = 2.1029  \n",
      "\n",
      "Fold: 2  Epoch: 14  Training loss = 2.4222  Validation loss = 2.0819  \n",
      "\n",
      "Fold: 2  Epoch: 15  Training loss = 2.4194  Validation loss = 2.1271  \n",
      "\n",
      "Fold: 2  Epoch: 16  Training loss = 2.4131  Validation loss = 2.0914  \n",
      "\n",
      "Fold: 2  Epoch: 17  Training loss = 2.4092  Validation loss = 2.1399  \n",
      "\n",
      "Fold: 2  Epoch: 18  Training loss = 2.4034  Validation loss = 2.1638  \n",
      "\n",
      "Fold: 2  Epoch: 19  Training loss = 2.4086  Validation loss = 2.1931  \n",
      "\n",
      "Check model:  Fold: 2  Optimal epoch: 12  \n",
      "\n",
      "Fold: 3  Epoch: 1  Training loss = 1.3861  Validation loss = 2.6327  \n",
      "\n",
      "Fold: 3  Epoch: 2  Training loss = 1.3804  Validation loss = 2.6251  \n",
      "\n",
      "Fold: 3  Epoch: 3  Training loss = 1.3753  Validation loss = 2.5959  \n",
      "\n",
      "Fold: 3  Epoch: 4  Training loss = 1.3744  Validation loss = 2.5359  \n",
      "\n",
      "Fold: 3  Epoch: 5  Training loss = 1.3748  Validation loss = 2.4576  \n",
      "\n",
      "Fold: 3  Epoch: 6  Training loss = 1.3631  Validation loss = 2.4874  \n",
      "\n",
      "Fold: 3  Epoch: 7  Training loss = 1.3558  Validation loss = 2.4967  \n",
      "\n",
      "Fold: 3  Epoch: 8  Training loss = 1.3520  Validation loss = 2.4782  \n",
      "\n",
      "Fold: 3  Epoch: 9  Training loss = 1.3452  Validation loss = 2.4584  \n",
      "\n",
      "Fold: 3  Epoch: 10  Training loss = 1.3465  Validation loss = 2.4014  \n",
      "\n",
      "Fold: 3  Epoch: 11  Training loss = 1.3359  Validation loss = 2.3865  \n",
      "\n",
      "Fold: 3  Epoch: 12  Training loss = 1.3305  Validation loss = 2.3685  \n",
      "\n",
      "Fold: 3  Epoch: 13  Training loss = 1.3274  Validation loss = 2.3351  \n",
      "\n",
      "Fold: 3  Epoch: 14  Training loss = 1.3242  Validation loss = 2.3569  \n",
      "\n",
      "Fold: 3  Epoch: 15  Training loss = 1.3192  Validation loss = 2.3093  \n",
      "\n",
      "Fold: 3  Epoch: 16  Training loss = 1.3243  Validation loss = 2.3460  \n",
      "\n",
      "Fold: 3  Epoch: 17  Training loss = 1.3276  Validation loss = 2.3606  \n",
      "\n",
      "Fold: 3  Epoch: 18  Training loss = 1.3217  Validation loss = 2.3399  \n",
      "\n",
      "Fold: 3  Epoch: 19  Training loss = 1.3127  Validation loss = 2.2894  \n",
      "\n",
      "Fold: 3  Epoch: 20  Training loss = 1.3109  Validation loss = 2.2515  \n",
      "\n",
      "Fold: 3  Epoch: 21  Training loss = 1.3095  Validation loss = 2.2357  \n",
      "\n",
      "Fold: 3  Epoch: 22  Training loss = 1.3058  Validation loss = 2.2326  \n",
      "\n",
      "Fold: 3  Epoch: 23  Training loss = 1.3024  Validation loss = 2.2009  \n",
      "\n",
      "Fold: 3  Epoch: 24  Training loss = 1.3034  Validation loss = 2.2085  \n",
      "\n",
      "Fold: 3  Epoch: 25  Training loss = 1.2970  Validation loss = 2.1388  \n",
      "\n",
      "Fold: 3  Epoch: 26  Training loss = 1.2976  Validation loss = 2.1552  \n",
      "\n",
      "Fold: 3  Epoch: 27  Training loss = 1.2962  Validation loss = 2.1535  \n",
      "\n",
      "Fold: 3  Epoch: 28  Training loss = 1.2986  Validation loss = 2.1625  \n",
      "\n",
      "Fold: 3  Epoch: 29  Training loss = 1.2938  Validation loss = 2.1180  \n",
      "\n",
      "Fold: 3  Epoch: 30  Training loss = 1.3013  Validation loss = 2.1581  \n",
      "\n",
      "Fold: 3  Epoch: 31  Training loss = 1.2970  Validation loss = 2.1285  \n",
      "\n",
      "Fold: 3  Epoch: 32  Training loss = 1.2947  Validation loss = 2.0986  \n",
      "\n",
      "Fold: 3  Epoch: 33  Training loss = 1.2863  Validation loss = 2.0286  \n",
      "\n",
      "Fold: 3  Epoch: 34  Training loss = 1.2903  Validation loss = 2.0053  \n",
      "\n",
      "Fold: 3  Epoch: 35  Training loss = 1.2868  Validation loss = 2.0025  \n",
      "\n",
      "Fold: 3  Epoch: 36  Training loss = 1.2840  Validation loss = 1.9868  \n",
      "\n",
      "Fold: 3  Epoch: 37  Training loss = 1.2844  Validation loss = 1.9981  \n",
      "\n",
      "Fold: 3  Epoch: 38  Training loss = 1.2814  Validation loss = 2.0160  \n",
      "\n",
      "Fold: 3  Epoch: 39  Training loss = 1.2839  Validation loss = 1.9995  \n",
      "\n",
      "Fold: 3  Epoch: 40  Training loss = 1.2836  Validation loss = 2.0295  \n",
      "\n",
      "Fold: 3  Epoch: 41  Training loss = 1.2778  Validation loss = 1.9640  \n",
      "\n",
      "Fold: 3  Epoch: 42  Training loss = 1.2861  Validation loss = 2.0536  \n",
      "\n",
      "Fold: 3  Epoch: 43  Training loss = 1.2757  Validation loss = 1.9887  \n",
      "\n",
      "Fold: 3  Epoch: 44  Training loss = 1.2764  Validation loss = 2.0080  \n",
      "\n",
      "Fold: 3  Epoch: 45  Training loss = 1.2756  Validation loss = 1.9901  \n",
      "\n",
      "Fold: 3  Epoch: 46  Training loss = 1.2827  Validation loss = 2.0602  \n",
      "\n",
      "Fold: 3  Epoch: 47  Training loss = 1.2746  Validation loss = 2.0023  \n",
      "\n",
      "Fold: 3  Epoch: 48  Training loss = 1.2756  Validation loss = 1.9668  \n",
      "\n",
      "Fold: 3  Epoch: 49  Training loss = 1.2748  Validation loss = 2.0194  \n",
      "\n",
      "Fold: 3  Epoch: 50  Training loss = 1.2825  Validation loss = 2.0626  \n",
      "\n",
      "Check model:  Fold: 3  Optimal epoch: 41  \n",
      "\n",
      "Fold: 4  Epoch: 1  Training loss = 1.2775  Validation loss = 3.1730  \n",
      "\n",
      "Fold: 4  Epoch: 2  Training loss = 1.2635  Validation loss = 3.1339  \n",
      "\n",
      "Fold: 4  Epoch: 3  Training loss = 1.2578  Validation loss = 3.0659  \n",
      "\n",
      "Fold: 4  Epoch: 4  Training loss = 1.2539  Validation loss = 3.0778  \n",
      "\n",
      "Fold: 4  Epoch: 5  Training loss = 1.2520  Validation loss = 3.0663  \n",
      "\n",
      "Fold: 4  Epoch: 6  Training loss = 1.2455  Validation loss = 3.0423  \n",
      "\n",
      "Fold: 4  Epoch: 7  Training loss = 1.2378  Validation loss = 3.0007  \n",
      "\n",
      "Fold: 4  Epoch: 8  Training loss = 1.2390  Validation loss = 3.0051  \n",
      "\n",
      "Fold: 4  Epoch: 9  Training loss = 1.2337  Validation loss = 2.9978  \n",
      "\n",
      "Fold: 4  Epoch: 10  Training loss = 1.2400  Validation loss = 3.0271  \n",
      "\n",
      "Fold: 4  Epoch: 11  Training loss = 1.2309  Validation loss = 3.0068  \n",
      "\n",
      "Fold: 4  Epoch: 12  Training loss = 1.2313  Validation loss = 2.9956  \n",
      "\n",
      "Fold: 4  Epoch: 13  Training loss = 1.2250  Validation loss = 2.9591  \n",
      "\n",
      "Fold: 4  Epoch: 14  Training loss = 1.2209  Validation loss = 2.9411  \n",
      "\n",
      "Fold: 4  Epoch: 15  Training loss = 1.2151  Validation loss = 2.9272  \n",
      "\n",
      "Fold: 4  Epoch: 16  Training loss = 1.2131  Validation loss = 2.9144  \n",
      "\n",
      "Fold: 4  Epoch: 17  Training loss = 1.2286  Validation loss = 2.9383  \n",
      "\n",
      "Fold: 4  Epoch: 18  Training loss = 1.2130  Validation loss = 2.9133  \n",
      "\n",
      "Fold: 4  Epoch: 19  Training loss = 1.2093  Validation loss = 2.8728  \n",
      "\n",
      "Fold: 4  Epoch: 20  Training loss = 1.2049  Validation loss = 2.8728  \n",
      "\n",
      "Fold: 4  Epoch: 21  Training loss = 1.2022  Validation loss = 2.8761  \n",
      "\n",
      "Fold: 4  Epoch: 22  Training loss = 1.1997  Validation loss = 2.8688  \n",
      "\n",
      "Fold: 4  Epoch: 23  Training loss = 1.2034  Validation loss = 2.8828  \n",
      "\n",
      "Fold: 4  Epoch: 24  Training loss = 1.1979  Validation loss = 2.8525  \n",
      "\n",
      "Fold: 4  Epoch: 25  Training loss = 1.1945  Validation loss = 2.8385  \n",
      "\n",
      "Fold: 4  Epoch: 26  Training loss = 1.1949  Validation loss = 2.8263  \n",
      "\n",
      "Fold: 4  Epoch: 27  Training loss = 1.1940  Validation loss = 2.7999  \n",
      "\n",
      "Fold: 4  Epoch: 28  Training loss = 1.1917  Validation loss = 2.7859  \n",
      "\n",
      "Fold: 4  Epoch: 29  Training loss = 1.1867  Validation loss = 2.7636  \n",
      "\n",
      "Fold: 4  Epoch: 30  Training loss = 1.1827  Validation loss = 2.7523  \n",
      "\n",
      "Fold: 4  Epoch: 31  Training loss = 1.1833  Validation loss = 2.7588  \n",
      "\n",
      "Fold: 4  Epoch: 32  Training loss = 1.1817  Validation loss = 2.7569  \n",
      "\n",
      "Fold: 4  Epoch: 33  Training loss = 1.1800  Validation loss = 2.7363  \n",
      "\n",
      "Fold: 4  Epoch: 34  Training loss = 1.1913  Validation loss = 2.7221  \n",
      "\n",
      "Fold: 4  Epoch: 35  Training loss = 1.1845  Validation loss = 2.7281  \n",
      "\n",
      "Fold: 4  Epoch: 36  Training loss = 1.1761  Validation loss = 2.7350  \n",
      "\n",
      "Fold: 4  Epoch: 37  Training loss = 1.1750  Validation loss = 2.7576  \n",
      "\n",
      "Fold: 4  Epoch: 38  Training loss = 1.1777  Validation loss = 2.7150  \n",
      "\n",
      "Fold: 4  Epoch: 39  Training loss = 1.1726  Validation loss = 2.7268  \n",
      "\n",
      "Fold: 4  Epoch: 40  Training loss = 1.1863  Validation loss = 2.7885  \n",
      "\n",
      "Fold: 4  Epoch: 41  Training loss = 1.1760  Validation loss = 2.7788  \n",
      "\n",
      "Fold: 4  Epoch: 42  Training loss = 1.1706  Validation loss = 2.7554  \n",
      "\n",
      "Fold: 4  Epoch: 43  Training loss = 1.1759  Validation loss = 2.7543  \n",
      "\n",
      "Fold: 4  Epoch: 44  Training loss = 1.1694  Validation loss = 2.7305  \n",
      "\n",
      "Fold: 4  Epoch: 45  Training loss = 1.1718  Validation loss = 2.7552  \n",
      "\n",
      "Fold: 4  Epoch: 46  Training loss = 1.1743  Validation loss = 2.7557  \n",
      "\n",
      "Fold: 4  Epoch: 47  Training loss = 1.1639  Validation loss = 2.7167  \n",
      "\n",
      "Fold: 4  Epoch: 48  Training loss = 1.1638  Validation loss = 2.7324  \n",
      "\n",
      "Fold: 4  Epoch: 49  Training loss = 1.1624  Validation loss = 2.7317  \n",
      "\n",
      "Fold: 4  Epoch: 50  Training loss = 1.1593  Validation loss = 2.7065  \n",
      "\n",
      "Fold: 4  Epoch: 51  Training loss = 1.1682  Validation loss = 2.7171  \n",
      "\n",
      "Fold: 4  Epoch: 52  Training loss = 1.1993  Validation loss = 2.7413  \n",
      "\n",
      "Fold: 4  Epoch: 53  Training loss = 1.1690  Validation loss = 2.6985  \n",
      "\n",
      "Fold: 4  Epoch: 54  Training loss = 1.1785  Validation loss = 2.6854  \n",
      "\n",
      "Fold: 4  Epoch: 55  Training loss = 1.1586  Validation loss = 2.6733  \n",
      "\n",
      "Fold: 4  Epoch: 56  Training loss = 1.1576  Validation loss = 2.6742  \n",
      "\n",
      "Fold: 4  Epoch: 57  Training loss = 1.1555  Validation loss = 2.6787  \n",
      "\n",
      "Fold: 4  Epoch: 58  Training loss = 1.1514  Validation loss = 2.6409  \n",
      "\n",
      "Fold: 4  Epoch: 59  Training loss = 1.1582  Validation loss = 2.6671  \n",
      "\n",
      "Fold: 4  Epoch: 60  Training loss = 1.1581  Validation loss = 2.6720  \n",
      "\n",
      "Fold: 4  Epoch: 61  Training loss = 1.1477  Validation loss = 2.6408  \n",
      "\n",
      "Fold: 4  Epoch: 62  Training loss = 1.1470  Validation loss = 2.6472  \n",
      "\n",
      "Fold: 4  Epoch: 63  Training loss = 1.1464  Validation loss = 2.6634  \n",
      "\n",
      "Fold: 4  Epoch: 64  Training loss = 1.1448  Validation loss = 2.6601  \n",
      "\n",
      "Fold: 4  Epoch: 65  Training loss = 1.1427  Validation loss = 2.6623  \n",
      "\n",
      "Fold: 4  Epoch: 66  Training loss = 1.1415  Validation loss = 2.6576  \n",
      "\n",
      "Fold: 4  Epoch: 67  Training loss = 1.1420  Validation loss = 2.6453  \n",
      "\n",
      "Fold: 4  Epoch: 68  Training loss = 1.1436  Validation loss = 2.6523  \n",
      "\n",
      "Fold: 4  Epoch: 69  Training loss = 1.1506  Validation loss = 2.6342  \n",
      "\n",
      "Fold: 4  Epoch: 70  Training loss = 1.1419  Validation loss = 2.6633  \n",
      "\n",
      "Fold: 4  Epoch: 71  Training loss = 1.1384  Validation loss = 2.6905  \n",
      "\n",
      "Check model:  Fold: 4  Optimal epoch: 69  \n",
      "\n",
      "Fold: 5  Epoch: 1  Training loss = 1.2102  Validation loss = 2.0841  \n",
      "\n",
      "Fold: 5  Epoch: 2  Training loss = 1.2031  Validation loss = 2.0537  \n",
      "\n",
      "Fold: 5  Epoch: 3  Training loss = 1.2005  Validation loss = 2.0496  \n",
      "\n",
      "Fold: 5  Epoch: 4  Training loss = 1.1913  Validation loss = 1.9988  \n",
      "\n",
      "Fold: 5  Epoch: 5  Training loss = 1.1925  Validation loss = 1.9515  \n",
      "\n",
      "Fold: 5  Epoch: 6  Training loss = 1.1903  Validation loss = 1.9673  \n",
      "\n",
      "Fold: 5  Epoch: 7  Training loss = 1.1771  Validation loss = 1.9099  \n",
      "\n",
      "Fold: 5  Epoch: 8  Training loss = 1.1775  Validation loss = 1.8769  \n",
      "\n",
      "Fold: 5  Epoch: 9  Training loss = 1.1766  Validation loss = 1.8830  \n",
      "\n",
      "Fold: 5  Epoch: 10  Training loss = 1.1755  Validation loss = 1.8827  \n",
      "\n",
      "Fold: 5  Epoch: 11  Training loss = 1.1691  Validation loss = 1.8625  \n",
      "\n",
      "Fold: 5  Epoch: 12  Training loss = 1.1626  Validation loss = 1.8567  \n",
      "\n",
      "Fold: 5  Epoch: 13  Training loss = 1.1664  Validation loss = 1.8417  \n",
      "\n",
      "Fold: 5  Epoch: 14  Training loss = 1.1571  Validation loss = 1.7993  \n",
      "\n",
      "Fold: 5  Epoch: 15  Training loss = 1.1553  Validation loss = 1.7642  \n",
      "\n",
      "Fold: 5  Epoch: 16  Training loss = 1.1594  Validation loss = 1.7975  \n",
      "\n",
      "Fold: 5  Epoch: 17  Training loss = 1.1516  Validation loss = 1.7743  \n",
      "\n",
      "Fold: 5  Epoch: 18  Training loss = 1.1455  Validation loss = 1.7857  \n",
      "\n",
      "Fold: 5  Epoch: 19  Training loss = 1.1425  Validation loss = 1.7520  \n",
      "\n",
      "Fold: 5  Epoch: 20  Training loss = 1.1426  Validation loss = 1.7255  \n",
      "\n",
      "Fold: 5  Epoch: 21  Training loss = 1.1344  Validation loss = 1.6788  \n",
      "\n",
      "Fold: 5  Epoch: 22  Training loss = 1.1506  Validation loss = 1.6432  \n",
      "\n",
      "Fold: 5  Epoch: 23  Training loss = 1.1543  Validation loss = 1.6201  \n",
      "\n",
      "Fold: 5  Epoch: 24  Training loss = 1.1458  Validation loss = 1.5596  \n",
      "\n",
      "Fold: 5  Epoch: 25  Training loss = 1.1256  Validation loss = 1.5596  \n",
      "\n",
      "Fold: 5  Epoch: 26  Training loss = 1.1260  Validation loss = 1.5561  \n",
      "\n",
      "Fold: 5  Epoch: 27  Training loss = 1.1311  Validation loss = 1.5594  \n",
      "\n",
      "Fold: 5  Epoch: 28  Training loss = 1.1181  Validation loss = 1.5306  \n",
      "\n",
      "Fold: 5  Epoch: 29  Training loss = 1.1150  Validation loss = 1.5202  \n",
      "\n",
      "Fold: 5  Epoch: 30  Training loss = 1.1221  Validation loss = 1.4571  \n",
      "\n",
      "Fold: 5  Epoch: 31  Training loss = 1.1163  Validation loss = 1.4415  \n",
      "\n",
      "Fold: 5  Epoch: 32  Training loss = 1.1090  Validation loss = 1.4580  \n",
      "\n",
      "Fold: 5  Epoch: 33  Training loss = 1.1440  Validation loss = 1.4166  \n",
      "\n",
      "Fold: 5  Epoch: 34  Training loss = 1.1001  Validation loss = 1.4204  \n",
      "\n",
      "Fold: 5  Epoch: 35  Training loss = 1.1172  Validation loss = 1.4576  \n",
      "\n",
      "Fold: 5  Epoch: 36  Training loss = 1.0979  Validation loss = 1.4518  \n",
      "\n",
      "Fold: 5  Epoch: 37  Training loss = 1.1185  Validation loss = 1.3825  \n",
      "\n",
      "Fold: 5  Epoch: 38  Training loss = 1.0921  Validation loss = 1.4138  \n",
      "\n",
      "Fold: 5  Epoch: 39  Training loss = 1.1037  Validation loss = 1.5097  \n",
      "\n",
      "Fold: 5  Epoch: 40  Training loss = 1.0981  Validation loss = 1.3766  \n",
      "\n",
      "Fold: 5  Epoch: 41  Training loss = 1.0922  Validation loss = 1.5079  \n",
      "\n",
      "Fold: 5  Epoch: 42  Training loss = 1.0979  Validation loss = 1.5158  \n",
      "\n",
      "Fold: 5  Epoch: 43  Training loss = 1.0939  Validation loss = 1.4306  \n",
      "\n",
      "Fold: 5  Epoch: 44  Training loss = 1.0899  Validation loss = 1.5217  \n",
      "\n",
      "Check model:  Fold: 5  Optimal epoch: 40  \n",
      "\n",
      "Fold: 6  Epoch: 1  Training loss = 1.1417  Validation loss = 0.8186  \n",
      "\n",
      "Fold: 6  Epoch: 2  Training loss = 1.1256  Validation loss = 0.7634  \n",
      "\n",
      "Fold: 6  Epoch: 3  Training loss = 1.1292  Validation loss = 0.8134  \n",
      "\n",
      "Fold: 6  Epoch: 4  Training loss = 1.1163  Validation loss = 0.7632  \n",
      "\n",
      "Fold: 6  Epoch: 5  Training loss = 1.1090  Validation loss = 0.7687  \n",
      "\n",
      "Fold: 6  Epoch: 6  Training loss = 1.1417  Validation loss = 1.0434  \n",
      "\n",
      "Fold: 6  Epoch: 7  Training loss = 1.1227  Validation loss = 0.8947  \n",
      "\n",
      "Fold: 6  Epoch: 8  Training loss = 1.1005  Validation loss = 0.8128  \n",
      "\n",
      "Fold: 6  Epoch: 9  Training loss = 1.1503  Validation loss = 1.1275  \n",
      "\n",
      "Fold: 6  Epoch: 10  Training loss = 1.0950  Validation loss = 0.8066  \n",
      "\n",
      "Fold: 6  Epoch: 11  Training loss = 1.0962  Validation loss = 0.8159  \n",
      "\n",
      "Fold: 6  Epoch: 12  Training loss = 1.0876  Validation loss = 0.7843  \n",
      "\n",
      "Fold: 6  Epoch: 13  Training loss = 1.0856  Validation loss = 0.7959  \n",
      "\n",
      "Fold: 6  Epoch: 14  Training loss = 1.0959  Validation loss = 0.9249  \n",
      "\n",
      "Fold: 6  Epoch: 15  Training loss = 1.0842  Validation loss = 0.7910  \n",
      "\n",
      "Fold: 6  Epoch: 16  Training loss = 1.0820  Validation loss = 0.8338  \n",
      "\n",
      "Fold: 6  Epoch: 17  Training loss = 1.0918  Validation loss = 0.9006  \n",
      "\n",
      "Fold: 6  Epoch: 18  Training loss = 1.0855  Validation loss = 0.8957  \n",
      "\n",
      "Fold: 6  Epoch: 19  Training loss = 1.0955  Validation loss = 0.7759  \n",
      "\n",
      "Fold: 6  Epoch: 20  Training loss = 1.0844  Validation loss = 0.7761  \n",
      "\n",
      "Fold: 6  Epoch: 21  Training loss = 1.0727  Validation loss = 0.8319  \n",
      "\n",
      "Fold: 6  Epoch: 22  Training loss = 1.0811  Validation loss = 0.7719  \n",
      "\n",
      "Fold: 6  Epoch: 23  Training loss = 1.1102  Validation loss = 0.9543  \n",
      "\n",
      "Fold: 6  Epoch: 24  Training loss = 1.0633  Validation loss = 0.7567  \n",
      "\n",
      "Fold: 6  Epoch: 25  Training loss = 1.0641  Validation loss = 0.7726  \n",
      "\n",
      "Fold: 6  Epoch: 26  Training loss = 1.0621  Validation loss = 0.8907  \n",
      "\n",
      "Fold: 6  Epoch: 27  Training loss = 1.0698  Validation loss = 0.9424  \n",
      "\n",
      "Fold: 6  Epoch: 28  Training loss = 1.0534  Validation loss = 0.8688  \n",
      "\n",
      "Fold: 6  Epoch: 29  Training loss = 1.0574  Validation loss = 0.9178  \n",
      "\n",
      "Fold: 6  Epoch: 30  Training loss = 1.0631  Validation loss = 0.9686  \n",
      "\n",
      "Check model:  Fold: 6  Optimal epoch: 24  \n",
      "\n",
      "Fold: 7  Epoch: 1  Training loss = 1.0368  Validation loss = 1.3145  \n",
      "\n",
      "Fold: 7  Epoch: 2  Training loss = 1.0210  Validation loss = 1.5987  \n",
      "\n",
      "Fold: 7  Epoch: 3  Training loss = 1.0229  Validation loss = 1.6309  \n",
      "\n",
      "Fold: 7  Epoch: 4  Training loss = 1.0395  Validation loss = 1.0641  \n",
      "\n",
      "Fold: 7  Epoch: 5  Training loss = 1.0430  Validation loss = 0.9663  \n",
      "\n",
      "Fold: 7  Epoch: 6  Training loss = 1.0152  Validation loss = 1.4606  \n",
      "\n",
      "Fold: 7  Epoch: 7  Training loss = 1.0297  Validation loss = 1.5369  \n",
      "\n",
      "Fold: 7  Epoch: 8  Training loss = 1.0118  Validation loss = 1.5398  \n",
      "\n",
      "Fold: 7  Epoch: 9  Training loss = 1.0184  Validation loss = 1.1423  \n",
      "\n",
      "Fold: 7  Epoch: 10  Training loss = 1.0047  Validation loss = 1.3429  \n",
      "\n",
      "Fold: 7  Epoch: 11  Training loss = 1.0071  Validation loss = 1.7034  \n",
      "\n",
      "Fold: 7  Epoch: 12  Training loss = 1.0023  Validation loss = 1.3714  \n",
      "\n",
      "Fold: 7  Epoch: 13  Training loss = 1.0245  Validation loss = 1.0665  \n",
      "\n",
      "Fold: 7  Epoch: 14  Training loss = 0.9966  Validation loss = 1.3678  \n",
      "\n",
      "Fold: 7  Epoch: 15  Training loss = 0.9984  Validation loss = 1.2618  \n",
      "\n",
      "Fold: 7  Epoch: 16  Training loss = 0.9866  Validation loss = 1.5778  \n",
      "\n",
      "Fold: 7  Epoch: 17  Training loss = 0.9883  Validation loss = 1.7186  \n",
      "\n",
      "Check model:  Fold: 7  Optimal epoch: 5  \n",
      "\n",
      "Fold: 8  Epoch: 1  Training loss = 0.9887  Validation loss = 4.5008  \n",
      "\n",
      "Fold: 8  Epoch: 2  Training loss = 1.0572  Validation loss = 4.8581  \n",
      "\n",
      "Fold: 8  Epoch: 3  Training loss = 1.0121  Validation loss = 4.2275  \n",
      "\n",
      "Fold: 8  Epoch: 4  Training loss = 0.9755  Validation loss = 4.4797  \n",
      "\n",
      "Fold: 8  Epoch: 5  Training loss = 1.0286  Validation loss = 4.1166  \n",
      "\n",
      "Fold: 8  Epoch: 6  Training loss = 0.9572  Validation loss = 4.4449  \n",
      "\n",
      "Fold: 8  Epoch: 7  Training loss = 1.0488  Validation loss = 4.2265  \n",
      "\n",
      "Fold: 8  Epoch: 8  Training loss = 0.9534  Validation loss = 4.3491  \n",
      "\n",
      "Fold: 8  Epoch: 9  Training loss = 0.9555  Validation loss = 4.3001  \n",
      "\n",
      "Fold: 8  Epoch: 10  Training loss = 0.9870  Validation loss = 4.2094  \n",
      "\n",
      "Fold: 8  Epoch: 11  Training loss = 1.0113  Validation loss = 4.1481  \n",
      "\n",
      "Fold: 8  Epoch: 12  Training loss = 0.9560  Validation loss = 4.5058  \n",
      "\n",
      "Fold: 8  Epoch: 13  Training loss = 0.9756  Validation loss = 4.3431  \n",
      "\n",
      "Fold: 8  Epoch: 14  Training loss = 0.9490  Validation loss = 4.5660  \n",
      "\n",
      "Fold: 8  Epoch: 15  Training loss = 0.9536  Validation loss = 4.6613  \n",
      "\n",
      "Fold: 8  Epoch: 16  Training loss = 1.0166  Validation loss = 4.8297  \n",
      "\n",
      "Fold: 8  Epoch: 17  Training loss = 0.9362  Validation loss = 4.4495  \n",
      "\n",
      "Fold: 8  Epoch: 18  Training loss = 0.9550  Validation loss = 4.4294  \n",
      "\n",
      "Fold: 8  Epoch: 19  Training loss = 0.9354  Validation loss = 4.4477  \n",
      "\n",
      "Fold: 8  Epoch: 20  Training loss = 1.0641  Validation loss = 4.9338  \n",
      "\n",
      "Check model:  Fold: 8  Optimal epoch: 5  \n",
      "\n",
      "Fold: 9  Epoch: 1  Training loss = 1.4337  Validation loss = 7.4000  \n",
      "\n",
      "Fold: 9  Epoch: 2  Training loss = 1.4506  Validation loss = 7.5141  \n",
      "\n",
      "Fold: 9  Epoch: 3  Training loss = 1.4157  Validation loss = 7.1872  \n",
      "\n",
      "Fold: 9  Epoch: 4  Training loss = 1.4050  Validation loss = 7.1727  \n",
      "\n",
      "Fold: 9  Epoch: 5  Training loss = 1.3892  Validation loss = 7.0910  \n",
      "\n",
      "Fold: 9  Epoch: 6  Training loss = 1.3815  Validation loss = 7.0690  \n",
      "\n",
      "Fold: 9  Epoch: 7  Training loss = 1.4059  Validation loss = 7.0112  \n",
      "\n",
      "Fold: 9  Epoch: 8  Training loss = 1.4059  Validation loss = 6.8885  \n",
      "\n",
      "Fold: 9  Epoch: 9  Training loss = 1.4263  Validation loss = 6.8665  \n",
      "\n",
      "Fold: 9  Epoch: 10  Training loss = 1.3655  Validation loss = 6.7669  \n",
      "\n",
      "Fold: 9  Epoch: 11  Training loss = 1.4777  Validation loss = 6.6447  \n",
      "\n",
      "Fold: 9  Epoch: 12  Training loss = 1.3707  Validation loss = 6.7995  \n",
      "\n",
      "Fold: 9  Epoch: 13  Training loss = 1.3391  Validation loss = 6.6814  \n",
      "\n",
      "Fold: 9  Epoch: 14  Training loss = 1.3497  Validation loss = 6.5828  \n",
      "\n",
      "Fold: 9  Epoch: 15  Training loss = 1.4030  Validation loss = 6.6930  \n",
      "\n",
      "Fold: 9  Epoch: 16  Training loss = 1.3287  Validation loss = 6.8465  \n",
      "\n",
      "Fold: 9  Epoch: 17  Training loss = 1.3535  Validation loss = 6.8111  \n",
      "\n",
      "Fold: 9  Epoch: 18  Training loss = 1.3459  Validation loss = 6.8594  \n",
      "\n",
      "Fold: 9  Epoch: 19  Training loss = 1.3535  Validation loss = 6.7887  \n",
      "\n",
      "Fold: 9  Epoch: 20  Training loss = 1.3427  Validation loss = 6.8048  \n",
      "\n",
      "Fold: 9  Epoch: 21  Training loss = 1.3811  Validation loss = 6.6666  \n",
      "\n",
      "Fold: 9  Epoch: 22  Training loss = 1.3193  Validation loss = 6.8948  \n",
      "\n",
      "Check model:  Fold: 9  Optimal epoch: 14  \n",
      "\n",
      "Fold: 10  Epoch: 1  Training loss = 2.0247  Validation loss = 2.3622  \n",
      "\n",
      "Fold: 10  Epoch: 2  Training loss = 2.0393  Validation loss = 2.2637  \n",
      "\n",
      "Fold: 10  Epoch: 3  Training loss = 1.9944  Validation loss = 2.4225  \n",
      "\n",
      "Fold: 10  Epoch: 4  Training loss = 1.9602  Validation loss = 2.4441  \n",
      "\n",
      "Fold: 10  Epoch: 5  Training loss = 1.9369  Validation loss = 2.5044  \n",
      "\n",
      "Fold: 10  Epoch: 6  Training loss = 1.9307  Validation loss = 2.4513  \n",
      "\n",
      "Fold: 10  Epoch: 7  Training loss = 1.9183  Validation loss = 2.4411  \n",
      "\n",
      "Fold: 10  Epoch: 8  Training loss = 1.9167  Validation loss = 2.6264  \n",
      "\n",
      "Fold: 10  Epoch: 9  Training loss = 1.8978  Validation loss = 2.3513  \n",
      "\n",
      "Fold: 10  Epoch: 10  Training loss = 1.8968  Validation loss = 2.5905  \n",
      "\n",
      "Fold: 10  Epoch: 11  Training loss = 1.8789  Validation loss = 2.1260  \n",
      "\n",
      "Fold: 10  Epoch: 12  Training loss = 1.8629  Validation loss = 2.2695  \n",
      "\n",
      "Fold: 10  Epoch: 13  Training loss = 1.8661  Validation loss = 2.5991  \n",
      "\n",
      "Fold: 10  Epoch: 14  Training loss = 1.8879  Validation loss = 3.1072  \n",
      "\n",
      "Fold: 10  Epoch: 15  Training loss = 1.8463  Validation loss = 2.7516  \n",
      "\n",
      "Fold: 10  Epoch: 16  Training loss = 1.7955  Validation loss = 2.2575  \n",
      "\n",
      "Fold: 10  Epoch: 17  Training loss = 1.7989  Validation loss = 1.8284  \n",
      "\n",
      "Fold: 10  Epoch: 18  Training loss = 1.7777  Validation loss = 2.4866  \n",
      "\n",
      "Fold: 10  Epoch: 19  Training loss = 1.7498  Validation loss = 2.1525  \n",
      "\n",
      "Fold: 10  Epoch: 20  Training loss = 1.8074  Validation loss = 2.0346  \n",
      "\n",
      "Fold: 10  Epoch: 21  Training loss = 1.7141  Validation loss = 2.1265  \n",
      "\n",
      "Fold: 10  Epoch: 22  Training loss = 1.6924  Validation loss = 2.0909  \n",
      "\n",
      "Fold: 10  Epoch: 23  Training loss = 1.6932  Validation loss = 1.9509  \n",
      "\n",
      "Fold: 10  Epoch: 24  Training loss = 1.6789  Validation loss = 2.1650  \n",
      "\n",
      "Fold: 10  Epoch: 25  Training loss = 1.6768  Validation loss = 2.1462  \n",
      "\n",
      "Fold: 10  Epoch: 26  Training loss = 1.6517  Validation loss = 1.9030  \n",
      "\n",
      "Fold: 10  Epoch: 27  Training loss = 1.6407  Validation loss = 1.9080  \n",
      "\n",
      "Fold: 10  Epoch: 28  Training loss = 1.6283  Validation loss = 1.8831  \n",
      "\n",
      "Fold: 10  Epoch: 29  Training loss = 1.6343  Validation loss = 2.4110  \n",
      "\n",
      "Fold: 10  Epoch: 30  Training loss = 1.6244  Validation loss = 2.2332  \n",
      "\n",
      "Fold: 10  Epoch: 31  Training loss = 1.6014  Validation loss = 1.8527  \n",
      "\n",
      "Fold: 10  Epoch: 32  Training loss = 1.5851  Validation loss = 1.9130  \n",
      "\n",
      "Fold: 10  Epoch: 33  Training loss = 1.5617  Validation loss = 1.9291  \n",
      "\n",
      "Fold: 10  Epoch: 34  Training loss = 1.5552  Validation loss = 1.9284  \n",
      "\n",
      "Fold: 10  Epoch: 35  Training loss = 1.5634  Validation loss = 1.8775  \n",
      "\n",
      "Fold: 10  Epoch: 36  Training loss = 1.5463  Validation loss = 1.9152  \n",
      "\n",
      "Fold: 10  Epoch: 37  Training loss = 1.5250  Validation loss = 1.9230  \n",
      "\n",
      "Fold: 10  Epoch: 38  Training loss = 1.5078  Validation loss = 1.9618  \n",
      "\n",
      "Fold: 10  Epoch: 39  Training loss = 1.5109  Validation loss = 1.9479  \n",
      "\n",
      "Fold: 10  Epoch: 40  Training loss = 1.4921  Validation loss = 1.9815  \n",
      "\n",
      "Fold: 10  Epoch: 41  Training loss = 1.4980  Validation loss = 1.9530  \n",
      "\n",
      "Fold: 10  Epoch: 42  Training loss = 1.4907  Validation loss = 1.9587  \n",
      "\n",
      "Fold: 10  Epoch: 43  Training loss = 1.5256  Validation loss = 1.9188  \n",
      "\n",
      "Fold: 10  Epoch: 44  Training loss = 1.4877  Validation loss = 2.0718  \n",
      "\n",
      "Fold: 10  Epoch: 45  Training loss = 1.4514  Validation loss = 2.0428  \n",
      "\n",
      "Fold: 10  Epoch: 46  Training loss = 1.4532  Validation loss = 2.0839  \n",
      "\n",
      "Check model:  Fold: 10  Optimal epoch: 17  \n",
      "\n",
      "Fold: 11  Epoch: 1  Training loss = 1.5129  Validation loss = 1.7568  \n",
      "\n",
      "Fold: 11  Epoch: 2  Training loss = 1.9003  Validation loss = 1.7220  \n",
      "\n",
      "Fold: 11  Epoch: 3  Training loss = 1.8796  Validation loss = 1.6968  \n",
      "\n",
      "Fold: 11  Epoch: 4  Training loss = 1.8535  Validation loss = 1.7455  \n",
      "\n",
      "Fold: 11  Epoch: 5  Training loss = 1.8470  Validation loss = 1.7579  \n",
      "\n",
      "Fold: 11  Epoch: 6  Training loss = 1.5015  Validation loss = 1.7529  \n",
      "\n",
      "Fold: 11  Epoch: 7  Training loss = 1.4884  Validation loss = 1.7803  \n",
      "\n",
      "Fold: 11  Epoch: 8  Training loss = 1.4914  Validation loss = 1.7957  \n",
      "\n",
      "Fold: 11  Epoch: 9  Training loss = 1.4724  Validation loss = 1.7819  \n",
      "\n",
      "Fold: 11  Epoch: 10  Training loss = 1.4680  Validation loss = 1.6487  \n",
      "\n",
      "Fold: 11  Epoch: 11  Training loss = 1.5126  Validation loss = 1.5095  \n",
      "\n",
      "Fold: 11  Epoch: 12  Training loss = 1.4896  Validation loss = 1.6222  \n",
      "\n",
      "Fold: 11  Epoch: 13  Training loss = 1.8100  Validation loss = 1.8460  \n",
      "\n",
      "Fold: 11  Epoch: 14  Training loss = 1.6456  Validation loss = 1.6425  \n",
      "\n",
      "Fold: 11  Epoch: 15  Training loss = 1.6171  Validation loss = 1.2177  \n",
      "\n",
      "Fold: 11  Epoch: 16  Training loss = 1.5757  Validation loss = 1.6458  \n",
      "\n",
      "Fold: 11  Epoch: 17  Training loss = 1.5284  Validation loss = 1.6606  \n",
      "\n",
      "Fold: 11  Epoch: 18  Training loss = 1.5351  Validation loss = 1.5796  \n",
      "\n",
      "Fold: 11  Epoch: 19  Training loss = 1.4398  Validation loss = 1.9363  \n",
      "\n",
      "Check model:  Fold: 11  Optimal epoch: 15  \n",
      "\n",
      "Fold: 12  Epoch: 1  Training loss = 1.5114  Validation loss = 1.2618  \n",
      "\n",
      "Fold: 12  Epoch: 2  Training loss = 1.5121  Validation loss = 1.6369  \n",
      "\n",
      "Fold: 12  Epoch: 3  Training loss = 1.5062  Validation loss = 1.8058  \n",
      "\n",
      "Fold: 12  Epoch: 4  Training loss = 1.4741  Validation loss = 1.5929  \n",
      "\n",
      "Fold: 12  Epoch: 5  Training loss = 1.5532  Validation loss = 1.8069  \n",
      "\n",
      "Fold: 12  Epoch: 6  Training loss = 1.5338  Validation loss = 1.5916  \n",
      "\n",
      "Fold: 12  Epoch: 7  Training loss = 1.5132  Validation loss = 1.8283  \n",
      "\n",
      "Fold: 12  Epoch: 8  Training loss = 1.4465  Validation loss = 1.8405  \n",
      "\n",
      "Fold: 12  Epoch: 9  Training loss = 1.7846  Validation loss = 1.8044  \n",
      "\n",
      "Fold: 12  Epoch: 10  Training loss = 1.8050  Validation loss = 1.7988  \n",
      "\n",
      "Fold: 12  Epoch: 11  Training loss = 1.8942  Validation loss = 1.8288  \n",
      "\n",
      "Fold: 12  Epoch: 12  Training loss = 1.7546  Validation loss = 1.7297  \n",
      "\n",
      "Fold: 12  Epoch: 13  Training loss = 1.8058  Validation loss = 1.7408  \n",
      "\n",
      "Fold: 12  Epoch: 14  Training loss = 1.7786  Validation loss = 1.7899  \n",
      "\n",
      "Fold: 12  Epoch: 15  Training loss = 1.7334  Validation loss = 1.7155  \n",
      "\n",
      "Fold: 12  Epoch: 16  Training loss = 1.7207  Validation loss = 1.6926  \n",
      "\n",
      "Fold: 12  Epoch: 17  Training loss = 1.7105  Validation loss = 1.6991  \n",
      "\n",
      "Fold: 12  Epoch: 18  Training loss = 1.7422  Validation loss = 1.5029  \n",
      "\n",
      "Fold: 12  Epoch: 19  Training loss = 1.7547  Validation loss = 1.4741  \n",
      "\n",
      "Fold: 12  Epoch: 20  Training loss = 1.6658  Validation loss = 1.6240  \n",
      "\n",
      "Fold: 12  Epoch: 21  Training loss = 1.6123  Validation loss = 1.7542  \n",
      "\n",
      "Fold: 12  Epoch: 22  Training loss = 1.4077  Validation loss = 1.5602  \n",
      "\n",
      "Fold: 12  Epoch: 23  Training loss = 1.4086  Validation loss = 1.6571  \n",
      "\n",
      "Fold: 12  Epoch: 24  Training loss = 1.4783  Validation loss = 1.5181  \n",
      "\n",
      "Fold: 12  Epoch: 25  Training loss = 1.5561  Validation loss = 1.3877  \n",
      "\n",
      "Fold: 12  Epoch: 26  Training loss = 1.5081  Validation loss = 1.3974  \n",
      "\n",
      "Fold: 12  Epoch: 27  Training loss = 1.6637  Validation loss = 1.7226  \n",
      "\n",
      "Fold: 12  Epoch: 28  Training loss = 1.6487  Validation loss = 1.6867  \n",
      "\n",
      "Fold: 12  Epoch: 29  Training loss = 1.6272  Validation loss = 1.8319  \n",
      "\n",
      "Check model:  Fold: 12  Optimal epoch: 1  \n",
      "\n",
      "Fold: 13  Epoch: 1  Training loss = 1.6820  Validation loss = 3.8128  \n",
      "\n",
      "Fold: 13  Epoch: 2  Training loss = 1.6496  Validation loss = 3.3730  \n",
      "\n",
      "Fold: 13  Epoch: 3  Training loss = 1.6241  Validation loss = 3.5753  \n",
      "\n",
      "Fold: 13  Epoch: 4  Training loss = 1.6479  Validation loss = 3.3868  \n",
      "\n",
      "Fold: 13  Epoch: 5  Training loss = 1.4352  Validation loss = 3.9734  \n",
      "\n",
      "Fold: 13  Epoch: 6  Training loss = 1.3465  Validation loss = 3.7304  \n",
      "\n",
      "Fold: 13  Epoch: 7  Training loss = 1.3582  Validation loss = 3.7376  \n",
      "\n",
      "Fold: 13  Epoch: 8  Training loss = 1.3918  Validation loss = 4.0282  \n",
      "\n",
      "Fold: 13  Epoch: 9  Training loss = 1.3723  Validation loss = 3.5670  \n",
      "\n",
      "Fold: 13  Epoch: 10  Training loss = 1.3405  Validation loss = 3.6736  \n",
      "\n",
      "Fold: 13  Epoch: 11  Training loss = 1.3266  Validation loss = 3.6857  \n",
      "\n",
      "Fold: 13  Epoch: 12  Training loss = 1.3194  Validation loss = 3.8253  \n",
      "\n",
      "Fold: 13  Epoch: 13  Training loss = 1.3369  Validation loss = 4.0785  \n",
      "\n",
      "Fold: 13  Epoch: 14  Training loss = 1.3794  Validation loss = 4.0891  \n",
      "\n",
      "Fold: 13  Epoch: 15  Training loss = 1.3122  Validation loss = 3.9475  \n",
      "\n",
      "Fold: 13  Epoch: 16  Training loss = 1.3876  Validation loss = 4.1054  \n",
      "\n",
      "Check model:  Fold: 13  Optimal epoch: 2  \n",
      "\n",
      "Fold: 14  Epoch: 1  Training loss = 1.5950  Validation loss = 5.1741  \n",
      "\n",
      "Fold: 14  Epoch: 2  Training loss = 1.5827  Validation loss = 5.5351  \n",
      "\n",
      "Fold: 14  Epoch: 3  Training loss = 1.7253  Validation loss = 5.9604  \n",
      "\n",
      "Fold: 14  Epoch: 4  Training loss = 1.7583  Validation loss = 5.2436  \n",
      "\n",
      "Fold: 14  Epoch: 5  Training loss = 1.5862  Validation loss = 5.4234  \n",
      "\n",
      "Fold: 14  Epoch: 6  Training loss = 1.5797  Validation loss = 5.5033  \n",
      "\n",
      "Fold: 14  Epoch: 7  Training loss = 1.6600  Validation loss = 4.8641  \n",
      "\n",
      "Fold: 14  Epoch: 8  Training loss = 1.6021  Validation loss = 5.1863  \n",
      "\n",
      "Fold: 14  Epoch: 9  Training loss = 1.6247  Validation loss = 5.1894  \n",
      "\n",
      "Fold: 14  Epoch: 10  Training loss = 1.5742  Validation loss = 5.8644  \n",
      "\n",
      "Fold: 14  Epoch: 11  Training loss = 1.5699  Validation loss = 5.1458  \n",
      "\n",
      "Fold: 14  Epoch: 12  Training loss = 1.5250  Validation loss = 6.0627  \n",
      "\n",
      "Fold: 14  Epoch: 13  Training loss = 1.5069  Validation loss = 5.8735  \n",
      "\n",
      "Fold: 14  Epoch: 14  Training loss = 1.7595  Validation loss = 6.6426  \n",
      "\n",
      "Fold: 14  Epoch: 15  Training loss = 1.5248  Validation loss = 5.2745  \n",
      "\n",
      "Fold: 14  Epoch: 16  Training loss = 1.5374  Validation loss = 5.5455  \n",
      "\n",
      "Fold: 14  Epoch: 17  Training loss = 1.5013  Validation loss = 5.4862  \n",
      "\n",
      "Fold: 14  Epoch: 18  Training loss = 1.4723  Validation loss = 5.6564  \n",
      "\n",
      "Fold: 14  Epoch: 19  Training loss = 1.7930  Validation loss = 6.8011  \n",
      "\n",
      "Check model:  Fold: 14  Optimal epoch: 7  \n",
      "\n",
      "Fold: 15  Epoch: 1  Training loss = 2.2581  Validation loss = 6.0775  \n",
      "\n",
      "Fold: 15  Epoch: 2  Training loss = 2.2507  Validation loss = 6.0291  \n",
      "\n",
      "Fold: 15  Epoch: 3  Training loss = 2.0537  Validation loss = 4.3548  \n",
      "\n",
      "Fold: 15  Epoch: 4  Training loss = 1.9538  Validation loss = 4.2855  \n",
      "\n",
      "Fold: 15  Epoch: 5  Training loss = 1.8338  Validation loss = 4.3552  \n",
      "\n",
      "Fold: 15  Epoch: 6  Training loss = 1.9334  Validation loss = 4.3764  \n",
      "\n",
      "Fold: 15  Epoch: 7  Training loss = 2.2448  Validation loss = 6.2089  \n",
      "\n",
      "Fold: 15  Epoch: 8  Training loss = 2.1766  Validation loss = 6.1158  \n",
      "\n",
      "Fold: 15  Epoch: 9  Training loss = 2.2903  Validation loss = 6.0842  \n",
      "\n",
      "Fold: 15  Epoch: 10  Training loss = 2.1336  Validation loss = 5.9638  \n",
      "\n",
      "Fold: 15  Epoch: 11  Training loss = 2.1048  Validation loss = 5.8873  \n",
      "\n",
      "Fold: 15  Epoch: 12  Training loss = 2.1114  Validation loss = 5.8514  \n",
      "\n",
      "Fold: 15  Epoch: 13  Training loss = 2.0761  Validation loss = 5.8915  \n",
      "\n",
      "Fold: 15  Epoch: 14  Training loss = 1.9314  Validation loss = 4.3851  \n",
      "\n",
      "Fold: 15  Epoch: 15  Training loss = 2.1313  Validation loss = 5.9928  \n",
      "\n",
      "Fold: 15  Epoch: 16  Training loss = 2.0547  Validation loss = 5.8122  \n",
      "\n",
      "Fold: 15  Epoch: 17  Training loss = 2.0944  Validation loss = 5.8210  \n",
      "\n",
      "Fold: 15  Epoch: 18  Training loss = 2.0404  Validation loss = 5.7480  \n",
      "\n",
      "Fold: 15  Epoch: 19  Training loss = 2.1096  Validation loss = 5.8938  \n",
      "\n",
      "Fold: 15  Epoch: 20  Training loss = 2.0648  Validation loss = 5.7614  \n",
      "\n",
      "Fold: 15  Epoch: 21  Training loss = 2.0791  Validation loss = 6.2742  \n",
      "\n",
      "Check model:  Fold: 15  Optimal epoch: 4  \n",
      "\n",
      "Fold: 16  Epoch: 1  Training loss = 2.5316  Validation loss = 3.4227  \n",
      "\n",
      "Fold: 16  Epoch: 2  Training loss = 2.4670  Validation loss = 3.5518  \n",
      "\n",
      "Fold: 16  Epoch: 3  Training loss = 2.4587  Validation loss = 3.4964  \n",
      "\n",
      "Fold: 16  Epoch: 4  Training loss = 2.5517  Validation loss = 3.4415  \n",
      "\n",
      "Fold: 16  Epoch: 5  Training loss = 2.4173  Validation loss = 3.6403  \n",
      "\n",
      "Fold: 16  Epoch: 6  Training loss = 2.4039  Validation loss = 3.5586  \n",
      "\n",
      "Fold: 16  Epoch: 7  Training loss = 2.3479  Validation loss = 3.4310  \n",
      "\n",
      "Fold: 16  Epoch: 8  Training loss = 2.3560  Validation loss = 3.3352  \n",
      "\n",
      "Fold: 16  Epoch: 9  Training loss = 2.3457  Validation loss = 3.3009  \n",
      "\n",
      "Fold: 16  Epoch: 10  Training loss = 2.3342  Validation loss = 3.2373  \n",
      "\n",
      "Fold: 16  Epoch: 11  Training loss = 2.2943  Validation loss = 3.2915  \n",
      "\n",
      "Fold: 16  Epoch: 12  Training loss = 2.2988  Validation loss = 3.2350  \n",
      "\n",
      "Fold: 16  Epoch: 13  Training loss = 2.3161  Validation loss = 3.3271  \n",
      "\n",
      "Fold: 16  Epoch: 14  Training loss = 2.4226  Validation loss = 3.3321  \n",
      "\n",
      "Fold: 16  Epoch: 15  Training loss = 2.2376  Validation loss = 3.2309  \n",
      "\n",
      "Fold: 16  Epoch: 16  Training loss = 2.2750  Validation loss = 3.4074  \n",
      "\n",
      "Fold: 16  Epoch: 17  Training loss = 2.2224  Validation loss = 3.2295  \n",
      "\n",
      "Fold: 16  Epoch: 18  Training loss = 2.3038  Validation loss = 3.0890  \n",
      "\n",
      "Fold: 16  Epoch: 19  Training loss = 2.2000  Validation loss = 3.2242  \n",
      "\n",
      "Fold: 16  Epoch: 20  Training loss = 2.2039  Validation loss = 3.1835  \n",
      "\n",
      "Fold: 16  Epoch: 21  Training loss = 2.2021  Validation loss = 3.1170  \n",
      "\n",
      "Fold: 16  Epoch: 22  Training loss = 2.1901  Validation loss = 3.1930  \n",
      "\n",
      "Fold: 16  Epoch: 23  Training loss = 2.1439  Validation loss = 3.0655  \n",
      "\n",
      "Fold: 16  Epoch: 24  Training loss = 2.1530  Validation loss = 3.0492  \n",
      "\n",
      "Fold: 16  Epoch: 25  Training loss = 2.1179  Validation loss = 3.0214  \n",
      "\n",
      "Fold: 16  Epoch: 26  Training loss = 2.1537  Validation loss = 3.3032  \n",
      "\n",
      "Fold: 16  Epoch: 27  Training loss = 2.1297  Validation loss = 3.3373  \n",
      "\n",
      "Fold: 16  Epoch: 28  Training loss = 2.1854  Validation loss = 3.4687  \n",
      "\n",
      "Check model:  Fold: 16  Optimal epoch: 25  \n",
      "\n",
      "Fold: 17  Epoch: 1  Training loss = 2.2131  Validation loss = 4.3927  \n",
      "\n",
      "Fold: 17  Epoch: 2  Training loss = 2.1892  Validation loss = 4.8231  \n",
      "\n",
      "Fold: 17  Epoch: 3  Training loss = 2.2781  Validation loss = 4.1793  \n",
      "\n",
      "Fold: 17  Epoch: 4  Training loss = 2.2239  Validation loss = 4.3551  \n",
      "\n",
      "Fold: 17  Epoch: 5  Training loss = 2.1676  Validation loss = 4.5701  \n",
      "\n",
      "Fold: 17  Epoch: 6  Training loss = 2.4064  Validation loss = 4.0763  \n",
      "\n",
      "Fold: 17  Epoch: 7  Training loss = 2.1252  Validation loss = 4.5126  \n",
      "\n",
      "Fold: 17  Epoch: 8  Training loss = 2.1714  Validation loss = 4.4797  \n",
      "\n",
      "Fold: 17  Epoch: 9  Training loss = 2.1387  Validation loss = 4.3982  \n",
      "\n",
      "Fold: 17  Epoch: 10  Training loss = 2.2994  Validation loss = 4.0104  \n",
      "\n",
      "Fold: 17  Epoch: 11  Training loss = 2.1983  Validation loss = 4.5313  \n",
      "\n",
      "Fold: 17  Epoch: 12  Training loss = 2.1265  Validation loss = 4.4083  \n",
      "\n",
      "Fold: 17  Epoch: 13  Training loss = 2.0888  Validation loss = 4.5136  \n",
      "\n",
      "Fold: 17  Epoch: 14  Training loss = 2.0832  Validation loss = 4.5244  \n",
      "\n",
      "Fold: 17  Epoch: 15  Training loss = 2.0690  Validation loss = 4.5183  \n",
      "\n",
      "Fold: 17  Epoch: 16  Training loss = 2.1571  Validation loss = 3.8844  \n",
      "\n",
      "Fold: 17  Epoch: 17  Training loss = 2.0950  Validation loss = 4.1870  \n",
      "\n",
      "Fold: 17  Epoch: 18  Training loss = 2.0722  Validation loss = 4.6397  \n",
      "\n",
      "Check model:  Fold: 17  Optimal epoch: 16  \n",
      "\n",
      "Fold: 18  Epoch: 1  Training loss = 2.3114  Validation loss = 1.5165  \n",
      "\n",
      "Fold: 18  Epoch: 2  Training loss = 2.3120  Validation loss = 1.5541  \n",
      "\n",
      "Fold: 18  Epoch: 3  Training loss = 2.4055  Validation loss = 1.5239  \n",
      "\n",
      "Fold: 18  Epoch: 4  Training loss = 2.4083  Validation loss = 1.9216  \n",
      "\n",
      "Fold: 18  Epoch: 5  Training loss = 2.4463  Validation loss = 1.9259  \n",
      "\n",
      "Fold: 18  Epoch: 6  Training loss = 2.2396  Validation loss = 1.5216  \n",
      "\n",
      "Fold: 18  Epoch: 7  Training loss = 2.2039  Validation loss = 1.4568  \n",
      "\n",
      "Fold: 18  Epoch: 8  Training loss = 2.3355  Validation loss = 1.4966  \n",
      "\n",
      "Fold: 18  Epoch: 9  Training loss = 2.1638  Validation loss = 1.4663  \n",
      "\n",
      "Fold: 18  Epoch: 10  Training loss = 2.1317  Validation loss = 1.5154  \n",
      "\n",
      "Fold: 18  Epoch: 11  Training loss = 2.1302  Validation loss = 1.6336  \n",
      "\n",
      "Fold: 18  Epoch: 12  Training loss = 2.2928  Validation loss = 1.5528  \n",
      "\n",
      "Fold: 18  Epoch: 13  Training loss = 2.2719  Validation loss = 1.5556  \n",
      "\n",
      "Fold: 18  Epoch: 14  Training loss = 2.1540  Validation loss = 1.5590  \n",
      "\n",
      "Fold: 18  Epoch: 15  Training loss = 2.3067  Validation loss = 1.6830  \n",
      "\n",
      "Fold: 18  Epoch: 16  Training loss = 2.3774  Validation loss = 1.7379  \n",
      "\n",
      "Fold: 18  Epoch: 17  Training loss = 2.2031  Validation loss = 1.8637  \n",
      "\n",
      "Fold: 18  Epoch: 18  Training loss = 2.2537  Validation loss = 1.7655  \n",
      "\n",
      "Fold: 18  Epoch: 19  Training loss = 2.0983  Validation loss = 1.7798  \n",
      "\n",
      "Fold: 18  Epoch: 20  Training loss = 2.3424  Validation loss = 1.7380  \n",
      "\n",
      "Fold: 18  Epoch: 21  Training loss = 2.2271  Validation loss = 1.8007  \n",
      "\n",
      "Fold: 18  Epoch: 22  Training loss = 2.0961  Validation loss = 1.8324  \n",
      "\n",
      "Fold: 18  Epoch: 23  Training loss = 2.2858  Validation loss = 1.6806  \n",
      "\n",
      "Fold: 18  Epoch: 24  Training loss = 2.2477  Validation loss = 1.6562  \n",
      "\n",
      "Fold: 18  Epoch: 25  Training loss = 2.0974  Validation loss = 1.5944  \n",
      "\n",
      "Fold: 18  Epoch: 26  Training loss = 2.0269  Validation loss = 1.5434  \n",
      "\n",
      "Fold: 18  Epoch: 27  Training loss = 2.0953  Validation loss = 1.5849  \n",
      "\n",
      "Fold: 18  Epoch: 28  Training loss = 2.0163  Validation loss = 1.5811  \n",
      "\n",
      "Fold: 18  Epoch: 29  Training loss = 2.1314  Validation loss = 1.5908  \n",
      "\n",
      "Fold: 18  Epoch: 30  Training loss = 2.0984  Validation loss = 1.5187  \n",
      "\n",
      "Fold: 18  Epoch: 31  Training loss = 2.3967  Validation loss = 1.4846  \n",
      "\n",
      "Fold: 18  Epoch: 32  Training loss = 2.3925  Validation loss = 1.5523  \n",
      "\n",
      "Fold: 18  Epoch: 33  Training loss = 2.0941  Validation loss = 1.8287  \n",
      "\n",
      "Fold: 18  Epoch: 34  Training loss = 2.0318  Validation loss = 1.5100  \n",
      "\n",
      "Fold: 18  Epoch: 35  Training loss = 1.9111  Validation loss = 1.4035  \n",
      "\n",
      "Fold: 18  Epoch: 36  Training loss = 1.9196  Validation loss = 1.3874  \n",
      "\n",
      "Fold: 18  Epoch: 37  Training loss = 1.9066  Validation loss = 1.4050  \n",
      "\n",
      "Fold: 18  Epoch: 38  Training loss = 1.9008  Validation loss = 1.4131  \n",
      "\n",
      "Fold: 18  Epoch: 39  Training loss = 1.9533  Validation loss = 1.3705  \n",
      "\n",
      "Fold: 18  Epoch: 40  Training loss = 1.9411  Validation loss = 1.3691  \n",
      "\n",
      "Fold: 18  Epoch: 41  Training loss = 2.0491  Validation loss = 1.5253  \n",
      "\n",
      "Fold: 18  Epoch: 42  Training loss = 1.9658  Validation loss = 1.4580  \n",
      "\n",
      "Fold: 18  Epoch: 43  Training loss = 1.9842  Validation loss = 1.6698  \n",
      "\n",
      "Fold: 18  Epoch: 44  Training loss = 1.9071  Validation loss = 1.4393  \n",
      "\n",
      "Fold: 18  Epoch: 45  Training loss = 1.9176  Validation loss = 1.4533  \n",
      "\n",
      "Fold: 18  Epoch: 46  Training loss = 1.8415  Validation loss = 1.4311  \n",
      "\n",
      "Fold: 18  Epoch: 47  Training loss = 2.0493  Validation loss = 1.4082  \n",
      "\n",
      "Fold: 18  Epoch: 48  Training loss = 1.8848  Validation loss = 1.3115  \n",
      "\n",
      "Fold: 18  Epoch: 49  Training loss = 1.8492  Validation loss = 1.3028  \n",
      "\n",
      "Fold: 18  Epoch: 50  Training loss = 1.8781  Validation loss = 1.5036  \n",
      "\n",
      "Fold: 18  Epoch: 51  Training loss = 1.8911  Validation loss = 1.3719  \n",
      "\n",
      "Fold: 18  Epoch: 52  Training loss = 1.8801  Validation loss = 1.4087  \n",
      "\n",
      "Fold: 18  Epoch: 53  Training loss = 1.8188  Validation loss = 1.2693  \n",
      "\n",
      "Fold: 18  Epoch: 54  Training loss = 1.8550  Validation loss = 1.1449  \n",
      "\n",
      "Fold: 18  Epoch: 55  Training loss = 1.8470  Validation loss = 1.1606  \n",
      "\n",
      "Fold: 18  Epoch: 56  Training loss = 1.9746  Validation loss = 1.1315  \n",
      "\n",
      "Fold: 18  Epoch: 57  Training loss = 1.8669  Validation loss = 1.2224  \n",
      "\n",
      "Fold: 18  Epoch: 58  Training loss = 1.8899  Validation loss = 1.2815  \n",
      "\n",
      "Fold: 18  Epoch: 59  Training loss = 1.9530  Validation loss = 1.3498  \n",
      "\n",
      "Fold: 18  Epoch: 60  Training loss = 1.8640  Validation loss = 1.1656  \n",
      "\n",
      "Fold: 18  Epoch: 61  Training loss = 1.7670  Validation loss = 1.2994  \n",
      "\n",
      "Fold: 18  Epoch: 62  Training loss = 1.7139  Validation loss = 1.3268  \n",
      "\n",
      "Fold: 18  Epoch: 63  Training loss = 1.7518  Validation loss = 1.3799  \n",
      "\n",
      "Fold: 18  Epoch: 64  Training loss = 2.0221  Validation loss = 1.3194  \n",
      "\n",
      "Fold: 18  Epoch: 65  Training loss = 1.7003  Validation loss = 1.2222  \n",
      "\n",
      "Fold: 18  Epoch: 66  Training loss = 1.7872  Validation loss = 1.4436  \n",
      "\n",
      "Check model:  Fold: 18  Optimal epoch: 56  \n",
      "\n",
      "Fold: 19  Epoch: 1  Training loss = 1.7189  Validation loss = 1.5551  \n",
      "\n",
      "Fold: 19  Epoch: 2  Training loss = 1.8748  Validation loss = 1.5019  \n",
      "\n",
      "Fold: 19  Epoch: 3  Training loss = 1.7259  Validation loss = 1.6151  \n",
      "\n",
      "Fold: 19  Epoch: 4  Training loss = 1.7189  Validation loss = 1.7001  \n",
      "\n",
      "Fold: 19  Epoch: 5  Training loss = 2.3289  Validation loss = 1.7196  \n",
      "\n",
      "Fold: 19  Epoch: 6  Training loss = 1.6948  Validation loss = 1.6496  \n",
      "\n",
      "Fold: 19  Epoch: 7  Training loss = 1.8591  Validation loss = 1.5234  \n",
      "\n",
      "Fold: 19  Epoch: 8  Training loss = 1.6866  Validation loss = 1.7162  \n",
      "\n",
      "Fold: 19  Epoch: 9  Training loss = 1.6397  Validation loss = 1.7152  \n",
      "\n",
      "Fold: 19  Epoch: 10  Training loss = 1.8942  Validation loss = 1.5941  \n",
      "\n",
      "Fold: 19  Epoch: 11  Training loss = 1.6951  Validation loss = 1.4989  \n",
      "\n",
      "Fold: 19  Epoch: 12  Training loss = 1.9310  Validation loss = 1.1396  \n",
      "\n",
      "Fold: 19  Epoch: 13  Training loss = 1.8111  Validation loss = 1.3023  \n",
      "\n",
      "Fold: 19  Epoch: 14  Training loss = 1.7182  Validation loss = 1.4006  \n",
      "\n",
      "Fold: 19  Epoch: 15  Training loss = 1.7991  Validation loss = 1.2831  \n",
      "\n",
      "Fold: 19  Epoch: 16  Training loss = 1.8822  Validation loss = 1.3435  \n",
      "\n",
      "Fold: 19  Epoch: 17  Training loss = 1.6786  Validation loss = 1.4384  \n",
      "\n",
      "Fold: 19  Epoch: 18  Training loss = 1.7025  Validation loss = 1.4212  \n",
      "\n",
      "Fold: 19  Epoch: 19  Training loss = 1.6065  Validation loss = 1.4111  \n",
      "\n",
      "Fold: 19  Epoch: 20  Training loss = 1.7323  Validation loss = 1.4916  \n",
      "\n",
      "Fold: 19  Epoch: 21  Training loss = 1.6090  Validation loss = 1.5383  \n",
      "\n",
      "Fold: 19  Epoch: 22  Training loss = 1.6754  Validation loss = 1.4878  \n",
      "\n",
      "Fold: 19  Epoch: 23  Training loss = 1.7214  Validation loss = 1.4823  \n",
      "\n",
      "Fold: 19  Epoch: 24  Training loss = 1.7149  Validation loss = 1.3941  \n",
      "\n",
      "Fold: 19  Epoch: 25  Training loss = 1.9968  Validation loss = 1.5953  \n",
      "\n",
      "Check model:  Fold: 19  Optimal epoch: 12  \n",
      "\n",
      "Fold: 20  Epoch: 1  Training loss = 1.6612  Validation loss = 1.3344  \n",
      "\n",
      "Fold: 20  Epoch: 2  Training loss = 2.3357  Validation loss = 1.5013  \n",
      "\n",
      "Fold: 20  Epoch: 3  Training loss = 1.6472  Validation loss = 1.4231  \n",
      "\n",
      "Fold: 20  Epoch: 4  Training loss = 1.7825  Validation loss = 1.6221  \n",
      "\n",
      "Fold: 20  Epoch: 5  Training loss = 1.6700  Validation loss = 1.4509  \n",
      "\n",
      "Fold: 20  Epoch: 6  Training loss = 1.6843  Validation loss = 1.5409  \n",
      "\n",
      "Fold: 20  Epoch: 7  Training loss = 1.8080  Validation loss = 1.5869  \n",
      "\n",
      "Fold: 20  Epoch: 8  Training loss = 1.6296  Validation loss = 1.4878  \n",
      "\n",
      "Fold: 20  Epoch: 9  Training loss = 1.5915  Validation loss = 1.5949  \n",
      "\n",
      "Fold: 20  Epoch: 10  Training loss = 2.0257  Validation loss = 2.0219  \n",
      "\n",
      "Fold: 20  Epoch: 11  Training loss = 1.9353  Validation loss = 1.3616  \n",
      "\n",
      "Fold: 20  Epoch: 12  Training loss = 1.6308  Validation loss = 1.2905  \n",
      "\n",
      "Fold: 20  Epoch: 13  Training loss = 1.6696  Validation loss = 1.2699  \n",
      "\n",
      "Fold: 20  Epoch: 14  Training loss = 1.6146  Validation loss = 1.4314  \n",
      "\n",
      "Fold: 20  Epoch: 15  Training loss = 1.6588  Validation loss = 1.4410  \n",
      "\n",
      "Fold: 20  Epoch: 16  Training loss = 1.5488  Validation loss = 1.4549  \n",
      "\n",
      "Fold: 20  Epoch: 17  Training loss = 1.6150  Validation loss = 1.5219  \n",
      "\n",
      "Fold: 20  Epoch: 18  Training loss = 1.5432  Validation loss = 1.4486  \n",
      "\n",
      "Fold: 20  Epoch: 19  Training loss = 1.5809  Validation loss = 1.4908  \n",
      "\n",
      "Fold: 20  Epoch: 20  Training loss = 1.7106  Validation loss = 1.3938  \n",
      "\n",
      "Fold: 20  Epoch: 21  Training loss = 1.6266  Validation loss = 1.4336  \n",
      "\n",
      "Fold: 20  Epoch: 22  Training loss = 1.6688  Validation loss = 1.5156  \n",
      "\n",
      "Fold: 20  Epoch: 23  Training loss = 1.8547  Validation loss = 1.5573  \n",
      "\n",
      "Fold: 20  Epoch: 24  Training loss = 1.8072  Validation loss = 1.6543  \n",
      "\n",
      "Fold: 20  Epoch: 25  Training loss = 1.7852  Validation loss = 1.5022  \n",
      "\n",
      "Fold: 20  Epoch: 26  Training loss = 1.6338  Validation loss = 1.3700  \n",
      "\n",
      "Fold: 20  Epoch: 27  Training loss = 1.5756  Validation loss = 1.4005  \n",
      "\n",
      "Fold: 20  Epoch: 28  Training loss = 1.6753  Validation loss = 1.4356  \n",
      "\n",
      "Fold: 20  Epoch: 29  Training loss = 1.5432  Validation loss = 1.6409  \n",
      "\n",
      "Fold: 20  Epoch: 30  Training loss = 1.6726  Validation loss = 1.5005  \n",
      "\n",
      "Fold: 20  Epoch: 31  Training loss = 1.5791  Validation loss = 1.6468  \n",
      "\n",
      "Fold: 20  Epoch: 32  Training loss = 1.5443  Validation loss = 1.7148  \n",
      "\n",
      "Check model:  Fold: 20  Optimal epoch: 13  \n",
      "\n",
      "Fold: 21  Epoch: 1  Training loss = 1.5570  Validation loss = 4.5904  \n",
      "\n",
      "Fold: 21  Epoch: 2  Training loss = 1.5678  Validation loss = 4.7283  \n",
      "\n",
      "Fold: 21  Epoch: 3  Training loss = 1.5662  Validation loss = 4.8728  \n",
      "\n",
      "Fold: 21  Epoch: 4  Training loss = 1.5234  Validation loss = 4.6567  \n",
      "\n",
      "Fold: 21  Epoch: 5  Training loss = 1.6064  Validation loss = 4.3443  \n",
      "\n",
      "Fold: 21  Epoch: 6  Training loss = 1.4991  Validation loss = 4.6486  \n",
      "\n",
      "Fold: 21  Epoch: 7  Training loss = 1.7252  Validation loss = 4.8518  \n",
      "\n",
      "Fold: 21  Epoch: 8  Training loss = 1.5874  Validation loss = 4.9852  \n",
      "\n",
      "Fold: 21  Epoch: 9  Training loss = 1.5405  Validation loss = 4.7929  \n",
      "\n",
      "Fold: 21  Epoch: 10  Training loss = 1.5673  Validation loss = 4.3383  \n",
      "\n",
      "Fold: 21  Epoch: 11  Training loss = 1.5873  Validation loss = 4.5320  \n",
      "\n",
      "Fold: 21  Epoch: 12  Training loss = 1.5401  Validation loss = 4.8042  \n",
      "\n",
      "Fold: 21  Epoch: 13  Training loss = 1.5124  Validation loss = 4.7834  \n",
      "\n",
      "Fold: 21  Epoch: 14  Training loss = 1.5308  Validation loss = 4.6068  \n",
      "\n",
      "Fold: 21  Epoch: 15  Training loss = 1.5055  Validation loss = 4.7856  \n",
      "\n",
      "Fold: 21  Epoch: 16  Training loss = 1.5071  Validation loss = 4.5522  \n",
      "\n",
      "Fold: 21  Epoch: 17  Training loss = 1.5075  Validation loss = 4.6811  \n",
      "\n",
      "Fold: 21  Epoch: 18  Training loss = 1.5756  Validation loss = 4.9963  \n",
      "\n",
      "Check model:  Fold: 21  Optimal epoch: 10  \n",
      "\n",
      "Fold: 22  Epoch: 1  Training loss = 1.7758  Validation loss = 2.9369  \n",
      "\n",
      "Fold: 22  Epoch: 2  Training loss = 1.7370  Validation loss = 2.7117  \n",
      "\n",
      "Fold: 22  Epoch: 3  Training loss = 1.7595  Validation loss = 2.7243  \n",
      "\n",
      "Fold: 22  Epoch: 4  Training loss = 1.8247  Validation loss = 2.6852  \n",
      "\n",
      "Fold: 22  Epoch: 5  Training loss = 1.7592  Validation loss = 2.6089  \n",
      "\n",
      "Fold: 22  Epoch: 6  Training loss = 1.8281  Validation loss = 2.6058  \n",
      "\n",
      "Fold: 22  Epoch: 7  Training loss = 1.8043  Validation loss = 2.5361  \n",
      "\n",
      "Fold: 22  Epoch: 8  Training loss = 1.7798  Validation loss = 2.8902  \n",
      "\n",
      "Fold: 22  Epoch: 9  Training loss = 1.7374  Validation loss = 2.6822  \n",
      "\n",
      "Fold: 22  Epoch: 10  Training loss = 1.7501  Validation loss = 2.3378  \n",
      "\n",
      "Fold: 22  Epoch: 11  Training loss = 1.7031  Validation loss = 2.4152  \n",
      "\n",
      "Fold: 22  Epoch: 12  Training loss = 1.8309  Validation loss = 2.3165  \n",
      "\n",
      "Fold: 22  Epoch: 13  Training loss = 1.7094  Validation loss = 2.6769  \n",
      "\n",
      "Fold: 22  Epoch: 14  Training loss = 1.6896  Validation loss = 2.4417  \n",
      "\n",
      "Fold: 22  Epoch: 15  Training loss = 1.6804  Validation loss = 2.5759  \n",
      "\n",
      "Fold: 22  Epoch: 16  Training loss = 1.8058  Validation loss = 4.0134  \n",
      "\n",
      "Check model:  Fold: 22  Optimal epoch: 12  \n",
      "\n",
      "Fold: 23  Epoch: 1  Training loss = 1.7978  Validation loss = 3.1581  \n",
      "\n",
      "Fold: 23  Epoch: 2  Training loss = 1.7533  Validation loss = 2.1885  \n",
      "\n",
      "Fold: 23  Epoch: 3  Training loss = 1.7372  Validation loss = 2.5261  \n",
      "\n",
      "Fold: 23  Epoch: 4  Training loss = 1.7243  Validation loss = 3.1956  \n",
      "\n",
      "Fold: 23  Epoch: 5  Training loss = 1.6854  Validation loss = 2.3956  \n",
      "\n",
      "Fold: 23  Epoch: 6  Training loss = 1.6805  Validation loss = 2.4961  \n",
      "\n",
      "Fold: 23  Epoch: 7  Training loss = 1.8755  Validation loss = 1.7945  \n",
      "\n",
      "Fold: 23  Epoch: 8  Training loss = 1.7266  Validation loss = 3.0709  \n",
      "\n",
      "Fold: 23  Epoch: 9  Training loss = 1.6793  Validation loss = 2.0783  \n",
      "\n",
      "Fold: 23  Epoch: 10  Training loss = 1.6560  Validation loss = 2.0954  \n",
      "\n",
      "Fold: 23  Epoch: 11  Training loss = 1.6380  Validation loss = 1.6425  \n",
      "\n",
      "Fold: 23  Epoch: 12  Training loss = 1.6718  Validation loss = 2.0012  \n",
      "\n",
      "Fold: 23  Epoch: 13  Training loss = 1.6740  Validation loss = 2.1300  \n",
      "\n",
      "Fold: 23  Epoch: 14  Training loss = 1.6739  Validation loss = 2.1825  \n",
      "\n",
      "Fold: 23  Epoch: 15  Training loss = 1.6629  Validation loss = 2.6622  \n",
      "\n",
      "Fold: 23  Epoch: 16  Training loss = 1.8034  Validation loss = 1.8543  \n",
      "\n",
      "Fold: 23  Epoch: 17  Training loss = 1.8256  Validation loss = 3.8439  \n",
      "\n",
      "Check model:  Fold: 23  Optimal epoch: 11  \n",
      "\n",
      "Fold: 24  Epoch: 1  Training loss = 2.3784  Validation loss = 3.1198  \n",
      "\n",
      "Fold: 24  Epoch: 2  Training loss = 2.0042  Validation loss = 3.1808  \n",
      "\n",
      "Fold: 24  Epoch: 3  Training loss = 1.7749  Validation loss = 3.0232  \n",
      "\n",
      "Fold: 24  Epoch: 4  Training loss = 2.1992  Validation loss = 1.8182  \n",
      "\n",
      "Fold: 24  Epoch: 5  Training loss = 2.0364  Validation loss = 1.6666  \n",
      "\n",
      "Fold: 24  Epoch: 6  Training loss = 2.2965  Validation loss = 3.3581  \n",
      "\n",
      "Fold: 24  Epoch: 7  Training loss = 1.9592  Validation loss = 3.3202  \n",
      "\n",
      "Fold: 24  Epoch: 8  Training loss = 1.8579  Validation loss = 3.1946  \n",
      "\n",
      "Fold: 24  Epoch: 9  Training loss = 1.9450  Validation loss = 3.0331  \n",
      "\n",
      "Fold: 24  Epoch: 10  Training loss = 1.7523  Validation loss = 3.0036  \n",
      "\n",
      "Fold: 24  Epoch: 11  Training loss = 1.6866  Validation loss = 3.0936  \n",
      "\n",
      "Fold: 24  Epoch: 12  Training loss = 1.6935  Validation loss = 1.8410  \n",
      "\n",
      "Fold: 24  Epoch: 13  Training loss = 1.6395  Validation loss = 1.9040  \n",
      "\n",
      "Fold: 24  Epoch: 14  Training loss = 2.3125  Validation loss = 3.3958  \n",
      "\n",
      "Fold: 24  Epoch: 15  Training loss = 2.1892  Validation loss = 3.2051  \n",
      "\n",
      "Fold: 24  Epoch: 16  Training loss = 1.9038  Validation loss = 3.1321  \n",
      "\n",
      "Fold: 24  Epoch: 17  Training loss = 1.8474  Validation loss = 3.2034  \n",
      "\n",
      "Fold: 24  Epoch: 18  Training loss = 1.7201  Validation loss = 3.1189  \n",
      "\n",
      "Fold: 24  Epoch: 19  Training loss = 1.6032  Validation loss = 2.4525  \n",
      "\n",
      "Fold: 24  Epoch: 20  Training loss = 1.5973  Validation loss = 2.6157  \n",
      "\n",
      "Fold: 24  Epoch: 21  Training loss = 1.6142  Validation loss = 2.4249  \n",
      "\n",
      "Fold: 24  Epoch: 22  Training loss = 1.5771  Validation loss = 2.7983  \n",
      "\n",
      "Fold: 24  Epoch: 23  Training loss = 1.5731  Validation loss = 2.5774  \n",
      "\n",
      "Fold: 24  Epoch: 24  Training loss = 1.5756  Validation loss = 2.3905  \n",
      "\n",
      "Fold: 24  Epoch: 25  Training loss = 1.7647  Validation loss = 1.9513  \n",
      "\n",
      "Fold: 24  Epoch: 26  Training loss = 1.5372  Validation loss = 2.4684  \n",
      "\n",
      "Fold: 24  Epoch: 27  Training loss = 1.5761  Validation loss = 2.6034  \n",
      "\n",
      "Fold: 24  Epoch: 28  Training loss = 1.5600  Validation loss = 2.7184  \n",
      "\n",
      "Fold: 24  Epoch: 29  Training loss = 1.5526  Validation loss = 2.7691  \n",
      "\n",
      "Fold: 24  Epoch: 30  Training loss = 1.5156  Validation loss = 2.4602  \n",
      "\n",
      "Fold: 24  Epoch: 31  Training loss = 1.5781  Validation loss = 3.5042  \n",
      "\n",
      "Check model:  Fold: 24  Optimal epoch: 5  \n",
      "\n",
      "Fold: 25  Epoch: 1  Training loss = 1.7254  Validation loss = 2.3861  \n",
      "\n",
      "Fold: 25  Epoch: 2  Training loss = 1.6507  Validation loss = 2.8228  \n",
      "\n",
      "Fold: 25  Epoch: 3  Training loss = 1.6640  Validation loss = 2.7212  \n",
      "\n",
      "Fold: 25  Epoch: 4  Training loss = 1.5493  Validation loss = 2.9722  \n",
      "\n",
      "Fold: 25  Epoch: 5  Training loss = 1.9534  Validation loss = 3.3101  \n",
      "\n",
      "Fold: 25  Epoch: 6  Training loss = 2.0377  Validation loss = 1.8065  \n",
      "\n",
      "Fold: 25  Epoch: 7  Training loss = 1.8088  Validation loss = 2.4949  \n",
      "\n",
      "Fold: 25  Epoch: 8  Training loss = 2.2032  Validation loss = 2.9741  \n",
      "\n",
      "Fold: 25  Epoch: 9  Training loss = 1.6707  Validation loss = 2.5707  \n",
      "\n",
      "Fold: 25  Epoch: 10  Training loss = 1.7068  Validation loss = 3.0313  \n",
      "\n",
      "Fold: 25  Epoch: 11  Training loss = 1.7048  Validation loss = 3.0344  \n",
      "\n",
      "Fold: 25  Epoch: 12  Training loss = 1.7790  Validation loss = 1.5480  \n",
      "\n",
      "Fold: 25  Epoch: 13  Training loss = 1.6224  Validation loss = 3.3694  \n",
      "\n",
      "Fold: 25  Epoch: 14  Training loss = 1.4974  Validation loss = 1.9738  \n",
      "\n",
      "Fold: 25  Epoch: 15  Training loss = 1.6749  Validation loss = 3.2345  \n",
      "\n",
      "Fold: 25  Epoch: 16  Training loss = 1.4891  Validation loss = 1.9832  \n",
      "\n",
      "Fold: 25  Epoch: 17  Training loss = 1.5667  Validation loss = 2.9562  \n",
      "\n",
      "Fold: 25  Epoch: 18  Training loss = 1.5126  Validation loss = 2.6121  \n",
      "\n",
      "Fold: 25  Epoch: 19  Training loss = 1.5566  Validation loss = 2.7134  \n",
      "\n",
      "Fold: 25  Epoch: 20  Training loss = 1.7354  Validation loss = 1.8116  \n",
      "\n",
      "Fold: 25  Epoch: 21  Training loss = 1.4525  Validation loss = 2.0658  \n",
      "\n",
      "Fold: 25  Epoch: 22  Training loss = 1.4593  Validation loss = 1.9700  \n",
      "\n",
      "Fold: 25  Epoch: 23  Training loss = 1.4733  Validation loss = 2.7466  \n",
      "\n",
      "Fold: 25  Epoch: 24  Training loss = 1.4725  Validation loss = 2.7325  \n",
      "\n",
      "Fold: 25  Epoch: 25  Training loss = 1.4343  Validation loss = 1.8589  \n",
      "\n",
      "Fold: 25  Epoch: 26  Training loss = 1.4603  Validation loss = 2.5128  \n",
      "\n",
      "Fold: 25  Epoch: 27  Training loss = 1.4729  Validation loss = 2.5705  \n",
      "\n",
      "Fold: 25  Epoch: 28  Training loss = 1.4459  Validation loss = 2.3449  \n",
      "\n",
      "Fold: 25  Epoch: 29  Training loss = 1.6002  Validation loss = 2.0466  \n",
      "\n",
      "Fold: 25  Epoch: 30  Training loss = 1.4058  Validation loss = 2.3845  \n",
      "\n",
      "Fold: 25  Epoch: 31  Training loss = 1.4281  Validation loss = 2.5409  \n",
      "\n",
      "Fold: 25  Epoch: 32  Training loss = 1.4018  Validation loss = 2.1877  \n",
      "\n",
      "Fold: 25  Epoch: 33  Training loss = 1.4570  Validation loss = 3.1942  \n",
      "\n",
      "Check model:  Fold: 25  Optimal epoch: 12  \n",
      "\n",
      "Fold: 26  Epoch: 1  Training loss = 1.6485  Validation loss = 4.4863  \n",
      "\n",
      "Fold: 26  Epoch: 2  Training loss = 1.4899  Validation loss = 3.2803  \n",
      "\n",
      "Fold: 26  Epoch: 3  Training loss = 1.8313  Validation loss = 4.8674  \n",
      "\n",
      "Fold: 26  Epoch: 4  Training loss = 1.6271  Validation loss = 2.7019  \n",
      "\n",
      "Fold: 26  Epoch: 5  Training loss = 1.5520  Validation loss = 2.8105  \n",
      "\n",
      "Fold: 26  Epoch: 6  Training loss = 1.4886  Validation loss = 3.5976  \n",
      "\n",
      "Fold: 26  Epoch: 7  Training loss = 1.6957  Validation loss = 2.8270  \n",
      "\n",
      "Fold: 26  Epoch: 8  Training loss = 1.5574  Validation loss = 3.5371  \n",
      "\n",
      "Fold: 26  Epoch: 9  Training loss = 1.4686  Validation loss = 3.5378  \n",
      "\n",
      "Fold: 26  Epoch: 10  Training loss = 1.4559  Validation loss = 3.6570  \n",
      "\n",
      "Fold: 26  Epoch: 11  Training loss = 2.0037  Validation loss = 5.5427  \n",
      "\n",
      "Fold: 26  Epoch: 12  Training loss = 1.9256  Validation loss = 2.6883  \n",
      "\n",
      "Fold: 26  Epoch: 13  Training loss = 2.1823  Validation loss = 5.3578  \n",
      "\n",
      "Fold: 26  Epoch: 14  Training loss = 1.6570  Validation loss = 4.3839  \n",
      "\n",
      "Fold: 26  Epoch: 15  Training loss = 1.6074  Validation loss = 4.4370  \n",
      "\n",
      "Fold: 26  Epoch: 16  Training loss = 1.6189  Validation loss = 4.6211  \n",
      "\n",
      "Fold: 26  Epoch: 17  Training loss = 1.5281  Validation loss = 3.5308  \n",
      "\n",
      "Fold: 26  Epoch: 18  Training loss = 1.6430  Validation loss = 4.5892  \n",
      "\n",
      "Fold: 26  Epoch: 19  Training loss = 1.8541  Validation loss = 2.2176  \n",
      "\n",
      "Fold: 26  Epoch: 20  Training loss = 2.0498  Validation loss = 4.2137  \n",
      "\n",
      "Fold: 26  Epoch: 21  Training loss = 1.9818  Validation loss = 3.4963  \n",
      "\n",
      "Fold: 26  Epoch: 22  Training loss = 1.6683  Validation loss = 3.9966  \n",
      "\n",
      "Fold: 26  Epoch: 23  Training loss = 1.5859  Validation loss = 4.1517  \n",
      "\n",
      "Fold: 26  Epoch: 24  Training loss = 1.4852  Validation loss = 3.5818  \n",
      "\n",
      "Fold: 26  Epoch: 25  Training loss = 1.5684  Validation loss = 2.3931  \n",
      "\n",
      "Fold: 26  Epoch: 26  Training loss = 1.6080  Validation loss = 2.3048  \n",
      "\n",
      "Fold: 26  Epoch: 27  Training loss = 1.5677  Validation loss = 4.3837  \n",
      "\n",
      "Fold: 26  Epoch: 28  Training loss = 1.4868  Validation loss = 3.3531  \n",
      "\n",
      "Fold: 26  Epoch: 29  Training loss = 1.5922  Validation loss = 4.8305  \n",
      "\n",
      "Check model:  Fold: 26  Optimal epoch: 19  \n",
      "\n",
      "Fold: 27  Epoch: 1  Training loss = 1.9541  Validation loss = 1.4840  \n",
      "\n",
      "Fold: 27  Epoch: 2  Training loss = 1.9119  Validation loss = 1.5029  \n",
      "\n",
      "Fold: 27  Epoch: 3  Training loss = 1.8977  Validation loss = 1.3797  \n",
      "\n",
      "Fold: 27  Epoch: 4  Training loss = 1.5804  Validation loss = 1.2623  \n",
      "\n",
      "Fold: 27  Epoch: 5  Training loss = 2.0723  Validation loss = 1.3590  \n",
      "\n",
      "Fold: 27  Epoch: 6  Training loss = 1.9206  Validation loss = 1.1815  \n",
      "\n",
      "Fold: 27  Epoch: 7  Training loss = 1.9845  Validation loss = 0.9467  \n",
      "\n",
      "Fold: 27  Epoch: 8  Training loss = 1.9575  Validation loss = 1.0521  \n",
      "\n",
      "Fold: 27  Epoch: 9  Training loss = 1.8640  Validation loss = 1.0353  \n",
      "\n",
      "Fold: 27  Epoch: 10  Training loss = 1.8080  Validation loss = 1.1783  \n",
      "\n",
      "Fold: 27  Epoch: 11  Training loss = 1.7732  Validation loss = 1.0051  \n",
      "\n",
      "Fold: 27  Epoch: 12  Training loss = 1.8087  Validation loss = 1.2834  \n",
      "\n",
      "Fold: 27  Epoch: 13  Training loss = 1.8305  Validation loss = 1.1886  \n",
      "\n",
      "Fold: 27  Epoch: 14  Training loss = 1.7890  Validation loss = 1.0275  \n",
      "\n",
      "Fold: 27  Epoch: 15  Training loss = 1.7991  Validation loss = 0.8902  \n",
      "\n",
      "Fold: 27  Epoch: 16  Training loss = 1.8559  Validation loss = 1.2539  \n",
      "\n",
      "Fold: 27  Epoch: 17  Training loss = 1.8336  Validation loss = 1.1672  \n",
      "\n",
      "Fold: 27  Epoch: 18  Training loss = 1.7979  Validation loss = 0.9610  \n",
      "\n",
      "Fold: 27  Epoch: 19  Training loss = 1.8123  Validation loss = 1.0641  \n",
      "\n",
      "Fold: 27  Epoch: 20  Training loss = 2.1959  Validation loss = 1.7607  \n",
      "\n",
      "Check model:  Fold: 27  Optimal epoch: 15  \n",
      "\n",
      "Fold: 28  Epoch: 1  Training loss = 2.0132  Validation loss = 0.9021  \n",
      "\n",
      "Fold: 28  Epoch: 2  Training loss = 1.9538  Validation loss = 0.8179  \n",
      "\n",
      "Fold: 28  Epoch: 3  Training loss = 2.0259  Validation loss = 1.4905  \n",
      "\n",
      "Fold: 28  Epoch: 4  Training loss = 2.0407  Validation loss = 1.4572  \n",
      "\n",
      "Fold: 28  Epoch: 5  Training loss = 2.1403  Validation loss = 1.1470  \n",
      "\n",
      "Fold: 28  Epoch: 6  Training loss = 1.9602  Validation loss = 1.1786  \n",
      "\n",
      "Fold: 28  Epoch: 7  Training loss = 2.1401  Validation loss = 1.2488  \n",
      "\n",
      "Fold: 28  Epoch: 8  Training loss = 2.1009  Validation loss = 1.4625  \n",
      "\n",
      "Fold: 28  Epoch: 9  Training loss = 2.0330  Validation loss = 1.0714  \n",
      "\n",
      "Fold: 28  Epoch: 10  Training loss = 1.8226  Validation loss = 1.1615  \n",
      "\n",
      "Fold: 28  Epoch: 11  Training loss = 1.8075  Validation loss = 1.1390  \n",
      "\n",
      "Fold: 28  Epoch: 12  Training loss = 1.9902  Validation loss = 1.4125  \n",
      "\n",
      "Fold: 28  Epoch: 13  Training loss = 1.9678  Validation loss = 1.4664  \n",
      "\n",
      "Fold: 28  Epoch: 14  Training loss = 2.0048  Validation loss = 1.5593  \n",
      "\n",
      "Fold: 28  Epoch: 15  Training loss = 1.7863  Validation loss = 1.1338  \n",
      "\n",
      "Fold: 28  Epoch: 16  Training loss = 1.8687  Validation loss = 0.9698  \n",
      "\n",
      "Fold: 28  Epoch: 17  Training loss = 1.8810  Validation loss = 0.7786  \n",
      "\n",
      "Fold: 28  Epoch: 18  Training loss = 1.8014  Validation loss = 0.6247  \n",
      "\n",
      "Fold: 28  Epoch: 19  Training loss = 1.7343  Validation loss = 1.2781  \n",
      "\n",
      "Fold: 28  Epoch: 20  Training loss = 1.7301  Validation loss = 1.2255  \n",
      "\n",
      "Fold: 28  Epoch: 21  Training loss = 1.7100  Validation loss = 1.0048  \n",
      "\n",
      "Fold: 28  Epoch: 22  Training loss = 1.7465  Validation loss = 1.3080  \n",
      "\n",
      "Fold: 28  Epoch: 23  Training loss = 1.6949  Validation loss = 1.1635  \n",
      "\n",
      "Fold: 28  Epoch: 24  Training loss = 1.8233  Validation loss = 1.0846  \n",
      "\n",
      "Fold: 28  Epoch: 25  Training loss = 1.7516  Validation loss = 1.1853  \n",
      "\n",
      "Fold: 28  Epoch: 26  Training loss = 1.7735  Validation loss = 0.7475  \n",
      "\n",
      "Fold: 28  Epoch: 27  Training loss = 1.7069  Validation loss = 0.8777  \n",
      "\n",
      "Fold: 28  Epoch: 28  Training loss = 1.6761  Validation loss = 0.9457  \n",
      "\n",
      "Fold: 28  Epoch: 29  Training loss = 1.6695  Validation loss = 1.1113  \n",
      "\n",
      "Fold: 28  Epoch: 30  Training loss = 1.6523  Validation loss = 0.9015  \n",
      "\n",
      "Fold: 28  Epoch: 31  Training loss = 1.6781  Validation loss = 1.1178  \n",
      "\n",
      "Fold: 28  Epoch: 32  Training loss = 1.7851  Validation loss = 0.9031  \n",
      "\n",
      "Fold: 28  Epoch: 33  Training loss = 1.6823  Validation loss = 0.9397  \n",
      "\n",
      "Fold: 28  Epoch: 34  Training loss = 1.6615  Validation loss = 0.9747  \n",
      "\n",
      "Fold: 28  Epoch: 35  Training loss = 2.0593  Validation loss = 0.6768  \n",
      "\n",
      "Fold: 28  Epoch: 36  Training loss = 1.7239  Validation loss = 0.9071  \n",
      "\n",
      "Fold: 28  Epoch: 37  Training loss = 1.7246  Validation loss = 0.8685  \n",
      "\n",
      "Fold: 28  Epoch: 38  Training loss = 1.7523  Validation loss = 1.1433  \n",
      "\n",
      "Fold: 28  Epoch: 39  Training loss = 2.0422  Validation loss = 1.4107  \n",
      "\n",
      "Check model:  Fold: 28  Optimal epoch: 18  \n",
      "\n",
      "Fold: 29  Epoch: 1  Training loss = 1.7940  Validation loss = 0.6879  \n",
      "\n",
      "Fold: 29  Epoch: 2  Training loss = 1.6178  Validation loss = 0.6189  \n",
      "\n",
      "Fold: 29  Epoch: 3  Training loss = 1.7872  Validation loss = 0.6810  \n",
      "\n",
      "Fold: 29  Epoch: 4  Training loss = 1.8130  Validation loss = 1.2459  \n",
      "\n",
      "Fold: 29  Epoch: 5  Training loss = 1.6524  Validation loss = 0.7765  \n",
      "\n",
      "Fold: 29  Epoch: 6  Training loss = 1.5951  Validation loss = 0.8178  \n",
      "\n",
      "Fold: 29  Epoch: 7  Training loss = 1.5788  Validation loss = 0.7269  \n",
      "\n",
      "Fold: 29  Epoch: 8  Training loss = 1.6002  Validation loss = 0.7273  \n",
      "\n",
      "Fold: 29  Epoch: 9  Training loss = 1.5392  Validation loss = 0.8444  \n",
      "\n",
      "Fold: 29  Epoch: 10  Training loss = 1.6505  Validation loss = 0.7468  \n",
      "\n",
      "Fold: 29  Epoch: 11  Training loss = 1.5849  Validation loss = 0.8179  \n",
      "\n",
      "Fold: 29  Epoch: 12  Training loss = 1.5862  Validation loss = 0.7251  \n",
      "\n",
      "Fold: 29  Epoch: 13  Training loss = 1.7180  Validation loss = 0.7256  \n",
      "\n",
      "Fold: 29  Epoch: 14  Training loss = 1.6415  Validation loss = 0.6889  \n",
      "\n",
      "Fold: 29  Epoch: 15  Training loss = 1.7343  Validation loss = 0.7173  \n",
      "\n",
      "Fold: 29  Epoch: 16  Training loss = 1.6873  Validation loss = 0.7804  \n",
      "\n",
      "Fold: 29  Epoch: 17  Training loss = 1.6183  Validation loss = 0.6658  \n",
      "\n",
      "Fold: 29  Epoch: 18  Training loss = 1.5633  Validation loss = 0.7005  \n",
      "\n",
      "Fold: 29  Epoch: 19  Training loss = 1.5498  Validation loss = 0.8393  \n",
      "\n",
      "Fold: 29  Epoch: 20  Training loss = 1.6384  Validation loss = 1.1289  \n",
      "\n",
      "Check model:  Fold: 29  Optimal epoch: 2  \n",
      "\n",
      "Fold: 30  Epoch: 1  Training loss = 1.7130  Validation loss = 1.6740  \n",
      "\n",
      "Fold: 30  Epoch: 2  Training loss = 1.6020  Validation loss = 1.7223  \n",
      "\n",
      "Fold: 30  Epoch: 3  Training loss = 1.5454  Validation loss = 1.5455  \n",
      "\n",
      "Fold: 30  Epoch: 4  Training loss = 1.5022  Validation loss = 1.2893  \n",
      "\n",
      "Fold: 30  Epoch: 5  Training loss = 1.5049  Validation loss = 1.4836  \n",
      "\n",
      "Fold: 30  Epoch: 6  Training loss = 1.5833  Validation loss = 1.2644  \n",
      "\n",
      "Fold: 30  Epoch: 7  Training loss = 1.5374  Validation loss = 1.6582  \n",
      "\n",
      "Fold: 30  Epoch: 8  Training loss = 1.4756  Validation loss = 1.5073  \n",
      "\n",
      "Fold: 30  Epoch: 9  Training loss = 1.5144  Validation loss = 1.5789  \n",
      "\n",
      "Fold: 30  Epoch: 10  Training loss = 1.5198  Validation loss = 1.9085  \n",
      "\n",
      "Fold: 30  Epoch: 11  Training loss = 1.5062  Validation loss = 1.6928  \n",
      "\n",
      "Fold: 30  Epoch: 12  Training loss = 1.4742  Validation loss = 1.6078  \n",
      "\n",
      "Fold: 30  Epoch: 13  Training loss = 1.4416  Validation loss = 1.5215  \n",
      "\n",
      "Fold: 30  Epoch: 14  Training loss = 1.4967  Validation loss = 1.6063  \n",
      "\n",
      "Fold: 30  Epoch: 15  Training loss = 1.5342  Validation loss = 1.1949  \n",
      "\n",
      "Fold: 30  Epoch: 16  Training loss = 1.4484  Validation loss = 1.3072  \n",
      "\n",
      "Fold: 30  Epoch: 17  Training loss = 1.4723  Validation loss = 1.2235  \n",
      "\n",
      "Fold: 30  Epoch: 18  Training loss = 1.4665  Validation loss = 1.5138  \n",
      "\n",
      "Fold: 30  Epoch: 19  Training loss = 1.4689  Validation loss = 1.6540  \n",
      "\n",
      "Fold: 30  Epoch: 20  Training loss = 1.4706  Validation loss = 1.6106  \n",
      "\n",
      "Fold: 30  Epoch: 21  Training loss = 1.4423  Validation loss = 1.3751  \n",
      "\n",
      "Fold: 30  Epoch: 22  Training loss = 1.4534  Validation loss = 1.2346  \n",
      "\n",
      "Fold: 30  Epoch: 23  Training loss = 1.4236  Validation loss = 1.3337  \n",
      "\n",
      "Fold: 30  Epoch: 24  Training loss = 1.4073  Validation loss = 1.3810  \n",
      "\n",
      "Fold: 30  Epoch: 25  Training loss = 1.4371  Validation loss = 1.6338  \n",
      "\n",
      "Fold: 30  Epoch: 26  Training loss = 1.4405  Validation loss = 1.6912  \n",
      "\n",
      "Check model:  Fold: 30  Optimal epoch: 15  \n",
      "\n",
      "Fold: 31  Epoch: 1  Training loss = 1.4694  Validation loss = 2.0168  \n",
      "\n",
      "Fold: 31  Epoch: 2  Training loss = 1.4513  Validation loss = 2.0870  \n",
      "\n",
      "Fold: 31  Epoch: 3  Training loss = 1.4667  Validation loss = 1.8148  \n",
      "\n",
      "Fold: 31  Epoch: 4  Training loss = 1.4540  Validation loss = 1.8597  \n",
      "\n",
      "Fold: 31  Epoch: 5  Training loss = 1.5683  Validation loss = 1.2646  \n",
      "\n",
      "Fold: 31  Epoch: 6  Training loss = 1.4886  Validation loss = 1.8509  \n",
      "\n",
      "Fold: 31  Epoch: 7  Training loss = 1.4742  Validation loss = 2.0654  \n",
      "\n",
      "Fold: 31  Epoch: 8  Training loss = 1.4621  Validation loss = 2.1497  \n",
      "\n",
      "Fold: 31  Epoch: 9  Training loss = 1.4229  Validation loss = 1.9410  \n",
      "\n",
      "Fold: 31  Epoch: 10  Training loss = 1.4958  Validation loss = 1.8913  \n",
      "\n",
      "Fold: 31  Epoch: 11  Training loss = 1.5487  Validation loss = 1.8061  \n",
      "\n",
      "Fold: 31  Epoch: 12  Training loss = 1.4402  Validation loss = 1.7660  \n",
      "\n",
      "Fold: 31  Epoch: 13  Training loss = 1.4522  Validation loss = 1.9324  \n",
      "\n",
      "Fold: 31  Epoch: 14  Training loss = 1.4203  Validation loss = 1.7226  \n",
      "\n",
      "Fold: 31  Epoch: 15  Training loss = 1.4466  Validation loss = 1.8279  \n",
      "\n",
      "Fold: 31  Epoch: 16  Training loss = 1.4250  Validation loss = 1.7568  \n",
      "\n",
      "Fold: 31  Epoch: 17  Training loss = 1.4040  Validation loss = 1.8149  \n",
      "\n",
      "Fold: 31  Epoch: 18  Training loss = 1.4399  Validation loss = 1.8569  \n",
      "\n",
      "Fold: 31  Epoch: 19  Training loss = 1.4211  Validation loss = 1.4843  \n",
      "\n",
      "Fold: 31  Epoch: 20  Training loss = 1.3788  Validation loss = 1.8806  \n",
      "\n",
      "Fold: 31  Epoch: 21  Training loss = 1.4009  Validation loss = 1.7538  \n",
      "\n",
      "Fold: 31  Epoch: 22  Training loss = 1.4691  Validation loss = 2.1950  \n",
      "\n",
      "Check model:  Fold: 31  Optimal epoch: 5  \n",
      "\n",
      "Fold: 32  Epoch: 1  Training loss = 1.3541  Validation loss = 3.8225  \n",
      "\n",
      "Fold: 32  Epoch: 2  Training loss = 1.2751  Validation loss = 2.8725  \n",
      "\n",
      "Fold: 32  Epoch: 3  Training loss = 1.2525  Validation loss = 2.9860  \n",
      "\n",
      "Fold: 32  Epoch: 4  Training loss = 1.2838  Validation loss = 3.3789  \n",
      "\n",
      "Fold: 32  Epoch: 5  Training loss = 1.2676  Validation loss = 2.9570  \n",
      "\n",
      "Fold: 32  Epoch: 6  Training loss = 1.2806  Validation loss = 3.3899  \n",
      "\n",
      "Fold: 32  Epoch: 7  Training loss = 1.2958  Validation loss = 3.5609  \n",
      "\n",
      "Fold: 32  Epoch: 8  Training loss = 1.2581  Validation loss = 2.9006  \n",
      "\n",
      "Fold: 32  Epoch: 9  Training loss = 1.2630  Validation loss = 2.9502  \n",
      "\n",
      "Fold: 32  Epoch: 10  Training loss = 1.2592  Validation loss = 2.9974  \n",
      "\n",
      "Fold: 32  Epoch: 11  Training loss = 1.2416  Validation loss = 2.8349  \n",
      "\n",
      "Fold: 32  Epoch: 12  Training loss = 1.2574  Validation loss = 2.7656  \n",
      "\n",
      "Fold: 32  Epoch: 13  Training loss = 1.3132  Validation loss = 3.1436  \n",
      "\n",
      "Fold: 32  Epoch: 14  Training loss = 1.2482  Validation loss = 3.2267  \n",
      "\n",
      "Fold: 32  Epoch: 15  Training loss = 1.2559  Validation loss = 2.8351  \n",
      "\n",
      "Fold: 32  Epoch: 16  Training loss = 1.2515  Validation loss = 2.7599  \n",
      "\n",
      "Fold: 32  Epoch: 17  Training loss = 1.3131  Validation loss = 3.7016  \n",
      "\n",
      "Check model:  Fold: 32  Optimal epoch: 16  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 2. Train model\n",
    "# ==================================\n",
    "sess = tf.InteractiveSession()  # Launch Graph\n",
    "sess.run(tf.global_variables_initializer())  # Initialise all variables\n",
    "\n",
    "print(\"Start training\", \n",
    "      \"\\nHyperparameters:\",\n",
    "      \"\\nDimension of recurrent unit =\", n_hidden,\n",
    "      \"\\nLearning rate =\", learning_rate,\n",
    "      \"\\nEpochs =\", epochs,\n",
    "      \"\\nBatch size =\", batch_size,\n",
    "      \"\\nEarly stopping epochs =\", early_stop_iters,\n",
    "      \"\\nLearning rate =\", learning_rate)\n",
    "\n",
    "total_batch = int(window_length / batch_size)\n",
    "validation_fold_error = [] # store validation error of each fold\n",
    "optimal_epochs = []\n",
    "\n",
    "# loop through folds\n",
    "for fold in range(1, len(valIndex)):\n",
    "    validx = train_features[valIndex[fold-1]:valIndex[fold]]\n",
    "    validy = train_target[valIndex[fold-1]:valIndex[fold]]\n",
    "    trainx = train_features[(fold-1)*4:valIndex[(fold-1)]]\n",
    "    trainy = train_target[(fold-1)*4:valIndex[(fold-1)]]\n",
    "\n",
    "    loss_list = [] # store validation loss after each epoch for early stopping\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(total_batch):\n",
    "            # Backprop\n",
    "            batch_xs, batch_ys = next_batch(num=batch_size, data=trainx, labels=trainy)\n",
    "            optimizer.run(feed_dict={x:batch_xs, y:batch_ys, lr:learning_rate})\n",
    "\n",
    "        # Loss\n",
    "        loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "        loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "        loss_list.append(loss_valid)\n",
    "\n",
    "        print(\"Fold: {0:d}\".format(fold),\n",
    "              \" Epoch: {0:d}\".format(epoch+1),\n",
    "              \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "              \" Validation loss = {0:.4f}\".format(np.sqrt(loss_valid)),\n",
    "              \" \\n\")\n",
    "\n",
    "        if all(j <= loss_valid for j in loss_list[-early_stop_iters:]) and len(loss_list)>early_stop_iters:\n",
    "            break\n",
    "    \n",
    "    epoch_hat = np.argmin(loss_list) + 1\n",
    "    optimal_epochs.append(epoch_hat) # store optimal number of epochs for each fold\n",
    "    \n",
    "    # RMSE\n",
    "    loss_train = sess.run(loss, feed_dict={x:trainx, y:trainy})\n",
    "    loss_valid = sess.run(loss, feed_dict={x:validx, y:validy})\n",
    "    validation_fold_error.append(np.sqrt(loss_valid))\n",
    "    \n",
    "    print(\"Check model:\",\n",
    "          \" Fold: {0:d}\".format(fold),\n",
    "          \" Optimal epoch: {0:d}\".format(epoch_hat),\n",
    "          \" \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average number of epochs: 17\n",
      "Average validation error: 3.02566\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 3. Optimal epoch choice\n",
    "# ==================================\n",
    "# Pick number of epochs to train model for out-of-sample testing\n",
    "epoch_hat = int(np.mean(optimal_epochs))\n",
    "print(\"\\nAverage number of epochs:\", epoch_hat)\n",
    "# Average validation error\n",
    "print(\"Average validation error:\", np.mean(validation_fold_error))\n",
    "\n",
    "# Save validated model\n",
    "saveModel(sess, \n",
    "          MODEL_FILENAME + \"lr_\" + str(learning_rate) + \"/\" + str(epoch_hat) + \"_validated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed scheme:\n",
      "Epoch: 1  Training loss = 1.4168  Test loss = 3.1773  \n",
      "\n",
      "Epoch: 2  Training loss = 1.3458  Test loss = 3.0889  \n",
      "\n",
      "Epoch: 3  Training loss = 1.3019  Test loss = 3.0287  \n",
      "\n",
      "Epoch: 4  Training loss = 1.2731  Test loss = 2.9849  \n",
      "\n",
      "Epoch: 5  Training loss = 1.2533  Test loss = 2.9519  \n",
      "\n",
      "Epoch: 6  Training loss = 1.2392  Test loss = 2.9266  \n",
      "\n",
      "Epoch: 7  Training loss = 1.2289  Test loss = 2.9071  \n",
      "\n",
      "Epoch: 8  Training loss = 1.2212  Test loss = 2.8919  \n",
      "\n",
      "Epoch: 9  Training loss = 1.2152  Test loss = 2.8802  \n",
      "\n",
      "Epoch: 10  Training loss = 1.2106  Test loss = 2.8710  \n",
      "\n",
      "Epoch: 11  Training loss = 1.2068  Test loss = 2.8637  \n",
      "\n",
      "Epoch: 12  Training loss = 1.2036  Test loss = 2.8579  \n",
      "\n",
      "Epoch: 13  Training loss = 1.2009  Test loss = 2.8533  \n",
      "\n",
      "Epoch: 14  Training loss = 1.1985  Test loss = 2.8495  \n",
      "\n",
      "Epoch: 15  Training loss = 1.1963  Test loss = 2.8463  \n",
      "\n",
      "Epoch: 16  Training loss = 1.1944  Test loss = 2.8436  \n",
      "\n",
      "Epoch: 17  Training loss = 1.1926  Test loss = 2.8414  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# 4. Fixed scheme\n",
    "# ==================================\n",
    "print(\"\\nFixed scheme:\")\n",
    "for epoch in range(epoch_hat):\n",
    "    x_train = train_features[-window_length:]\n",
    "    y_train = train_target[-window_length:]\n",
    "    optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:test_features, y:test_target})\n",
    "    print(\"Epoch: {0:d}\".format(epoch+1),\n",
    "          \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "          \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "          \" \\n\")\n",
    "\n",
    "# Forecasts\n",
    "yhat_test_fixed = pred.eval(feed_dict={x:test_features})\n",
    "yhat_train_fixed = pred.eval(feed_dict={x:x_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGBe7mVlhXOxmZoX5L4OLOcFkFmrKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc1ff198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train set\n",
    "plt.plot(y_train, 'black')\n",
    "plt.plot(yhat_train_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXd4VGXa/z/PJBMSIIQQINQ0INTQBARpS3FFEUEBxQau\nvaC7vqhbLLvq6uvPta3o64piAbGAgqCAAsIqHUJPgAQSQi8hhRRInfP745kzmZnMZCbJlJTnc11c\nCVPOnDmZ+Z77fO/7uW+haRoKhUKhaDgY/L0DCoVCofAsStgVCoWigaGEXaFQKBoYStgVCoWigaGE\nXaFQKBoYStgVCoWigaGEXaFQKBoYStgVCoWigaGEXaFQKBoYgf540datW2sxMTH+eGmFQqGot+za\nteuipmltXD3OL8IeExNDYmKiP15aoVAo6i1CiOPuPE5ZMQqFQtHAUMKuUCgUDQwl7AqFQtHAUMKu\nUCgUDQwl7AqFQtHAUMKuUCgUDQwl7AqFQtHAUMJeG44fhx9/9PdeKBQKhQ1K2GvDm2/C1Kmg5sYq\nFIo6hBL22pCeDiUlkJvr7z1RKBQKC0rYa0NGhvx54YJfd0OhUCisUcJeUzRNCbtCoaiTKGGvKRcv\nQmGh/F0Ju8Jkgi++gAcegKIif++NopGjhL2m6NE6KGFv7GzdCsOGwd13w8cfU7J6tb/3SNHI8Yiw\nCyFaCiG+FUIcFkIcEkIM88R26zRK2D2OyWQiKyvL37vhPpcuwe23wzXXwKlTfDpiBIVA0dKl/t4z\nRSPHUxH7v4GfNE3rAfQDDnlou3UXXdiDgyEz06+70lD44osviImJIT8/39+74h5vvgnffAPPP0/5\nwYP8OSWFdUDI+vWqBFbhV2ot7EKIMGAUMB9A07QSTdMafv1fRga0agXR0Spi9xCHDh2ioKCAtLQ0\nf++KazQNvvwSxo6Fl15iW1ISmZmZrASMZ87AwYP+3kNFI8YTEXsskAl8KoTYI4T4WAjRzAPbrdsc\nOwYxMdC2rRJ2D3Hu3DkA0tPT/bwnbrBzJ6SlwR13ALBixQoALO76qlX+2S+FAs8IeyAwEPhA07QB\nQCHwF/sHCSEeFEIkCiESMxuCdZGRoYTdw5w/fx6oJ8L+5ZcQFAS33ALA8uXLadKkCaeAwq5dYeVK\n/+6folHjCWE/BZzSNG27+f/fIoXeBk3T5mmaNkjTtEFt2ricxVq30WvYlbB7FF3Yjx075uc9cUF5\nOXz9NUycCC1bkpKSQkpKChMmTAAge+hQ2LRJJlcVCj9Qa2HXNO0ccFII0d180zigYRuMmZlw5UqF\nsGdnQ1mZv/eq3lNvrJgNG+D8eYsNs3z5cgCmTp0KQObgwVL816yxfV5+vrRonCVWi4vh55+9ttuK\nxoOnqmIeBxYJIfYD/YFXPbTduoleERMbK4Vd06A+lenVQUwmExfMVz51PmL/8ksIDZURO1LYBwwY\nQJcuXQC42LUrhIfb+uyaJk8EEyfCunWOt/uvf8GECXD0qLffgaKB4xFh1zRtr9lm6atp2hRN03I8\nsd06iy7sMTGg20rKjnHN6dPwxhtwqHI1bHZ2NmVlZTRt2pRjx45hMpn8sINuUFQE330nvfWQEC5c\nuMDWrVuZPHkyRqMRgBKTCa67Tgq7/j7efVe2eA4IgA8+qLzdsjL48EP5u/UaCYWiBqiVpzVBjyij\no2XEDkrY3eG99+Dpp6FXLxg8GObOtawB0P31wYMHU1JSwtmzZ32/f5cvw3//W/VjVq2CvDyLDfPj\njz+iaRqTJ08mKCgIgNLSUhmZX7gAu3bJf08/DTfdBP/zP7BihTzJWbNyJZw6JX+3v0+hqCZK2GtC\nRgZERMjLcSXs7rN9O/TpA2+9JSPUJ56Abt3g+HGLsA8bJhct+8Vn//xzGDMG9u93/pgvv5R/87Fj\nAWnDREVF0a9fv4qIvaREWipCyAVMM2ZAZCR88gk89JD03z/+2Ha7H3wA7drJ35WwK2qJEvaaoFfE\ngBJ2dykvl7Xfo0fDk0/Cnj3y/yUlMGeOJXGqC7tffPbUVPnz228d33/pkrRTbrsNAgO5fPkya9eu\n5aabbkIIYRH20tJSaN0arr5ark5NT5cnhIgI6NJF2jQffVSRcE9Lk0nThx+Wi970yF2hqCFK2GuC\ntbCHh0vf1B+1+VlZsHGj71+3Jhw6BAUFUux0Bg2CZ5+F776jyW+/ATBkyBCEEP6J2PWTyXffOb7/\nu+9k5crttwOwdu1arly5wuTJkwFsrRiwJFf5xz9g5MiK7TzyiIzK9bGKH34oP0MPPAAdO6qIXVFr\nlLBXF72GPTZW/t9gkAlUf0Ts//63tA7qwwSn7eZlDtbCDjBnDnTpwsglS2hmNBIZGUnHjh39E7Gn\np8u/58GDDhO8fPQR9OgBQ4cCsGjRIlq3bs3o0aMBbK0YgMcek8/5299stzNxInTqJO2XoiJp0UyZ\nAh06yNuVsCtqiRL26nL+vPwy6hE7+E/Yjx+XFseOHb5/7eqyfTu0bAldu9reHhwM//43bbOz+UvT\npgghiIuL833ErmkyYjevJK0Ute/fD9u2wYMPghDk5OSwYsUK7rjjDoug21gxIK/m7r9fRuPWBAbK\n6HzNGnjtNXnl9cgj8r6OHZUVo6g1Stiri3Wpo46/Vp+eOSN/bt3q+9euLtu3w5AhMiK2Z+JEtrdp\nw5P5+XDmDLGxsb6P2LOypFU0fLjsrW4v7PPmQZMmMHMmAEuWLKG4uJiZ5v+DAyumKu67Twr+iy9C\nfLwlGUvHjvKzpEf9CkUNUMJeXeqisG/b5vvXrg6FhZCUVNmGseKVNm0wAtxzDw+kpTH39GlMPXtC\n377S1/Y2+okkNhamToW9e8nasYO5c+dSnpcHCxfCtGkyAQosWLCAXr16MXBgRfeMSlZMVXTsCGZv\nnocflhU0IK0YTQN/lHsqGgxK2KuLLuzR0RW3+UvYdS9227aKhTB1kV275P5VIeyJOTn81K8frF3L\n0K1b6Q6UGAxw4ID0vr2N/hpxcVLYgeSXXuKJJ57g18cek7XrDz0EQFpaGps3b2bmzJkIXZBxYMW4\n4m9/g3Hj4J57Km7r2FH+VD67ohYoYa8uGRmylK1584rb2raVfUB8OeuysFCW38XHy+RpSorvXru6\n6InTIUMc3q23E9gxYQKcOMG2X36hN7DnD3+QDzh+3Pv7aB2xx8TAVVcRtXMnAKFffUVZt24wYgQA\nCxcuRAjBnXfeabOJwMBAoBrCftVVsr1AeHjFbZ06yZ9K2BW1QAl7ddH7sFuj17L7suRRt2GmTZM/\n67LPvn27FEwnXT2zs7MpLy+nbbt20Lkzsd26AZCqWxq+WGJ/7JjtCXvqVGIuXODOli0ZXF7ON2Fh\nIASaprFgwQLGjRtHJ12Ezei17G5ZMc7QI3aVQFXUAiXs1cW61FHHH/1idGH/3e9kxFeXffbt26u0\nYfTFSe3MKy/btWtHcHAwSVlZYDR6TdjPnTvH+++/j6Zp0oqx/rua7Zj3L1+mNCCAxxMT+fXXX9m0\naRPHjh2zSZpaYzQa3Y/YHREeLiuFfBCxZ2dnM3/+fP/15dE0NWnKSyhhrw4mk7QFnEXsnhb2LVvg\nqacc36d/8Tt1kqLpi4hd02S74upw5oyMPqsQdr2dQGRkJAAGg4GYmBjSMzIgKsprVszf/vY3Zs+e\nzdGjR2XEHhdnuU/r1o0kIQgrKUFMn05YTAyPPvoo8+fPp1mzZtx8880OtxkUFFQ7YRdC/k29HLHn\n5uby+9//nvvvv5+dZsvJ5/z6K/TuDcnJ/nn9BowS9upw/rys0PCVsH/8sVySnuOgWaYesXfsKMvz\nkpO9P9jhww+l6BQUuP8cvcbejYhdF3agopY9JsYrEfu5c+dYtGgRACkHD8KJEzYR+7lz51hi7pse\n+NhjvPvuuxw8eJDPP/+cqVOn0tw6x2JFra0Y8Prq0/z8fK6//np2794NyFmzfkH/u5444Z/Xb8A0\nbmEvKIAjR9x/vKNSR/Cex75nj/zpqKb79Glo1kw2Ihs2TEbT3o68vvtODhXZtMn952zfLhfkDBjg\n9CF6xK5bMQCxsbGkp6ejeSlif++99yyR9dnERCgttRH29PR03gQSn38ehg9n0qRJ3HTTTQBObRjw\ngBUDXhX2wsJCJk6cyM6dO1m8eDFBQUH+E/aLF+XP7Gz/vH4DpnEL+wsvyKqSBx5wHBXb40zYmzeX\nvqgnI/aSkopLVEflfmfOSAEQQlabCOFdO+bKlYq+NK5a25r58ccf2fnee5j69pXHxwnnz58nKCiI\nsLAwy21xcXHk5eVxpV07WdPtwYqjwsJC/u///o8pU6YQERFB3t69+otaHnPs2DEKgWa3326pMZ83\nbx7vv/8+Y8aMcbrtWlsxUNFWwNmkpRpSVFTE5MmT2bx5M4sWLWLatGl069aNw4cPe/R13EYXdjWk\nxuM0bmHfulV20/v0U9kD5Ouvq/4yWfdht0YIz9eyHzoko0hwLuwdOsjfw8Jkj3NvCvuWLdKGCgmR\no+HcYOWKFfQoKCCvR48qH3fu3DkiIyNtasJjzdHz+SZN5A0evFz/7LPPyMnJYc6cOcTHx1OmX7VZ\nRez6ytcYq5N4ZGQkjz76KAZHq2fNeMyKKSmpED4PsWDBAn755Rfmz5/PbbfdBkCPHj38H7ErYfc4\nHhN2IUSAEGKPEOJHT23Tq5SVwb59MGsWJCbKJN3tt8Nddzl/TlKSjKaaNat8n6f7xeg2jMHgWNhP\nn64QdpB2zLZtVZ6Yjh49ygeOpve4w9q10lJ56CG54Cgvr/JjduyQ1SQvvwy//ELZ1q2EAsfbt69y\n0+fPn7exYUBG7AAZ+g0esmPKy8t5++23ufrqq7nmmmvo3r07QadPy+McFWV5XHp6Ou3btyckJKRa\n2/eIFeOlWvbvv/+euLg4Zs2aZbmtR48epKenU+yL1b32KCvGa3gyYv8j4NtTf3Ky8xarrjh8WNoL\nAwdC//5SFB95RPbNdibQ27ZZOvtVwtMR+9690LSp9KbthV3TKqwYnaFDpZ2k9xR3wNy5c3n00Ue5\nfPly9fdn3Tp58pg0STYec9Qu+P/9Pzkd6O9/h/Hj+SgpCYADLsTx/PnzNolTqIjYD+kWjIcSqCtW\nrCAtLY05c+YghCA+Pp42BQWYOnaUpZVmjh07Zjm5VAePWDHu1rJnZ8tWB25YNgUFBfzyyy9MnjzZ\n5sqoZ8+elJeXk5aWVps9rhl6TkpF7B7HI8IuhOgETAQ+dvVYj3Hlimx1escdNfNfzRUB6L0+AgIq\novXNmys//vx5OHaMHc4uw70h7H37yglD9sKenS1tEfuIHaq0Yw4cOADIUrdqkZUlj9f48fJ1goIq\n2zG5uXK822OPQU4O5z7/nH8gp5rvys+vcvO6FWNNixYtiIiIkLXsAQEei9jffPNNYmJiLOWK3bt3\nJxYo1BPgZtLT0y0nl+rgMSsGqo7YL16UjcNmzpRXni5Ys2YNJSUllgSwTg+zTeYXO6YhROwnTlSv\nSsxHeCpifwd4BnC60kEI8aAQIlEIkZjpieqRV16R09xLSqQIVpfdu2VE3L17xW1XXSU7+DkSdvMC\noD8tXuy4pWzbtjIC8UTCS9Pke+rfXyb0jh+vmLYDtqWOOj16SK/dyUIlTdMswp7jTqLYmvXr5T5d\ne6302IcNqyzsy5bJk80dd0BYGDvDw3kR+HtgIOlVdGo0mUxkZmZWsmJARu1HMzKgc2ePROzbt29n\n8+bNPPnkk5bl//Hx8cQCmVbliyUlJZw6darGwl7riL1dO2kNOYvYMzOlqJv/nu60k1ixYgXh4eGM\nMLdF0Olu/vz7JYFa1zz20lLZG9/dtRpvvCHzbS1aQM+ecOed8P77FbkxP1JrYRdC3Ahc0DRtV1WP\n0zRtnqZpgzRNG9TGydJyt0lOlpf9EybI/9dk1eWuXVI4rXtlN2kip/o4EPayTZsoAfYgk1CVaNtW\nXjlUcfZevnw5t912m1zpWBUZGbImfcAAKexlZbZfcj2Ss47YDYYqFyqdP3+ei+YvUrWFfd06WVY5\neLD8/5gxMgdgvZ0vv5Rj38yP0U8io0aNqvIyPysri/Ly8koRO0if/ZjewsENYS8rK7O8R0dsNR+b\nGTNmWG7r2rEjHYDjVldiJ06cQNM0/1kxgYFS3B1F7OfPy+N/9Ch8/728rQr7DWRe4ccff2TixImW\nE5pO8+bN6dy5s++Fvays4vNTVyL2tWtlO+Wnn3b92K++ko+bPFlOyIqPh99+g9mz5dB2P+OJiH04\ncJMQIgP4GhgrhPjCA9t1jMkkE3hhYdJfjIqqfjWIySSFyarlqoURI6To2521L/38M3uBZhERfP75\n55WXYbuxSOmtt95i8eLF7HV1haHfr0fsIOdi6jiK2AFGjZJRnIPBG7rQQg2FfcwYKTggf9e0Cp/9\n7FkZ1d9xh6U0cP/+/cTExNC/f39Zj+7kZOZocZJObGwsx48fx+Sill3TNL7//nsSEhKIjo4m34n1\nc8m8gKtVq1aW24LNr3/Q6u+tX5H5zYoBx5OUsrJkpJ6eLsfqTZokP/8uhH3Lli1kZWVVsmF0/FIZ\nk5MjP0NGo/ci9pIS+L//cz+C1k9u778PP/3k/HEbNsiii9Gj5bDyF16A5cvh5En593n99eqv0PYw\ntRZ2TdP+qmlaJ03TYoAZwHpN06ooLaklH38sI+o33pBNm4YOrX7EfuSI7I541VWV7xs+XH4QEhMr\nbisro/mhQyQajbz++utkZGTwm3lGpwXzVchX//63w5fMzMxkk3lhzw8//FD1/u3ZIyPwPn0qhN3a\n/tG/8PbVJrNnyyj+3nsr9TDfv3+/5fdqeezp6fLftddW3Hb11bIuXbdjFi+WJ0vzLFCQJ5KEhAS6\ndOnClStXOOukv7ijxUk63bp1o7S0lJwWLeR7diCYmzdvZsSIEdx8882cPHmSy5cvc8HJyTUvL49m\nzZrZRq1mm2iX1clOL3X0W8QOjicpffih7K2ycmXFYI74eJfCvmLFCoxGI9ddd53D+3v27Mnhw4dd\nX0l6Ev3KqksXeXVqbTV6ip9+kjmfdevce3xqquzV06cP/OEPjstNDxyQub34eGk/6uW4Oi+8AOfO\nSZ3yI/Wrjv3cOfjzn2XjK71ka+hQmcDQo1hrvv8eHn208u32iVNrrrlG/rRaXWnav58mZWWUDBjA\njBkzCA0N5fPPP7d5WknLlgB8PXcuyQ56X6xYsQKTyURkZKRrYd+7V3rmTZvKyC0w0FbYz5yRJzX7\nD1VYmPzyJyfDq6/a3HXgwAGaNm0KVDNi178U48dX3NakiTwB6sL+5Zfy6qJnTwCKi4tJSUmhb9++\nFnF0Nuquqoi9f//+8rnl5TK6sxO65cuXM2LECI4dO8a8efP47LPPACngjrh06RItWrSwvdEs4htP\nnbIIW3p6OkajkQ7WVpebeCxid7T6dNkyeVK1XiClC3sVorxixQrGjh1b+b2b6dGjB4WFhZzyZUdJ\nXTTj4+XP6l5FusPRo/KnuzZTSor83n3xhbyKeOihiuOqabB6NVx/vVyQuHq1bbtlndGj5ZXza6/5\nto23HR4Vdk3T/qtp2o2e3KYNf/kLXL4M//lPxcQZvfxQ7/ltzWuvyYHB9lUDu3ZJcTILkQ0REfKP\na+Wzn/jmGwCib7uNpk2bctttt7FkyRIKrPz0eWa/s50QfPrpp5U2u2zZMmJiYnjiiSdITEzkjKMT\nkY6eOAWZA4iJqSzszkRn4kRZ3fPqqzbv+8CBAww1H6tqC3unTrZJZpAn1337pO2zY4e0YcwcPnyY\n8vJyS8QOOPXZ7RuAWdO7d28CAwPZp/fAsfPZV65cScuWLTly5AgPPPAAEebpRpec9MzJy8uzWd0K\nQHo6ZUYjRwsLLSeZY8eOERMTQ4D9rFI38EjyFOQxv3SpImdz4oS8irRvPqb343eSW0hJSSE1NdWp\nDQMyYgcfJ1D1/dU/V96wY/TPnIsrGgupqfJ49usH//wnLF0Kn30mA8TBg+GGG2SQtXq1TOg744UX\n5HfUgQ74ivoVsf/jH/D557YiM2CA9Ons7ZhTpyrE3j7ZuXu3/ONZ1S3bMHy4XGlp9tFzVq/mHDDa\nfJUwa9YsCgsL+c5cQ3/y5ElemDsXgFE9e7JgwQKbL3d+fj5r167l5ptvZtKkSYBcbu+QrCzp1enC\nDtKOsbdiqoom33lHrqi9914oK6O8vJyDBw/Sv39/QkND3Rf28nL45RcZrZt7kZeXl8v79KhRvyKy\nSkjqto/ueRsMhiqFvUmTJpUFFwgODqZnz55s0U+CdsKemJjIoEGDaGZeMKZvw5mwO4vYi8yWVqpZ\nAI4dO1Yjfx1qZ8WkpKRw++23yxOMfcmjnih1JOzgVLyWL18OYPncOUIvefSLsOv7740Eqv6dcWcI\njXnerkVb5syRkfe998pjnpsL8+fL49y3b9XbGjtWXvn/7//6bXZt/RL2mBgbAQGk1ztwYGVh178I\n/frBokUVHp6mSWF3ZMPojBghLw3NH/TwlBSORkTQyhwRDh8+nC5duljsmKeeeoormoYpNJRhXbuS\nmZnJypUrLZtbvXo1JSUl3HzzzfTp04eYmBjndoyeOLVummUv7PaLk+yJiJAJoN274Y03OHr0KEVF\nRSQkJBAeHu6+x753r/zCmW2YhQsXEhYWJrsiDh4sraJdu+QXwCqCOXDgAEFBQXTr1o2goCCioqKc\nCrujdgLW9O/fn19SUmTOwSqBWlRURFJSEoMGDbLcpot2tSL2Y8cwmK8qUswCUNMadqidFfPll1/y\n9ddfc/3111Og76cu7MuWyRa3uhDquBD2FStWMGDAADpXEWFGRkYSFhbm2wSqLyN2d4Rdbyuh709A\ngAwIp06V1szhw1LkzQPLq0QIGbWfPCkDUT9Qv4TdGUOHys6G1gmYpUtl/5S//12WiK1dK29PT5eX\nuFUJ+/Dh8ufmzRzfvZuYkhI0q7azQgjuueceNmzYwKeffsrixYv561//iiEykpimTWnfvj3z58+3\nPH7ZsmW0adOGa665BiEEkyZNYt26dY5XgOrC3q9fxW1xcVJgc3Plezx/vuqIHeRkpVtugRdf5JD5\nykUXdrcj9vXr5c9x4wDYsGEDhYWF3HXXXfzx6acx6cfJyoYBKey9evWyzACNi4urMmJ3ZMPo9O/f\nnxNnz1Levr1NxH7gwAFKS0tthL3aEbt5wEZIz54EBweTmprKpUuXyM7OrlHiFGpnxWzcuJHIyEiS\nkpJ4+J//lDeeOiXr1n/7Tf497YmOlleeDoQ9MzOTLVu2MFkfmu0EIYQlgeozLl6UrTn0AMXTEXt5\nufy8BAfLQMjFIjmL+FufOKOj4dtvZX26XZmoS37/e9mc79VXpX3sYxqOsF+5UrFg4+JF+UW4+Wbp\ni7VqVWHH6IlTRxUxOl27yiqXzZvZ++GHAMTaiZc+yPi+++4jNjaWp59+Gtq0wZCZyaxZs1i1ahVn\nzpyhuLiYlStXctNNN1k820mTJlFUVMT6VasqRxN798oPu3Wtvy4yx45JUTeZqo7YdZ54AoqKKPrx\nRwwGA7169aqesO/cKRtjmStWkpOTGTVqFE8++STvvvsu76alYWrWzDJtSEeviNHp0qWL0+Spoz4x\n1ugJ1LzwcJuIPdFctVQdYa8UsefkQF4eIi6Obt26kZKSYqmI8bUVU1JSwrZt25gxYwafffYZS80l\nq6aTJ+GHH+Tf3NFwj8BAWVniQNjXr1+PpmlMnDjR5ev7vOTx4kVZAKCXnno6Yj95Ula3/e538v+u\n2nOnpMhIu2tXz7y+EPDii/Lk0rEjzJ7NqR9+YPr06U6rtjxJwxF2qKhn/+EHeca+5RaZJJ0xQ1oz\nly5JYTca5WWtM4SQUfumTeStWUM50GnKFJuHREVFMXbsWDRN4+2335bNosxtBf7whz9gMplYuHAh\n69evJz8/32bizujRowkNDaXZCy/IBO5HH1VseM+eyr3LrUserRYnLVq0iAEDBjgXkmuugbAwWm/f\nTrdu3QgJCaFly5buC7uVZWUymTh48CD9+vXjrbfe4quvvuL5s2fpGxbGJatcRXZ2NqdPn64k7JmZ\nmQ7ryx21E7Cmn/nK5UxQkE3EnpiYSOvWrYmyatwVFBREcHCw+1Ux+orYuDi6d+9OampqrUodoeZW\nzO7du7ly5QojR47kzjvv5NW33yYb+O2rr+TVZ0yMbd7FGiclj4cOHUIIYfO3cEbPnj05d+5c9dtN\n1BRd2MPCpO3h6YhdDySuv17+dGXHpKbKCL2K9tLVZsIE+PVXTBMmUPbhh3S66SaeXbqUjC+/9Nxr\nOKFhCHt0NERGVvjsS5fK23SBnDlTlh59+60Uqz59LKWCTkVx+HBIS6NfRgbn27Z12NHxtdde4/XX\nX6+oOGjbFs6eJT4qihEjRvDJJ5+wdOlSmjdvzjiznQFSgCaPH8+gw4fRgoPhwQflpKQrV6SXZ/8F\nthZ2q8VJq1evZu/evfz666+O34PRCNddR7/Tp+nbpw+A+xF7bq70KM1XNidPnqSwsJDe5hPijBkz\nWPPLLySfOcNbb71leZq+EMpe2KFyZUx5eTmZmZlVCntERASdO3cmtbhY2hJmu01PnNp48x98wImS\nEp786CNZd/+HP8hEsjnpW1BQYBux68IeG0t8fDzp6emWBGptPPaaROz6ugh9yf+f/vQniiMiKE9O\nxrRmjYzWneQh6NZNRqR2i+ZSU1OJiYmhiX1ZrAP0BGpKVQJ45IhckKMn0GuDLuxCyLJBT0fs+mft\nuuvka7gS9pSUypVfHmBPaCiDU1NpU1bGhwkJ9OrZkyH6inkv0jCEXYiKhUr5+bBmjYzW9S/CkCHy\nj/b55zLZZxarN954g6CgIJo1a0ZUVBT9+/dn3LhxTJ8+ndfMdex9AYPuJdsxaNAgnn766QpxueYa\n+YHt3Zt/DBhAamoqCxYs4IYbbiDYLhJ4uE0bQjWNlNdfh+nT5WzTmTPll8Ze2MPC5CWrtbB36MBB\n8yDg7/VEsQOKx4+nTXk515oTv1UlT1etWsXo0aMpKyurZFnptfm9ra50hg0bxvTp03nrrbfQ+/84\nEnY9+rXpmq5OAAAgAElEQVQX9qysLEwmU5VWDEg7Zk92tjw2p09z+fJlkpOTbWwYPvsMHn2U40FB\nHG/eXJYJrloFTz4JycmWqwWbiH3/fpmU7dpV9mUvK2P9+vW0aNGCcEc1ym5QUytm48aNxMfH25zk\nQrp2ZQxgKC117K/rxMfLBWknT9rcnJqaSrx9stUJesljJTvm1CkZdAwaJF9nxoxKayRqhC7sIJP9\n3hB2o1FaK9HRVZc8alpFqaMHOXz4MEOGDOH06dPMW7yYB/ftIygpSZZTe5mGIewgG1MdOSLbDJSU\n2H4RhJCiuXGj/ACZ7YXVq1cTHR3Nww8/zLhx44iKiqKoqIjk5GTe27IFfVFw2ypqgG245x55UmnS\nhHFz57IuIIA4czWMPUMOHCAVWHTunOw7ce+98ooCHI+R0ytjTp+GgADKIyIsX8Lvv//e6arB5Kgo\nTMAIs+8cHh5OYWGhQ/H573//y2+//SZFfJe59Y/5WOknkV69etk856WXXuLy5cu89tprgBT2Vq1a\n2Szu0SN2e5+9qsVJ1gwYMIBt5seSkcG+ffsoLy+vEPalS2WPj2uv5YlevXixTx9py23ZIu/fuNFi\nz9hE7Bs3ymPdvLmlGdavv/5KXFyc0yodVxiNRsrLyyu3nKgCk8nE5s2bGTlypM3tTbp0wQBc1scf\nOsNBZYymaaSkpFjelytiY2MxGo22CdSffpIW0FNPye/Qm2/CrbfKsmNHbZurg7Wwt2rlHSsmNlba\nPN27Vx2xnz0rAwEPR+w//vgjZWVlbN++nenTp9f4M1UTGo6w6z77Sy9JS8T+i2A9QGPgQDRNY9eu\nXUyYMIE333yTTz/9lBUrVrB582YOHjzIqQsXCDZ/0ZxF7A659lq5cOeddxgWEMAOYKLZBrFw5AjG\nrVvZEBPDDz/+KD98H31UsarWfvQeVAj7mTPQvj0ZJ05QVFTEqFGjOH36NLt2Oe7BtufUKbYDseaT\nQEvzCllHdoye1NmxY4eM2KOiLF++5ORk2rVrZ9NnBeQl/KxZs3j//fc5deqUJXFq/SEOCwsjIiKi\nUsRe1eIka/r370+6fuI6ftw2cbpmjYwihw6FZctoGh5ekTyNi5OJ302bLLdZIvaSErnOwWx96JFt\ncXFxjW0YwFIJVJ2oPTk5mZycnErCHmw+Ie6NjrZtVmePA2E/d+4cBQUFbkfsgYGBtmPySkvhj3+U\nEW9qqkyk/8//yKXycXGyEqqmUXZJiRzU4u2IXbcwu3evenWuftw8LOzr16+nR48eRNtPXPMBDUfY\nBw2Sl9Xnz8teDvZfhKgouagmIAD69iUtLY1Lly7ZXs7bIaZMkX/s6mbKjUb44x8xJSbSrGlTQv/8\nZ9sP1SefQEAARbfdxr59+6Q1YjDIlbIbNsjf7enSRSYPT5yADh0s1sgzzzxDQECAUzvmwIEDrDEa\nCd6/H86ft1gMVQn79u3bbSwrkBG7fbSu88ILL2AymXjxxRcrVcRU7H4Xp8LujhVjMRkyMkhMTKRd\nu3Z0OHZMes+9esn+Kc2aERYWViHsQsDIkY4j9t27ZU7DLKatWrWitVloapo4BWnFQPWEfaM5+rUX\ndmGuPV9j1VLYIe3byxyQlbDruQJ3hR2kHWOxYubNk9t74w3p4euEhkqf/cIFmcOoSX8ZXcSthd2T\nEbumSWE3nxiJj5cRubPV3o5KHWtJaWkpGzdurHI+rjdpOMLerFnFijBnfuSbb8oPbEiIw3K5SvzP\n/8hkZg0voZonJGB49VXp9X79tbyxrEx6/TfcQLdRowDbBl1O0dv3JiZCx44WYR8xYgSjRo1yKuz7\n9+/nqP6BXbXKIuyOfHZd2A9u3SptLbOwa5rGwYMHbfx1a2JiYnjooYf4+OOPKSgocCjsjmrZ9+7d\ni8FgoL2L0XkxMTEEt2hBbrNmFmG/vWtXxA03yKX3P/8M5iuRsLAw26qYkSPh5EmKzeVulohd7wVk\ndTWmi6AnIvbqVMZs3LiRDh06VH7dadP4uHdvlrmqwRaiUmWMngR114oBefWVlpZGSWamtFvGjJEt\nKuwZOBD+9S9ZfWZece2MckeJVn1xkrUV42bErgcDVZKdLSvgdGHXj4Eznz01Vc4Z0EcSeoDExEQK\nCgoYqzdr8zENR9hBfhDbtLFtkmTNgAHSy0Ye+CZNmjgVK48xe7ZM3j7xhPxAr14tPb1777XUaO9z\nYwKO5bIyL8+SOO3UqRNhYWFMmTKF5ORkjtjV6urDNZpcfbVc0LRypVsRe7AetVlVxBQUFDiN2AGe\nffZZy3zQvg6WXHfp0oUTJ05YItn8/Hw+/vhjpk6dSmhoaJVvXQghFyoZDJSlp2M8eJBXEhOlMPzy\ni6yIMmMTsYMlIg8xn8gtEfumTfJKzOpqQRfB2kTs1bViNE1j48aNjBw5srIHGx5O8rXXkpaR4brz\nop2wp6amEhwcTKdqiFWvXr0oLy8n++mnpdC++abzoObxx2Xb4DlzZEnhW2/JZLTVfm7YsIHQ0NDK\nCVl7YY+IkN1WHc1dtdre+vXrad++veUKxyl6Lsde2J357Ckp8qqkiiHl1WWDuUHe7/Q6eh/TsIT9\nlVfkh8uNZb+JiYn079/f8kX0GgEB0pfMzZVfgvnzpRBNnEj79u1p06aN6/7sUCHsYInY9ZOSvrJQ\n7wuiow/XSOjbV0Zea9bQ0kmHR03TuHDhAr1796a//mUyJ04dVcTY065dO+bMmUOzZs3oY59TQAp7\neXk5J06cAODjjz/m0qVLPPXUU67fO9KOOXz5Mqa9e1kLssPeL79UirJatGhBfn5+RaSYkAAtWhBm\nvipq0aKFLAvctMki+jqeiNira8VkZGRw+vTpSjaMTlxcHIWFhbicOhYfL606szimpqbSrVs3DNUQ\nq+uuu45uRiMRX3wBd9/tOImvI4S88pw9W77unDlytXRUlOzCipyxe+XKFdmCwhpHETtUtmP27ZPW\nj9n3X7p0KZqmMW/evKrfiH5lqH9nOnaU7S+qEnYv+Ot9+/a12Hu+pmEJe0iITQTmDJPJxK5du6q2\nYTxJQoLsTLlggbx8nTkTjEaEEPTr18+9iF1v3wuY2rXj0KFDlgg6OjqaAQMGsGzZMpun2JQeTpwI\n+flEmqN6e2HPz8+nuLiYiRMnchWQFxZmGR7irCLGnhdffJG0tDSaO/CErWvZS0tLeeeddxg1ahRD\nhgxx/d6Rwn60vJygS5coA/KXLZNVD3boEbllMVRAAFxzDW3N0WxYWJj8ImdlWRKnOnfffTcvvfRS\ntXxpe6prxejRp/3IOh1XbY8txMfLE5b5cdWpiNGJiIjg0w4dKCsvp+i551w/ITwc3n4bDh2SpZav\nvWZpvnf+/HlLP6QlS5bYXnE4itihsrBv2yYj+dWr0TTN0n/p22+/rXohlb2wGwwyIndkxZSUyPUM\nHvTXi4uL2bx5s99sGGhowu4mqampFBQU+E7YAZ59VkYFJpPFDgIpWElJSa4jvMBAWY8LnAsIoKio\nyCaCvvnmm9m6daulhPDKlSuW/uQJCQmy30tQEC3M3nJuTo7sQWJOKOk2TJ8+fRgaGMhhqwVZycnJ\nREZGWtriOsNgMDitcLGuZV+yZAknTpxwO1oHeZx+A1INBu5s25a2ToTQYVuBkSNpff48rYWQnSD1\nS3m7KLljx448//zz1Ypy7amuFbNx40bCwsIcXuVAxdWDW8IOkJpKaWkp6enp1T9B7djB8OPHeRNY\nUt3hNZ06wcMPy98PH2bhwoWUlZXx+OOPk5qaajPByyLs+ufJWVsBvZ/6r7+SkpJCRkYG999/P0VF\nRXyt56wckZYmE8rmq1PAYcmjpmnsWrJEro+oYcTuyCLbtm0bRUVFfkucQiMVdrcSp54mOFjWqf/n\nPzYLFPr162cZTOESszimmKNR6wh6ypQpaJrGDz/8wI4dOxg4cCBffvklc+bMoU2bNtK6GDOGwC++\nYJsQ/Omf/5QRedeukJFhEfb2zZsTW1bGxsJCy7atbZ+a0qFDB5o0aUJaWhpvvPEGPXr0cKuHiU7v\n3r1ZbzTS3WSiRRU13Q6F3XwSuLZpU+ljb9pU8d49THWtmI0bNzJ8+HCnvd9jzKWvLoVdr1xJTSUj\nI4OysrLqCbvJBE88gRYZyXdxca7tDkeEhUH79miHDjF//nyGDRvGs88+i8Fg4Ft9jQZIYQ8Lq2ib\nrQu8vbDrkffGjaw0R//PP/88CQkJNk32KpGeXuGv63TvLiNzKx9/y5YtvKiXQddA2I8cOULr1q0t\nfrrO+vXrMRgMjDIXR/gDTwyz7iyE2CCEOCiESBZC/NETO+ZNEhMTadq0qWUZtc/o00dOZbGiJgnU\nvWa/1VrY+/TpQ1xcHP/4xz8YNmwYBQUFrFmzhjfeeKPi+Q88AKGhlBqN7IiJkbMZNQ2ee84i7FFZ\nWRiAXy5d4ty5c5aKGFc2jCsMBgNxcXF89dVX7Nmzhzlz5lQrMg4KCrLsQ1UnZF3YbSpjhgyh1GBg\ntP56GzfKaN0LC0aqY8VcuHCBlJQUp/46QFNzt1CXwh4eLgsHUlNrVBHDokWwfTvitde44+GH2bRp\nU6WkZ2FhId9++23VidyePSnYuZPDhw9z3333ERkZyahRo2ztGOvFSeDcY09Lk1eq2dkc/vZb+vTp\nQ1RUFPfddx+JiYnOq8msa9h19Ktlq8qss2fPYjlCNbBiduzYQXZ2No888gjFVieMDRs2MHDgQMua\nEX/giYi9DJijaVovYCjwmBCidirgZRITExkwYEClie3+oHv37jRp0sS9BOrUqXDnnew+etRSEaMj\nhGDatGmcOXOGWbNmkZSUxLXWc0r15x87xkNduzK3Vy85Zf3JJ2HRIkzmboJtzU3GdiM/uHpFjCeq\nh7p06cKZM2eIjIzkLusFY26inwTdEXabiD04mKMtWzKsrEx6wBkZlfx1T1EdK2aruWldVcIO0sbS\nm5NVibkypto17Pn5cnHckCEwcyazZs3CaDTykVVzupKSEm655RamT59eed6vNT16EHDkCM2aNuXW\nW28FYPr06Rw+fLhiZKS9sJsj9py0NK677jpOnjxZUYt+oxzI1iwxkRtuuAGAO++8E6PR6HBSGUVF\ncnW2fcTuYBFXTk4O8cB54FgNRvPpVWgpKSmWfkmFhYVs27bNr/46eGaY9VlN03abf88HDgFu9JT1\nPAsXLuS6666zDI12RFlZGXv27PGtDVMFRqOR3r17uxexX3stfPEFB60Sp9boC4Q++eQThxOJdGz6\nxfz5z9C6NQPNnmVoaiqm9u25GBDAjh073E6cuoPusz/++OOVeue4w/DhwwkODq7yb+ds2Mbe5s3p\nVVQkV6pCJX/dU1THijltPol2dWEJxcXFuY7YwUbYIyIiKq0Sdsqrr8oS3HffBYOBtm3bMmXKFBYs\nWEBRUREmk4l7772XNeZjV5WwF8XG0rS0lAdvuslSxnrLLbcghGDJkiXyQfbC3qwZBAVxev9+1qxZ\nw/PPPy8XGhYWwrhxXG7dmhEmk0XYW7duzZQpU1i4cKFNpAxIu0XTHFsxYOOz5+bm0h1IBccnCRcc\nOXKE6OhopkyZwssvv8yJEyfYvHkzpaWlfvXXwcMeuxAiBhgAOBhA6l3Kysp49tlnWbNmDSNHjmTq\n1Kkc1ZMvVhw+fJjLly/XGWEHGYnu3bvXrSnxJpOJQ4cOOYygg4ODnSbhrLHp8BgWBn//OzHHjnFb\ns2YE7NmDYdAg+vTpw44dO9wqdXSXkSNH0rlzZx7Wk2zV5N577+Xo0aNVlpA568m+LSiIQE2T9dbN\nm9sOMvEg1bFi9JOrq0v2uLg4Tp486Xqb8fFw7hwnk5Pdj9aPHpXHZOZMOSjbzAMPPEBWVhbLli3j\nqaeeYtGiRbzyyiv069evSmFfb07e36sPhUeWwo4aNarCZ7cXdiGgVStKzM9dsGAB6foJuEsX9rVs\nyWghuMYqt3LvvfeSlZVVeRKZfgK0t2JatJAVc1bCnpOTQ3egsGNHPv30U8eLqarg6NGjdOvWjXfe\neQeQHTk3bNhAYGCg0yonX+ExYRdCNAe+A/6kaVqlhthCiAeFEIlCiESXNbk1YPny5Zw8eZIvv/yS\nl19+mZ9//plevXrx3HPP2QimXxKnLujXrx+ZmZmWipaqyMjI4MqVK7WKoCv1ZH/oIc40b87/lpTI\nmuGrruLqq69m586dJCUl0bZtW5cVMe4wbdo0jh8/XuNtBQQE0NHFgBFnwr6xvBwTQHKy7CPkJRuu\nOlZMTk4OISEhLtvqxsXFoWkax60GjTjEXHcek5Tkvr8+Z45c92Fu4qYzbtw4YmNjefzxx3n77bd5\n/PHH+etf/8rIkSPZsmWL0/f3H3Misbdd/mTatGkcPHhQXgHaCztARARaVhYtW7akRYsWrHj7bQC0\nLl34NjOTNpqG0cofv/baa+nUqROffPKJ7Xb0x9hH7FDRM8ZM0blzRAIdxozh1KlTlisSdzly5Ahd\nu3YlOjqa559/nmXLljFv3jyuvvpqhyW/vsQjwi6EMCJFfZGmaUsdPUbTtHmapg3SNG1QG+vpQB5i\n7ty5xMTEcOutt/Lcc89x9OhRZsyYwSuvvMKH5ilIALt27aJ58+a1qlX2NLp37I7P7okIulJPdqOR\nD6KjiS0tlZexAwcyZMgQcnNzWbVqlUdX53q7w11ISAiBgYGVhm2cLizkjG5NeDGaqo4Vk5ub61aC\nze2Sx2uvpbxnTx7LzSXenYqfLVtgxQp47jlZHmiFwWDg/vvvJysri1tvvZV33nkHIQSjRo3i8uXL\n7Nmzp9LmDh06xA+7d1PcpAnCbsze1KlTEUKwbNEiOSrOXthbtSIwN5f4+HieeeYZ8vbuRTMYOJCX\nx3L9JG01dyAgIIB77rmHn3/+WXryOmlp8orMkcbEx1dE7BcuMM7c/bPn5Mm0bt266kobO7KyssjJ\nyaGbuRppzpw5dO/enezsbL/bMOCZqhgBzAcOaZr2lqvHe4P9+/fz66+/8uijj1rKxtq1a8dnn33G\nhAkT+NOf/mT5ICYmJnLVVVfVqlbZ0+hTgqoj7Hr/7JoQHh5OXl6eTWvZpSYTh/RI+qqrLAuH9NWo\n9QUhROW2AsgI/oQ+bclL/jpUz4rJyclxS9jdXqRkMHDi7rvpBYxxp6nWf/8rfzqxxp588kk+//xz\nFixYYPm+6IleR3bMV199hcFgQPTsKRctWdG+fXtGjBjBBt1ndxCxN7l8mejoaP74xz/SJziYs0Yj\nP65ZQxpQ3q6djbAD3HfffZVXourNvxwFEN27y6uF++6D6GiuT0nhl/BwjDfeyMyZM1m+fLnbY+v0\nxKku7EFBQXzwwQc0adKESZMmubUNb+IJdRsO3A2MFULsNf+7wQPbdZv33nuPkJAQ7rvvPpvbDQYD\nCxcupHXr1tx6661kZWWxd+/eOmXDgLQPYmJi3EqgHjx4kI4dO9aqlCo8PBxN02zE70JmJl9fdx28\n9x506ECvXr3kYh48kzj1JS1atLB5b8XFxXKtwNVXyyHDeotnL1AdKyY3N9etgR7t27enSZMmblXG\n7IiO5jDQ78cfK01UqsTu3bL+3UmiPSQkhJkzZ9pYRe3atSM+Pr6SsGuaxuLFixk9ejRBffta2gBY\nM336dLL1fkZ2wq6FhxNaUkJMTAzNmjVjeLt2JBcX8+abbzJw4EACxoyRwm5lq8bExHDjjTcyb968\niiRqenplf11HD4a++ALuuosZCQm8PngwBAdz3333UVZWxsKFC6s6Yhb0/F03q86XY8aMIT8/3+3V\n1N7EE1UxmzRNE5qm9dU0rb/53ypP7Jw7ZGdn88UXX3DnnXc6rAJo3bo1X3/9NceOHWPChAkUFRXV\nOWGHigSqKzyxWMi+J3tZWRlZWVmIbt3gsccAeal7lbkJWH2K2KFyIzDdlino3Vt2gjQ3K/MG3rBi\nDAYDsbGxblXGpBw9yqtASGqqbF9RFbt2WfoBVYeRI0eyadMmmyu+AwcOkJKSIksce/SQJYd2dtjw\n4cOxyLmdsBeGhBABRJuvqtrm53OxRQuys7NlNcyoUbJyx65D6OzZs7lw4YJMzOotFRz56yDH5H3x\nhSx3/egj9hUXW06svXr1YtiwYXz88cduFTEcOXLE8nexxuu9p9yk7vgRNeSTTz7hypUrPP74404f\nM2LECF555ZU6mTjV6devH6mpqRRarfi0R6+IqW0Ebd/hMSsrC03TaGvuDaMz1BzZ1reI3ZmwV1UC\n6imqa8W4O4LP3ZLHlJQUNkdFSXF76SXn/dKzs6XA1UDYR40aRU5OTkVdOrB48WIMBgO33HJLRWRs\nt5q6W7duToU9GwgGYtu1g9xcRFYWCVOmYDAY5ASy0aPlA+3smPHjx9O9e3fee+892R6juNi5sAcE\nwJ13WvIJ9lbY/fffz+HDh9m5c6fLY3DkyBGioqLcmifrD+q1sJeXl/P+++8zatQoh61irXn66ae5\n8cYbad++vaUhVV2if//+aJpGUlKS08foFTG1jaDte7LrvqK9sM+ZM4fly5f7rUNdTbEX9krTk7xI\nda0Ydy01vZ+9q2gyNTWVrj16wN/+Jq2W1asdP1BPftZQ2KHCZ9dtmDFjxsjPkL6i285nDw0NJU5v\n0Wz3mTpvPl5xYWGWqLzPlClcvHiRgQMHym22bVtJ2A0GA4899hjbtm0j44MP5I1uJI41Tat0Yp08\neTJCCH7++WeXz9dLHesq9VrYV65cSUZGBrNnz3b5WIPBwNKlS9m3b59PZw+6izutBfQIydMRuzNh\nb9u2LTe5O++1DmE/bMOXEbu7VozJZHLbYwdZGZOXl+ewj76OpmkVA6zvvls2jXv5ZcdRuz6svKrW\nvE6Ijo6mU6dOFmHft28fR44c4bbbbpMP6NJFlpM68Nm7hYfLslO7932qqAiATk2b2pQsWo6PENKO\nsRN2gFmzZnFrcDCd//d/ZWTvRo+WwsJCysrKbI5/REQEAwcOZN26dVU+V9M0S6ljXaXeCntJSQnP\nPvusZeWXOxiNRrxRaukJoqOjCQsLq9Jn95Sw23vszoS9vlIXInZXVkxBQQEmk6laETtUXRlz/vx5\n8vLypLAbjfDMM7L1raN5uLt3S+GvwZoCvexx48aNaJrGN998Q0BAQMXQdqNRRs32AzaAqGbNyBGi\n0ujK4+aTb/Pi4oqujvZJ0NGj5WjIBQtkR0YzLXbs4IuSEnYBFz/91K15DPrVqv2Jddy4cWzdupWC\nggKnz83KyiI3N1dF7N7gX//6F0lJScydO7fOJCxqg96bvSph37NnD9HR0bVuLuRuxF5fadGiBXl5\neRbbwh8eu6uIXT/2nhR2vUeMZXHS9Oky0l3loJZh9+4a2TA6o0aN4uzZsxw9epTFixczbtw4W8uu\nZ0+HEXt7o5FMTavUT/2IXp6ZnS0j9nbtZD26NdOnQ+/eMGuW/LlggYzgJ0+mvGtXJmgaH3/zjVv7\n7+z4jx8/3jKv1BmOKmLqGvVS2FNTU3n55ZeZPn16nagZ9RT9+/dn//79Tpc279q1y1KpUhuaNWtG\nYGCgjbAHBgb6tRudJwkLC6O8vNySiPZlxO6uFeMsYnSGXn1RVcmjbuNZrujatJFtAswDKizk5ckV\nmLUUdoB33nmH9PR0S8MvCz16yMjb7jhEaBoXodIYx8N6/XhWlu0gamsiI+WEtCVLZBvsWbPgd7+D\nTp0I/u03BowdywcffEBZWZnL/dc/+/bHf8SIETRp0oRffvnF6XPta9jrIvVO2E0mEw8++CAhISG8\n++67/t4djzJ48GAKCwstjbesyc3NJS0tzSPCLoSwaQR24cIF2rRpU6cWbdUG+7YCvozY9QVyrqwY\nd/vE6ISGhtKmTZsqI/ZNmzbRuXNn2zmnN9wAO3eC9cIbPY9TC2Hv0aMHrVu35j//+Q+BgYGV7dCe\nPeXwdbvyxNDi4krCrmka+80N0cjKkicEZwUOBgNMmyaTv8uXy6E169ZBZCSPPPKIpRGXK5ydWENC\nQhg+fHiVPruzUse6RL37Js+fP59ff/2Vf/3rX7RzYwxefUIvL9TbuVqz25zs8lSppnW/mAsXLjQY\nGwYqC/ulS5cICgrySWmaEAKj0ei2FeNuxA5VlzxqmsamTZsqN5+64QaZPLWu9NATp7UIEoQQjBw5\nEpPJxPjx4yv3/9FLHu189ib5+WRRYRuBFNmLBQWUBgXJ+vfTp11XtggBN90kZwh37gzIOnnAeZ92\nK6qywsaPH8++ffucrkI9evQo0dHRlquzuki9EvazZ8/y9NNP87vf/a7SKtOGQJcuXWjdujXbHIwl\n22VOgHkiYgfbfjENVdj1SD0vL88n0bpOUFCQ21ZMdeyvqoT9+PHjnDlzprKwDxggLQxrn333bujQ\nQd5eC3Q7ppINAxVtcq19dk1DXLxIaViYjbDrzc1KQ0PBvNbEacReBe3atSMiIsJ2DJ8Tqjqxjh8/\nHpCTkBxx5MiROm3DQD0T9r/85S8UFRUxb968OlmyWFuEEAwdOtRhxJ6YmEh0dLRHuixC4xB264jd\nF/66jtFodGnF1CRij42N5fjx4w49ZH0GQSVhNxjg+uvhp5+kNQK1Tpzq3H777cyePZvp06dXvjM0\nVM5BtY7Y8/OhtJSAyEiHwk6rVqAXD9RA2IUQJCQkuCXs+onV0Qlfn37kyI6pD6WOUM+E/ZVXXmHR\nokV1/mxZG4YOHcrhw4cr1St7KnGqY++xNyRhtx+24euI3R0rJjc3FyFEtU44cXFxlJeXk2bnW4MU\n9hYtWjhevHbDDZCbK0sfL1+Ggwc9IuyRkZHMnTvXeYvaHj1sI3bzEOuQqChSU1MtVUsZGRkABEZG\ngn5CrKFwJiQkkJSUZNPuwBE5OTm0aNHC4azZgIAAxo4dy9q1aystCMvKyuLSpUt1XoPqlbB36tSJ\nqVOn+ns3vMow8zCBHeZRdeDZxKmO7rFfvnyZgoKCBiXsjiL2umbF6MJSnYS1Pm7t+++/r3Tfpk2b\nuOzsLisAABUfSURBVOaaaxwPxf7972Xd+KpVsqrEZPKIsLtEL3nUxdE8h6Flly7k5+dz/vx5QEbs\nISEhGPWcWVhYxRzUapKQkEBBQYHL3vWu2jmMGzeOEydOVDqJ1oeKGKhnwt4YGDx4MAaDwcaO0ROn\nno7Yc3JyGlwNO9QPK6Y6q051YmNjufrqq/nGrlZb79vidGpPWJjsQb9qVUXi1BfC3qOHtF9GjZKL\njcz7F2G+qtBF8vjx40RHRyN0MXfWdtcN9AliruwYV8df99nt7Rgl7IoaERoaSp8+fWwSqJ5OnIIU\n9vLycksyriEJe/PmzRFC+M2KcTdir8m6gdtuu409e/bYeNRbzAMjqhzHdsMNssxxxQrZp8W6JNJb\njB0rk6jl5XJq1TPPwBdf0M48u1R/D7qwW1bB1sK/dlfYXR3/bt260blzZ4fCbjAYiImJqfE++gIl\n7HWQoUOHsm3bNotPuGvXLqKjoz3ajEuPVvQvV0MSdoPBYFl9Cv6J2N3x2KsbsYOsQBFC2ETtmzZt\nIjAwkMGDBzt/ollM+flnGa37ovhA99i3bIFFi+CVV+DOO4mKiSEoKKiysFtH7DUkNDSUmJgYt4S9\nquMvhGD8+PGsX7/eZsHg0aNHiTHvf11GCXsdZNiwYVy6dIkUc9tTfeqTJ9GjFf01GpKwQ8WwDU3T\n/JI8dceKqUnE3rFjR0aOHMlXX31lSext2rSJq666iqZNmzp/Yu/elnpvn9gwVRAQEEDXrl0tbaov\nXrxoG7HXsvuqO5Ux7pxYx48fT05ODv/+978tx7o+lDqC52aeThBCpAghjgoh/uKJbTZmrBcqeSNx\nChURuy7sdbU5Wk3RG4EVFhZiMpl8GrF704oBacccOnSIpKQkiouL2blzZ9U2DMgIXY/a/SzsAPHx\n8aSmpnLixAlATkOy1NXXch5xQkICqampFVOVHOBOL/ybb76Z66+/njlz5jBp0iQuXLhQL0odwTMz\nTwOA94HrgV7A7UKI+jWZoY4RHx9PeHg427Zt80riFGyFvVmzZpYxeA0FXdh92U5Ax5tWDMC0adMw\nGAx888037Nq1i+LiYtfCDnDXXbKnuReHebtLfHw8aWlplt430dHRsnpnyZJa719CQgJlZWWWoMWe\nkpISLl++7PLEGhISwsqVK5k7dy7r1q2jV69e5OXlNZqIfQhwVNO0dE3TSoCvgcke2G6jxWAwcPXV\nV7N161avJE6hQtgzMjIanA0DFcLuywZgOq6smNLSUgoLC2scsbdt25axY8fy9ddfW7oQXnPNNa6f\nOGIEnD9vmSDkT7p160ZxcbFl/6Ojo2UP92nTau3/JyQkAM4TqNVZHCaEYPbs2SQmJtKhQwegdoPk\nfYUnhL0jcNLq/6fMtylqwdChQ0lOTmbDhg1ERUV5fIqRLiomk6lBC7s/InZXVkx1Ozs6YsaMGaSl\npfHhhx8SHx9f7/6G8Wa7Ze3atQQGBtLegyeb+Ph4jEajU2GvyfHv06cPO3bs4KeffuLaa6/1yH56\nE58lT4UQDwohEoUQiZnmhQoK5wwbNgxN0/jpp588Hq2DFDq9LUN9EwV30Kco1cWIvbq92B1x8803\nYzQaOXbsmHs2TB1DF/bdu3fTuXNnxwuraojRaKRnz54uI/bqHv/g4GCuu+66etHOxBPCfhrobPX/\nTubbbNA0bZ6maYM0TRvU0BJ13mDIkCGA7E3hDWE3GAyWKLYhCrteFVMXPXZPROytWrXi97//PeCi\nfr2OEhkZSWhoKJqmSRvGw/Tp08cjVkx9xRPCvhPoJoSIFUIEATOAFR7YbqOmZcuWloEJnmrVa4/+\nwW6Iwh4WFkZJSYllZW1dqoqpSWdHR9x7770EBQUxZsyYWm3HHwghLFG7N4Q9ISGBkydPVprUBJ45\nsdZ1ai3smqaVAbOBn4FDwGJN05Jru11FRdmjNyJ2qBCWhirsgKWcri7VsXvCigG45ZZbyMzMrPOr\nIJ2hV5d4S9gBkpKSKt3nqeNflwn0xEY0TVsFOBisqKgNc+bMYcCAAR5PnOroEUtkLfty10V0IT95\nUub1Q0NDffbavrBidHx5JeJpvB2xg6yMsbeqGoMV4xFhV3iHXr16Vcyv9AIN3YoBGbE3b97co8k5\nV7iyYhpDxOgO3hT2zp07ExYW5tBnz83NJSQkxCcTtfyFainQiGkMwn7y5EmfR7WurJjc3FyCgoII\nCQnx4V7VPSZNmsTzzz/PyJEjPb5tIYTTBGptVv3WF5SwN2Iasseui/np06d96q+DaytGF5b6UDbn\nTVq0aMFLL73ktYZa+tAN+2EZ7rQTqO8oYW/EJCQk0LFjR4+N26tL6GJeVlbm84jdnaqYhi4sdYGE\nhARyc3M5fdq2+roxHH8l7I2YmTNncurUKQIDG16qxTpK90fE7sqKaehWQF1AT6Du37/f5nZlxSgU\n9RTrKN0fHrvJZHI6d7MxCEtdwNnQDWXFKBT1lMDAQEvHSl9H7Lpn7MyOaQxWQF0gPDyczp07VxL2\nxnD8lbArGix6pO6PiB1waseoiN13JCQk2FgxJpOJS5cuKWFXKOoreqTuD48dHEfsmqY1ioixrtC3\nb18OHz5sOcnqU7Ua+olVCbuiwaILuj+qYsCxsBcWFlJWVtbghaWukJCQQGlpqWXoRmNYdQpK2BUN\nGH9H7I6smMbQgKou0bdvX6AigdpYjr8SdkWDxV8Re1VWjGon4Fu6d++O0Wi0+OyN5fgrYVc0WPwV\nsVdlxXiqZa/CPYxGIz169LBE7MqKUSjqOXWxKqaxWAF1ib59+1oi9sZy/JWwKxos/vbYlRVTN0hI\nSODUqVPk5OQ0muOvhF3RYNF74Pg6OnPHimnoEWNdwjqBmpOTQ0BAAM2bN/fzXnmXhtckRKEwc9dd\nd9GxY0efDxKpyorRI0ZfX0U0ZqyHbuhrCBp6Z81aRexCiH8JIQ4LIfYLIZYJIRr29Y2iXhEWFsaU\nKVN8/rpVWTG5ubk0b968QTZeq6t07NiR8PBw9u/f32hW/dbWilkL9NE0rS+QCvy19rukUNRvqrJi\nGkMDqrqGEIKEhASLFdMYjn+thF3TtDXmYdYA24BOtd8lhaJ+46oqpjFEjHWNvn37kpSURHZ2thL2\nanIvsNrZnUKIB4UQiUKIxMzMTA++rEJRt3BlxTQGYalrJCQkkJ+fT3JycqM4/i6FXQixTgiR5ODf\nZKvHPAuUAYucbUfTtHmapg3SNG1QmzZtPLP3CkUdxJUVoyJ236NXxly+fLlRHH+XGRxN08ZXdb8Q\n4h7gRmCcZj9cUKFohLiyYhpDxFjX6N27t+X3xnD8a1sVMwF4BrhJ07TLntklhaJ+42qBUmOIGOsa\noaGhxMXFAUrY3eE9IBRYK4TYK4T4jwf2SaGo1zizYsrKysjPz1fC7if0evbGcPxrVUyraVpXT+2I\nQtFQcGbFXLp0CWgcEWNdJCEhgeXLlzeK469aCigUHsZZxK46O/oXPYGqhF2hUFQbZx676hPjX268\n8UZeffVVRo4c6e9d8TpqXbNC4WGcWTGNpbNgXSUkJIS//rVxLI5XEbtC4WECAgIQQqiIXeE3lLAr\nFB5GCIHRaHQq7Kqzo8LbKGFXKLyA0WisZMXk5+cDvp/opGh8KGFXKLxAUFBQpYg9Ly8PoMEPeVD4\nHyXsCoUXcGTF5OfnExISonqxK7yOEnaFwgs4s2KUDaPwBUrYFQov4MyKCQ0N9dMeKRoTStgVCi/g\nzIpRwq7wBUrYFQov4MiKycvLU1aMwicoYVcovIAjK0ZF7ApfoYRdofACzqwYFbErfIESdoXCCziz\nYlTErvAFStgVCi+grBiFP/GIsAsh5gghNCFEa09sT6Go79hH7KWlpRQVFSkrRuETai3sQojOwO+B\nE7XfHYWiYWDvset9YlTErvAFnojY30YOtNY8sC2FokFgb8UoYVf4kloJuxBiMnBa07R9HtofhaJB\nYG/FqM6OCl/ishuREGId0M7BXc8Cf0PaMC4RQjwIPAgQFRVVjV1UKOof9laM3tlRRewKX+BS2DVN\nG+/odiFEAhAL7BNCAHQCdgshhmiads7BduYB8wAGDRqkbBtFg8aZFaMidoUvqHH/UE3TDgBt9f8L\nITKAQZqmXfTAfikU9Rp7K0ZF7ApfourYFQovoKpiFP7EYx3/NU2L8dS2FIr6jrJiFP5ERewKhRdQ\nVozCnyhhVyi8gCMrJjg4GKPR6Me9UjQWlLArFF4gKCgIk8lEeXk5oPrEKHyLEnaFwgvokbketavO\njgpfooRdofAC9sKuerErfIkSdoXCCwQFBQG2wq4idoWvUMKuUHgBPWLXK2OUFaPwJUrYFQovoKwY\nhT9Rwq5QeAF7K0ZF7ApfooRdofAC9laMitgVvkQJu0LhBaytmPLyci5fvqwidoXPUMKuUHgBaytG\nNQBT+Bol7AqFF7C2YlQDMIWvUcKuUHgBaytGRewKX6OEXaHwAtZWjOrsqPA1HuvHrlAoKrC2YvRG\nYMqKUfiKWkfsQojHhRCHhRDJQojXPbFTCkV9R0XsCn9Sq4hdCDEGmAz00zStWAjR1tVzFIrGgCOP\nXUXsCl9R24j9EeA1TdOKATRNu1D7XVIo6j+OqmJUxK7wFbUV9nhgpBBiuxDiVyHEYE/slEJR31FW\njMKfuLRihBDrgHYO7nrW/PxWwFBgMLBYCBGnaZrmYDsPAg8CREVF1WafFYo6j70VExQURJMmTfy8\nV4rGgkth1zRtvLP7hBCPAEvNQr5DCGECWgOZDrYzD5gHMGjQoErCr1A0JOytGBWtK3xJba2Y74Ex\nAEKIeCAIuFjbnVIo6jv2VoxKnCp8SW3r2D8BPhFCJAElwCxHNoxC0diwt2JUxK7wJbUSdk3TSoC7\nPLQvCkWDQVkxCn+iWgooFF5AWTEKf6KEXaHwAgEBAQghlBWj8AtK2BUKL2E0GikpKVFj8RQ+Rwm7\nQuElgoKCLBG7smIUvkQJu0LhJYxGI8XFxRQUFKiIXeFTlLArFF7CaDSSk5MDqAZgCt+ihF2h8BJB\nQUFkZ2cDqk+MwrcoYVcovITRaCQrKwtQwq7wLUrYFQovYS3syopR+BIl7AqFl1BWjMJfKGFXKLyE\n0WgkNzcXUMKu8C1K2BUKL6H3iwFlxSh8ixJ2hcJL6P1iQEXsCt+ihF2h8BIqYlf4CyXsCoWX0IU9\nMDBQjcVT+BQl7AqFl9CtmNDQUIQQft4bRWNCCbtC4SX0iF3ZMApfUythF0L0F0JsE0LsFUIkCiGG\neGrHFIr6ji7sKnGq8DW1jdhfB17UNK0/8IL5/wqFggorRkXsCl9TW2HXAP1TGwacqeX2FIoGg4rY\nFf6iVsOsgT8BPwsh3kCeJK5x9kAhxIPAgwBRUVG1fFmFou6jhF3hL1wKuxBiHdDOwV3PAuOAJzVN\n+04IcSswHxjvaDuaps0D5gEMGjRIq/EeKxT1BGXFKPyFS2HXNM2hUAMI8f/buZsQq8o4juPfH5m9\nWPiSItJImoniIkcdTEl6MQodwlWLpIULqY0LhSCUgaBlm8pFBGEvBGGhvYmLysxNLbTxrdRp0shQ\nU8ciEYoi69/iPEOXQUbHO+Pz3NPvA4d7nufcmflxn7n/Ofd/7xm9BaxNwy3ApmHKZdbyfMZuuTTb\nY/8JuD/tLwWONvn9zGrDhd1yabbH/iSwUdIo4A9SD93M3IqxfJoq7BHxBbBgmLKY1YrP2C0XX3lq\nNkJ85anl4sJuNkIa/1eM2bXkwm42QtyKsVxc2M1GiN88tVxc2M1GiM/YLRcXdrMR0tnZSVdXFzNm\nzMgdxf5nFHHtr+7v6OiI7u7ua/5zzcxamaS9EdFxufv5jN3MrGZc2M3MasaF3cysZlzYzcxqxoXd\nzKxmXNjNzGrGhd3MrGZc2M3MaibLBUqSzgE/XuWXTwR+HsY4w835muN8zXG+5pWc8Y6ImHS5O2Up\n7M2Q1H0lV17l4nzNcb7mOF/zWiHj5bgVY2ZWMy7sZmY104qF/dXcAS7D+ZrjfM1xvua1QsZBtVyP\n3czMBteKZ+xmZjaIlirskpZJ6pV0TNL6AvK8LqlP0qGGuQmSdkg6mm7HZ8w3VdIuSUckHZa0tqSM\nkm6UtEfSwZTvuTQ/XdLutM7vShqdI19Dzusk7Ze0vbR8ko5L+kbSAUndaa6I9U1ZxknaKulbST2S\nFpeST9Ks9Lj1bxckrSslXzNaprBLug54GVgOzAFWSpqTNxVvAssGzK0HdkbETGBnGudyEXg6IuYA\ni4A16TErJeOfwNKImAu0A8skLQKeB16MiLuAX4HVmfL1Wwv0NIxLy/dgRLQ3fESvlPUF2Ah8HBGz\ngblUj2MR+SKiNz1u7cAC4Hfgg1LyNSUiWmIDFgOfNIw3ABsKyDUNONQw7gWmpP0pQG/ujA3ZPgIe\nLjEjcDOwD7iH6uKQUZda9wy52qie3EuB7YAKy3ccmDhgroj1BcYCP5Deyyst34BMjwBflppvqFvL\nnLEDtwMnGsYn01xpJkfE6bR/BpicM0w/SdOAecBuCsqY2hwHgD5gB/A9cD4iLqa75F7nl4BngH/S\n+DbKyhfAp5L2SnoqzZWyvtOBc8AbqZW1SdKYgvI1ehzYnPZLzDckrVTYW05Uf/Kzf+xI0i3Ae8C6\niLjQeCx3xoj4O6qXwm3AQmB2riwDSXoU6IuIvbmzDGJJRMynalGukXRf48HM6zsKmA+8EhHzgN8Y\n0NbI/fsHkN4jWQFsGXishHxXo5UK+ylgasO4Lc2V5qykKQDpti9nGEnXUxX1tyPi/TRdVEaAiDgP\n7KJqbYyTNCodyrnO9wIrJB0H3qFqx2yknHxExKl020fVH15IOet7EjgZEbvTeCtVoS8lX7/lwL6I\nOJvGpeUbslYq7F8BM9MnEkZTvXTaljnTpWwDVqX9VVR97SwkCXgN6ImIFxoOFZFR0iRJ49L+TVT9\n/x6qAv9Y7nwRsSEi2iJiGtXv2+cR8UQp+SSNkXRr/z5Vn/gQhaxvRJwBTkialaYeAo5QSL4GK/mv\nDQPl5Ru63E3+Ib7B0Ql8R9WH7Sogz2bgNPAX1dnJaqoe7E7gKPAZMCFjviVULyO/Bg6krbOUjMDd\nwP6U7xDwbJq/E9gDHKN6eXxDAWv9ALC9pHwpx8G0He5/TpSyvilLO9Cd1vhDYHxh+cYAvwBjG+aK\nyXe1m688NTOrmVZqxZiZ2RVwYTczqxkXdjOzmnFhNzOrGRd2M7OacWE3M6sZF3Yzs5pxYTczq5l/\nAZhG9k90gBvDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xce00cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: fixed scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_test_fixed, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed scheme RMSE: 2.84135494019 \n",
      "Fixed scheme MAE:  1.97188402012\n"
     ]
    }
   ],
   "source": [
    "rmse_fixed = np.sqrt(np.mean((yhat_test_fixed[:,0]-test_target)**2))\n",
    "mae_fixed = np.mean(np.abs((yhat_test_fixed[:,0]-test_target)))\n",
    "print(\"Fixed scheme RMSE:\", rmse_fixed,\n",
    "     \"\\nFixed scheme MAE: \", mae_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stack training and test set, this makes updating scheme easier to train\n",
    "all_features = np.concatenate((train_features[-window_length:], test_features), axis=0)\n",
    "all_target = np.concatenate((train_target[-window_length:], test_target), axis=0)\n",
    "\n",
    "# Vectors to store loss and forecasts\n",
    "test_loss = np.zeros(len(test_target))\n",
    "yhat_update = np.zeros(len(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test period = 1  Training loss = 1.1926  Test loss = 2.7613  Model updated 0 times  \n",
      "\n",
      "Test period = 2  Training loss = 1.2339  Test loss = 2.7696  Model updated 0 times  \n",
      "\n",
      "Test period = 3  Training loss = 1.2808  Test loss = 0.5845  Model updated 0 times  \n",
      "\n",
      "Test period = 4  Training loss = 1.2825  Test loss = 1.7503  Model updated 0 times  \n",
      "\n",
      "Test period = 5  Training loss = 1.1032  Test loss = 1.2437  Model updated 1 times  \n",
      "\n",
      "Test period = 6  Training loss = 1.1060  Test loss = 1.1355  Model updated 1 times  \n",
      "\n",
      "Test period = 7  Training loss = 1.1026  Test loss = 0.6770  Model updated 1 times  \n",
      "\n",
      "Test period = 8  Training loss = 1.0847  Test loss = 0.3577  Model updated 1 times  \n",
      "\n",
      "Test period = 9  Training loss = 0.9648  Test loss = 0.0073  Model updated 2 times  \n",
      "\n",
      "Test period = 10  Training loss = 0.9637  Test loss = 2.6202  Model updated 2 times  \n",
      "\n",
      "Test period = 11  Training loss = 1.0133  Test loss = 0.9279  Model updated 2 times  \n",
      "\n",
      "Test period = 12  Training loss = 1.0198  Test loss = 0.6022  Model updated 2 times  \n",
      "\n",
      "Test period = 13  Training loss = 0.9626  Test loss = 2.0291  Model updated 3 times  \n",
      "\n",
      "Test period = 14  Training loss = 0.9948  Test loss = 1.0318  Model updated 3 times  \n",
      "\n",
      "Test period = 15  Training loss = 1.0009  Test loss = 1.8001  Model updated 3 times  \n",
      "\n",
      "Test period = 16  Training loss = 1.0027  Test loss = 4.5998  Model updated 3 times  \n",
      "\n",
      "Test period = 17  Training loss = 1.0434  Test loss = 0.5397  Model updated 4 times  \n",
      "\n",
      "Test period = 18  Training loss = 1.0429  Test loss = 0.3195  Model updated 4 times  \n",
      "\n",
      "Test period = 19  Training loss = 1.0436  Test loss = 0.1041  Model updated 4 times  \n",
      "\n",
      "Test period = 20  Training loss = 0.9832  Test loss = 0.2535  Model updated 4 times  \n",
      "\n",
      "Test period = 21  Training loss = 0.8968  Test loss = 1.5202  Model updated 5 times  \n",
      "\n",
      "Test period = 22  Training loss = 0.9002  Test loss = 3.6187  Model updated 5 times  \n",
      "\n",
      "Test period = 23  Training loss = 0.9880  Test loss = 1.7859  Model updated 5 times  \n",
      "\n",
      "Test period = 24  Training loss = 1.0100  Test loss = 1.0341  Model updated 5 times  \n",
      "\n",
      "Test period = 25  Training loss = 0.9517  Test loss = 0.1775  Model updated 6 times  \n",
      "\n",
      "Test period = 26  Training loss = 0.9508  Test loss = 0.8585  Model updated 6 times  \n",
      "\n",
      "Test period = 27  Training loss = 0.9567  Test loss = 1.3352  Model updated 6 times  \n",
      "\n",
      "Test period = 28  Training loss = 0.9663  Test loss = 1.0169  Model updated 6 times  \n",
      "\n",
      "Test period = 29  Training loss = 0.9334  Test loss = 1.9113  Model updated 7 times  \n",
      "\n",
      "Test period = 30  Training loss = 0.9597  Test loss = 1.2588  Model updated 7 times  \n",
      "\n",
      "Test period = 31  Training loss = 0.9702  Test loss = 2.4546  Model updated 7 times  \n",
      "\n",
      "Test period = 32  Training loss = 0.9954  Test loss = 0.3779  Model updated 7 times  \n",
      "\n",
      "Test period = 33  Training loss = 0.9666  Test loss = 0.1396  Model updated 8 times  \n",
      "\n",
      "Test period = 34  Training loss = 0.9668  Test loss = 0.1883  Model updated 8 times  \n",
      "\n",
      "Test period = 35  Training loss = 0.9423  Test loss = 0.1056  Model updated 8 times  \n",
      "\n",
      "Test period = 36  Training loss = 0.9400  Test loss = 5.8369  Model updated 8 times  \n",
      "\n",
      "Test period = 37  Training loss = 1.1363  Test loss = 1.2951  Model updated 9 times  \n",
      "\n",
      "Test period = 38  Training loss = 1.1256  Test loss = 1.7432  Model updated 9 times  \n",
      "\n",
      "Test period = 39  Training loss = 1.1253  Test loss = 1.0554  Model updated 9 times  \n",
      "\n",
      "Test period = 40  Training loss = 1.1313  Test loss = 1.8004  Model updated 9 times  \n",
      "\n",
      "Test period = 41  Training loss = 0.9854  Test loss = 3.5575  Model updated 10 times  \n",
      "\n",
      "Test period = 42  Training loss = 1.0797  Test loss = 1.1961  Model updated 10 times  \n",
      "\n",
      "Test period = 43  Training loss = 1.0890  Test loss = 2.4487  Model updated 10 times  \n",
      "\n",
      "Test period = 44  Training loss = 1.1255  Test loss = 11.3935  Model updated 10 times  \n",
      "\n",
      "Test period = 45  Training loss = 1.8661  Test loss = 8.2273  Model updated 11 times  \n",
      "\n",
      "Test period = 46  Training loss = 2.1256  Test loss = 2.2646  Model updated 11 times  \n",
      "\n",
      "Test period = 47  Training loss = 2.1440  Test loss = 0.6399  Model updated 11 times  \n",
      "\n",
      "Test period = 48  Training loss = 2.1434  Test loss = 0.4100  Model updated 11 times  \n",
      "\n",
      "Test period = 49  Training loss = 1.7998  Test loss = 2.4765  Model updated 12 times  \n",
      "\n",
      "Test period = 50  Training loss = 1.8254  Test loss = 3.5647  Model updated 12 times  \n",
      "\n",
      "Test period = 51  Training loss = 1.8781  Test loss = 0.8467  Model updated 12 times  \n",
      "\n",
      "Test period = 52  Training loss = 1.8790  Test loss = 1.6171  Model updated 12 times  \n",
      "\n",
      "Test period = 53  Training loss = 1.8484  Test loss = 3.9183  Model updated 13 times  \n",
      "\n",
      "Test period = 54  Training loss = 1.9104  Test loss = 3.0098  Model updated 13 times  \n",
      "\n",
      "Test period = 55  Training loss = 1.9458  Test loss = 0.9000  Model updated 13 times  \n",
      "\n",
      "Test period = 56  Training loss = 1.9343  Test loss = 0.2064  Model updated 13 times  \n",
      "\n",
      "Test period = 57  Training loss = 1.8176  Test loss = 0.2446  Model updated 14 times  \n",
      "\n",
      "Test period = 58  Training loss = 1.8174  Test loss = 1.9794  Model updated 14 times  \n",
      "\n",
      "Test period = 59  Training loss = 1.8330  Test loss = 0.2300  Model updated 14 times  \n",
      "\n",
      "Test period = 60  Training loss = 1.8332  Test loss = 1.1350  Model updated 14 times  \n",
      "\n",
      "Test period = 61  Training loss = 1.7841  Test loss = 0.9884  Model updated 15 times  \n",
      "\n",
      "Test period = 62  Training loss = 1.7881  Test loss = 4.2620  Model updated 15 times  \n",
      "\n",
      "Test period = 63  Training loss = 1.8595  Test loss = 1.8078  Model updated 15 times  \n",
      "\n",
      "Test period = 64  Training loss = 1.8707  Test loss = 0.0651  Model updated 15 times  \n",
      "\n",
      "Test period = 65  Training loss = 1.8090  Test loss = 1.4596  Model updated 16 times  \n",
      "\n",
      "Test period = 66  Training loss = 1.8176  Test loss = 0.7310  Model updated 16 times  \n",
      "\n",
      "Test period = 67  Training loss = 1.8180  Test loss = 1.2502  Model updated 16 times  \n",
      "\n",
      "Test period = 68  Training loss = 1.8240  Test loss = 2.3337  Model updated 16 times  \n",
      "\n",
      "Test period = 69  Training loss = 1.8124  Test loss = 4.9765  Model updated 17 times  \n",
      "\n",
      "Test period = 70  Training loss = 1.9131  Test loss = 0.5760  Model updated 17 times  \n",
      "\n",
      "Test period = 71  Training loss = 1.9144  Test loss = 0.9369  Model updated 17 times  \n",
      "\n",
      "Test period = 72  Training loss = 1.9165  Test loss = 2.5838  Model updated 17 times  \n",
      "\n",
      "Test period = 73  Training loss = 1.8477  Test loss = 2.7177  Model updated 18 times  \n",
      "\n",
      "Test period = 74  Training loss = 1.8738  Test loss = 0.8936  Model updated 18 times  \n",
      "\n",
      "Test period = 75  Training loss = 1.8756  Test loss = 0.4346  Model updated 18 times  \n",
      "\n",
      "Test period = 76  Training loss = 1.8752  Test loss = 1.0576  Model updated 18 times  \n",
      "\n",
      "Test period = 77  Training loss = 1.8015  Test loss = 2.1433  Model updated 19 times  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrain_count = 0\n",
    "for t in range(len(test_target)):\n",
    "    x_test = np.reshape(test_features[t], newshape=(1,4,3))\n",
    "    y_test = test_target[t]\n",
    "    test_loss[t] = np.sqrt(sess.run(loss, feed_dict={x:x_test, y:y_test}))\n",
    "    yhat_update[t] = pred.eval(feed_dict={x:x_test})\n",
    "\n",
    "    x_train = all_features[t:(window_length+t)]\n",
    "    y_train = all_target[t:(window_length+t)]\n",
    "\n",
    "    loss_train = sess.run(loss, feed_dict={x:x_train, y:y_train})\n",
    "    loss_test = sess.run(loss, feed_dict={x:x_test, y:y_test})\n",
    "    \n",
    "    print(\"Test period = {0:d}\".format(t+1),\n",
    "      \" Training loss = {0:.4f}\".format(np.sqrt(loss_train)),\n",
    "      \" Test loss = {0:.4f}\".format(np.sqrt(loss_test)),\n",
    "      \" Model updated {0:d} times\".format(retrain_count),\n",
    "      \" \\n\")\n",
    "\n",
    "    if (t+1)%4==0:\n",
    "        retrain_count += 1\n",
    "        for epoch in range(epoch_hat):\n",
    "            for i in range(total_batch):\n",
    "                optimizer.run(feed_dict={x:x_train, y:y_train, lr:learning_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXlcVXX+/5+fCxcUEFARTQxBcUtcSAVTx8rKzEotc6Y9\nbSptsaZ1Zmyab9OvZpqmZfp+WyYrK7NGyyyXSrPccl9wCXdMxQUVUQFBtnvP74/PPZe73wvcy4XL\n5/l4+BDucu65l3Nf531e7+UjNE1DoVAoFKGDIdg7oFAoFAr/ooRdoVAoQgwl7AqFQhFiKGFXKBSK\nEEMJu0KhUIQYStgVCoUixFDCrlAoFCGGEnaFQqEIMZSwKxQKRYgRHowXTUhI0FJSUoLx0gqFQtFk\n2bJly2lN09p5e1xQhD0lJYXNmzcH46UVCoWiySKEOOzL45QVo1AoFCGGEnaFQqEIMZSwKxQKRYih\nhF2hUChCDCXsCoVCEWIoYVcoFIoQQwm7QqFQhBhK2BUKP1FUVMQnn3wS7N1QKJSwKxT+4s0332Ti\nxIkcPXo02LuiaOb4RdiFEPFCiLlCiD1CiN1CiMv8sV1F82LHjh1MmTIFk8kU7F2pE4sWLQLgwoUL\nQd4TRXPHXxH7m8BiTdN6Av2A3X7arqIZMX/+fN577z0OH/apa7pRkZ+fz6ZNmwCoqqoK8t4omjv1\nFnYhRBwwHPgQQNO0Sk3TztV3u4rmx8mTJwE4dOhQcHekDnz77bfWn5WwK4KNPyL2VKAA+EgIsVUI\n8YEQItoP21U0M06dOgXAwYMHg7wntUe3YUAJuyL4+EPYw4FLgXc1TcsASoE/OT5ICPGAEGKzEGJz\nQUGBH15WEWo01Yi9vLycpUuX0rVrV0AJuyL4+EPYjwJHNU3bYPl9LlLo7dA0bbqmaQM1TRvYrp3X\nccJNgyNH4Pvvg70XIYMu7E0tYl++fDllZWXcfPPNgBJ2RfCpt7BrmnYCOCKE6GG56SpgV3232yT4\n+99h3Dgwm4O9JyFBU43YFy5cSHR0NCNHjgSgsrIyyHukaO74a6GNqcBnQogI4Fdgkp+227j55Reo\nrISCAmjfPth706SprKzk3DmZc29KEbumaSxatIhrrrmGmJgYQEXsiuDjl3JHTdO2WWyWvpqmjdM0\n7aw/ttuo0TTIyZE/Hz8e3H0JAfTEaadOnTh+/Djl5eVB3iPf2LFjB0eOHOHGG2/EaDQCStgVwUd1\nntaVY8egqEj+rIS93ug2TFZWFgB5eXnB3B2f0athRo8erYRd0WhQwl5X9GgdlLD7AT1i14W9qdgx\nCxcuJDMzkw4dOhAREQF4EPYVK+Cii8ByElMoAoUS9rqihN2vOEbsTSGBevLkSTZu3MgNN9wAYI3Y\n3SZPN26EEydg4cKG2kVFM0UJe13JyYGOHSExUQm7H9CFvX///hiNxiYRsS9cuBBN05yE3W3Erg8H\ns2lmUigCgRL2uvLLL5CeLsVdCXu9OXXqFFFRUcTGxtK5c+cmEbF//vnnpKWl0b9/f8AHYT92TP6/\ndCk0keSwommihL0umEywa5cSdj9y8uRJ2ltKRlNSUhp9xH706FFWrFjBnXfeiRAC8FHYo6KgrEz6\n7QpFgFDCXhd+/VVGXErY/YatsKempjZ6Yf/vf/+Lpmnccccd1tu8Jk+PHYMxY6S4KztGEUCUsNcF\nPXGqC/vJk6BK3OrFqVOnSExMBKSwFxQUUFpaGtR9OnjwIE8//TQVFRVO93322WdkZWWRlpZmvc1j\n8tRkgvx86NoVrr5aCrumBWzfFc0bJex1QRf2Sy6Rwq5pqoStnjhaMRD8ypi5c+fy6quv8vrrr9vd\nnpOTw/bt2+2idfBixZw8KcU9KQluuAEOH4adOwO274rmjRL2upCTA126QHS0FHZQdkw9MJlMFBQU\n2FkxEHxh1+2gF1980W65u88++4ywsDB+97vf2T0+LCwMcCPseuI0KQlGj5Y/KztGESCUsNeFnBzo\n00f+nJQk/1fCXmcKCwsxm81WK0aP2IPtsx86dIhOnTphNpt56qmnADCbzXz++eeMHDnSur86QgiM\nRqN3YU9KgksvVcKuCBhK2GtLRQXs2yf9dVARux/Qu071iL19+/a0aNEi4MJ+4sQJ3n77bTQ3XvfB\ngwfJzMzkT3/6E3PmzGHFihWsXr2avLw8JxtGx6uwd+ok/7/hBli3Dk6f9sdbqRNnz57lww8/xKym\nk4YcSthry969UF1dI+zt2kFYmBL2eqA3J+nCLoQgJSUl4FbMX/7yFx555BFyc3Od7tM0jUOHDpGS\nksIzzzxDSkoKU6dO5ZNPPiE6Oppx48a53GZERIRrYT96FIxGebyAFHazGRYv9udb8pmSkhKuvfZa\n7rvvPutarYrQQQl7bbGtiAEp6h06KGGvB7qw21obgS55LCgoYNasWQDs37/f5T6Vl5eTmppKy5Yt\nef3118nJyWHGjBmMGzeO6GjXqz8ajUbXVTHHjsk5MQbLV27AADnqOQh2zIULF7jxxhutgr5nz54G\n3wdFYFHCXltyciA8HLp3r7lN1bLXC0crBgh4xP7ee+9Zyxj37dvndL/+2rrfP27cOK655hoA7rzz\nTrfb9WjF6PkYkAJ//fUyYr9woW5vog5UVlYyYcIEVq1axSeffEJ4eDh79+5tsNdXNAxK2GtLTg70\n6AGWZhRACbsbVq1axZgxY6iurvb4uJMnTxIeHk7r1q2tt6WmpnL27FmK9NHIfqSyspK3336bkSNH\nEhcX5zJi168W9AodIQQffPABL7zwglXgXeFR2HV/XeeOO+To57FjZTdqgDGZTNx99918++23vPvu\nu9x999107dpVCXsIooS9tuTk1NgwOoES9hkzoG/fJtvIMnfuXBYuXOh1tvrJkydJTEy0tuZDYGvZ\nv/jiC06cOMHjjz9O9+7dXQq7/rqdO3e23pacnMxzzz1nLWt0hUth1zTpsdtG7AAjRsBHH8GPP8oS\nyJKSOr8nX5gzZw5z5szh5ZdfZvLkyQD06NFDWTEhiN+EXQgRJoTYKoRoOjVcZWU1E/d84fx5OHjQ\ntbAXFsqKGX/y1Vdy2JjFqqgvZ86cYfny5X7Zli/kWPIRv/76q8fHnTp1ys6GgZpIuV4++9dfwz33\nyOULLWiaxhtvvEHPnj0ZOXIk3bp1c2vFJCQkWJe78xWXydPiYigtdRZ2gIkT4bPPYPVquPbamsVb\nAsD8+fPp0KEDTz/9tPW2Hj16kJubi8lkCtjrKhoef0bsjwG7/bg975w9K4dx1ZXnnoOePaVY+4L+\nWq6EHfwbtZvNsH69/NlPUetrr73GyJEjXbbIBwJd2L2Js23XqU69hf3f/4bx42HmTNi+3XrzmjVr\nyM7O5rHHHsNgMNC9e3fy8vKcluI7ePCgdR/cUlICH3xgZ6O4TJ7a1rC74rbb4IsvYPNmuOqqgHju\nVVVVLF68mOuvvx6DoeZr37NnTyorK4PeDKbwL34RdiFEJ+B64AN/bM8nNA3GjYNhw2Srdl34+WcZ\nST34oE92R9nGjfKHhhD2ffvgzBn5s5++dNu3b6e6utq6aHQgOXXqFAUFBYD3iN2VsLdp04aYmJja\nC47ZDE88AY8/DldcIW/Lzrbe/eabb9K6dWvuuusuALp164amaU77qJc6emTWLLj/fsjMtI4HcGnF\neBN2gJtvhjlzYMsWeOstb++y1qxevZri4mLr7HidHj16ACifPcTwV8T+b+AZoOE6HWbNglWrZNRe\nF4+wogK2bYOUFFiyBD7/3OtT1r/3HqXAWZskHxAYYV+3rubnw4f9skk9gm4IYc+xWWHKKprFxfD6\n6zBlihyElZKC1qULPU+ccNnFWeuSx/JyuPVWeOMNePRROfe8dWsplsDhw4eZN28eDzzwgLVcsVu3\nboB9ZYzZbObw4cPeI/bjx2V1S0EBDBoE06djDA93Fnbd7nNMnjpy001w3XXwj3+An/9GixYtIiIi\ngquvvtru9kYl7Hl58Omn8uSsqBf1FnYhxA3AKU3Ttnh53ANCiM1CiM16JFdnzp6Fp54C/YunR9K1\nYds2OZHxlVdg8GD4wx88dgFqmkb83r1sBr786iv7O30U9r179/Lxxx/bvw+baNKOtWshPl4Kkx8i\n9pKSEg5bThABE/aFC2V9dlGRVdj79etXI+xTp8KTT8LcuTJfMXQo5vBwvquqYtT+/U5XTbUueXzi\nCfjyS3jtNWnFhIXJ/bEI+7fffovZbOa+++6zPkUXdtsEan5+PpWVld4j9vx8WYu+fbu8cpw8mRcO\nHKDanRWjHyee+Mc/5HHxyiveH1sLFi1axJVXXumUM0hISKBNmzbBTaDu2wf33isnX959t8w3KOqF\nPyL2ocAYIcQhYDYwQggxy/FBmqZN1zRtoKZpA9vp3Xd15S9/kSL85ZcQFwcbNtR+G/rJ4LLL4P33\nZdLqiSfcPnzb+vWkV1WxHvj000/t72zbVpY/ehH2v/71r0yaNIlj+hf9n/+UJ5XCQucHr10r9y01\n1S/CvssmFxEwYZ8/X56o/vlPcnJySEhIYPDgwTLq3rNHXmU98YT8261fD599xsHZs/kWGPHNNzBp\nkt3KQnrE7q7l35bzCxfCu+/yVXIylY88AnqFzaWXygR0ZaX1fV988cXW58XHx9OuXTs7YXcsdXTL\niROyOa1DB1mP/qc/cXVBAWlnz9o/7tgxeYy0aOH1fdCvH9x+uzwx5ed7f7wP7Nu3j3379jnZMDo9\nevQITsReWirfa69e8N//woQJ8vZGPou/KVBvYdc07c+apnXSNC0FuBVYpmma+w6O+rJlC7z7Ljz8\nsIzGBg2qW8S+caPsBExKkp75n/4kLwN/+MHlw7d9+CERQMKYMaxevdreIhACOnakOi/PZXs6QEVF\nBd9//z0A3333nbwxJ0deNXzzjf2Dz52TidohQ6RV5AcrxtYaOesoPP5Cv/p44w1ObtlC79696dq1\nK4WFhVQ++yy0bCk/ZxvyS0u5Cci96y745BO4/HKrDZGamsr58+etDUyuqKqq4p1//YuCcePYD9yV\nl8eJEydqHjBggPyMc3IoLi4mIiKCyMhIu204VsY4Nie5JT9fHkMgLZnHHgPgElfC7slfd+SFF+Q+\n/7//5/tzPPDtt98CcP3117u8v2fPnsER9lmzpKA//rgMXj76SN6uErn1pmnVsZtMMtGZmFhz0Gdl\nwY4drhs8jh2TCVJXbNwok156ZDdtmmw8eughl4nU4iVLALj2r38FsLajW+nYkd3LltGvXz+XEfHK\nlSspKSnBYDBYv2joX6a5c+0frFfDXHYZdO4sD/R61rLbCntAIvbKSnmi+t3v0Mxmbtmxg/T0dLp0\n6UI6EDFvnhQ+h6u1U6dOoQGlTz4J8+bB1q1ylaELF+hjmaD5yy+/uHzJ3NxcevfuTdUzz5BqNrPq\n7ru5ABQXF9c86NJL5f/Z2RQXFxMbG+u0Hcdadv2kbVvD7hI9Ytfp0IH8qCj6Otaj11bYu3aFBx6Q\nV5JuAoXasGjRInr37u32CqRHjx6cOHEiIM1gHvnhB7j4YvjXv6SlFRkp7Sol7PXGr8KuadoKTdNc\nX+/5g/ffh02bZAIuLk7elpkpBX/rVufHP/GETNI5HrBnz0pfLzOz5rYWLeCPf4QDB+zK40BOAUw6\nepSzrVvTacAArrjiCmbOnGlnERQYjYSfOkVZWRlffPGF067Mnz+fqKgo7r77bn788UcqSkrkJWeL\nFrJBxTbKW7dORoCZmTJiLyur9xTAnJwcevfuDdRD2L/6Sgqlq1koO3fKKPOmmyi55x7uqK5meOvW\npKam8jegKipK+usO2A0Au+kmedW0ejXcfjt9L7kEkNU8rpgxYwYdf/2VxwDt4Ye52DJx0U7Yu3aV\nx8qWLRQVFbkU9m7dunH8+HHKVqyAsWM5vWcPHTp0oGXLlu4/C7NZLp5hK+zAnnbt6H/+vP2J+OhR\n74lTR557Ttp7zz1Xu+c5UFRUxKpVq9zaMBCkBKrJBMuWwTXX1ARXII93Jez1pmlF7JWVcr7GbbfV\n3KaLs6PPXlEB330nn7Nggf19+jS7rCz726+7Tv6vR9QWvvv2Wy4DGUEDd911F7m5uWywvKbJZGLp\nzp0kCUGPHj345JNP7J6vaRoLFizg2muv5ZZbbqG0tJTNc+bIg/uBB+S0yPnza56wdq2c996qlTzQ\nwfXBvmcP/N//Od/ugpycHAYNGkRkZGTdrZgPPpAn0M2bne/TT6wZGWy46iqKgKt//JFuxcXcDGwa\nNgzatHF62smTJxFCkJCQIG/43e/gzTfhm29o99e/clGHDuzYscPl7uzctImZYWGQmop4+WWraNtF\nnkJARoY1Yo/TAwIb9ASq6Y9/hAULmPDDD6R6s2FOn5Z/P92KsZCbmEhbk6km0q6slA1mPkTslZWV\nrF+/XgYMHTpIi2L2bJlLqiNLliyhurq68Qn75s3Schs50v52Jex+oWkJ+6OPysoL2zN8hw6QnOws\n7MuXy8qLsDDnL4buyQ8caH97hw7yNt0Dt7D+yy9JAuJHjQLglltuoUWLFtYk6owZM9h++jSxmsaU\nO+9k7dq1dp7t1q1bOXr0KGPGjOHKK6+kRYsW7NZ99dtvl3aLbseYTPK9DBkif9ftAFc++//9n/xM\nvLTsFxYWcuLECdLT04mPj69bxF5ainnZMgDKly51vn/rVoiJgbQ0tuXl8RLQZv16Wk2ezBkh+Mom\nYWnLqVOnaNu2LeHh4TU3Tp0Kzz4LH3zAq9HRLiN2TdPouW4dyZWV8kouJsYq7HYRO0iffft2Ss+d\nc2vF9AVabdwIGRkMO32aSd56I3Qf3yFiP6ALvV7ZoSdAfRD2Tz/9lMsuu4zXXntN3jBtmqy2ueMO\nsORnasuiRYto06YNgwcPdvuYrl27EhYW1rDC/sMP8nt81VX2t6ekwJEjMthR1JmmJexgL+o6WVnO\nCdRvvpFL191/v6xTt/2yb9woO05dRG+MHi09bkulSnl5ORUrVsiXtohtbGwsY8eOZfbs2RQUFPDs\ns8/SyjLt8fYrr8RgMDBz5kzrJufPn4/BYOCGG24gKiqKESNGcEavU+/ZE265RR7oRUXS0igpsV4d\nWIXdVRSjC97KlW4/LoCdluaZegn7smUYKiupBNa/+qrzfJXsbOjfHwwGcnJy+LpjR3nC3bePzzp2\nZJdeCeSAq+YkQOZQ7r6b2w8c4OzOnU7dnEePHiW9tJTzcXFWcdCjcSev+NJLoaKChIICl8KelpbG\n40Cl0Uj14sX8KAT3bNsm7Tp36ILtELGfTkjgjMFQI+y+NCdZWGn5Oz799NPMnj0boqLkWN8+fWQD\n06pVNQ/ev19e3SQmuq2eMZlMfPfdd1x33XX2J04HIiIi6NKlS8MK+9Kl8kpKv1LTSUmRwY2b40Xh\nG01P2F2RlSWFT6+eMJul/TJqFNx1l7wcXrhQ3qdpNYlTV4weLZ9vSZauXLmS/hUVmCIj5UAuC3ff\nfTdnzpxh1KhRnD59mt89/jgAiVVVXHvttcycOdO6Ms38+fMZOnSo1W64/vrrSThzhuqEBHlyueUW\n6U8vWCBtGKiJ2OPj5T9HYTebZdIYwHLicYeeOO3duzetW7eum7AvWkRZWBjftGhBRmkpgwcNYonl\nM8JkkicZS6IyJyeHbn36yCuK3/yGjYMGue0+1QeAOSGETJQDl1ZXO4nOli1bGAhU6ksUgueIHUg5\nc8alsEcXF3M7sKprV45duMDdmoY5IkJeTbnKJ4DbiN0YEcHG8HBYs0be4GtzErI7dPTo0QwfPpx7\n7rmHFStWyONj8WIpeDfcIK8mH35YLqT+9deyOcrN33/jxo0UFhZ6tGF0GnQYWEmJzCM52jDg2XoM\nARrq5Bkawq6LtB61b9oko5hx42SdeFJSjR1z5IhMerkT9kGDZOWGxY5ZtGgRQ4WQFo1lFXrAuuZl\ndnY29957L90uv1zecfw499xzD0eOHGH58uUcPnyY7du3M2bMGOtzr7/+enoAJ3SRycqS1QFz58oD\nvl07uVi2jivf8dAhKCnBHBbmk7DHxcWRlJREfHx87T12TYPvvmNVZCQF/foRp2mMSExk9OjRvP76\n6zJ6LC2FjAxMJhO7du0iPT1dVresWkXH7t05dOiQyyXYXA0As9K/P+bwcDJxTqDuXLeOHkDMlVda\nb4uJiUEI4Szs3bpBTAzdS0pcCjvvvEM4ML1lSw4dOkQ+sOfJJ2Vp7fPPu943d8JuNLLOYJAVTwUF\nPkfs+fn5HDx4kKuuuopvvvmGtLQ0xo0bJ0/K7drJBHvbtjLHNH26zM0cPCivSm27lG3YYmnMGj58\nuMfXBins+/fvb5hhYCtWSKvF1fhjXdj91G3dWCgrK+OJJ56gV69eLHDM+QWA0BD2Sy+VXrrus8+f\nL38fPVpWl9xyi4x6iotrHuOYONUxGGSkv3gxWnU1PyxYQAYQNnSo3cPCw8OZOHEi8fHxvPTSS3bd\np2PHjiUuLo6PP/7Y+kccO3as9bmdO3fmkrAwtunNOELIgVVLlshKgSFD7C2nzp2dD3SL0M02meDX\nXz367Dk5OaSnpyOEqJsVs2MHHD3Kl2VlVFssolmTJzNu3DiefPJJcvWT5qWX8uuvv1JeXi6F3UKX\nLl2orKzkuIsGLrdWDECLFoh+/RgshJOwl6xahQGI0C0r5BiC2NhYZyvGYICMDHpXVDgLe1kZvPsu\nOzp3ZsXRo9ZSx5i77pLdkC+/7Hq6Zn6+TG47rKRkNBpZo/+yZo0U9hYtZAexB9ZYIvyhQ4fSunVr\nvv/+e6Kjo7nuuuso1SdDLlsm+wB27YK335a3ZWbWXOU5sH//fmJiYrjIwS5yRY8ePaioqPA6Ytkv\nLF0qexocvlOAtO8gpCL2lStX0rdvX9544w2mTJnClTbBSKAIDWGPjpZNRnrEPn8+DB9eU4UxYYKs\nklm0SD4mIgL69iUvL4933nmHzz//nCVLlrB582YOHjxI2ZVXQmEhB+fMoW1eHkZNq/G8bXjxxRc5\ncOCAFKbYWOmJHj9OixYtuPXWW/nqq6/47LPP6NWrl7XyAoDCQlqbTKw8caImutT38ehR59fSI3bb\nErodOzAD1nFRbnx2TdOswg54FPbS0lLXNeOW5du+BZKGDIFOnYjcuJGPP/6YhIQEts2YIWuQe/Wy\n2j6Owg7Ow8AuXLhASUmJayvGgsjKYpAQ5DgIewtL3kC3WXRiY2OdI3agun9/+mkacY5jeD/9FAoL\n2X3ddRQUFLBt2zaEELI79be/lZ+5q8tnxxp2C0ajkQ0mk/w8dGFPSnKdG7JhzZo1tGjRgoyMDEDO\nfv/f//1fjh49WvM3SU2VIwdsj6UhQ+R4jNJSp23u37+ftLQ0uzn37ujZsyfQQFbB0qWyEc2hUQwI\nqVp2k8nE1KlTueKKK9A0jWXLlvHOO+/QqlWrgL92aAg71CRQ9+2TEY3tYsOXXSYPlrlz5WMyMiAi\nghdeeIGHH36YO+64g1GjRjFo0CC6dOlC0r33YgK+nDQJay2Bi6oCo9FIG/3kYek+1ccKTJw4kQsX\nLrBhwwY7GwawCsUus5mleoXJ4ME1Ub/ur+ukpMgKH33aI8D27RwKD2c9UGw0urVj8vPzOXv2rFVo\nW7duzdmzZ2VJ3aZNUigsJ4x3332XAQMGOAv/t99S2KULJ4Fel1wCv/kN/PwzrWJieOaZZ4g/dIiS\n1FQwGsnJyUEIQa9evaxP1xtjHIXd1ZJ4TmRmEmM2U2YzUyc/P58eJSUUt24tk4c2xMXFuWy0KevR\ngyigs+1IXLNZtu4PGEDUtdcCsHTpUpKSkmR3ateuWHbceb9su05tiIiIoLS6Gm3QIJlAdbXAhgtW\nr15NVlYWETYrc+li63EQ2pAhMsfhogR1//79dLddwtEDDVbyeOSILNP1sApVg5Y8emv8q0dj4OLF\ni3nrrbeYMmUKO3bsaJBIXSd0hD0zU9bFvvqq/N3G+sBgkFbH99/LL4DFX9+yZQtXXnkle/bsYc2a\nNcyfP58ZM2bwl1df5UinTtwWF8cDffrIA81FdOaEjbBnZWVZv1S2NgxgFfb8Vq1qulANBlnlEBXl\nXIbpIqFk3raNzdXVRERGstxkspYiOuIYQcfHx1NdXU358uWymmTaNKtfnJeXR1VVldWbBaxzXbYn\nJREWFiavPIYPl8J24AAPPfgglwrBSkuUnJOTQ5cuXewWe05OTsZgMDgJu11zkjssllmXwkLr410l\nTnXcReznLCKdbDuA7v33pcg8/jjdLH+rXbt21YwSSE6Wf5cDB5z3y0PErmka2mWXSY/+wAGvidPS\n0lK2bt3KUAdrQt8Pj8KuBxwOdkxVVRUHDx60v1L0QLt27YiPj69dAlXTai/AeiDjKnGq01DCvnKl\nvApy152+bJn8Pg4eDC++KK+MaiH0K1asIDIykjfeeMPt4ueBInSEXffMZ8yQZXeO7eATJsgBU2Vl\nkJVFeXk5OTk5DB48mB49ejBkyBDGjBnDpEmTePLJJ0l5+GGST5+m55EjLm0YlyQlWYVdCMHTTz/N\nkCFDyHRM1O7dC0Yjva67ju+++66mg1U/eBw7Hh1r2YuLMRw8yHZg0qRJ/GQ2Yzh0yGXCSS911LtO\n4+PjGQxEjh0rRQvkkCxqIujNttHf4sWgaSwOCyMtLU1GlL/5jbzv55+JLiykjabx7fHjrFixws72\n0YmIiODiiy92EihdqD1ZMXTvTlV0tF0CddeaNXQDWrmIgOLi4lwK++m2bSkFLsrPl5H6n/8sxwdf\ncQX89rd06dLFallYW+8jIqS4u4rYPQg7QFVWlqx0ys/3GrFv3LgRk8nkJOzR0dEkJiZ6FvY2bWTJ\nrIOwHzx4EJPJ5LOwC0tzXa0i9q+/lsLoJnnrkh9+kFc6luPRJQ1Ry15cLFfXOnxY/n/+vPP9kybJ\nUQeaJjuAMzKkDeZjKeaKFSvIysqihS/D3/xM6Ah7r16yQcZkso/WdYYOrbl0zswkJyeH6upqLtVn\niTgyerT8/9w534X94otlEtNS4nbfffexZs0a5zUy9+6FtDSGXn45J0+erJn2GBVl75/qOEbsFiHe\nDkyePJmc5OFlAAAgAElEQVQtum/swmfPyckhMTERfaJm11OnWAJUtW1bUxdtKZvUxylv0jtzQfrr\n7dvz7YkTNfZKr15SUH7+2dpxmte2LdOmTWPfvn1Owg7SZ6+TFWMwoA0cSBZYO1BLLO8zctgwp4e7\nTJ4CxaWlbAPa7dsnRxe8/HJNj4PRSIsWLayzYeyGf3Xp4hyxl5XJL74LK0YX9gpb79+LsOuJ08tc\nHGepqaneRxcPGSLF1Saa1PsMfBV2qMOUR31e0gc+rq9jNsNPP8kxH558/5QUKeqBXCD+8cflyeO1\n1+T3yma5QECOIzl6VC5+smGDPJG/8448FnyoaikqKiI7O5sr9MVeGpjQEfawsBoLw9Zf1zEYZE17\ncjKkpVnthgEOyTcrffrUXEL7KuyTJ8v9sNS0u2XvXujRg76Wunh3s1CsxMfL5Kz+Bbc8fqfBwCWX\nXELn66/njBCYHdczPXeOlOXL+VtsLPz1r/Doo1z+j39wEtj+xhuyLv+ii5widquwV1fDkiWYR41i\nX25ujbAbDLIj8uefZWOSwcCN06axbt06qqurXQp7amqqk7D/+OOPxMbGeq3aiBg6lL7ALsvfrKWb\nxCm4t2KKi4vJBmL37ZMnq//9X3jvPRmVW9BF0G5YVteuzsLuptQRbCL2Vq1krTn4JOx6j4EjPi02\nctllsqHOpmlM73z21WMH6ekfP36cEl8W1S4pkSXB4eFyWT8XyVuz2cw622h+2zZp7XmyYcBtLXtV\nVRVz5sypf0nmwoXyyv6Pf5QC/tRT8J//WHtX+O47+PBDeOaZGiegfXt5hde+fc2QPg+sWbMGs9ms\nhN0vTJggL6379XN9/0svyc5OIcjOzqZ169buR7MKIeuwY2Pdb8+RLl3kJdvcuU5jCaxUV8s5Ij16\nWKcXupuFYrcvtuN7d+zgvNFIpMUauXHsWFZoGpW2I4fLytBGjuS5gweZkpsrbZ6ZMynv0oUrgVN6\nTX7fvlZhLygoICwsjLy8PCnya9fCuXMcz8igurqaS3ShAumz5+bK99mrFxMfeohOlhOhu4j9xIkT\nlFmmcObl5fHll19y//332yUMXZKZSThQvXEjp06doltxMWfbtnVZQugueVpcXMw3QOXFF8tcy9Sp\nTlGjLuxOEXtBgRQyHTddp4D1vVRVVcmTH3gUdpPJxNq1axnm4upD35e8vDzPYqYn223smP379xMf\nH0/btm3dP88B/Xhc66Z80o4FC2QV14svShvDcUIpMHPmTIYMGVKzgPqPP8r/HccIOOKm23rOnDnc\neuut9kP2fv21dgnOwkJ5pda3L/zP/8jbXnhBnoR//3vZG3D//dIqcuxhEEL67T4I+4oVK4iIiPA4\nyiGQhJawP/SQnBHj7jIvPFzaNcgE3KWXXuq5FOzll2XliE1jkleeekpaFY884nqU8MGD0nvt0YO4\nuDhSUlK8CzvUjO8F2L6dXeHhskIFGDVqFCuFoMXx41L8zWbpG27ezG+Bj//9b3lCOXeO/HnzOIbN\nhMc+fWDXLsyVlZw+fZohFpHYvHmz9EPDwsi2iINtpYvVZ9+yBTIyaNGiBa+88gr9+/d3GSXqJY+6\nrfDWW2+haRpTp071/t4tOYrEQ4dYv369TJzadAHbEhsby4ULF5yWpysuLmYZcHbTJrcRo/7+uurV\nMPIX+b/t1YYPEXtlZaXsFHVnr1nYuXMnxcXFTv66TmpqKlVVVTV2nSt69pRXdQ7C3q1bN59KHXWu\nueYa2rRpw0f6XHRPfPGFPGE99ZR8fy6eM2PGDAC+1PscfvpJCqa3unq9lt0hZ7R48WKAmiF7y5bJ\nv8+ECb4vJfjQQ7K6bObMmnLLFi3kWgAnTkgf/eRJ+burcszBg2XlnavFcWzQ/XWPE0IDSGgJu49U\nVlbyyy+/uPfXdVq1glpcygLy0v7dd6WAv/SS8/26h2kpL+vbt693KwZqKgXMZrRffmFDebk1gm7d\nujXndRtq5UrrVcOuiRP5EuiRmWlNlOqX+9bu0z59oKKC4i1bMJlMXHvttQghpB2zejVkZLDDIsZ6\n+R0gvwBRUfJny+d42223sXXrVpcRuG0t+/nz55k+fTrjx4/3PvMc4KKLKG3blgFmMwtmzCAViHVT\nOqbPi3G0Y/TfXXaeWrj33ntZvHgxybqwyB3HsuM1t/lixVRVwY03ynHMjvNQbLBtTHKFbgt5tGMM\nBik4NraHLuy1ITIykjvuuIOvv/6aQk/CVVQkk+oTJkjrceJEedzZWFa5ubn8/PPPRERE8PXXX2O+\ncEFad96idZBCe9FF9lVgZjNLlizBaDSydOlSeaL76Sf53ufPl8ejtwV3li+XJ6Tnn3e+Ch84UA6e\nKyqS/7uzaPUI3MNrFRcXs2XLlqDZMNBMhX2nZaiUW3+9vlx+uYyY//Uv2L3b/j4Xwr53717KbZaE\nc0lKikzYbdmCKC1lq6bZRdB9br2V00DVc8/B3/9O2Z138uihQwgh7CwUXfisEbsl8i21iEKXLl3o\n2bMnWzdskAfv0KHs3r2bzp0725dsGY01uQdLU40nbIX9448/pqioiMe95SJsMA0YQBZwwlIe2lK/\nYnDA3byY4uJiwsPDPVYoREVFca2lnt2KHrHb+uz5+VLQXAi2nbCDnYfvijVr1tChQwe3i2D4JOwg\n7ZidO+HcOcrLy8nLy6uVv67z+9//nsrKSj73tLj7ggVyhs7vfid/v/tuKbA26/nOnDkTg8HAiy++\nyIkTJ9j14Ydw4YJvwg5OJY/Z2dmcPn2av/zlL5jNZrnQzbp18tj7+WdpxwwbJhcyd2fNfPKJtFYt\nS2BWV1fz0Ucf1QyYe+45aRd5moE/cKB8rxY75oxtb4mFYPvr0EyFPdvS7OI1Yq8P//qXtH2mTLFf\ndX3vXjnzw2Jv9O3bF7PZbLcmqUv0yNYyt30H9tbIjWPHshIw5uVR0KcPXb//nrXr1vHuu+/azSA3\nGo1ER0fXCHuvXhAWhmnbNkDWMw8aNIiKDRvkF9Ei7HY2jM7VV8vL1f79vX4cCQkJREdHk5uby5tv\nvklWVpbLKhB3RI8YQRfgar0Ezs3fzpOwx8bG1sqaAGoWFHeM2BMTpbg74CTsXli9ejVDhw51u1/J\nyckIIXwTdk2DDRs4cOAAmqbVOmIHufj4gAED+PDDD92vNfvFF9Iu0ROLnTrJhqNPPgGTCbPZzCef\nfMLIkSOZPHkyERER5M+aJQVRn6nkDQdhX7x4MUIIHnzwQYYNG8anH32EtmGDDC4GD5bVWaNHS9F2\nWE8BkMfyvHmyn8Vycl+1ahX33ntvzSTW8HB54vEwCZOYGHmVu349O3bsICEhgbkO+YUVK1ZgNBqD\n5q+DH4RdCHGxEGK5EGKXEGKnEOIxf+xYINmyZQuxsbH2Xqq/addOllKtWiVXfNKxVMTo9LNcEnr1\n2fWE3vz5mIVgJ/bWSNeuXVmWnMxPYWF0/+UXLkpOZsuWLUyePNlpU3ZjBSIjoUcPwi1XFomJiQwa\nNIhelkjEfNll7Nmzx7WwP/GETLzGx3ved2SddJcuXZg1axa5ubk84WHhcFeEWb4kE4HCdu1k5OUC\nd6N73S2L5xOOlTFuuk7BIXnqhWPHjnH48GG3iVN9e506dfIu7Lrdtm5dnUodbbn33nvZvn27NQDS\nOXPmDO//619oS5ZIG8b2ZDRpkiwfXLaM5cuXk5eXx8SJE4mNjeXqq6+m7bZtaJmZrkdluyIlRZYO\nW5LGixcvZuDAgbRr14577rkH4969iLKymsRx69Yygdupk1yoxZGFC2UC3LLKFsBpy6pkPuUUbMnK\ngg0b2LplC5qm8cQTT8h5Pjt3QseOtJs9m6zMTKJ0qzII+CNirwae1DTtEmAw8LAQ4hIvzwkq2dnZ\nZGRkYDAE+IJl4kQZIUybJpOM4CTsXbt2pWXLlt59dl3Yc3I43qoV7R2tEaD973/PSE3joWnTWL9+\nvX0Viw36WAErffoQYxGOdu3aMXDgQIYCpYmJHK6q4sKFC663FRHhMTHoSJcuXTh79izJycncfPPN\nPj8PgAEDMAtBPO4Tp+A9Yq8TXbs6R+xuOpFrE7HrK3ANcRwh4UBKSor3WvZWrWQkuXZtvYX99ttv\np0WLFnz44YfW2y5cuMCNN97ImmeeQVRVyTk6towdK8X1o4/4+OOPiYuLs3Zc3zp6NH0rKjjpqSkJ\nOHDgAIMGDeLAgQN2texnz55l3bp1VptswoQJXK4XNNhe9YWHy+Tojz/KsSK2fPaZPBnb2CN6cOO4\nMI5XBg+GoiKKLD77kSNHePnll2VuLT+fp/LyeLW8XBZJBIl6K5umafmapmVbfi4BdgO1WLnXf7z/\n/vsMGzaMH/WyKhdUV1ezffv2wPnrtgghR6y2by+X8zt2TGbcbYQ9LCyM9PR07xF7mzbWip4cS/26\nI9OmTePIkSO89NJLHksInQaB9elDbGEhrZCWSf9+/RgK7GvXjt2WSN5lxF5LdJ996tSpHhd+cElM\nDOcsZYNxI0a4fVhAIvYuXaQtoNtA+flehd1xYRBXnLAkYd2W3FrwqZYdZPS6fj379+yxjgioC/Hx\n8YwfP57PP/+cCxcuYDKZuPPOO1m3bh2/Bc7Gxcnx1ra0aAG33442bx7L5s7ltttus+YzxsbHEw58\n6yWPtHr1ajZv3sxzzz1nV8v+008/YTabGWVZwSwuLo4JSUnkC0G5Y3Pb/ffLq9C33qq57cwZWeJ6\n++129pn+HRBC8LFNfsArlqvHiK1bSUlJ4Y477uB/X3kF06efkj98OP8EsrZskdZQIBaO9wG/hqxC\niBQgA9jg+ZH+p6qqiueff541a9ZwzTXXMHbsWHJdrPC+e/duysvLA+uv29KmjezQy82Vq+CAnbBD\nTWWMW08T5EnC4rOvOX/epdCGh4fTUR8k5gEnYbdEwENatcJoNNLi2DE6AKsss9XBP8I+fPhwunXr\nxn333Ven57e2lClGeZgv7i5id7eQtU907SpF/ehRaQ2cOuXWiqlNxK7/DVytw2pLamoqx44do6Ki\nwvMGhw+HkhJ6r1pV52hd595776WoqIh58+bx5JNPMm/ePN564QWuAX6Ii3NdUjx5MlRW8ofyciZO\nnGi9OXbTJioMBv7P1Vq5Nugjg2fPns1e/b0eOsTixYuJi4sjy2bUdkZFBWs1jYWWyaNWEhKkgM+c\nWSOqX34po2cbGwbk5x8eHs7o0aOZOXOm741PPXpAXByJv/5KWloa//znPxknBGHFxXzboQPPGY1U\n/Oc/slJo8GDZB9HA+E3YhRAxwFfAHzRNc2r9E0I8IITYLITYXBCAN7pw4UKOHz/OnDlz+Pvf/86y\nZcu45JJLeP755+0E02vHaSC4/HI5m0QvkXIQ9n79+lnXJfWIJYrZUl1dL6F1WmzD0piSqdfcWkrw\n5hw7xq5du2jfvn3NFMt6MG7cOPbt21fnSFLcfLNM9no4KXuyYrwJqFv0kscDB2T9ssnkFyvm3Llz\ntGzZUk6S9EBqaiqapnmflT5hAtx4I4/k5jKhnvNJrrjiClJTU3nsscd48803efyRR3goOxsj8Oap\nU67fX58+fNeuHY8JQaZtxdCPP3Kye3e2791rvQJ0RV5eHvHx8bRq1Yq/TJ8OgHbwIIsXL+aaa66p\nuco7eZKo/Hx2WdY8cGLqVNkJq3vnn30mjxuHJP+5c+eIj49n0qRJHDt2zOOVvh0GA2Rl0e3MGdLS\n0khKSuJ/OnfmMDDtxx/JzMwkcvJk2cm6d6/LGv9A4xdhF0IYkaL+maZp81w9RtO06ZqmDdQ0baA+\nt8SfvPPOOyQnJzN+/Hj+/Oc/s2/fPsaPH8/f/vY3u/VHs7OziY6OrndEU2uef14muCIiakroLPg8\nWsAi7NvBrX/uC07L43XuTGlYGP30y9Q1a6iIimJ9SQnff/+9X6J1v3D99dI79ZCUatmyJeHh4f5P\nnoL02T10nULthd2Xk5zPJY9hYZx//32ygYd+/tnlKF9fMRgMTJo0icLCQiaMG8drx4/D11+TPXEi\n68rLXR6r+/fv5/5TpyA8HPHnP8sbT5yAnTuJt1ytzpvnUh4AOHz4MN27d+epp55i7qJFVCYkcHb7\ndo4dO2a1YQBrvX67MWNYsmQJ+Y5rvmZkyNLHt96S/SQ//yyjdYerjLNnz9K6dWtuuOEG3xuzLFzo\n149LTCZ6XXwxHDlC6v79LGzThoIzZ2rKHK+8UiZa//tfn7frL/xRFSOAD4Hdmqa97u3xgWDv3r38\n9NNPTJ482Tpw66KLLmLWrFlcccUVPPzww9bkyJYtW8jIyHAezBVojEZZhvXTT061zT6PFrj1VrKH\nDOE49bNG4uPjKSoqqlmqTgj2RUTQU/eF16yhIiMDDekDNxph9wF9FSW/Jk+TkuTf78ABj81JULuq\nGL8LO5Cbn8+NQFV8vOx89XX87fnzUoAef9w6LuDxxx/nvbff5vOwMMS8efDGG7R/8UVA+uGOzJkz\nh3ygfOpUaX+sXSu7Q4HYm27isssu8yjseXl5JCcn84c//IGEhAT2V1VRYvlO2PUXrF0LERFc/cwz\nmEwma4erHVOnyhPxpEny99tvd3qI/vnrjVnffPONz8tGHklKIgy41GyGmTMRmkafV1/FYDBw3XXX\n1TzwttvkjJyGWk/Wgj8i9qHAXcAIIcQ2y7/Rftiuz/znP//BaDTy+9//3u72sLAwZs2aZV3RqKys\njG3btjWcv+5IQkLN/BAb2rRpQ6dOnbwL+7BhvNWjB+3bt3c5MMpX4uPj0TTNbtjTdiClpERaDbt2\nET1ypDX5VZ+rg2DgOC+msrKS8vLyugt7WJgcT2sbsfsheeqrsHfs2BGj0eiTsO/fv5+TQN5//iNn\nuYweLWu4XaFpsi/it7+Vdfm33y4XIB87FhITiXn4YR5YupTwr76S6xz84Q8kJSWRmprqUtjnzp3L\nkCFDiH3hBbk2wRNPyAqV+HjIyODmm28mOzubwy7GS+tWU3JyMq1ateLPf/4z24uKMB88SHp6unUO\nESAj9ksvJS09nauvvpr33nvP2R+/6SZ5Ql65UiaVXTSA2X7+EydOpKKigtmzZ3v9jAF+sdiWaadP\nS6vlyiu5fNIkzpw5Y99F/NvfSuumgaN2f1TFrNY0TWia1lfTtP6Wf24mYPmf0tJSPvroI8aPH+9y\n/GtSUhIfffQRW7duZcKECZSVlTWsv+4j/fr182lmzK5du+ottI5jBcxmM5vKy4murLQu+h12+eX0\nt3iSTSliB+cJj/oJrM7CDjXje71E7IGwYsLCwkhOTvZe8kjNuN6LR46UEwp373a7bCLffCMnoa5Y\nISPbVaukN/3997JMd8EC+Zh//hOefNL6tGHDhrF69Wq73FVubi7bt2/nlltukUtVvvSSHHf76acw\nYgSEhfEbS7ewKxunsLCQCxcuWEdMPPjggxTGxJBkMnGd7Wyfyko5v8lS5vjggw9y5MiRmgVrdIxG\nePBB+fOdd7p8+7aff0ZGBn379vXZjtl96hR7gcS5c+VxYbkycMrj6CWW//1vvVZjqi1NvvN09uzZ\nFBUV8dBDD7l9zI033sijjz7Kd5aJi0GL2D3Qt29fdu/e7bHyQdM0912gtUA/mHWf/cyZM+zQD7p3\n35X1wIMGMchS0tbUhd2XOTFe0WvZT5xwuYi1TiCEHXwvedy3bx8dO3YkJiYGrr1W/i31ufuOfP+9\nbBg6dkwujv2b38hSwVGj5FjbkyelhfDMM3ZPGzZsGCdPnpT15ha++uorAMaPHy9vuPtumaysrraO\nEdDzWvttxgvr6IlhfU5Py5Yt6Td2LBHATbYdnNu2ySsRS+3/mDFj6NixI++++67z+5s6VY6rvusu\nl2/f9vMXQjBp0iQ2bdrkvQsceSLb0bIlhmPH5PGgv29X3HabHKns0PAVSJq0sGuaxjvvvEN6errH\n7j3AOnkwJibGfphVI6Fv375UV1d7XJosPz+f4uLiekfsjsJeUFCAdQnrHTtk1UlUFI888givvPKK\nT6vcNyYcrRi/CHuXLrJ8btcuj9MJgy3sdsO/oqPlbBN3EfvKlbJE0t300ogIpwouwPpds7Vj5s6d\ny6BBg2oGqBkM0tbp0kV6/UjLsU2bNi6FXbdnbAew/cYScV+2aJG1A9U66MwSsYeHh3P//fezZMkS\np3n/xMbC3/5m7f9wRE+e6ugnpSX6XHYP5ObmclQfx3zrrR4T+owfLz/jBrRjmrSwb9q0iezsbB58\n8EGvM0AiIyNZsmQJy5Ytq31zTAOgV8Z4smP8VVOui4luxZw6dYoioFyvVrJ4hN27d+fpp5+u/XyV\nIBOwiB2ksHhY/9bX5KmmabUW9oKCAs47LuHmgNNUx8svl9aF4wjp48fl+FlfZ7fY0LNnT9q0aWMV\n9sOHD7N582Zpw9gybJi0KWzEulu3bi77SxwjdgAxdKiclT5zprwCqK6WidPkZLsZ9/fffz8Gg4H3\n3nvP5/dQXl5ORUWF3ed/8cUX061bN5a5WT/YlgMHDnCyf3+5Lw8/7PnBrVvLq6DZs+3nRgWQJi3s\nL730EjExMdzpxkNzRJ+D0hjp3r07kZGRHoXdX12gepRiG7EDVOrb9XL109gJSMSuC/v58x6F3dfk\nqT4z3tfaer0yxpPPfu7cOQoKCuyFffhw2ZyzwaFnUI/i6zCB0GAwMHToUKuwO9kwHkhLS3NrxbRs\n2dJ5YZDnnoN//AM+/1yWLK5d67SiWVJSEmPGjGHGjBnep6Ra0I99xxPriBEjWLlyJdUe1ls9f/48\nJ06cIDYjQ86M92UhHr3z3N3C2X6myQr7/PnzWbBgAc8991z9vrCNhPDwcHr37u2xln3Hjh20adOG\nDh6ExRdcWTEABr2Bw8vsksZOQCJ226oKP1gx7oTF/ct7L3nUBdNuXO/QobJ+29FnX7FCWhU+TOZ0\nxbBhw9i7dy8FBQXMnTuX/v37+zRUr1u3bhw5csRJgPPy8ujcubPrq8M//UlW5Xzxhez+dXF8Pvjg\ng5w+fdpp0qI7PAl7SUmJtZHRFXpuIS0tzafXAuRqbFFRDWbHNElhP3/+PFOnTiU9Pb1WM70bO94W\n3cjOzva+6pMP6ONr9YNbX+s08o9/lEvd1fPEEWxiY2OprKy0JqL16L3Onacg/Wr9c/EhYve3sOvz\nZDwJu35FZyfscXFSvB2FfeVKmSytYz+HXtL35Zdfsm7dOmcbxg3dunVD0zQnP/zw4cP2C5w48uST\n8O9/y/fjYgWsq666irS0NNdJVBe4+/z15iJPdkydhD06Wor73LkNMhysSQr73/72N44cOcJ7771n\n/SKFAhkZGZw6dYqjR4863VdZWUlOTo5fKnoMBgOxsbFWj72goIA2bdpg7NgRbJsrmiiOg8D8ErFD\nzWiBIETsiYmJREVFeRT29evX06pVK3o4JjyHD5e5Ad0eys+Xre71WAhi4MCBREZG8rxlXdDaCDs4\nV8boNeweeewxOdDLRfGDwWBgypQprF271uPYAh392HfsB0lMTKRPnz4ehV3PEdR67Pdtt8k+kaVL\na/e8OtDkhH379u288cYb3H///V7HnTY19CFHGxz9UGTitLKy0m+lmrZjBU6dOkUgxjwEC8d5McXF\nxRgMhvrPx9a/yB4idiEE4eHhfhd2IYTX8b1r165l8ODBzl3Vw4fLJiXdXqiHv64TGRnJoEGDKCgo\noHfv3s4nEzfoUa6tsJeXl3Py5Envwg7WJR5dMXq07Ivc7MMoBU+f/4gRI1i9erXb0uPc3FwSEhJq\nfwU4ahS88451Dd9A0qSE3Ww2M2XKFNq0aSPnH4cY/fv3JyIigo0u1lPUFz3I8GEZOl+wnfBYUFBA\nYmKiX7bbGHAVsddp9SRH9Ijdi1VlNBq9Jk9rK+zgueSxuLiYX375xXWwoy8jqNsxK1fK2us6+us6\netmjr9E6yICibdu2dsKuX6H6tP6tB9LS0oiIiCAnJ8frY70Je3l5Oesty985cuDAgdrZMDoREbJp\nysMauP6iSQn7+++/z/r163nttdf8Mm2wsREZGUn//v1dRuxbt24lJiambgeUC2wnPDaHiN0vCfYR\nI2Rdt5dLcKPR6PeIHWqE3dV4540bN2I2m10Le7t2crqhLuwrVkixr2fZ7w033EDLli257bbbavU8\nx5JHVzXsdcFoNNKzZ896C/vw4cMxGAxu7Zjc3Fy/fQ8DRZMS9oqKCq6//nqfyxubIllZWWzevNlp\n9oW/V32ytWIKCgpCUtgdI/Z6M3y47MR00/CiUxthr83lfGpqKsXFxS4HVa1duxYhhN3McjuGD4fV\nq2XJ3Z499bJhdIYOHUpJSYnPNoxOt27d7CJ2VzXsdaV3797s3LnT6+POnj1LixYtXC5uHh8fz4AB\nA1wKe0VFBUeOHAnsspp+oEkJ+6OPPsrChQubXMNMbcjMzKS0tNTu4DSZTGzbts1vNgzUWDEmk4nC\nwsKQtGL8HrH7iK/C7k5Y3KFPAXU1gGvt2rWkp6e7P1EMHw7FxbIbFOrUmOSKukxJTUtL48iRI1yw\nDCfLy8tDCEGSTdNRXUlPT+fw4cN2A+5c4a05bMSIEaxfv16uZWqDfsWkInY/E8qiDq4TqPv27aOs\nrMyvM250YT9z5gxmszkkI/ZgCXtERIRPwl7bBUeuuOIKWrduzZeWQW06ZrOZdevWeS4m0H32t9+W\nVxxBnJekV8boJY95eXl06NDB64IjvpCeng7gdd6LL8JeXV3tdBLVLSQl7IpakZaWRps2bewSqFu3\nbgX8O7wsPj6e8+fPc/z4cYCQitgDZsX4iK8Re22F3Wg0ctNNN7FgwQK7io1du3ZRXFzsWdgvvlg2\nWZ0/7xd/vT44ljzqzUn+oLdlwWxvPru3z3/o0KEYjUYnO0YJu6JOCCHIzMy0i9izs7OJjIz06/Ay\nvX5X/3KFUsQeGRlJZGRkUK0YX6pi6rJE4IQJEyguLuaHH36w3rZ27VoA7+W/+lqxfrJh6opjyaPX\n5iEITGsAABMLSURBVKRakJqaSsuWLest7NHR0QwePNhJ2A8cOEBsbKzz6INGhhL2RkhWVhY7d+60\nDnzKzs6mb9++fm3G0g/qUBR2kFG7HrEXFRXVr+u0lgQqYgfZYelox6xdu5Z27dp5T+iNGKFvpNav\n60/i4+PlCkn799stsOEPDAaDTwlUx8mOrhgxYgTZ2dl2yWq9IqaxW8JK2BshWVlZmM1mNm/ejKZp\nbN261e8z5HVR0ZcMDCUrBmQCtbi4mOrqasrKykLCitG3PW7cOObPn2+1Y9auXcvQoUO9i80dd8gh\nVAMH1vp1/Y1e8lhQUEBFRYXfhB2kHVPfiB1kw5PZbGbUqFHWfEBTKHUE/y1mPUoIsVcIkSuE+JM/\nttmc0SdQbtiwgUOHDnHu3Dm/VsSAs7A39kvL2qIPAvPL6km1JJDCDvZ2TEFBAfv37/etCzssrNFM\n7tRLHvVSR3957CATqPn5+Zw5c8bl/b6OTM7MzGTu3Lns27ePjIwMPvvsMw4dOtToSx3BP4tZhwFv\nA9cBlwC3CSGa1iKZjYyEhAS6du3Kxo0brR2n/o7YbT32tm3bNsoZ9fVBH93rtzkxtcBbVUxtZ7E7\nYmvHrLMsPNHUxmukpaVx9OhR68Iy/o7YAbd2TFlZGdXV1T59/uPHj2fbtm307t2bO++8k+rq6mYT\nsWcCuZqm/appWiUwGxjrh+02a7KystiwYQNbt24lLCzMWsPsL/SDOtSak3T0iD0Ywu4tearPYq+r\nsEdERFjtmOXLl2M0GhvlOr6e0Ctjli9fDvhX2PWSR3d2TG27fjt37szKlSuZNm0aUVFR7pvAGhH+\nEPYk4IjN70cttynqQVZWFseOHWPhwoX07t27Vo0svmB7UIeqsAcrYvdmxdRlnIAjuh0zffp0BgwY\n4PfjI9Dowv7TTz8RHR3tNZFZGzp16kRsbKzbiN3dZEdPGI1GXnrpJc6fP2+9ImjMNFjyVAjxgBBi\nsxBis76wg8I9elSwY8cOv/vrIMu5dPsl1BKnUJM8DVVhv+qqq4iPj6esrKzJ2TBQU/J4+PBh9wts\n1BEhhMcEan0+/8ZeDaPjD2E/Blxs83sny212aJo2XdO0gZqmDQzFCNHf6JMewf/+OsgDVD+wQ/Hv\noVsxesljYxJ2fZ/qI+y6HQNNz18HeeLVjzt/2jA66enp5OTkuByY5o8Ta2PHH8K+CegmhEgVQkQA\ntwIL/LDdZo0+6RECI+xQc2CHasRuMpk4ceIE0LiSp/4SlilTptCnTx/rqj9NDd2OCZSwFxYWWlcH\ns0UJuw9omlYNPAIsAXYDX2ia5n28msIrgwcPxmAw0M+XxXLrQKhH7FAz67sxJU/rMtnRFVlZWezY\nsaPJlqoGUtg9jRaoi8fe1PCLx65p2neapnXXNK2rpmkv+WObCpg2bRqLFy+mVatWAdm+fmCHsrAf\nOXIEIQQxXkbt+pOG8NhDgUBH7OC65NFfJ9bGjOo8bcS0b9+ea665JmDbD3UrBqSwt2rVym9z7H1B\nCbtv6MKuL9TtTxITE0lISHAZsZ87d47o6OiQWi/ZESXszZjmYsU0pA0Dvgl7ZGRkkytR9Ddjx47l\nvffeC0jy11NlTH2aw5oKStibMboVE8oR+/Hjxxtc2H1Jnoa6sPhCZGQkDzzwQJ0W6/CF9PR0du7c\n6VQZ0xw+fyXszZh+/fqRlpbWZJNvntDF3GQyNcqIPdSFpTHQu3dviouLrQl0HV8mOzZ1lLA3Y26/\n/Xb2798fsIgpmNiKeTCE3VtVjBL2wONutEBz+PyVsCtCkmALu9lsxmw2u7y/OQhLY0AJu0IRYoSH\nhxMVFQUER9gBt3ZMcxCWxkDr1q1JSkpSwq5QhBJ6AjUYyVNQwt4Y0EcL6JjNZoqKikL+81fCrghZ\ndEFvTBF7fWexK2pHeno6u3btwmQyAVBSUoLZbFbJU4WiqRJsYXeVQC0vL6eyslIJewORnp5OeXk5\nBw4cAJpPc5gSdkXIolsxDd067iliby7C0lhwTKA2l89fCbsiZAl2xK6EPfhccsklCCGUsCsUoUKw\nkqdK2BsPUVFRdO3aVQm7QhEqBCti91QV01yEpTFhWxnTHEb2ghJ2RQgTbCvGVfJUCXvDk56ezr59\n+6ioqGg2n78SdkXIoqwYBUhhN5lM7Nmzx/r5N/Qx0dAoYVeELCNHjuSOO+6gY8eODfq6StgbF7aV\nMefOnSM2NjYk5yPZUi9hF0L8SwixRwixQwjxtRBCHa2KRkOfPn2YNWsW4eHhDfq63oRdzWJvWLp3\n747RaCQnJ6dZTHaE+kfsS4F0TdP6AvuAP9d/lxSKpo235KmK1hsWo9FIz549rRF7c/j86yXsmqb9\nYFnMGmA90Kn+u6RQNG28RezNQVgaG+np6fzyyy/N5vP3p8d+L/C9H7enUDRJvFXFhPIiyo2V9PR0\nDh8+TF5enhJ2ACHEj0KIHBf/xto85lmgGvjMw3YeEEJsFkJsLigo8M/eKxSNEBWxNz70BOqhQ4ea\nxefvNaukadrVnu4XQkwEbgCu0hwXF7TfznRgOsDAgQPdPk6haOp4E/aUlJQG3iOFLuwQ+s1JUP+q\nmFHAM8AYTdPK/LNLCkXTRiVPGx8pKSlER0cDzaPUtL4e+1tAK2CpEGKbEOI/ftgnhaJJ4y5iV7PY\ng4fBYKB3795A8xD2ehX4apqW5q8dUShCBXfJUzWLPbikp6ezcePGZvH5q85ThcLPuIvYVddpcNF9\n9ubw+SthVyj8jBL2xklWVhYAnTt3DvKeBJ6G7bVWKJoB7pKnRUVFgBL2YDFkyBB+/fVXUlNTg70r\nAUdF7AqFn1ERe+OlOYg6KGFXKPyOwWDAYDA4JU+VsCsaCiXsCkUAMBqNbq2YUJ8Frgg+StgVigDg\nSthLSkoAaNWqVTB2SdGMUMKuUAQAT8IeExMTjF1SNCOUsCsUASAiIsKlsEdHR2MwqK+dIrCoI0yh\nCABGo9EpeVpSUqJsGEWDoIRdoQgA7qwYJeyKhkAJu0IRAJSwK4KJEnaFIgAoYVcEEyXsCkUAcJc8\nVcKuaAiUsCsUAUBF7IpgooRdoQgAqipGEUyUsCsUAUBF7Ipg4hdhF0I8KYTQhBAJ/tieQtHUcRT2\n6upqLly4oIRd0SDUW9iFEBcDI4G8+u+OQhEaOCZPz58/D6g5MYqGwR8R+xvAM4Dmh20pFCGBY8Su\nBoApGpJ6CbsQYixwTNO07X7aH4UiJHBMniphVzQkXpfGE0L8CHRwcdezwDSkDeMVIcQDwAMAycnJ\ntdhFhaLpoSJ2RTDxKuyapl3t6nYhRB8gFdguhADoBGQLITI1TTvhYjvTgekAAwcOVLaNIqRRwq4I\nJnVezFrTtF+ARP13IcQhYKCmaaf9sF8KRZNGCbsimKg6doUiADhWxShhVzQkdY7YHdE0LcVf21Io\nmjoqeaoIJipiVygCgLJiFMFECbtCEQBcCbvBYKBly5ZB3CtFc0EJu0IRAIxGI9XV1WiaLADT58RY\nKsgUioCihF2hCAARERGAnBEDagCYomFRwq5QBACj0QhgtWOUsCsaEiXsCkUA0IVdr4xRwq5oSJSw\nKxQBQEXsimCihF2hCABK2BXBRAm7QhEA9OSpEnZFMFDCrlAEABWxK4KJEnaFIgCo5KkimChhVygC\ngG3EXlFRQVVVlRJ2RYOhhF2hCAC2wq7mxCgaGiXsCkUAsE2eKmFXNDRK2BWKAKAidkUwUcKuUAQA\n2+SpEnZFQ+O3hTYUCkUNthG7PghMCbuioah3xC6EmCqE2COE2CmEeMUfO6VQNHWUFaMIJvWK2IUQ\nVwJjgX6aplUIIRK9PUehaA4oYVcEk/pG7A8CL2uaVgGgadqp+u+SQtH0UVUximBSX2HvDvxGCLFB\nCLFSCDHIHzulUDR1VMSuCCZerRghxI9ABxd3PWt5fhtgMDAI+EII0UXT1wOz384DwAMAycnJ9dln\nhaLR41gVExERYY3iFYpA41XYNU272t19QogHgXkWId8ohDADCUCBi+1MB6YDDBw40En4FYpQwjFi\nV9G6oiGprxXzDXAlgBCiOxABnK7vTikUTR0l7IpgUt869hnADCFEDlAJ3OPKhlEomhuOyVMl7IqG\npF7CrmlaJXCnn/ZFoQgZVMSuCCZqpIBCEQAck6dK2BUNiRJ2hSIAhIWFASpiVwQHJewKRQAQQmA0\nGpWwK4KCEnaFIkBEREQoYVcEBSXsCkWAMBqNVFZWcv78eSXsigZFCbtCESCMRiNFRUWYzWYl7IoG\nRQm7QhEgjEYjZ86cAdScGEXDooRdoQgQRqORs2fPAkrYFQ2LEnaFIkBERESoiF0RFJSwKxQBQlkx\nimChhF2hCBBGo5HCwkJACbuiYVHCrlAECKPRqBayVgQFJewKRYDQ58WAEnZFw6KEXaEIEErYFcFC\nCbtCESBsl8KLiYkJ4p4omhtK2BWKAKFH7FFRUdZpjwpFQ6CEXaEIELqwKxtG0dDUS9iFEP2FEOuF\nENuEEJuFEJn+2jGFoqmjhF0RLOobsb8C/E3TtP7AXy2/KxQKlLArgkd9hV0DYi0/xwHH67k9hSJk\n0JOnStgVDU29FrMG/gAsEUK8ijxJDKn/LikUoYGK2BXBwquwCyF+BDq4uOtZ4CrgcU3TvhJC/Bb4\nELjazXYeAB4ASE5OrvMOKxRNBSXsimDhVdg1TXMp1ABCiJnAY5ZfvwQ+8LCd6cB0gIEDB2q1202F\noumhhF0RLOrrsR8HLrf8PALYX8/tKRQhgxJ2RbCor8d+P/CmECIcKMditSgUCpU8VQSPegm7pmmr\ngQF+2heFIqRQEbsiWKjOU4UiQChhVwQLJewKRYBQwq4IFkrYFYoAoYRdESyUsCsUAUIJuyJYKGFX\nKAKEqopRBAsl7ApFgFDCrggWStgVigBx3XXX8eyzz9K1a9dg74qimSE0reG7+wcOHKht3ry5wV9X\noVAomjJCiC2apg309jgVsSsUCkWIoYRdoVAoQgwl7AqFQhFiKGFXKBSKEEMJu0KhUIQYStgVCoUi\nxFDCrlAoFCGGEnaFQqEIMYLSoCSEKAAO1/HpCfD/2zefEKvqKI5/vjiZNYWjJTI00hiJMoscDUxJ\nooxilHDVImnhQmjjQiEIhyBo2aZyEUH0bxMW2T+ZRWWTqxZjo441Oo0aDTiivgpFKIis0+L+Hl0e\nMuN4hd+5j/OBH/f3O/ctPtzz3nn3nnsvv91EnZtN+FUj/KoRftXx7HivmS2Z7UNZCnsVJI1ez5tX\nuQi/aoRfNcKvOnVwnI1oxQRBELQZUdiDIAjajDoW9rdyC8xC+FUj/KoRftWpg+OM1K7HHgRBEMxM\nHc/YgyAIghmoVWGXNCBpUtIZSXsc+LwrqSFpvBRbLOmgpNNpuyij3zJJhySdlHRC0i5PjpIWSDos\n6XjyeznFl0saSXn+SNL8HH4lz3mSjkka8uYnaUrSj5LGJI2mmIv8JpcuSfsl/SRpQtIGL36SVqbj\n1hxXJO324leF2hR2SfOAN4DNQB+wTVJfXiveBwZaYnuAYTNbAQyndS6uAs+bWR+wHtiZjpkXx7+A\nTWa2GugHBiStB14BXjOz+4FLwI5Mfk12AROltTe/x8ysv/SInpf8AuwFvjSzVcBqiuPows/MJtNx\n6wceBP4EPvPiVwkzq8UANgBfldaDwKADr15gvLSeBLrTvBuYzO1YcvsCeMKjI3A7cBR4iOLlkI5r\n5T2DVw/Fj3sTMATImd8UcHdLzEV+gYXAL6R7ed78WpyeBL7z6jfXUZszduAe4GxpPZ1i3lhqZufT\n/AKwNKdME0m9wBpgBEeOqc0xBjSAg8DPwGUzu5o+kjvPrwMvAP+m9V348jPga0lHJD2XYl7yuxz4\nFXgvtbLeltTpyK/MM8C+NPfoNyfqVNhrhxV/+dkfO5J0B/AJsNvMrpT35XY0s3+suBTuAdYBq3K5\ntCLpKaBhZkdyu8zARjNbS9Gi3CnpkfLOzPntANYCb5rZGuAPWtoaub9/AOkeyVbg49Z9HvxuhDoV\n9nPAstK6J8W8cVFSN0DaNnLKSLqFoqh/YGafprArRwAzuwwcomhtdEnqSLty5vlhYKukKeBDinbM\nXvz4YWbn0rZB0R9eh5/8TgPTZjaS1vspCr0XvyabgaNmdjGtvfnNmToV9u+BFemJhPkUl04HMjtd\niwPA9jTfTtHXzoIkAe8AE2b2ammXC0dJSyR1pfltFP3/CYoC/3RuPzMbNLMeM+ul+L59a2bPevGT\n1Cnpzuacok88jpP8mtkF4KyklSn0OHASJ34ltvF/Gwb8+c2d3E3+Od7g2AKcoujDvujAZx9wHvib\n4uxkB0UPdhg4DXwDLM7ot5HiMvIHYCyNLV4cgQeAY8lvHHgpxe8DDgNnKC6Pb3WQ60eBIU9+yeN4\nGieavwkv+U0u/cBoyvHnwCJnfp3A78DCUsyN342OePM0CIKgzahTKyYIgiC4DqKwB0EQtBlR2IMg\nCNqMKOxBEARtRhT2IAiCNiMKexAEQZsRhT0IgqDNiMIeBEHQZvwHJZ8hLcVAz5QAAAAASUVORK5C\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd8a2128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test set: updating scheme\n",
    "plt.plot(test_target, 'black')\n",
    "plt.plot(yhat_update, 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating scheme RMSE: 2.53031978108 \n",
      "Updating scheme MAE:  1.75470817904\n"
     ]
    }
   ],
   "source": [
    "rmse_update = np.sqrt(np.mean((yhat_update-test_target)**2))\n",
    "mae_update = np.mean(np.abs((yhat_update-test_target)))\n",
    "print(\"Updating scheme RMSE:\", rmse_update,\n",
    "     \"\\nUpdating scheme MAE: \", mae_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
